,Job Title,Salary Estimate,Job Description,Rating,Company Name,Location,Size,Founded,Type of ownership,Industry,Sector,Revenue
0,Senior Data Engineer (Remote),Employer Provided Salary:$50.00 - $70.00 Per Hour,"Company Overview: IT Giants INC is a dynamic and innovative technology company at the forefront of delivering advanced data solutions across various industries. We are seeking an experienced Data Engineer with expertise in cloud platforms, programming languages, database technologies, and analytics tools. If you also have a strong background in the airline domain, this is an opportunity to join our dedicated team and shape the future of data-driven decision-making in the aviation sector.
Position Overview: As a Data Engineer at IT Giants INC, you will play a pivotal role in designing, building, and maintaining sophisticated data pipelines and analytical solutions for our airline industry clients. Leveraging your deep understanding of both data engineering and the airline domain, you will empower our clients to harness the power of their data to improve operations and customer experiences.
Key Responsibilities:
Data Pipeline Development: Architect, develop, and maintain end-to-end data pipelines, integrating data from diverse sources, including cloud platforms (GCP, Azure, AWS), APIs, and various databases (Teradata, Oracle, PostgreSQL, NoSQL).
Data Transformation and Modeling: Apply your expertise in SQL, Python, and Alteryx to transform raw data into meaningful insights, while ensuring data integrity and accuracy.
Cloud Expertise: Utilize your proficiency in GCP, Azure, and AWS to design and implement scalable and cost-efficient cloud-based solutions that meet the unique demands of the airline industry.
Database Management: Manage relational and NoSQL databases to store and retrieve data efficiently, ensuring optimal performance and security.
Analytics and Reporting: Collaborate with data scientists and analysts to develop interactive dashboards and reports using tools like Tableau, QuickSight, and CloudWatch, enabling stakeholders to make informed decisions.
Data Quality and Governance: Implement data quality checks, validation processes, and adherence to industry regulations to ensure data accuracy and compliance.
Version Control and Collaboration: Utilize GitLab and other version control tools to collaborate with cross-functional teams on code management and deployment.
Real-time Monitoring: Develop real-time data processing solutions using technologies like CloudWatch to enable timely insights into operational metrics.
Linux Environment: Leverage your proficiency in Linux to optimize system performance and troubleshoot issues as they arise.
Domain Knowledge: Apply your deep understanding of the airline industry to structure data effectively and drive actionable insights from the data.
Qualifications:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
years of proven experience in data engineering, with a focus on cloud platforms (GCP, Azure, AWS), Python, SQL, and relevant database technologies.
Experience with Alteryx, Tableau, and data visualization tools for creating impactful dashboards.
Proficiency in managing databases such as Teradata, Oracle, PostgreSQL, and NoSQL databases.
Strong command of Linux environments and version control using GitLab.
Familiarity with CloudWatch, QuickSight, and real-time data processing is advantageous.
Domain expertise in the airline industry, including a solid understanding of aviation data, standards, and operational challenges.
Excellent problem-solving skills and the ability to troubleshoot complex data issues.
Effective communication skills to collaborate with technical and non-technical teams.
Proven ability to work independently and in a team, delivering high-quality solutions.
Join Our Team: If you are a driven and knowledgeable Data Engineer with a passion for both technology and the airline industry, IT Giants INC offers an exciting opportunity to contribute to transformative data solutions. Join us in our mission to revolutionize data analytics within the aviation sector and drive meaningful impact.
Job Types: Full-time, Contract
Pay: $50.00 - $70.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Work Location: Remote",5.0,"IT Giants
5.0",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
1,BI Data Engineer (Remote),Employer Provided Salary:$75.00 - $85.00 Per Hour,"Stefanini Group is hiring!
Exciting opportunity awaits, let us help you get started!
Click Apply now or you may call: (248) 728-2611 / email: palak.agrawal@stefanini.com for faster processing!

Job Description:
We require a reliable, talented, highly motivated BI/Data Engineer with excellent SQL, PowerBI, Python skills, The firm is putting a huge focus on data and reporting, with an increased demand of data related tasks.
As a global leader in Asset management, you will be working as part of an innovative, fast paced technology driven culture who are always looking at ways to enhance how technology is used within their organization.
You will be a valued member of an effective data team working to deliver growth against ambitious targets.
You will play a key role in extracting useful information from data, providing important insights to global technology teams.
The role will focus on creating best practice, high quality data solutions in collaboration with numerous business stakeholders and technology partners.
We are looking for someone who enjoys problem solving, who has an inquisitive nature, a team player and can work in a collaborative environment with good communication skills.
Ability to help extract and analyze data is required for the job.



Requirements:
We are looking for a good mix of the following skills:
Excellent experience in the Microsoft BI stack, Excel.
Excellent experience in building data pipelines and integrations (ETL/ELT).
Excellent knowledge of data warehousing and analysis concepts, data normalization, dimensional modelling, and associated practices.
Knowledge and experience of data manipulation languages such as DAX or MDX, Visualization or dashboarding experience.
Python experience is a must. Azure cloud experience would be a bonus.
We believe in building a culture supported by strong values, trust and independence. Within the team, we are highly collaborative, supportive, open and have high standards in the work we produce.

Education:
Bachelor's degree or equivalent experience.

*Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives*

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",3.3,"Stefanini, Inc
3.3","New York, NY",10000+ Employees,1987,Company - Private,Enterprise Software & Network Solutions,Information Technology,$1 to $5 billion (USD)
2,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
3,"Data Engineer (AWS, Python)",Employer Provided Salary:$50.00 - $52.00 Per Hour,"Title: Data Engineer (AWS, Python)
Location: Remote
Duration: 12+ Months
Job Description:
Typical task breakdown:-
Daily standup, collaborate with peers, work independently to complete sprint commitments, participate in agile ceremonies.
Interaction with team:-
Daily stand up meetings with team – working mainly with Helios Platform adoption team.
Work environment:
Education & Experience Required:
Bachelor's in computer science required plus 7+ Working experience .
Masters degree with 6+ years working expeirence acceptable as well.
Preferred: AWS cloud certifications
Technical Skills
(Required)
Exp with Object oriented programming - Python (4+ years)
AWS (3+ years overall exp) lambda and/or glue, EMR, S3
SQL (3+ years)
2+ years exp in test driven development, unit test, integration testing, regression testing
(Desired)
Event driven framework experience.
Kinesis or Kafka experience
Familiar with working in Agile methodology
Soft Skills
(Required)
Strong written and verbal communication, experience working as a member of agile engineering team, ability to commit to and deliver work within allocated sprint window.
Job Type: Contract
Pay: $50.00 - $52.00 per hour
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",5.0,"DSMH LLC
5.0",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
4,Data Engineer,Employer Provided Salary:$45.00 Per Hour,"Job Title: GCP Data Engineer
Duration: 12+ Months
Location: Hartford, CT (Onsite Role)
Tax term : W2 & 1099
Job Description:
As a GCP Data Engineer, you will play a pivotal role in designing, implementing, and optimizing data solutions on the Google Cloud Platform (GCP). Your expertise will be essential in leveraging various technologies to build robust data pipelines and contribute to the overall success of data-driven initiatives.
Roles and Responsibilities:
Design, develop, and maintain efficient data pipelines using Python or PySpark on the Google Cloud Platform.
Seamlessly integrate data solutions using GCP services like Google Cloud Storage, BigQuery, Dataflow, and Pub/Sub.
Utilize expertise in the Hadoop ecosystem, including HDFS and Hive, to craft seamless data workflows.
Transform raw data into insights by creating complex SQL queries for data transformation and analysis.
Implement data quality checks and validation procedures to ensure accuracy and completeness.
Optimize data pipelines and processes for maximum performance and efficiency.
Collaborate with cross-functional teams to understand data requirements and deliver tailored solutions.
Create comprehensive documentation for pipelines, workflows, and processes.
Stay updated with the latest data engineering and cloud technology trends.
Tackle complex challenges with a proactive problem-solving mindset.
Required Qualifications:
Expertise in Python/PySpark and hands-on experience with Google Cloud Platform (GCP) for data engineering tasks.
Strong understanding of the Hadoop ecosystem, including HDFS and Hive.
Proficiency in SQL for effective data manipulation and analysis.
Experience working with cloud platforms, preferably Google Cloud Platform (GCP).
Strong problem-solving skills and analytical thinking.
Effective communication and collaboration abilities.
Adaptability to evolving data engineering technologies and practices.
Job Type: Contract
Salary: From $45.00 per hour
Experience level:
9 years
Application Question(s):
What is your work authorization?
Experience:
Data engineer: 9 years (Required)
Google Cloud Platform: 5 years (Required)
Hadoop: 5 years (Required)
Python: 4 years (Preferred)
SQL: 4 years (Preferred)
Work Location: On the road",-1,Conglomerate IT,"Hartford, CT",-1,-1,-1,-1,-1,-1
5,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
6,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
7,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
8,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
9,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
10,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
11,Sr. Data Engineer,Employer Provided Salary:$105K - $110K,"Role : Data Engineer
Location : Remote
Note : We are looking Someone who can be technical and also be able to
structure and manage transformation projects as well.
JD :
* Data Engineer Skill: Spark, Python and Databricks
* 12+ years of experience in Data pipeline engineering for both batch
and streaming applications.
* Experience with data ingestion process, creating data pipelines
(should be able to move data between stages in data lake, apply
transformation rules) and performance tuning. Must be hands-on coding using
Python with Spark.
* Exposure working with complex data types like XML or JSON, CI/CD
pipeline and scheduling tools like Airflow.
* Proven experience in DWH (type-2 dimensions, aggregations, star
schema, etc.) design and implementation
* Proficient in writing complex SQL queries
* Effective communication and collaboration skills to work with
cross-functional teams and stakeholders.
* Oversee and monitor the work of offshore according to projects
timelines and, if necessary, resolve any issues as they arise
Job Type: Full-time
Salary: $105,000.00 - $110,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
technical and able to structure and manage projects: 5 years (Required)
Data pipeline engineering for both batch and streaming apps.: 10 years (Required)
data ingestion process, creating data pipelines: 5 years (Required)
Python and Spark: 4 years (Required)
Exposure of XML or JSON, CI/CD: 4 years (Required)
scheduling tools like Airflow: 5 years (Required)
writing complex SQL queries: 5 years (Required)
Oversee and monitor the work of offshore: 3 years (Required)
working on fulltime role: 2 years (Required)
Work Location: Remote",4.3,"keasis Inc
4.3",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
12,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
13,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
14,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
15,Azure Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Required Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 9 years (Required)
Python: 8 years (Required)
Data lake: 7 years (Required)
Data Bricks: 7 years (Required)
Work Location: Remote",-1,TekValue IT Solutions,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
16,Data Engineer,-1,"Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore.

Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office.
The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus.
Position Responsibilities
Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities
Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization
Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process
Qualifications & Desired Skills
Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance
Minimum 2 years' of development experience using either Python, C#/Java or C++
Minimum 2 years' years of database/SQL experience
Familiarity with object-oriented programming concepts
Must be comfortable with development across the application stack
Ability to complete complex projects independently
Detail-oriented with strong verbal and written communication skills
Ability to work effectively in a high-energy, time sensitive team environment
Interest or prior experience in the financial trading industry (particularly in fixed income) a plus
This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education.
Base Salary for this role is expected to be between:
$110,000—$150,000 USD",4.4,"Garda Capital Partners
4.4","New York, NY",51 to 200 Employees,-1,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
17,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
18,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department.
The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
19,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1,Percept Health,Remote,-1,-1,-1,-1,-1,-1
20,Data Engineer (MongoDB),$80K - $114K (Glassdoor est.),"CrossCountry Mortgage (CCM) is one of the nation’s top 3 retail mortgage lenders with more than 7,000 employees operating nearly 600 branches across all 50 states. We have been recognized nine times on the Inc. 5000 list of America’s fastest-growing private businesses and have received countless awards for our standout culture. Through our dedication to getting it done, we make every mortgage feel like a win.

A culture where you can grow! CCM has created an exceptional culture driving employee engagement, exceeding employee expectations, and directly impacting company success. At our core, our entrepreneurial spirit empowers every employee to be who they are to help us move forward together. You’ll get unwavering support from all departments and total transparency from the top down.

CCM offers eligible employees a competitive compensation plan and a robust benefits package, including medical, dental, vision, as well as a 401K with an employer match. We also offer company-provided short-term disability, an employee assistance program, and a wellness program.
Position Overview:
We are looking for an experienced MongoDB developer to work with our development teams to integrate innovative features on our new products and applications, as well as maintain existing applications. The MongoDB developer will engineer scripts that automate daily tasks and ensure the security of data sets.
Job Responsibilities:
Upgrade databases through patches.
Understand and convert company needs into technical specifications and creating scalable solutions using MongoDB.
Optimize database schema designs, ensuring data integrity, and promoting best practices to enhance the overall performance and scalability of MongoDB solutions.
Assist developers in identifying and resolving performance issues by utilizing Mongo Profiler to profile and fine-tune database queries and operations.
Leverage performance optimization techniques by implementing proper indexes, including B-Tree, Geospatial, and Text indexes, to enhance query performance and overall system efficiency.
Implement the most effective backup and recovery methods to safeguard critical data and ensure business continuity in case of data loss or system failures.
Collaborate with cross-functional teams to understand business requirements.
Stay updated with the latest MongoDB advancements, features, and best practices.
Qualifications and Skills:
Bachelor’s/Master’s degree in software engineering or Information Technology.
3+ years of experience in executing data solutions using MongoDB.
Recommend and implement best practices for Rest API integration framework/model.
Analytical and problem-solving skills.
Configuring schema and MongoDB data modelling.
Experience with database security management.
Administration of MongoDB in Atlas.
Thorough understanding of MongoDB architecture.
Excellent communication skills.
This job description is intended to convey information essential to understanding the scope of the job and the general nature and level of work performed by job holders within this job. However, this job description is not intended to be an exhaustive list of qualifications, skills, efforts, duties, responsibilities or working conditions associated with the position.
CrossCountry Mortgage, LLC offers MORE than a job, we offer a career. Apply now to begin your path to success! https://crosscountrymortgage.com/about-us/careers/apply/
California residents: Please see CrossCountry’s privacy statement for information about how CrossCountry collects and uses personal information about California consumers.
CrossCountry Mortgage, LLC. (NMLS3029) is an Equal Opportunity Employer committed to workforce diversity. Qualified applicants will receive consideration without regard to race, religion, creed, color, orientation, gender, age, national origin, veteran status, disability status, marital status, sexual orientation, gender identity, or gender expression, or any other protected status in accordance with all applicable federal, state and local laws. The collective sum of the individual differences, life experiences, knowledge, inventiveness, innovation, self-expression, unique capabilities and talent that our employees invest in their work represents a significant part of not only our culture, but our reputation. The Company is committed to fostering, cultivating and preserving a culture that welcomes diversity and inclusion. Employment is contingent upon successful completion of a background investigation. CrossCountry Mortgage, LLC. is an FHA Approved Lending Institution and is not acting on behalf of or at the direction of HUD/FHA or the Federal government. To verify licensing, please visit www.NMLSConsumerAccess.org.",3.3,"CrossCountry Mortgage, LLC.
3.3","Charlotte, NC",1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
21,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
22,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
23,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
24,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
25,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
26,Data Engineer,$76K - $101K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
The Data Governance/Data Quality Subject Matter Expert (SME) is a professional responsible for designing, implementing, and managing the client’s data governance strategy and ensuring high data quality standards across data-centric operations and decision-making.
The SME is in charge of developing and maintaining robust data pipelines, ensuring data observability, managing data within the Microsoft Azure environment, and implementing data governance principles in alignment with industry best practices and regulatory standards.
Primary Responsibilities and Accountabilities:
Data Governance & Quality: This includes developing and implementing data governance policies and procedures, monitoring data quality, managing data within the Microsoft Azure environment, and ensuring compliance with data privacy regulations.
Data Pipeline & Workflow Development: Designing and implementing efficient and secure data pipelines in Azure Data Factory and workflows, ensuring optimal performance and incorporating data quality principals.
Data Integration & Observability: Designing data integration strategies, integrating data from multiple sources, ensuring data quality and observability through tools like Azure Synapse.
Data Security: Defining and implementing data security policies and procedures, including access control and data encryption within the Azure environment.
Performance Tuning: Optimizing data access, indexing, and query performance within Azure Data Factory to ensure scalable and reliable data infrastructure.
Monitoring & Troubleshooting: Monitoring data systems, troubleshooting issues, identifying performance bottlenecks, and collaborating with team members to resolve problems.
Collaboration & Communication: Working closely with stakeholders to understand their data needs and develop solutions adhering to industry best practices and the needs of the business.
Automation & Tooling: Automating repetitive tasks and maintaining tools necessary for data processing and storage within the Azure environment.
Qualifications
A Bachelor's degree in a related field such as computer science, mathematics, engineering, or information systems.
3+ years of hands-on experience programming in SQL.
2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes within the Microsoft Azure environment.
Experience in Data Governance OR Data Quality Management is a must.
Skills
Competencies:
Expertise in Microsoft Azure data tools, with a focus on Azure Data Factory and Azure Synapse for data processing, storage, and analysis.
Strong SQL skills and knowledge of data warehousing concepts and ETL (Extract, Transform, Load) processes within the Azure environment.
Proficiency in programming languages such as Python, and experience with big data technologies such as Hadoop, Spark, and Kafka.
Ability to design, build, and maintain data pipelines, with a particular focus on Azure Data Factory.
Strong analytical and problem-solving skills, with the ability to identify and resolve data-related issues effectively.
Experience with data security and privacy regulations, and understanding of HIPAA compliance.
Strong experience with data governance, data quality, and data observability principles.
Excellent written and verbal communication skills, ability to explain complex technical concepts to non-technical stakeholders.
Excellent collaboration and teamwork skills, with the ability to work in a team involving client data engineers, data scientists, and data analysts.
Strong attention to detail and commitment to ensuring high data quality.
Continuous learning mindset and ability to stay up-to-date with new developments and technologies in data governance and data quality
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
27,GCP Data Engineer,Employer Provided Salary:$50.00 - $56.00 Per Hour,"JOB Description: -
ABOUT CONGLOMERATE
Conglomerate IT is a certified and a pioneer in providing premium end-to-end Global Workforce Solutions and IT Services to diverse clients across various domains. Visit us at ConglomerateIThttp://www.conglomerateit.comConglomerateIT
Conglomerate IT mission is to establish global cross culture human connections that further the careers of our employees and strengthen the businesses of our clients. We are driven to use the power of global network to connect business with the right people without bias. We provide Global Workforce Solutions with affability.
We offer a gamut of services across diverse domains, categories, skill sets with varying lengths of assignments /core capabilities included:
· Application Development Services
· Business Intelligence Services
· Enterprise Resource Planning
· Infrastructure Support Services
· Testing Services
· Program / Project Management
· Temporary & Permanent Staffing
Conglomerate IT is headquartered in Addison, Texas with branch offices across Canada, India, Sri Lanka, Argentina to further serve our clients.
JOB ROLE: GCP Data Engineer
Location: Job Type:Hartford, CT (Onsite Role)
Contract : 12+ Months
Work Authorization: w2 ,1099
Must have Skills: - Skills: Python/PySpark, GCP Cloud , HDFS, Hive, Big data
About Role: -
As a GCP Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and data architecture on the Google Cloud Platform. You will work closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and storage. Your expertise in GCP services and data engineering best practices will play a crucial role in maintaining a scalable, reliable, and high-performance data infrastructure.
Roles and Responsibilities:
· Design and implement data pipelines: Develop ETL (Extract, Transform, Load) processes to move data from various sources into GCP storage solutions such as BigQuery, Cloud Storage, and Datastore.
· Data modeling: Design and optimize data models to support data analytics, reporting, and machine learning initiatives. Ensure data accuracy, integrity, and appropriate access controls.
· Performance optimization: Fine-tune data pipelines and query performance to maximize efficiency and reduce latency, utilizing techniques like partitioning, indexing, and caching.
· GCP services utilization: Leverage GCP services like Dataflow, Dataproc, Pub/Sub, and Composer to create, schedule, and manage data processing and orchestration tasks.
· Data integration: Collaborate with cross-functional teams to integrate data from various sources, both internal and external, to enable comprehensive analysis and reporting.
· Data quality and governance: Implement data quality checks, monitoring, and validation processes to ensure data accuracy and consistency across the platform.
· Security and compliance: Implement security measures and access controls to protect sensitive data. Ensure compliance with industry standards and regulations.
· Documentation: Create and maintain technical documentation for data pipelines, workflows, and processes.
· Troubleshooting and support: Diagnose and resolve data-related issues, performance bottlenecks, and operational challenges in a timely manner.
Ideal Profile
· You have experience with Python Hive, Big data
· You have experience withYou GCP Cloud
· have strong experience with HDFS and other relevant tools/technologies.
· You have advanced python skills.
· You have PySparkknowledge.
· You have excellent communication and collaboration skills.
· You have attention to details.
· You are self-directed.
What's on Offer?
· Join a team of world class IT professionals
· Comprehensive salary package
· Great opportunity for growth and development
Job Type: Contract
Salary: $50.00 - $56.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Experience:
google cloud platform: 4 years (Required)
Work Location: In person",-1,CONGLOMERATE IT LLC,"Novi, MI",-1,-1,-1,-1,-1,-1
28,Snowflake Data Engineer,Employer Provided Salary:$70.00 - $80.00 Per Hour,"Job: Snowflake Data Engineer – Contract (6+ Months) – Stillwater, MN
REMOTE - HAVE TO WORK ONISTE WHEN EVER CLIENT CALL. Preferably local to MN NEEDED
EAD – GC – H4- EAD G C - CITIZEN
W2 Contract
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Expected hours: 40 per week
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 7 years (Required)
Snowflake API: 4 years (Required)
Snowflake administration: 6 years (Required)
Snowflake RBAC, RLS, CLS: 5 years (Required)
Willingness to travel:
50% (Required)
Work Location: Remote",-1,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
29,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
30,Data Engineer I,Employer Provided Salary:$90K - $120K,"Interested in joining a growing company where you will work with talented colleagues, enhance a supportive and energetic culture, and be part of the climate solution? At Energy Solutions, we focus on the big impacts. And we believe that market-based programs can be a powerful force to deliver large-scale energy, carbon, and water-use savings. For 28 years, we've harnessed that power to offer proven, performance-based solutions for our utility, government, and institutional customers.
Energy Solutions (ES) is currently seeking a full-time Data Engineer I to join our Business Operations Team. The Data Engineer role is part of the Business Operations Team and will work on critical projects such as the Corporate Operational Data Warehouse (CODW) and the new ERP system. The CODW project is one of ES's strategic goals, to build and maintain a system that enables ES to centralize, integrate, and analyze internal data from various sources. The goal is to provide a single source of truth for ES internal data and enable data-driven decision-making across the organization. This role will potentially lead the design and implementation of the infrastructure for internal reporting needs, driving operational efficiencies and helping our company's transition to operating internationally with 1000+ employees.
Energy Solutions has a remote-friendly work environment for staff located throughout the United States. We also have offices in Oakland and Orange, California, Boston, Massachusetts, New York, and Chicago, IL, for those that wish to work from one of our offices.

Daily responsibilities include but are not limited to:
Collaborate with stakeholders to understand data requirements and develop data-driven solutions that support business goals
Analyze data sources and pipelines needed for the CODW project
Design and implement scalable data pipelines, ETL processes, and data integration solutions for the CODW project using Python, SQL, and AWS technologies
Develop and maintain a data warehouse and optimize data storage, processing, and retrieval
Build schema, populate the database, and automate reporting
Document and maintain technical specifications, ETL processes, data mappings, and dictionaries
Support the Data Science Center of Excellence (DSCOE) with other data engineering needs

Minimum Qualifications:
BS/BA in Computer Science, Physics/Engineering, Business or Mathematics
Experience in building ETL data pipelines, real-time pipelines are a plus
Strong programming skills in Python and SQL
Minimum 2 years of data warehouse development and strong fundamentals in dimensional data modeling
Minimum 2 years of experience creating SQL queries and ETL design, implementation, and maintenance
Minimum 2 years of experience developing data pipelines using Python
Minimum 2 years of experience with AWS, AWS Redshift, AWS Data Engineering and ML Ops tools and services (ie: S3, Redshift, RDS)
Proven ability to aggregate, normalize, munge, analyze, and summarize value from disparate datasets
Strong oral, written, and interpersonal communication skills
High degree of accuracy and attention to detail
Ability to establish priorities, work independently with minimum supervision
Preferred Qualifications:
Ability to manage and communicate data warehouse plans to a non-technical audience
Expertise in cloud services architecting and designing secure AWS environments
Experience using BI tools like Tableau and Looker
Experienced in analyzing and troubleshooting data quality issues
Self-driven, highly motivated, and able to learn quickly
Familiarity with large data sets, cloud-based development and deployment, open-source practices and frameworks, and intelligent applications is desirable
Proven success in migrating data from legacy systems

Salary DOE: $90k - $120k / Annually
Compensation is commensurate with experience and includes a generous retirement package. Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP).
To see this and our other open positions, please visit https://energy-solution.com/company/careers/.
We encourage candidates from diverse backgrounds to apply. For more information about Energy Solutions and our Diversity, Equity and Inclusion efforts, please visit us on our website at www.energy-solution.com.
Background Check Information
Information will be requested to perform the compulsory background check. A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment. Energy Solutions is an equal opportunity employer.
Reasonable Accommodations
Energy Solutions is committed to providing access and reasonable accommodation for individuals with disabilities. If you require an accommodation in completing this application, interviewing, and/or completing any pre-employment testing, or otherwise participating in the employee selection process, please email accommodation@energy-solution.com.
Privacy Notice for Job Applicants",3.9,"Energy Solutions
3.9",Remote,10000+ Employees,2008,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$25 to $100 million (USD)
31,Apple Music - Data Engineer,-1,"Summary
Posted: Aug 18, 2023
Weekly Hours: 40
Role Number:200496819
This team is more than a group of engineers — it’s a group of music lovers. That passion has made Apple Music the world’s most complete music experience, with over 60 million songs, thousands of playlists, and daily selections from music experts for 115 countries. The team’s data-driven engineers focus relentlessly on the customer experience by running worldwide experiments and analyzing usage and latency, while collaborating with Apple’s product groups. As a result, someone can use the Shazam app to identify an intriguing song in a café in the morning, add it to their playlist from Apple Watch, listen to it through their AirPods on their commute, and share it with their family on HomePod at dinnertime. And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love. Apple Music is a huge part of Apple’s business because it’s a big part of people’s lives.
Key Qualifications
Expertise in designing, implementing and supporting highly scalable data systems and services in Scala
Experience with distributed computing technologies such as Hadoop MapReduce, Spark/Spark-SQL, YARN/MR2
Expertise building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Solid understanding of data-modeling and data-architecture optimized for big data patterns, such as efficient storage and query on HDFS Experience with distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues
Experience with low-latency NoSQL datastores, and traditional relational databases is a plus
Description
We are seeking a hardworking, performance-savvy, engineer to build out the big data platform and services, which power many of these customer features — existing and new. As a core member of the Data Engineering team you will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability. This will include crafting systems to model, ingest, process and compute large-scale, mission-critical data across Apple Music. High-throughput and reliability are essential. The computed datasets are produced for internal reporting used by executives and they are also shared across many teams within the ASE organization, such as the Search team, Recommendation team, AB team, Marketing team. You will also enjoy the benefits of working in a fast growing business where you are encouraged to “Think Different” to solve very interesting technical challenges and where your efforts play a key role in the success of Apple’s business. This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users. Come join us to help deliver the next amazing Apple product!
Education & Experience
BS CS or equivalent work experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $131,500 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",4.2,"Apple
4.2","Seattle, WA",10000+ Employees,1976,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
32,Data Scientist / ML Engineer (REMOTE /Contract),-1,"Role: Data Scientist / Machine Learning Engineer
Location: REMOTE
Duration: 6+ months
Scheduling Interviews
Skills/Experience:
6+ years of experience working in Data Science and Machine Learning field.
Extensive experience in applying data preprocessing, statistical analysis, data analytics tools, predictive modelling, model deployment and evidence-based approaches to find lean, actionable solutions to various real-world enterprise business problems.
Experience in the application of Supervised and Unsupervised Learning algorithms, Naïve Bayes, Regression Analysis, Neural Networks/Deep Learning, Support Vector Machines (SVM), Random Forest, K-Means, Hierarchical, Spectral clustering, DBSCAN, Collaborative Filtering and other advanced machine learning techniques.
Used AWS Cloud Services Sagemaker, ECS, EKS, S3, Redshift, QuickSight
Experience with Google Cloud Platform (GCP) AutoML, Vertex AI, BigQuery, Colab, DataProc, Data Studio,
Exposure to Azure ML, PowerBI, Azure SQL and DataBricks
Design custom BI reporting dashboards or interactive data visualizations and widgets in R and Python using Shiny, Tableau, Ggplot2, Plotly, Matplotlib, and Seaborn.
Produce custom BI reporting dashboards in R and Python using Shiny, and Plotly for rapid dissemination of actionable, data driven insights.
Strong experience in Software Development Life Cycle (SDLC), MLOPS
Experience in working with relational databases (Teradata, Oracle) with advanced SQL skills.",3.5,"Glow Networks
3.5",Remote,51 to 200 Employees,2003,Subsidiary or Business Segment,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
33,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
34,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
35,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
36,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
37,2024 Data Engineer I,-1,"2024 Data Engineer I
316114

Textron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.

Responsibilities:
Analyze, manipulate, or process large sets of data using statistical software.
Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces
Identify business problems or management objectives that can be addressed through data analysis.
Document data standards, semantic layer mapping, data flows and conceptual flows.
Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.
Perform statistical analysis to provide recommendations on best courses of action to business leaders.
Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.
Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.
Collaborate with business users to ensure data is available for self-service analytics.
Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.


Qualifications:
Education: Bachelor’s degree in computer science, engineering, mathematics, or similar field required
Years of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience required
Software Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferred
Experience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)
EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

Recruiting Company Textron Specialized Vehicles
Primary Location US-Georgia-Augusta
Job Field Information Technology
Schedule Full-time
Job Level Individual Contributor
Shift First Shift
Job Posting 08/24/2023, 8:20:05 AM",3.8,"Textron Specialized Vehicles
3.8","Augusta, GA",10000+ Employees,1923,Company - Public,Aerospace & Defense,Aerospace & Defense,$10+ billion (USD)
38,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
39,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
40,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
41,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
42,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
43,Data Engineer,-1,"SalesIntel is the top revenue intelligence platform on the market. Our unique approach to data collection, enhancement, verification and growth distinguishes us from the market, solidifying our position as the best B2B data partner.

The Data Engineer will be responsible for building and extending our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys working with big data and building systems from the ground up.

You will collaborate with our software engineers our software engineers, database architects, data analysts and data scientists to ensure our data delivery architecture is consistent throughout the platform. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What You’ll Be Doing:
Design and build parts of our data pipeline architecture for extraction, transformation, and loading of data from a wide variety of data sources using the latest Big Data technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with machine learning, data, and analytics experts to drive innovation, accuracy and greater functionality in our data system.
Qualifications:
Bachelor's degree in Engineering, Computer Science, or relevant field.
5+ years of experience in a Data Engineer role.
3+ years experience with Apache Spark and solid understanding of the fundamentals.
Deep understanding of Big Data concepts and distributed systems.
Strong coding skills with Scala, Python, Java and other languages and the ability to quickly switch between them with ease.
Advanced working SQL knowledge and experience working with a variety of relational databases such as Postgres and MySQL.
Experience working with data stored in many formats including TSV, CSV, JSON, Parquet.
Cloud Experience with DataBricks or similar service: S3, Athena, EC2, EMR, RDS, Redshift.
Comfortable working in a linux shell environment and writing scripts as needed.
Comfortable working in an Agile environment
Machine Learning knowledge is a plus.
Must be capable of working independently and delivering stable, efficient and reliable software.
Excellent written and verbal communication skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

This is a remote position.",-1,"SalesIntel Research, Inc.",Remote,51 to 200 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
44,Energy Data Engineer - Mid Level,$81K - $121K (Glassdoor est.),"About the Organization
Now is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.

The Data Engineer will support Redhorse in its execution of specific task areas related to the Federal Government Carbon Pollution Free Electricity (CFE) Technical and Strategy Support contract. The program will establish technical frameworks and analytical tools and methods to inform CFE procurement decisions for the entirety of the federal government. Work will be conducted primarily at Redhorse office locations (hybrid remote work) and on government client site as required. The ideal candidate will live in the DC/Virginia/Maryland area and willing to undergo a background check to obtain a Public Trust.
Primary Duties and Responsibilities for this position include
Review and analyze the US Federal Government Executive Agencies electric load data
Develop and test modeling techniques to understand an agency’s hourly load profile to inform the agency’s hourly CFE procurement requirement as an off taker from a new CFE technology source.
Support the configuration and ingestion of designated structured, unstructured, and semi-structured data repositories into capabilities that satisfy client requirements and support a data analytics pipeline to drive rapid delivery of functionality to the client.
Maintain all operational aspects of data transfers while accounting for the security posture of the underlying infrastructure and the systems and applications that are supported and monitoring the health of the environment through a variety of health tracking capabilities.
Automate configuration management, leverage tools, and stay current on data extract, transfer, and load (ETL) technologies and services.
Work under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.
Apply comprehension of data engineering-specific technologies and services, leverage expertise in databases and a variety of approaches to structuring and retrieving of data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically using vendor consoles.
Engage with multiple functional groups to comprehend client challenges, prototype new ideas and new technologies, help to create solutions to drive the next wave of innovation, and design, implement, schedule, test, and deploy full features and components of solutions.
Identify and implement scalable and efficient coding solutions.
Minimum Requirements for skills, Experience, and Credentials
Master’s Degree in Engineering, Computer Science, Mathematics or Statistics
Four (4) years of experience in enterprise data analysis with a preferred focus on utility data, to include advanced metering infrastructure (AMI) electricity data
Local to Washington D.C. metro area preferred, remote work is an option
Willingness to travel in support of on-site customer meetings
Willingness to obtain a Public Trust
Desired Basic Requirements for Skills, Experience, and Credentials include
Knowledge of U.S. Federal Government Energy Legislation and Executive Orders (especially EO 14057)
Experience in or knowledge of utility interval data and electricity pricing
Executive Order Requirement:
While currently on HOLD, Redhorse recognizes that we may need to comply with Executive Order 14042 which requires Federal Contractors to ensure all U.S. new hires be fully vaccinated for COVID-19. As required by the Executive Order, Redhorse will work in coordination with applicable contract agencies to consider requests for Reasonable Accommodations.
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.
EOE/M/F/Vet/Disabled",3.5,"Redhorse
3.5","Arlington, VA",51 to 200 Employees,2008,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
45,Data Engineer,Employer Provided Salary:$100K - $105K,"Job Description
▪ 4+ years of experience in Python with Data background
▪ BA/BS Computer Science or a technical/quantitative degree
▪ Excellent communication and collaboration skills
▪ Interest in learning and adopting new tools and techniques
▪ Humble enough to learn from others, confident enough to teach others new things
Full time position, Salary (+ Benefits ( Healthcare Insurance + PTO + Bonus + 401k) )
Job Type: Full-time
Salary: $100,000.00 - $105,000.00 per year
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",-1,Konnectingtree.Inc,Remote,-1,-1,-1,-1,-1,-1
46,AI/ML Data Automation Engineer,$88K - $125K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
47,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
48,Data Engineer II,$90K - $119K (Glassdoor est.),"Data Engineer II
Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What’s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.
Join us to do the best work of your career and make a profound social impact as a Data Engineer on our Growth Marketing team in Round Rock, Texas, Hopkinton, Massachusetts, or Remote United States
What you’ll achieve
As a Data Engineer II on a growing team, you will bring in your industry experience to develop, deploy, and maintain productionized data pipelines that support the training and productionization of machine-learning models.
You will:
Build, test, and implement data pipelines using tools like Apache Airflow, Spark, and Python for the training and inferencing of machine-learning models
Ensure data quality, monitor data pipelines, and troubleshoot production issues
Document metadata, data models, data pipelines and APIs to help data science teams understand and leverage data assets.
Work with data scientists and business analysts to understand data requirements
Take the first step toward your dream career
Every Dell Technologies team member brings something unique to the table. Here’s what we are looking for in this role:
Essential Requirements
U.S. citizen residing in the U.S.
Bachelor’s Degree in Computer Science, Engineering, IT, or equivalent professional experience
1 to 2 years experience developing production-quality Python and performant SQL, working with data warehouses, JSON, Apache Hive, Hadoop or Spark, or related technologies
1 to 2 years experience with orchestration tools, such as Apache AirFlow
Experience with monitoring tools and observability
Who we are

We believe that each of us has the power to make an impact. That’s why we put our team members at the center of everything we do. If you’re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we’re looking for you.

Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us.

Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. Read the full Equal Employment Opportunity Policy here.
Job ID:R227305

Dell’s Flexible & Hybrid Work Culture

At Dell Technologies, we believe our best work is done when flexibility is offered.

We know that freedom and flexibility are crucial to all our employees no matter where you are located and our flexible and hybrid work style allows team members to have the freedom to ideate, be innovative, and drive results their way. To learn more about our work culture, please visit our locations page.",4.2,"Dell Technologies
4.2","Round Rock, TX",10000+ Employees,1984,Company - Public,Information Technology Support Services,Information Technology,$10+ billion (USD)
49,Senior Data Engineer,Employer Provided Salary:$115K - $140K,"ABOUT THE JOB
Data is one of the foundations of Everside Health and plays an integral role in delivering first-class healthcare services to our patient population. We utilize a wide variety of tools to produce valuable data and provide better patient care as a result.
As a Senior Data Engineer, you will be an established thought leader through close partnerships with expert resources to design, develop, and implement data assets for a wide range of new initiatives at Everside Health. The role involves heavy data exploration, proficiency with SQL, ETL, knowledge of service-based deployments and APIs, and the ability to discover and learn quickly through collaboration. There is a need to think analytically and outside of the box while questioning current processes and continuing to build your business acumen. There will be a combination of team collaboration and independent work efforts. This role involves interaction with the Analytics team as well as a wide range of business areas across Everside.
We seek candidates with a strong quantitative background and excellent analytical and problem-solving skills. This position combines business and technical skills involving interaction with business customers, Analytics partners, internal and external data suppliers, and information technology partners.
ESSENTIAL DUTIES & RESPONSIBILITIES
Deliver data warehouse and analytic solutions by ingesting, integrating, and curating data to deliver information to the business and stakeholders
Conduct ETL design, development, and maintenance including data extraction, manipulation, analysis, source-target mapping, change data capture, code performance
Ensure seamless integration of data across the enterprise and drive automation of common and repeated tasks
Conduct performance tuning and optimization of all processes executed across the data platform
Develop large scale data structures and pipelines to organize, collect, and standardize data that helps generate insights and addresses reporting needs
Collaborate closely with business users and stakeholders to define data for the design, development, and deployment of new solutions that support strategic business priorities
Provide coaching and training to junior and new team members on ETL architecture, standards, and documentation
QUALIFICATIONS
Bachelor’s degree in Computer Science or related field and 5+ years of Data Engineering work experience. Experience designing and implementing ETL pipelines, working with a variety of data warehousing models and design fundamentals, working with Matillion, Azure Data Factory, Data Bricks, or similar ETL tool. Experience working in Snowflake, Synapse, or similar MPP platform and experience in DataOps/DevOps and agile methodologies
DESIRED ATTRIBUTES
Experience with messaging streaming systems (e.g., Kafka, Azure Event Hubs)
Experience in hybrid data processing methods (batch and streaming)
Experience with AWS or Azure application deployment
Experience with API integration
Pay Range: $115,000 - $140,000/yr
The actual offer may vary dependent upon geographic location and the candidate’s years of experience and/or skill level.
Everside Benefits Summary
We believe in empowering teammates to do their best work and build better healthcare. Below are some of our benefit offerings. Eligibility is based on 24/hr week.
Health and Well-Being: Free Everside membership for in person and virtual care, employer paid life and disability insurance, and choice in medical/dental plans, vision, employer funded HSA, FSA, and voluntary illness, accident and hospitalization plans. Benefits are effective on the first of the month following date of hire.
Financial Support: Competitive compensation, 401k match, access to financial coaching through our Employee Assistance Program
Lifestyle: Paid time off for vacation, sick leave, and more, holiday schedule
Learn more at
https://www.eversidehealth.com/careers/",3.2,"Everside Health
3.2",Remote,1001 to 5000 Employees,2001,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
50,Azure Data Engineer,Employer Provided Salary:$50.00 Per Hour,"7+ years of relevant experience
At least two years of experience building and leading highly complex, technical engineering teams.
Strong hands-on experience in Databricks
Experience managing distributed teams preferred.
Strong technical experience in large distributed systems, Data Warehousing, Data Lake at scale Project management skills: financial/budget management, scheduling and resource management experience with medium and large-scale projects
Comfortable working with ambiguity and multiple stakeholders.
Comfortable working cross functionality with product management and directly with customers; ability to deeply understand product and customer personas.
Architecture Design Experience for Cloud and Non-cloud platforms
Expertise on Azure Cloud platform
Knowledge on orchestrating workloads on cloud
Ability to set and lead the technical vision while balancing business drivers
Strong experience with PySpark, Python programming
Proficiency with APIs, containerization and orchestration is a plus.
Job Type: Contract
Salary: $50.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Work Location: Remote",3.8,"Visvak Solutions
3.8",Remote,51 to 200 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,Less than $1 million (USD)
51,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
52,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
53,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
54,Data Engineer,$79K - $116K (Glassdoor est.),"Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
Sr. Consultant /Data Engineer – Data Architecture & Engineering
Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
A Senior Consultant in the Data and Analytics Practice is someone that looks forward to being challenged and continuously grow their technical and team collaboration skills. You'll provide technical guidance, as needed, to other team members for quality assurance, mentoring and growth opportunities. If this is a path that excites you, let's chat!
Responsibilities
Technical delivery in both Agile and Waterfall methodologies
Implement solutions using the Microsoft Power BI tool suite and related technologies
Leverage advanced database systems including data warehouses, data marts, and models needed for business, financial and operational analysis and/or reporting
Develop high performing, reliable and scalable solutions
Ability to clearly communicate technical details to business and management personnel
Perform data analysis that will support and enhance Information Management systems
Author, oversee and gain approval of design documents for projects assigned
Develop reports, dashboards & scorecards, data exploration and visual analytics using existing and emerging tools and technologies.
Work independently or as part of a team to design and develop solutions
Assist business development team with pre-sales activities and RFPs
Provide meaningful feedback and coaching of other team members to successfully overcome technological challenges
Leading and demonstrating emerging technologies and concepts to teams
Requirements
Bachelor's Degree desired in Computer Science, Information Technology, or related field
2+ years of experience with data visualization
Expert knowledge of Data Management, Business Intelligence and Analytics concepts including:
Extract, Transform, Load (ETL)
Data Warehousing, Data Lakes, Data Marts, Data Stores, and Cubes
Data Quality and Profiling
Data Governance, Master Data Management, and Metadata Management
Data Visualization, Dashboard, Reporting, Self Service Business Intelligence (BI), Key Performance Indicators (KPI), and Scorecards
Predictive, Prescription, and Descriptive Analytics
Expert knowledge of Microsoft Business Intelligence stack including Power BI, Excel and SQL Server (SSAS, SSRS, SSIS)
Minimum of 3-5 years of experience (of which 2-3 years in consulting)
Experience with Azure Data Estate (Azure SQL DB, Cosmos DB, Azure SQL DW, Azure Data Lake) is a plus
Azure ML Studio/Services experience a plus
Certifications are a plus
Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and tuning is a plus
Knowledge of relational and dimensional database structures, theories, principles, and practice is a plus
Experience with Data Science tools, technologies and techniques (R, Python, algorithms) is a plus
Passionate about learning new technologies
Ability to learn new concepts and software quickly
Analytical approach to problem-solving; ability to use technology to solve business problems
Familiarity with database-centric applications
Ability to communicate effectively in both a technical and non-technical manner
Ability to work in a fast-paced environment
Ability to travel up to 30%
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways.
At this time, we cannot sponsor applicants for work visas. Candidates should be based in the United States.
About 3Cloud
3Cloud is a ""born in the cloud"" Gold-Certified Microsoft Azure technology consulting firm and Azure Expert Managed Services Provider that provides cloud strategy, design, implementation, acquisition and ongoing managed services to its clients across various industries. To meet the needs of its clients, 3Cloud offers comprehensive services across Cloud Infrastructure, DevOps & Automation, Cloud Application Development and Data, Analytics and AI. Founded by former Microsoft technology leaders, 3Cloud uniquely offers: (1) highly experienced and proven cloud architects and technologists, (2) strong business strategy, financial acumen and operational proficiency and (3) deep relationship and network into Microsoft engineering and field leadership.
3Cloud Total Rewards Highlights Include:
Flexible work location with a virtual first approach to work!
401(K) with match up to 50% of your 6% contributions of eligible pay
Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days
Three medical plan options to allow you the choice to elect what works best for you!
Option for vision and dental coverage
100% employer premium coverage for STD and LTD
Paid leave for birth parents and non-birth parents
Option for FSA, HSA, HRA and Dependent Care
$67.00 monthly tech and home office allowance
Utilization and/or discretionary bonus eligibility based on role
Robust Employee Assistance Program to help with everyday challenges
3Cloud offers competitive compensation. In addition to base pay employees are eligible to receive an annual discretionary/utilization bonus. If you are hired at 3Cloud your final base salary is based on factors such as skills, education, experience and/or geographic location. Please keep in mind that the range mentioned above includes the full base salary range for the role. It is not typical for offers to be made at or near the top of the range.
Base Salary Range
$90,000—$183,000 USD
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway.
At this time, we cannot sponsor applicants for work visas.",4.3,"3Cloud
4.3","Downers Grove, IL",501 to 1000 Employees,2016,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
55,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
56,Data Engineer,-1,"Hummingbird is a remote-first, fully distributed team united by the shared mission of helping fight financial crime. Since our launch in 2017, we’ve helped major financial institutions and tech-savvy trailblazers alike (e.g. Stripe, Affirm, etc.) orchestrate their compliance programs through our thoughtfully designed, intuitive SaaS product. We believe finding and stopping financial crime is a problem rooted in code, language and design, so we built the product that the heroes doing this work deserve.
We are customer-obsessed, and we love building and shipping great products. We set a high bar, challenge our assumptions, seek diverse opinions, and support each other to do our best work.
We do our best to write inclusive, descriptive and accurate job descriptions, but we’re not always perfect. If you’re interested in the role, we’d love to hear from you even if you don’t feel like you meet everything we’re looking for. We’re always iterating and improving, and it’s possible that your experience is even more impactful than we could have imagined.

About the Role
We are looking for a driven data engineer to join our team and champion the use of data at Hummingbird. Data plays a crucial role in our mission to fight financial crime and you will help us find new and innovative ways to leverage it to provide powerful tools for our users, and allow us to better understand the usage of our product. Hummingbird is uniquely positioned at the intersection of financial technology, security, policy, and law enforcement and as such we have built up a one of a kind data set that we can now use to give our customers the edge in their efforts to stay ahead of criminals. As an employee at a small startup, you will have the opportunity to wear many hats, working from product discovery through implementation.
What you’re looking to do:
Level up our use of data to make better decisions, build more powerful features, and fight financial crime
Leverage Infrastructure as Code (IaC) to manage and deploy infrastructure that supports a variety of different projects, such as data replication and orchestration for machine learning workflows
Build new data pipelines for ingesting data into our data warehouse via both batch and streaming architectures
Work closely with data science to enable us to build products that benefit our customers while keeping compliance and security at the forefront
Achieve goals through a combination of independent building, educating your peers, and influencing others to contribute towards your vision
What we’re looking for:
A data engineer with a history of taking projects from the earliest stages through successful rollout to production
Someone who is excited by the prospect of pioneering data as a practice at a fast growing startup and who is unafraid to dig in to discover what is possible
A flexible self starter that will cut across organizational lines to understand the business and identify the most valuable work
An engineer who brings a pragmatic approach to problem solving, favoring simplicity and shortening delivery cycles
Experience building data pipelines for sensitive data, including best practices for de-identification and data security
Experience deploying infrastructure via terraform or a similar infrastructure as code tool
Expertise in SQL and one or more programming languages, especially python
What’s in it for you:
The chance to help build from the ground up. The hires we’re making now are foundational to our growth as a company, so you will have an opportunity to help shape the future of Hummingbird.
Competitive compensation including cash and equity.
Remote-first, fully distributed company with flexible working hours.
Awesome health, vision & dental benefits, and 401k.
Safe, respectful & comfortable work environment with colleagues and leadership who prioritize diversity, equity, inclusion, and belonging.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please don't hesitate to contact us to request accommodation.",-1,Hummingbird,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
57,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
58,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1,Radiant System,Remote,-1,-1,-1,-1,-1,-1
59,Data Engineer (Onsite),Employer Provided Salary:$85K - $100K,"If you are seeking a career that has tremendous impact on manufacturing operations, then come be a part of Tokai Carbon GE's team to design, develop and implement our new manufacturing execution system. You'll play a critical role in our organization's success, providing easier access to data that engineers, analysts and decision-makers need to do their jobs. You will set the foundation of data for growth, relying on your programming and problem-solving skills to create scalable solutions. You will be part of the team building BI and Analytics tools that will take our operations to the next level. If you are excited about applying your knowledge and skills to meaningful challenges, apply today!
JOB SUMMARY
The Data Engineer will be responsible for designing, implementing, and supporting solutions used to collect, transform, store, and analyze data from a variety of company applications. They analyze and organize raw data, build data systems and pipelines, review and analyze data infrastructure, plan, and implement solutions to store and manage data. Also, they create reports, dashboards, and other analytics solutions. They will work closely with the lines of business and external partners, to ensure successful development and delivery of BI and data analytics solutions. This is a hands-on development and support position with ownership responsibility for data architecture and engineering for BI and analytics purposes.
ESSENTIAL FUNCTIONS
Work with business users and other technology teams to understand their data requirements, identify appropriate data sources, design, and develop data pipelines and design appropriate data structures to store the required data.
Design data pipelines using SQL, other programming languages and data integration tools.
Gather and combine data from a variety of sources to support actionable decision making. Prepare data for reporting, and predictive and prescriptive tools.
Address BI/EDW technical development working with operations, sales, supply chain, technical services, ERP applications, analytics, and finance & accounting teams.
Contribute to our suite of reports and data products while collaborating with business analysts, business process owners, and other team members, including UAT, and support for technical testing.
Work closely with senior team members defining data architecture, optimizing EDW performance, and updating EDW documentation, including outlining options to improve resilience and data quality monitoring.
Maintain the Enterprise Data Warehouse (EDW) and influence data quality and reliability.

COMPETENCIES
Data modeling, relational databases, data warehousing, business intelligence, stream processing.
SQL programming skills, including writing efficient and optimized SQL queries for data retrieval and manipulation.
ETL/ELT processes and data integration.
Strong analytical skills, development experience, and excellent verbal and written communications.
Manage all stages of the development lifecycle, including planning, requirements gathering, designing, developing, documenting, testing, training, deployment, and support.
Own individual and cross-functional team deliverables on small to large projects.
Project and change management methodologies including Agile/Scrum.
Willingness and availability to travel, and perform other duties as needed.
Must be a self-starter, manage workload autonomously, and balance competing priorities.
Ability and desire to continuously learn about recent technologies and Analytics trends.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Computer Engineering, Analytics, Information Systems, or equivalent experience.
1-3 years of experience working with SQL and relational databases (MS SQL Server preferred).
1-3 years of experience working with Microsoft Azure, particularly Azure Data Factory, Azure Synapse and Power Platform.
1-3 years of experience preferred in reporting, dashboards, and BI tools (Power BI preferred) for ERP and MES platforms.
1-3 years of experience preferred with Visual Studio, SSRS.
Experience with MS D365 and AVEVA or similar ERP and MES solutions preferred.
Proficient in coding and scripting in Python preferred.
Experience in Manufacturing, Supply Chain, Logistics, and Finance and Accounting solutions preferred.
How to stand out: Microsoft Azure Data Engineer Associate Certification
COMPENSATION AND BENEFITS
Available on your first day: Full medical, vision, dental benefits, short-term disability, and long-term disability
Paid time off
Monthly wellness reimbursement
Health Savings, Flexible Spending and Dependent Care accounts
401(k) retirement savings plan with employer match
Annual bonus",2.6,"Tokai Carbon GE LLC
2.6","Charlotte, NC",1001 to 5000 Employees,-1,Company - Public,Chemical Manufacturing,Manufacturing,$1 to $5 billion (USD)
60,Data Engineer,Employer Provided Salary:$77K - $113K,"Eaton’s Electrical division is currently seeking a Business Analyst to work out of our Raleigh, NC location.

Relocation within the United States will be offered.

The expected annual salary range for this role is $77,249 - $113,299 a year.
Please note the salary information shown above is a general guideline only. Salaries are based upon candidate skills, experience, and qualifications, as well as market and business considerations.
What you’ll do:

The Data Engineer will collaborate with cross-functional teams to understand data requirements, design and implement efficient data pipelines, and ensure the availability, reliability, and scalability of our data solutions. This position necessitates proficiency in data manipulation, data analysis, SQL (writing queries), data modeling, and visualization skills.

Job responsibilities
Gather data from various sources. Design and write SQL queries to retrieve and analyze data stored in various databases.
Develop ETL processes to clean, transform, and enrich data in PowerQuery, ensuring data quality, consistency, and accuracy for use in the PowerBI's Data model.
Design and develop data model, transform raw data into meaningful insights, create relationships, hierarchies, and calculated measures in PowerBI’s Data Model
Document data integration and transformation processes for future reference and troubleshooting.
Assists in the implementation of data governance processes and systems.
Qualifications:
Basic Qualifications:
Bachelor's degree from an accredited educational institution
Minimum three (3) years of SQL and Power BI experience
Must be legally authorized to work in the United States without company sponsorship now or in the future

Preferred Qualifications:
Bachelor’s degree in computer science or software engineering
PowerBI (PowerQuery, Modelling, DAX, creating interactive visuals), Relational databases, Data Governance, Visualization
Problem-solving, Communication, Attention to detail
#LI-JM3
We are committed to ensuring equal employment opportunities for all job applicants and employees. Employment decisions are based upon job-related reasons regardless of an applicant's race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, marital status, genetic information, protected veteran status, or any other status protected by law.
Eaton considers qualified applicants regardless of criminal histories, consistent with local laws. To request a disability-related reasonable accommodation to assist you in your job search, application or interview process, please call us at 1-800-836-6345 to discuss your specific need. Only accommodation requests will be accepted by this phone number.
We know that good benefit programs are important to employees and their families. Eaton provides various Health and Welfare benefits as well as Retirement benefits, and several programs that provide for paid and unpaid time away from work. Click here for more detail: Eaton Benefits Overview. Please note that specific programs and options available to an employee may depend on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.",3.9,"Eaton
3.9","Raleigh, NC",10000+ Employees,1911,Company - Public,Electronics Manufacturing,Manufacturing,$10+ billion (USD)
61,Data Engineer (multiple openings) - IHM,Employer Provided Salary:$88K - $157K,"Discover. A brighter future.
With us, you’ll do meaningful work from Day 1. Our collaborative culture is built on three core behaviors: We Play to Win, We Get Better Every Day & We Succeed Together. And we mean it — we want you to grow and make a difference at one of the world's leading digital banking and payments companies. We value what makes you unique so that you have an opportunity to shine.

Come build your future, while being the reason millions of people find a brighter financial future with Discover.

Job Description:
Employer: DFS Corporate Services LLC

Job Title: Data Engineer (multiple openings)

Job Location: Riverwoods, Illinois

Job Type: Full Time

Duties: Responsible for designing, developing, testing, and maintaining complex data solutions for the product. Mentor and influence peers to achieve commitments on data solutions on time and with quality. Telecommuting and/or working from home may be permissible pursuant to company policies.

Requirements: Employer will accept a Bachelor's degree in Computer Science, Computer Engineering, or a related field and 3 years of experience in Senior Data Engineer; ODI/DW/Batch Developer or related occupation.

Position required skills: Three (3) years of experience in the job offered or related occupation: working with Agile software development methodology and Agile management tools, including Jira; utilizing modern engineering practices, including the design and development of ETL data integration solutions in data warehouse environments; working with relational databases and Cloud-based technologies; utilizing design and architecture experience on large-scale ETL solutions; and working with Enterprise Data warehouse data models and dimensional modeling concepts, source to target mapping, and data integration architecture.

Position eligible for incentives under Employee Referral Program.

Rate of Pay: The base pay for this position generally ranges between $88,150.00 to $157,000.00. Additional incentives may be provided as part of a market competitive total compensation package. Factors, such as but not limited to, geographical location, relevant experience, education, and skill level may impact the pay for this position. We also offer a range of benefits and programs based on eligibility. Learn more at MyDiscoverBenefits.com .

QUALIFIED APPLICANTS: Please apply directly through our website by clicking on “Apply Now.” No calls. Equal Opportunity Employer/disability/vet.

What are you waiting for? Apply today!

All Discover employees place our customers at the very center of our work. To deliver on our promises to our customers, each of us contribute every day to a culture that values compliance and risk management.

Discover is committed to a diverse and inclusive workplace. Discover is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or other legally protected status. (Know Your Rights)",3.9,"Discover Financial Services
3.9","Riverwoods, IL",10000+ Employees,1985,Company - Public,Banking & Lending,Financial Services,$1 to $5 billion (USD)
62,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
63,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
64,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
65,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
66,AI/ML Data Automation Engineer,$88K - $125K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
67,Data Engineer,-1,"Remote
Contract
Opened 4 months ago
Job Description
Data Engineer (with Healthcare experience) Required Skills: SQL Databricks data engineering Snowflake data engineering QA experience for ETL experienced with data acquisition and ingestion using API and/or batch channels Experience with Healthcare",3.3,"Crackajack Solutions
3.3",Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
68,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
69,Data Engineer,-1,"ArchWell Health is a new, innovative healthcare provider devoted to improving the lives of our senior members. We deliver best-in-class care at comfortable, accessible neighborhood clinics where seniors can feel at home and become part of a vibrant, wellness-focused community. Our members experience greater continuity of care, as well as the comfort of knowing they will be treated with respect by people who genuinely care about them, their families, and their communities.

Duties/Responsibilities:
Build data integrations from internal and external sources to centralize data into a Data Warehouse environment.
Monitor data integration operations, data quality, troubleshoot, and resolve problems.
Profile data sources and map to target table formats.
Develop and monitor data quality processes and address problems.
Develop, unit test and system test integration components.
Create support documentation describing the functionality of the integrations.
Participating in technical design & requirements gathering meetings.
Participate in planning and implementing data integration and data migration activities.
Perform QA tests to ensure data integrity and quality.
Research data issues between source systems and the data warehouse.
Required Skills/Experience:
Bachelor’s degree required; Master's degree (in data science, computer science or MIS, mathematics, engineering, or related field) preferred.
5+ years of prior experience in Data Management / ETL / ELT / Data Warehousing
Experience in writing Data Quality routines for cleansing of data and capturing confidence score
Experience with master data management
Strong knowledge of Structured Query Language (SQL) and Transact-SQL (T-SQL)
Experience using scripting languages such as JavaScript or Python
Experience Healthcare data models, datasets, and source systems (e.g. EHR, claims, labs, etc.)
Experience with healthcare reference data (ICD, CPT etc.)
Experience with agile delivery methodologies
Data Modeling experience preferred.
Strong organizational, administrative, and analytical skills required.
Experience managing and working in cloud environments such as Amazon Web Services or Azure
Knowledge of HIPAA; ability to implement systems and processes in accordance with regulations
Excellent interpersonal communication skills, both written and verbal
ArchWell Health is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to their race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected classification.",4.0,"ArchWell Health
4.0",Remote,Unknown,-1,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
70,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
71,Data Engineer,-1,"Duration: 11+ months

Job Description:

Aviation connects the world and Connected Aviation Solutions (CAS) connects Aviation. Sustainably. Seamlessly. Securely. The Data Management & Data Science (DM&DS) team is tasked with the end to end responsibility to ensure that CAS data assets are managed with integrity and quality prior to consumption by our critical customer facing applications - whether via APIs, analytics and/or data visualizations. As a senior data engineer on the DM&DS team, you will be responsible for the design, development and maintenance of data processes and pipelines supporting critical CAS Strategic Business Unit (SBU) Data initiatives in support of the Digital Transformation of CAS as well as for the cross-SBU Connected Ecosystem. In this endeavor, you will be working closely with data architecture, data analytics & visualization teams leaders across CAS, SBU and Digital Technology (DT) teams to ensure the technical solutions are efficient, scalable and meet long term Connected Ecosystem needs.

Primary Responsibilities:
Design, develop and support the processes and pipelines for moving data throughout the CAS and cross SBU environments.
Develop automation and monitoring processes that support the data pipelines
Work closely with the architecture team to implement modern data repositories that support the CAS use cases (Pipelines, APIs, Data Science, Applications and Visualizations)
Work with internal business customers and software development teams to gather and document requirements for data publishing and data consumption via data warehouse, data lake, and analytics solutions
Support the operation of the CAS and Digital Technology owned Data Platforms, Data Warehouse and Data Lakes with a view to leveraging capabilities and resources over-time
Work with the CAS and DT Enterprise Data Architects to automate cloud deployments, as well as build CI/CD pipeline to support Cloud-Based workloads. Develop views, materialized views, and SQL scripts
Work with the CAS and DT Enterprise Data Architects to recommend investments or changes in technology, resources, procedures, equipment, systems, or other assets to improve the quality of the organizations projects.
May travel domestically and internationally up to 15%.

Qualifications / Required Skills:
Bachelors degree and 5 years of prior relevant experience OR Advanced Degree in a related technical field and minimum years 3 experience OR In absence of a degree, 10 years of relevant experience is required
3+ years of demonstrated engineering leadership in a relevant engineering function, such as software/service development and deployment, system design and integration, or data analytics.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.",3.8,"Capgemini
3.8",Remote,10000+ Employees,1967,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
72,Senior Data Engineer (Remote),Employer Provided Salary:$91K - $155K,"** This role is remote; if in area near an ICF office, hybrid may be recommended.**
Our Public Sector Group (PSG) is growing and we are looking for a talented, highly motivated Senior Data Engineer to join our Engineering and Emerging Technologies (EET) line of business for supporting a large government contract in the DC metro area.
Key Responsibilities:
Responsible for the data engineering on a large-scale technical modernization project in accordance with contract requirements and company policies, procedures and guidelines. Strong experience in I.T. modernization projects. Has a good mix of data requirements and data engineering, and knowledge of I.T. modernization efforts.
What you’ll be doing:
Support technical Data Warehouse/BI tasks for a major federal initiative, working as part of an extended system development team on project execution.
Develop extract, transform, and load (ETL) processing routines and data feeds, creating necessary data structures and data models to support data at all stages.
Perform extensive data profiling and analysis based on client data.
Work with UI and business analysis team members and the client to define BI and reporting requirements.
Design and implement custom data analytic and BI/reporting products, custom reports, and data visualization products.
What you must have:
Bachelor’s degree (e.g., Computer Science, Engineering or related discipline)
8+ years of experience in SQL and/or Python coding language.
8+ year of experience with procedural, functional, and object-oriented programming
3+ years of experience developing database ETL environments with business intelligence applications
3+ years of experience with AWS database, analytics, and compute services such as RDS/Aurora, AWS Glue, and Lambda
3+ years of experience with data warehouse design and development with Amazon Redshift
3+ years of experience working with BI tools such as Tableau, PowerBI or AWS Quicksight
3+ years of development experience in a DevSecOps environment, with programming languages such as Java, Node, or Python
Employment must be compliant with eligibility for Public Trust Clearance due to Government Contract.
Candidate must reside in the US and be a US Citizen
What we’d like you to know:
Understand ETL concepts of data flow, data enrichment, data consolidation, change data capture and transformation
Understand database concepts of referential integrity, indexes and keys and table metadata
Demonstrated experience showing strong critical thinking and problem-solving skills paired with a desire to take initiative
Knowledge of Big Data integration tools such as Hive, Airflow, Storm, Spark, AWS Kinesis, and Kafka
Experience building CI/CD pipelines with tools such as Jenkins and CodeBuild
Experience with containerization platforms including Docker
Experience with Agile development process
Technologies you’ll use in this role:
SQL, Node, Python, Tableau, Quicksight, Aurora, Lambda, Sequelize
Aurora, AWS Glue, S3, RedShift, AWS Kinesis
Git, Terraform, CodeBuild
Professional Skills:
Must have excellent written and oral communications skills.
Must be comfortable working in a fast-paced, matrixed team environment, with a client-centric culture, and an environment of high performers.
Must be able to multi-task and shift priorities as needed.
Why you’ll love working here:
Comprehensive health benefits
Generous vacation and retirement plans
Employee support program
Participation in charity initiatives
About ICF International:
ICF International (NASDAQ:ICFI) partners with government and commercial clients to deliver professional services and technology solutions in the energy and climate change; environment and infrastructure; health, human services, and social programs; and homeland security and defense markets. The firm combines passion for its work with industry expertise and innovative analytics to produce compelling results throughout the entire program life cycle, from research and analysis through implementation and improvement. Since 1969, ICF has been serving government at all levels, major corporations, and multilateral institutions. More than 4,000 employees serve these clients worldwide. ICF's Web site is
www.icf.com
ICF offers an excellent benefits package, an award-winning talent development project, and fosters a highly skilled, energized and empowered workforce.
ICF is an equal opportunity employer that values diversity at all levels. (EEO/AA – Minorities/Females/Veterans/Individuals with Disabilities)
For a listing of other career opportunities at ICF, please visit our Career Center at
www.icf.com/careers
Working at ICF
ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our
EEO & AA policy
.
Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email
icfcareercenter@icf.com
and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination:
Know Your Rights
and
Pay Transparency Statement.

Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
$90,940.00 - $154,598.00
Nationwide Remote Office (US99)",3.8,"ICF
3.8","Reston, VA",5001 to 10000 Employees,1969,Company - Public,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
73,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
74,Data Engineer,Employer Provided Salary:$30.05 - $44.13 Per Hour,"AMAZING BENEFITS PACKAGE!
Medical, Dental, & Vision Insurance
Paid Time Off (PTO) – up to 6 weeks of PTO per year
Childcare Stipend – contribution of $5,000 per year to the employee’s dependent care spending account
Loan Reimbursement - reimbursement for student loan payments up to $5,250 per year
Flexible Work Arrangements - opportunity to work a modified schedule, work part-time, or work from home
Retirement Plans - organization matches the employee’s contribution, up to 6% of gross wages
above benefits dependent upon eligibility criteria
Click here for more detail about the benefits package!
Pay Range: $30.05 - $44.13 USD hourly.

Purpose of Job
Improves the overall health of the communities we serve by providing superior customer service and advanced technical support to ensure quality patient care and user satisfaction as follows:
Essential Duties and Responsibilities
Designs and implements databases, ETL routines (using SSIS), stored procedures, and OLAP solutions in addition to providing support and enhancements to existing data solutions.
Designs and builds normalized and denormalized database solutions using healthcare industry best practices for data warehousing for specific client requirements, using Microsoft SQL Server Database programming (stored procedures and database design).
Works on multiple concurrent deadline-driven projects while ensuring data quality and meeting service level agreements.
Builds and maintains cubes (internal and client-facing).
Optimizes database schemas, queries, cubes, and reports, implementing complex logic requirements.
Conducts tuning reviews / assessments (DB and SQL tuning), reporting, and query monitoring.
Determines, enforces, and documents database policies, procedures, and standards.
Performs other duties as assigned, including supporting the CHAS Health Mission and Core Values.
Qualifications
Education/Experience: Associate’s degree or commensurate experience in a technical field required. Prior experience in database development using Microsoft SQL server, SSIS and SSAS preferred; understanding performance, deployment, configuration, security, migration, and troubleshooting; as well as proficiency in building and optimizing SQL queries preferred. Previous experience building and supporting business intelligence/data warehousing solutions; and knowledge of healthcare data models preferred.
Skills: Excellent logical and problem-solving abilities, verbal and written communication skills required. Ability to work independently in a self-directed environment, contribute to a team, maintain a positive attitude, demonstrate very high attention to detail, and a commitment to quality while working in a high availability environment is required. Commitment to supporting a safe, respectful, equitable, and inclusive environment required. Valid drivers’ license and insurance required.
Physical Demands
Required to stand, sit, and be mobile up to two-thirds of the time. Required to read from text and computer screen over two-thirds of the time. Climbing or balancing, stooping, kneeling, or crouching occurs less than one-third of the time. Communicating occurs constantly throughout the day. Lifting occurs about half the time up to 10 lbs. and less than one-third of the day from 25-40 lbs. Rarely is there a need to lift more than 41 lbs.",3.8,"CHAS Health
3.8","Spokane, WA",1001 to 5000 Employees,1994,Nonprofit Organization,Health Care Services & Hospitals,Healthcare,$25 to $100 million (USD)
75,Data Engineer,Employer Provided Salary:$112K - $154K,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

As a Data Engineer at DocuSign you will work with a group of like peers to help support the existing storage infrastructure, while designing the next version of the infrastructure. We have no down time and demand very high availability from our system (we strive for five 9s). When not supporting the existing systems, you will help collaborate with engineering, architecture and product management to create the next great platform features that will drive impact. You will share in an “On Call” rotation to support the platform 24X7. We are pushing the boundaries of what’s possible with file, NoSQL, Full text search and data warehouse storage. This position will push your skills to the next level.

This position is an individual contributor reporting to the Sr. Director, Database and Storage Solutions.

Responsibility
Protect the data under our control by making sure we meet all data storage best practices and policy requirements, which includes patching and other industry best practices
Ensure all storage systems meet (RPO) policies
Ensure all storage systems meet (RTO) policies
Operate and manage multiple 7x24 enterprise storage platforms
Maintain storage platform performance by identifying and resolving production and application development problems; calculating optimum performance measures; evaluating, integrating, and installing new releases, completing maintenance
Provide support by coding utilities, responding to user questions, and resolving problems
Troubleshoot storage service issues as they occur, including after-hours and weekends as part of an “On Call rotation”
Ensure ISO, Fed RAMP and PCI information security compliance
Plan and execute expansion to support rapid transaction growth
Work with engineering to design and optimize storage solutions
Work with the operations team to manage, install and configure server’s in multiple data centers around the world
Create and analyze operational reporting
Document the company’s storage environment
Update job knowledge by participating in educational opportunities, reading professional publications, maintaining personal networks, participating in professional organizations


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)

Positions at DocuSign are assigned a job designation of either In Office, Hybrid or Remote and are specific to the role/job. Preferred job designations are not guaranteed when changing positions within DocuSign. DocuSign reserves the right to change a position's job designation depending on business needs and as permitted by local law.


What you bring

Basic
5+ years experience as a server administrator and production support administrator
Understanding of server setup and configuration for optimal resource usage
Experience storage server monitoring, performance tuning, troubleshooting, and capacity planning
Experience with storage design, maintenance, security, management and analysis
Preferred
Experience with Cloud based storage tools and platforms
High level knowledge of Windows and Windows security
Excellent analytical and problem-solving skills
Excellent written and verbal communication skills
Self-motivated, able to work independently as well as in a collaborative environment
Experience with PowerShell scripting
Strong Kusto querying skills


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $118,300.00 - $182,775.00 base salary

Illinois and Colorado: $111,600.00 - $153,525.00 base salary

Washington and New York (including NYC metro area): $111,600.00 - $162,625.00 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.",3.7,"DocuSign
3.7","Seattle, WA",5001 to 10000 Employees,2003,Company - Public,Information Technology Support Services,Information Technology,$1 to $5 billion (USD)
76,Data Engineer,-1,"Blackwell Security Inc. is a start-up backed by venture capital, focused on bridging the technology gap in healthcare. Our purpose-built ecosystem provides comprehensive cybersecurity managed services for life sciences and healthcare. We are building a customizable product that ensures health systems have access to a suite of security solutions, with built-in visualization and optimization to ensure the safety of patient information.
As we continue to build out our core team, we are adding a Data Engineer to our Engineering Team. You will be working cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data models. You will design, implement, and scale data pipelines that transform billions of records into action and insight.
This is a unique opportunity to jump into an early-stage start-up at a pivotal time and make a meaningful impact. If you thrive in a small, growing environment and love the energy of start-ups, this is the role for you!
While our headquarters are in Detroit, Michigan, this is a remote role but ideally a candidate would live in Detroit or Minneapolis, location of our core Engineering Team. This role is not eligible for visa sponsorship.
What you will do in the Data Engineer role:
Collaborate with engineering to build and maintain an enterprise data ecosystem including ingestion, storage, organization and interface.
Analyze the business and technical requirements for data systems and applications; Coordinate the integration of IT policies, procedures and development practices.
Translate business requirements into data models that are easy to understand and used by different disciplines across the company.
Design, implement, build/enhance pipelines that deliver data with measurable quality under the SLA.
Partner with business domain experts, data analysts and engineering teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service.
Champion the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements.
Own and document foundational company metrics and benchmarks with clear definition and data lineage.
Identify, document and promote best practices.
Design and architect data systems, focusing not only on performance and scalability, but also on crafting a beautiful user experience.
Define/Implement data visualizations & UX for external/internal customers.
Taking a thoughtful approach to decision making; balance speed and quality, with a focus on tangible results.
Explore Blackwell’s data to discover trends and opportunities, identify what questions we should be asking of our data.
Analyze & evaluate transactional system data for transformation and use in reporting, analytics, and AI/ML.
Evaluate and establish early strategies and usage of AI (machine learning, generative AI, etc.).
Qualities and skills for success in the Data Engineer role:
Bachelor's degree in Computer Science, Engineering, or related technical or business field.
Attention to detail, and Agile development experience.
Experience with Python and AWS services.
Experience with various data storage systems, RDBMS, Document/NoSQL DBs, etc.
Experience implementing data pipelines via methods such as ETL, ELT, EL/TL, DaaS, Data Lake or ODS.
Experience experimenting with and applying AI (machine learning, generative AI, etc.) in an enterprise environment.
Experience working with multi-customer multi-tenant environments preferred. Experience with cybersecurity data is not required but is a plus.
Adaptable and focused on solutions.
Equal Employment Opportunity
We’re proud to be an equal opportunity employer and welcome our employee’s differences, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or Veteran status. Difference makes us better. Join us.",2.8,"Blackwell Security, Inc.
2.8",Remote,51 to 200 Employees,1999,Company - Private,Security & Protective,Management & Consulting,$1 to $5 million (USD)
77,Data Engineer,Employer Provided Salary:$120K,"Dimo
Worldwide
Permanent
120000 K
November 2022
Remote

Job description

What you will be doing:
Developing data analytics pipelines and query engines

Writing reusable code and libraries to help the team expand

Enhancing application for scalability and performance

Bridging the gap between requirements and technical implementation

Working closely with the leadership team on the product direction

We're looking for someone who has:
Passion about what decentralization technology can offer to humanity

Insatiable curiosity of the industry and self-motivation to pursue researching every new idea that floats across the industry

Strong engineering fundamentals

Experience in Flink/Kafka Streams/Apache Beam in high throughput scenarios

4+ years of experience in a similar role

Strong experience with software delivery and taking a set of requirements towards full implementation with minimal guidance

Ability to communicate effectively and work collaboratively with the team and internal/external organizations

Proficiency in code versioning tools such as Git

Experience in agile software development

Familiarity with Web3js

Excellent problem solving and analytical thinking skills

It'd be nice to find someone who also has:
Experience with web3 technologies

Knowledge/experience in heavily distributed systems

Work experience in Mobility/IoT space

Ability to explain abstract technical concepts in an easy to understand manner

Who we are ?
DIMO enables thousands of vehicle owners and operators to collect, use, and monetize the data their vehicles generate. We're building an open developer platform to transform the massive vehicle data and connected car market, and bootstrapping the supply and demand side with the $DIMO token.

Our core team is made up of experienced IoT, automotive, mobility, and web3 engineers, designeers, and operators, and we are committed to building a distributed organization.
Our values
We are a global, remote-friendly company. For those local to Detroit or New York, we have in-person office space for you to pull up a chair and work IRL. Team members can expect quarterly in-person events once individuals feel comfortable traveling again.

We are fueled by an immense intellectual curiosity of what technology can do and the impact it can have. We succeed as a team when we embrace individual differences. DIMO is an equal opportunity employer that is committed to fostering a diverse workforce. All qualified applicants will receive consideration for employment.",-1,Blockchain Talents,Remote,1 to 50 Employees,-1,Unknown,-1,-1,Unknown / Non-Applicable
78,Data Engineer,-1,"Hummingbird is a remote-first, fully distributed team united by the shared mission of helping fight financial crime. Since our launch in 2017, we’ve helped major financial institutions and tech-savvy trailblazers alike (e.g. Stripe, Affirm, etc.) orchestrate their compliance programs through our thoughtfully designed, intuitive SaaS product. We believe finding and stopping financial crime is a problem rooted in code, language and design, so we built the product that the heroes doing this work deserve.
We are customer-obsessed, and we love building and shipping great products. We set a high bar, challenge our assumptions, seek diverse opinions, and support each other to do our best work.
We do our best to write inclusive, descriptive and accurate job descriptions, but we’re not always perfect. If you’re interested in the role, we’d love to hear from you even if you don’t feel like you meet everything we’re looking for. We’re always iterating and improving, and it’s possible that your experience is even more impactful than we could have imagined.

About the Role
We are looking for a driven data engineer to join our team and champion the use of data at Hummingbird. Data plays a crucial role in our mission to fight financial crime and you will help us find new and innovative ways to leverage it to provide powerful tools for our users, and allow us to better understand the usage of our product. Hummingbird is uniquely positioned at the intersection of financial technology, security, policy, and law enforcement and as such we have built up a one of a kind data set that we can now use to give our customers the edge in their efforts to stay ahead of criminals. As an employee at a small startup, you will have the opportunity to wear many hats, working from product discovery through implementation.
What you’re looking to do:
Level up our use of data to make better decisions, build more powerful features, and fight financial crime
Leverage Infrastructure as Code (IaC) to manage and deploy infrastructure that supports a variety of different projects, such as data replication and orchestration for machine learning workflows
Build new data pipelines for ingesting data into our data warehouse via both batch and streaming architectures
Work closely with data science to enable us to build products that benefit our customers while keeping compliance and security at the forefront
Achieve goals through a combination of independent building, educating your peers, and influencing others to contribute towards your vision
What we’re looking for:
A data engineer with a history of taking projects from the earliest stages through successful rollout to production
Someone who is excited by the prospect of pioneering data as a practice at a fast growing startup and who is unafraid to dig in to discover what is possible
A flexible self starter that will cut across organizational lines to understand the business and identify the most valuable work
An engineer who brings a pragmatic approach to problem solving, favoring simplicity and shortening delivery cycles
Experience building data pipelines for sensitive data, including best practices for de-identification and data security
Experience deploying infrastructure via terraform or a similar infrastructure as code tool
Expertise in SQL and one or more programming languages, especially python
What’s in it for you:
The chance to help build from the ground up. The hires we’re making now are foundational to our growth as a company, so you will have an opportunity to help shape the future of Hummingbird.
Competitive compensation including cash and equity.
Remote-first, fully distributed company with flexible working hours.
Awesome health, vision & dental benefits, and 401k.
Safe, respectful & comfortable work environment with colleagues and leadership who prioritize diversity, equity, inclusion, and belonging.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please don't hesitate to contact us to request accommodation.",-1,Hummingbird,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
79,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
80,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
81,Data Engineer,-1,"Highlights
Experience
0 - 1 Year
Joining Date
Immediate or max 30 days’ Notice Period
Qualification
Bachelor's Degree or Post Graduation in Computer Science, IT, or a related field
Job Description
Beinex is looking for excellent Data Engineers who can make life easier for the league of extraordinary Data Analysts and Data Scientists of Beinex’s esteemed client line up. Out there, you get to be a part of an awesome work culture and robust work ethics. You will find yourself immersed in the latest trends and technologies expanding the frontiers of Data Engineering as we know it currently. And no one can beat the pay/ perks package either! Get to take the right decision, for you and for us! Read on and join us!
Responsibilities
Perform analytical and technical tasks to complete special and ongoing projects requiring extensive research, data collection, and detailed analysis, following departmental guidelines, policies, and procedures.
Need to provide complex analytical, technical, and administrative support to facilitate the day to-day operations.
Respond to complex inquiries from administrators
Exercise independent judgment to troubleshoot and resolve issues
Key Skills Requirements
0-1 year of experience in a similar role
Strong desire to learn cloud technologies, Python, and Spark to build resilient data pipelines
SQL experience would be a plus
Experience in cloud platforms is an added advantage
Solid analytical and problem-solving skills involving sound decision making and effective resolutions
Keen attention to detail
Strong planning and organizational skills involving the ability to manage multiple work streams effectively
Ability to work within a team to meet established project goals
Ability to communicate professionally, concisely and effectively, both verbally and in writing, to internal and external stakeholders
Self-starter with the ability to work independently while supporting a team environment
Data analysis and mapping skills with strong attention to detail and concern for data accuracy
Intermediate knowledge of SQL commonly used concepts, practices and procedures related to relational databases
Bachelor's Degree or Post Graduation in Computer Science, IT, or a related field

Perks: Careers at Beinex
Comprehensive Health Plans
Learning and development
Workation and outdoor training
Hybrid working environment
On-site travel Opportunity
Beinex Branded Merchandise",3.6,"Beinex
3.6",Remote,201 to 500 Employees,2017,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
82,"Data Engineer (Snowflake, PowerBi)-9583",Employer Provided Salary:$40.00 - $45.00 Per Hour,"Typical Day: -
Manage, and aggregate data for analytic consumption
Create and maintain data tables in Snowflake
Utilize data from multiple cloud sources and develop/maintain dashboards
Data movement and Extract, Transformation, Load (ETL) development
Education Requirements:
Associate’s degree in computer programming or a relevant field required.
Bachelor's degree preferred.
0-2 years experience required.
Technical Skills:
**Required: Basic knowledge of logical data modeling and physical data modeling. Data movement and Extract, Transformation, Load (ETL) development skills. Basic knowledge of computer software, such as SQL, Visual Basic, Scripting language - Python, Snowflake, etc. Experience with Microsoft Office tools. **Desired: Experience with other Microsoft tools, Teams, SharePoint (Office 360).
Soft Skills: Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. Basic ability to work independently and manage one’s time.
Job Type: Contract
Pay: $40.00 - $45.00 per hour
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Aurora, IL 60502: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 2 years (Required)
SQL: 2 years (Required)
Work Location: Hybrid remote in Aurora, IL 60502",5.0,"DSMH LLC
5.0","Aurora, IL",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
83,Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Title: Data Engineer
Experience in Retail merchandising analytics is a must,
Duration: 12 months
Location: Remote
Interview Process:
1*_st_ round – Hirevue Video Call*
2*_nd_ round – Hiring Manager*
Skills Required:
Data engineering, analytics, and data modeling experience
Python, Spark, Databricks, Azure, Power BI
Experience in retail merchandising analytics is a must
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
retail merchandising analytics: 8 years (Required)
Python: 8 years (Required)
Power BI: 8 years (Required)
Work Location: Remote",-1,zettalogix.Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
84,"Analytics Engineer (L4), Consumer Insights - Data Management & Analytics",$110K - $165K (Glassdoor est.),"Los Angeles, California
Consumer Insights
Los Angeles, CA or Los Gatos, CA (NOT OPEN TO REMOTE)

Netflix is revolutionizing entertainment and shaping the evolution of storytelling around the world. We are also a data-inspired company and use this competitive advantage to constantly improve our product and content for the benefit of our members.

The Data Management and Analytics pod sits within the Consumer Insights team and comprises quantitative-leaning, technical experts with specializations in data engineering and data science. This team's superpower is its unparalleled ability to bring clarity and a new perspective to nuanced and complicated business problems. They independently shine a light on opportunities and challenges or provide vital data to collaborative initiatives to ensure that insights inspire action and innovation across the company.

We’re looking for a passionate Analytics Engineer to join the Consumer Insights Data Management and Analytics team. The opportunity to leverage consumer research to impact critical decisions is unprecedented. In this role, you will maintain and develop next-generation data pipelines, models, and products to elevate consumer research and help answer Netflix’s most complex business challenges.

Visit our culture deck and our Research page to learn about what it’s like to work on Analytics at Netflix.
In this role, you will:
Maintain and help develop scalable data pipelines to manage and transform large amounts of survey, social and behavioral data for analytics use cases and company-wide reporting.
Create and maintain highly intuitive story-driven dashboards to answer some of the most challenging questions in entertainment through data visualization.
Partner closely with Researchers, Data Scientists, and Analytics Engineers to help identify critical analytical problems and find innovative solutions with data.
Support the development of data products and tools to facilitate self-service analytics and enable the broader use of key datasets.
Collaborate with Researchers, Engineers, and Data Scientists to conduct analyses and apply statistics to answer specific questions or support larger initiatives.
Support the execution of Quantitative and Qualitative research through research sampling design and targeting.
To be successful in this role, you have:
A BS/MS degree in a technical field (Statistics, Computer Science, Mathematics, or related field) with 2+ years of relevant industry experience.
A team player attitude and thrive in ambiguity, proactively identifying opportunities for impact.
A passion for connecting the dots across business needs and effectively collaborating with team members and direct partners to drive projects forward.
Strong communication skills and can turn insights into actionable, data-driven stories in both verbal and written form.
Experience giving and receiving feedback to strengthen ideas and partnerships.
Proficiency in programming and data manipulation skills using SQL, R and/or Python, and version control (Github, Stash).
Experience building multi-step ETL jobs/data pipelines and working with job scheduling systems.
Strong data visualization skills and experience with Tableau are required. This includes viz development, complex dashboard actions, and optimizations. Familiarity with other data visualization tools is a plus (e.g., R Shiny, Plotly, Looker).
At Netflix, we carefully consider a wide range of compensation factors to determine your personal top of market. We rely on market indicators to determine compensation and consider your specific job family, background, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location.

The overall market range for this role is typically $150,000 - $750,000.

This market range is based on total compensation (vs. only base salary), which is in line with our compensation philosophy. Netflix is a unique culture and environment. Learn more here.",4.2,"Netflix
4.2","Los Angeles, CA",5001 to 10000 Employees,1997,Company - Public,Internet & Web Services,Information Technology,$5 to $10 billion (USD)
85,Data Engineer,Employer Provided Salary:$100K - $160K,"Position Summary
The Effectual Data Engineer builds pipelines that are used to transport date from a data source to a data warehouse. These pipelines are crucial: they are what allow us to access and analyze an organizations data and use the insights to help them make decisions. Data pipelines transport and transform data according to established business rules or a line of exploratory analysis the business wants to undertake. As a Data Engineer, you will prepare and organize the data that organizations have built in their databases and other formats.
A Glimpse into the Daily Routine of a Data Engineer
As a Data Engineer, specializing in Confluent Kafka, you will take on big data challenges in an agile way. You will build data pipelines, utilizing Confluent Kafka, that enables our clients and their vision. You will also build data models to deliver insightful analytics while ensuring the highest standard in data integrity. You are encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.
Essential Duties and Responsibilities
Design, build and launch extremely efficient and reliable data pipelines, utilizing Confluent Kafka, to move data across several platforms including Data Lakes, Data Warehouses, and real-time systems.
Develop, construct, test and maintain data architectures from the data architect.
Analyze organic and raw data.
Build data systems and pipelines.
Build the infrastructure required for extraction, transformation, and loading of data from different data sources using SQL and AWS 'big data' technologies.
Write scripts for data architects, data scientists, and data quality engineers.
Data acquisition.
Identify ways to improve data reliability, efficiency, and quality.
Develop dataset processes.
Prepare data for prescriptive and predictive modeling.
Automate the data collection and analysis processes, data releasing and reporting tools.
Build algorithms and prototypes.
Develop analytical tools and programs.
Qualifications
MUST HAVE hands-on experience with Confluent Kafka including both administration and development.
Either Confluent Certified Administrator for Apache Kafka (CCAAK) or Confluent Certified Developer (CCDAK) for Apache Kafka certificates, however, both are preferred.
Bachelor's or master's degree in computer science, Engineering or a related field.
Experience working as a Data Engineer in a professional services or consulting environment.
Proficiency in programming languages such as Python, Java, or Scala, with expertise in data processing frameworks and libraries (e.g., Spark, Hadoop, SQL).
In-depth knowledge of database systems (relational and NoSQL), data modeling, and data warehousing concepts.
Strong knowledge of data architectures and data modeling and data infrastructure ecosystem.
Experience with cloud-based data platforms and services (e.g., AWS, Azure) including familiarity with relevant tools (e.g., S3, Redshift, BigQuery, etc.).
Proficiency in designing and implementing ETL processes and data integration workflows using tools like Apache Airflow, Informatica, or Talend.
Familiarity with data governance practices, data quality frameworks, and data security principles
Work minimal direction and turn a clients want and need into working stories, epics which can be performed upon during a sprint.
A firm understanding of the SDLC process.
An understanding of object-oriented programming.
The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex.
The ability to thrive in a dynamic environment. That means being flexible and willing to jump in and do whatever it takes to be successful.
Ability to travel, with strong preference to mid-west time zone or east coast.
Nice-to-Have Skills and Experience
Knowledge of batch and streaming data architectures.
Product mindset to understand business needs and come up with scalable engineering solutions.
AWS Certified Cloud Practitioner
AWS Certified Data Analytics Specialty
AWS Certified Machine Learning Specialty
AWS Certified Database Specialty
SnowPro Core Certification
Databricks Certified Data Engineer Associate
Company Offered Benefits
Full-time employees are eligible to participate in our employee benefit programs:
Medical, dental, and vision health insurances,
Short term disability, long term disability and life insurances,
401k with Company match
Paid time off (PTO) (120 hours PTO that accrue over one year)
Paid time off for major holidays (14 days per year)
These and any other employee benefit offerings are subject to management's discretion and may change at any time.
Physical Demands and Work Environment
The work is generally performed in an office environment. Physical demands include sitting, keyboarding, verbal communication, written communication. Employees are occasionally required to stand; walk; reach with hands and arms; climb or balance; and stoop, kneel, crouch, or crawl. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this position. Reasonable accommodation may be made to enable individuals with disabilities to perform the functions.
Salary Range for this position: $100,000-$160,000
""Salary ranges provided are for informational purposes only and may vary depending on factors such as experience, qualifications, and geographic location. The final salary offer will be determined based on the candidate's skills and alignment with the role requirements.""
This job description may not be inclusive of all assigned duties, responsibilities, or aspects of the job described, and may be amended anytime at the sole discretion of the Employer. Duties and responsibilities are subject to possible modification to reasonably accommodate individuals with disabilities. To perform this job successfully, the incumbents will possess the skills, aptitudes, and abilities to perform each duty proficiently. This document does not create an employment contract, implied or otherwise, other than an ""at will"" relationship. Effectual Inc. is an EEO employer and does not discriminate on the basis of any protected classification in its hiring, promoting, or any other job-related opportunity.",4.0,"Effectual
4.0",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
86,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1,Radiant System,Remote,-1,-1,-1,-1,-1,-1
87,Data Engineer,Employer Provided Salary:$80K - $120K,"As the Data Engineer at Digible, you will be responsible for designing, developing, and maintaining our data pipeline, ensuring that data is properly collected, processed, and analyzed to inform business decisions. You will work closely with cross-functional teams in order to understand their data needs and ensure that our data infrastructure supports those needs. You will also be responsible for ensuring the integrity and security of our data.
About Us:
Privately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.
At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.
We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.
What you'll do:
Design and implement scalable data pipelines, including data ingestion, transformation, and integration.
Collaborate with stakeholders to understand data requirements and develop data models aligned with Digible's business goals.
Build and maintain ETL/ELT workflows using dbt (Data Build Tool) and Prefect to transform raw data into structured datasets for analysis, reporting, and sustainability tracking purposes.
Implement data quality controls and validation processes to ensure the accuracy and integrity of Digible's data.
Monitor data pipelines, perform debugging and troubleshooting, and implement proactive measures to ensure data availability, accuracy, and efficient resource utilization.
Optimize and tune data pipelines for performance, scalability, and reliability.
Stay up to date on the latest advancements in data engineering technologies, tools, and best practices, and propose their adoption when they align with Digible's objectives
You should have:
Proven experience as a Data Engineer or similar role, with a strong understanding of data engineering concepts, data modeling, and database systems.
Cloud experience (i.e. AWS, GCP, Azure, etc.)
Proficiency with Git and source control
Proficiency with data transformation and modeling tools (i.e. dbt, dataform, PySpark, etc.)
Strong programming skills in Python and familiarity with related libraries and best practices.
Experience with SQL and working knowledge of relational databases and/or columnar databases.
Familiarity with data orchestration techniques and tools (i.e. Prefect, Dagster, Airflow, AWS Glue, etc.)
Data Warehouse Experience (i.e. BigQuery, Snowflake, Redshift, etc..)
Knowledge of data warehousing concepts and dimensional modeling.
Strong problem-solving and analytical skills, with the ability to work in a fast-paced and collaborative environment.
Familiarity with Docker
Experience with marketing and advertising API's & data (i.e. Google Suite, Facebook, etc.) is a big plus
Application experience of CI/CD implementations is a plus
Passion to make an impact and do some awesome work.
Digible's Core Values
Authenticity: The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.
Curiosity: The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.
Focus: The collective will to remain completely devoted and ultimately accountable to our deliverables.
Humility: The recognition and daily practice that ""we"" is always greater than ""I"".
Happiness: The decision to prioritize passion and love for what we do above everything else.
Pay, perks, and such:
Annual Salary of $80,000 - $120,000 depending on level, breadth and years of experience
4 Day work week (32 hour per week)
WFA (Work From Anywhere)
Profit Sharing Bonus
We offer 3 weeks of PTO as well as Sick leave, and Bereavement.
We offer 11 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Day before Thanksgiving, Thanksgiving, Day after Thanksgiving, Christmas Eve, and Christmas)
401(k) + 5% employer match
50% employer paid health benefits, including Medical, Dental, and Vision.
We provide $75/ month reimbursement for Physical Wellness
We provide $75/ month reimbursement for Mental Wellness
$1000/year travel fund for employees who have been with Digible 3+ years
Monthly subscription for financial wellness
Dog-Friendly Office
Paid Parental Leave
Company Sponsored Social Events
Company Provided weekly lunches and snacks for in office employees
Employee Development Program",4.7,"Digible
4.7",Remote,51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,$5 to $25 million (USD)
88,Data Engineer,Employer Provided Salary:$95K - $135K,"Your Job
KBX Technology Solutions, LLC, a provider of transportation technology to industry leaders, is seeking a Data Engineer. This role will be responsible for designing, developing, and maintaining data systems and infrastructure required to support data processing and analysis. You will work closely with a team of professionals to understand business requirements and build scalable solutions to handle large volumes of data. A successful candidate will have strong programming skills, experience with database technologies, and a deep understanding of data management and processing.
This role is not open to Visa Sponsorship now or in the future.
What You Will Do

Collaborate with cross-functional teams to understand data requirements and design data pipelines that align with business needs.
Develop and implement ETL processes to ingest, cleanse, and transform data from diverse sources into the data warehouse.
Optimize and tune data pipelines to ensure high performance and reliability in handling large volumes of data.
Troubleshoot and resolve issues related to data pipeline failures, data quality, and data integration challenges.
Work closely with data architects and database administrators to ensure seamless integration and data consistency.
Design and implement data models and schemas to support data warehouse solutions efficiently.
Monitor data pipeline performance and implement improvements to enhance data processing efficiency.
Ensure data security and compliance with data privacy regulations throughout the data pipeline process.
Continuously explore and evaluate new technologies and tools to enhance data pipeline capabilities.
Document data pipelines, data flows, and technical specifications for future reference and team collaboration.
Provide technical guidance and mentorship to junior team members in data engineering best practices.
Who You Are (Basic Qualifications)

Strong knowledge in Python, SQL, data warehouse systems, data lake systems, and data pipelines on AWS or similar cloud environments
Professional experience of data engineering concepts (ETL, data warehousing, near-/real-time streaming, data structures, metadata, and workflow management)
Strong experience with ETL tools like Apache Spark, Talend, or AWS Glue.
Strong programming skills and experience using source control platforms like Gitlab, GitHub, etc.
Knowledge of data management, stewardship, and governance concepts
Experience delivering advance analytics solutions, reporting, and managing big data
What Will Put You Ahead

Strong communication & collaboration skills
Familiarity with cloud platforms like Snowflake, AWS, Azure, or Google Cloud, and hands-on experience with relevant data services.
Understanding of data streaming platforms like Apache Kafka for real-time data processing.
Experience with API integration and handling semi-structured data
Experience developing with dockers in a Kubernetes environment.
An understanding of modern cloud infrastructure, container-based deployments, and storage architectures
Has worked in an Agile environment and is proficient using tools like Azure DevOps, Jira, etc.
Experience with data visualization tools such as Tableau or Power BI
Experience working in transportation management
At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.
For this role, we anticipate paying $95,000 - $135,000 per year. This role is eligible for variable pay, issued as a monetary bonus or in another form.
Hiring Philosophy
All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here .
Who We Are
As a Koch company, KBX provides the global transportation, logistics and technology solutions that help our customers deliver life's essentials to people all over the world. We develop and deploy cutting-edge technologies to deliver better solutions for increasingly complex supply chains. Our team of tenacious problem-solvers are driven to create real, long-term value for our customers.
At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.
Our Benefits
Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.
Equal Opportunities
Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information: http://www.kochcareers.com/doc/Everify.pdf",3.8,"KBX
3.8","Green Bay, WI",10000+ Employees,1940,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",$10+ billion (USD)
89,Data Scientist / ML Engineer (REMOTE /Contract),-1,"Role: Data Scientist / Machine Learning Engineer
Location: REMOTE
Duration: 6+ months
Scheduling Interviews
Skills/Experience:
6+ years of experience working in Data Science and Machine Learning field.
Extensive experience in applying data preprocessing, statistical analysis, data analytics tools, predictive modelling, model deployment and evidence-based approaches to find lean, actionable solutions to various real-world enterprise business problems.
Experience in the application of Supervised and Unsupervised Learning algorithms, Naïve Bayes, Regression Analysis, Neural Networks/Deep Learning, Support Vector Machines (SVM), Random Forest, K-Means, Hierarchical, Spectral clustering, DBSCAN, Collaborative Filtering and other advanced machine learning techniques.
Used AWS Cloud Services Sagemaker, ECS, EKS, S3, Redshift, QuickSight
Experience with Google Cloud Platform (GCP) AutoML, Vertex AI, BigQuery, Colab, DataProc, Data Studio,
Exposure to Azure ML, PowerBI, Azure SQL and DataBricks
Design custom BI reporting dashboards or interactive data visualizations and widgets in R and Python using Shiny, Tableau, Ggplot2, Plotly, Matplotlib, and Seaborn.
Produce custom BI reporting dashboards in R and Python using Shiny, and Plotly for rapid dissemination of actionable, data driven insights.
Strong experience in Software Development Life Cycle (SDLC), MLOPS
Experience in working with relational databases (Teradata, Oracle) with advanced SQL skills.",3.5,"Glow Networks
3.5",Remote,51 to 200 Employees,2003,Subsidiary or Business Segment,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
