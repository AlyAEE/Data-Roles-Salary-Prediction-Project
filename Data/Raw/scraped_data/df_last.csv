,Job Title,Salary Estimate,Job Description,Rating,Company Name,Location,Size,Founded,Type of ownership,Industry,Sector,Revenue
0,Senior Data Engineer (Remote),Employer Provided Salary:$50.00 - $70.00 Per Hour,"Company Overview: IT Giants INC is a dynamic and innovative technology company at the forefront of delivering advanced data solutions across various industries. We are seeking an experienced Data Engineer with expertise in cloud platforms, programming languages, database technologies, and analytics tools. If you also have a strong background in the airline domain, this is an opportunity to join our dedicated team and shape the future of data-driven decision-making in the aviation sector.
Position Overview: As a Data Engineer at IT Giants INC, you will play a pivotal role in designing, building, and maintaining sophisticated data pipelines and analytical solutions for our airline industry clients. Leveraging your deep understanding of both data engineering and the airline domain, you will empower our clients to harness the power of their data to improve operations and customer experiences.
Key Responsibilities:
Data Pipeline Development: Architect, develop, and maintain end-to-end data pipelines, integrating data from diverse sources, including cloud platforms (GCP, Azure, AWS), APIs, and various databases (Teradata, Oracle, PostgreSQL, NoSQL).
Data Transformation and Modeling: Apply your expertise in SQL, Python, and Alteryx to transform raw data into meaningful insights, while ensuring data integrity and accuracy.
Cloud Expertise: Utilize your proficiency in GCP, Azure, and AWS to design and implement scalable and cost-efficient cloud-based solutions that meet the unique demands of the airline industry.
Database Management: Manage relational and NoSQL databases to store and retrieve data efficiently, ensuring optimal performance and security.
Analytics and Reporting: Collaborate with data scientists and analysts to develop interactive dashboards and reports using tools like Tableau, QuickSight, and CloudWatch, enabling stakeholders to make informed decisions.
Data Quality and Governance: Implement data quality checks, validation processes, and adherence to industry regulations to ensure data accuracy and compliance.
Version Control and Collaboration: Utilize GitLab and other version control tools to collaborate with cross-functional teams on code management and deployment.
Real-time Monitoring: Develop real-time data processing solutions using technologies like CloudWatch to enable timely insights into operational metrics.
Linux Environment: Leverage your proficiency in Linux to optimize system performance and troubleshoot issues as they arise.
Domain Knowledge: Apply your deep understanding of the airline industry to structure data effectively and drive actionable insights from the data.
Qualifications:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
years of proven experience in data engineering, with a focus on cloud platforms (GCP, Azure, AWS), Python, SQL, and relevant database technologies.
Experience with Alteryx, Tableau, and data visualization tools for creating impactful dashboards.
Proficiency in managing databases such as Teradata, Oracle, PostgreSQL, and NoSQL databases.
Strong command of Linux environments and version control using GitLab.
Familiarity with CloudWatch, QuickSight, and real-time data processing is advantageous.
Domain expertise in the airline industry, including a solid understanding of aviation data, standards, and operational challenges.
Excellent problem-solving skills and the ability to troubleshoot complex data issues.
Effective communication skills to collaborate with technical and non-technical teams.
Proven ability to work independently and in a team, delivering high-quality solutions.
Join Our Team: If you are a driven and knowledgeable Data Engineer with a passion for both technology and the airline industry, IT Giants INC offers an exciting opportunity to contribute to transformative data solutions. Join us in our mission to revolutionize data analytics within the aviation sector and drive meaningful impact.
Job Types: Full-time, Contract
Pay: $50.00 - $70.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Work Location: Remote",5.0,"IT Giants
5.0",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
1,BI Data Engineer (Remote),Employer Provided Salary:$75.00 - $85.00 Per Hour,"Stefanini Group is hiring!
Exciting opportunity awaits, let us help you get started!
Click Apply now or you may call: (248) 728-2611 / email: palak.agrawal@stefanini.com for faster processing!

Job Description:
We require a reliable, talented, highly motivated BI/Data Engineer with excellent SQL, PowerBI, Python skills, The firm is putting a huge focus on data and reporting, with an increased demand of data related tasks.
As a global leader in Asset management, you will be working as part of an innovative, fast paced technology driven culture who are always looking at ways to enhance how technology is used within their organization.
You will be a valued member of an effective data team working to deliver growth against ambitious targets.
You will play a key role in extracting useful information from data, providing important insights to global technology teams.
The role will focus on creating best practice, high quality data solutions in collaboration with numerous business stakeholders and technology partners.
We are looking for someone who enjoys problem solving, who has an inquisitive nature, a team player and can work in a collaborative environment with good communication skills.
Ability to help extract and analyze data is required for the job.



Requirements:
We are looking for a good mix of the following skills:
Excellent experience in the Microsoft BI stack, Excel.
Excellent experience in building data pipelines and integrations (ETL/ELT).
Excellent knowledge of data warehousing and analysis concepts, data normalization, dimensional modelling, and associated practices.
Knowledge and experience of data manipulation languages such as DAX or MDX, Visualization or dashboarding experience.
Python experience is a must. Azure cloud experience would be a bonus.
We believe in building a culture supported by strong values, trust and independence. Within the team, we are highly collaborative, supportive, open and have high standards in the work we produce.

Education:
Bachelor's degree or equivalent experience.

*Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives*

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",3.3,"Stefanini, Inc
3.3","New York, NY",10000+ Employees,1987,Company - Private,Enterprise Software & Network Solutions,Information Technology,$1 to $5 billion (USD)
2,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1.0,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
3,"Data Engineer (AWS, Python)",Employer Provided Salary:$50.00 - $52.00 Per Hour,"Title: Data Engineer (AWS, Python)
Location: Remote
Duration: 12+ Months
Job Description:
Typical task breakdown:-
Daily standup, collaborate with peers, work independently to complete sprint commitments, participate in agile ceremonies.
Interaction with team:-
Daily stand up meetings with team – working mainly with Helios Platform adoption team.
Work environment:
Education & Experience Required:
Bachelor's in computer science required plus 7+ Working experience .
Masters degree with 6+ years working expeirence acceptable as well.
Preferred: AWS cloud certifications
Technical Skills
(Required)
Exp with Object oriented programming - Python (4+ years)
AWS (3+ years overall exp) lambda and/or glue, EMR, S3
SQL (3+ years)
2+ years exp in test driven development, unit test, integration testing, regression testing
(Desired)
Event driven framework experience.
Kinesis or Kafka experience
Familiar with working in Agile methodology
Soft Skills
(Required)
Strong written and verbal communication, experience working as a member of agile engineering team, ability to commit to and deliver work within allocated sprint window.
Job Type: Contract
Pay: $50.00 - $52.00 per hour
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",5.0,"DSMH LLC
5.0",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
4,Data Engineer,Employer Provided Salary:$45.00 Per Hour,"Job Title: GCP Data Engineer
Duration: 12+ Months
Location: Hartford, CT (Onsite Role)
Tax term : W2 & 1099
Job Description:
As a GCP Data Engineer, you will play a pivotal role in designing, implementing, and optimizing data solutions on the Google Cloud Platform (GCP). Your expertise will be essential in leveraging various technologies to build robust data pipelines and contribute to the overall success of data-driven initiatives.
Roles and Responsibilities:
Design, develop, and maintain efficient data pipelines using Python or PySpark on the Google Cloud Platform.
Seamlessly integrate data solutions using GCP services like Google Cloud Storage, BigQuery, Dataflow, and Pub/Sub.
Utilize expertise in the Hadoop ecosystem, including HDFS and Hive, to craft seamless data workflows.
Transform raw data into insights by creating complex SQL queries for data transformation and analysis.
Implement data quality checks and validation procedures to ensure accuracy and completeness.
Optimize data pipelines and processes for maximum performance and efficiency.
Collaborate with cross-functional teams to understand data requirements and deliver tailored solutions.
Create comprehensive documentation for pipelines, workflows, and processes.
Stay updated with the latest data engineering and cloud technology trends.
Tackle complex challenges with a proactive problem-solving mindset.
Required Qualifications:
Expertise in Python/PySpark and hands-on experience with Google Cloud Platform (GCP) for data engineering tasks.
Strong understanding of the Hadoop ecosystem, including HDFS and Hive.
Proficiency in SQL for effective data manipulation and analysis.
Experience working with cloud platforms, preferably Google Cloud Platform (GCP).
Strong problem-solving skills and analytical thinking.
Effective communication and collaboration abilities.
Adaptability to evolving data engineering technologies and practices.
Job Type: Contract
Salary: From $45.00 per hour
Experience level:
9 years
Application Question(s):
What is your work authorization?
Experience:
Data engineer: 9 years (Required)
Google Cloud Platform: 5 years (Required)
Hadoop: 5 years (Required)
Python: 4 years (Preferred)
SQL: 4 years (Preferred)
Work Location: On the road",-1.0,Conglomerate IT,"Hartford, CT",-1,-1,-1,-1,-1,-1
5,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
6,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
7,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
8,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
9,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
10,Sr. Data Engineer,Employer Provided Salary:$105K - $110K,"Role : Data Engineer
Location : Remote
Note : We are looking Someone who can be technical and also be able to
structure and manage transformation projects as well.
JD :
* Data Engineer Skill: Spark, Python and Databricks
* 12+ years of experience in Data pipeline engineering for both batch
and streaming applications.
* Experience with data ingestion process, creating data pipelines
(should be able to move data between stages in data lake, apply
transformation rules) and performance tuning. Must be hands-on coding using
Python with Spark.
* Exposure working with complex data types like XML or JSON, CI/CD
pipeline and scheduling tools like Airflow.
* Proven experience in DWH (type-2 dimensions, aggregations, star
schema, etc.) design and implementation
* Proficient in writing complex SQL queries
* Effective communication and collaboration skills to work with
cross-functional teams and stakeholders.
* Oversee and monitor the work of offshore according to projects
timelines and, if necessary, resolve any issues as they arise
Job Type: Full-time
Salary: $105,000.00 - $110,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
technical and able to structure and manage projects: 5 years (Required)
Data pipeline engineering for both batch and streaming apps.: 10 years (Required)
data ingestion process, creating data pipelines: 5 years (Required)
Python and Spark: 4 years (Required)
Exposure of XML or JSON, CI/CD: 4 years (Required)
scheduling tools like Airflow: 5 years (Required)
writing complex SQL queries: 5 years (Required)
Oversee and monitor the work of offshore: 3 years (Required)
working on fulltime role: 2 years (Required)
Work Location: Remote",4.3,"keasis Inc
4.3",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
11,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
12,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
13,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
14,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
15,Azure Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Required Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 9 years (Required)
Python: 8 years (Required)
Data lake: 7 years (Required)
Data Bricks: 7 years (Required)
Work Location: Remote",-1.0,TekValue IT Solutions,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
16,Data Engineer,-1,"Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore.

Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office.
The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus.
Position Responsibilities
Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities
Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization
Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process
Qualifications & Desired Skills
Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance
Minimum 2 years' of development experience using either Python, C#/Java or C++
Minimum 2 years' years of database/SQL experience
Familiarity with object-oriented programming concepts
Must be comfortable with development across the application stack
Ability to complete complex projects independently
Detail-oriented with strong verbal and written communication skills
Ability to work effectively in a high-energy, time sensitive team environment
Interest or prior experience in the financial trading industry (particularly in fixed income) a plus
This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education.
Base Salary for this role is expected to be between:
$110,000—$150,000 USD",4.4,"Garda Capital Partners
4.4","New York, NY",51 to 200 Employees,-1,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
17,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
18,Data center infrastructure Engineer,Employer Provided Salary:$60.00 - $68.00 Per Hour,"Requirements and Responsibilities:
Must have Bachelor’s degree in IS or equivalent.
Must have experience with VMWARE, DATA CENTER OPERATIONS, IT INFRASTRUCTURE, and VIRTUALIZATION.
Experience working with Linux (CentOS/RHEL/Rocky) .
Experience working with VMWare 6.x / 7.x and storage area network (SAN).
Manage and support Microsoft domain / Active Directory Services, RBAC, Group Policy, Windows Servers, desktops and services automate tasks through scripting technologies Support execution of Information
Support and manage the VMWare virtualization and storage (SAN - Dell Extreme IO & Pure Storage) infrastructure to meet and exceed development operations growth expectations.
Job Type: Contract
Pay: $60.00 - $68.00 per hour
Benefits:
Vision insurance
Experience:
Data center operations: 4 years (Required)
IT infrastructure: 3 years (Required)
Work Location: In person",3.0,"BJ Services
3.0","Exton, PA",1001 to 5000 Employees,1872,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",$1 to $5 billion (USD)
19,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department.
The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
20,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1.0,Percept Health,Remote,-1,-1,-1,-1,-1,-1
21,Data Engineer (MongoDB),$80K - $114K (Glassdoor est.),"CrossCountry Mortgage (CCM) is one of the nation’s top 3 retail mortgage lenders with more than 7,000 employees operating nearly 600 branches across all 50 states. We have been recognized nine times on the Inc. 5000 list of America’s fastest-growing private businesses and have received countless awards for our standout culture. Through our dedication to getting it done, we make every mortgage feel like a win.

A culture where you can grow! CCM has created an exceptional culture driving employee engagement, exceeding employee expectations, and directly impacting company success. At our core, our entrepreneurial spirit empowers every employee to be who they are to help us move forward together. You’ll get unwavering support from all departments and total transparency from the top down.

CCM offers eligible employees a competitive compensation plan and a robust benefits package, including medical, dental, vision, as well as a 401K with an employer match. We also offer company-provided short-term disability, an employee assistance program, and a wellness program.
Position Overview:
We are looking for an experienced MongoDB developer to work with our development teams to integrate innovative features on our new products and applications, as well as maintain existing applications. The MongoDB developer will engineer scripts that automate daily tasks and ensure the security of data sets.
Job Responsibilities:
Upgrade databases through patches.
Understand and convert company needs into technical specifications and creating scalable solutions using MongoDB.
Optimize database schema designs, ensuring data integrity, and promoting best practices to enhance the overall performance and scalability of MongoDB solutions.
Assist developers in identifying and resolving performance issues by utilizing Mongo Profiler to profile and fine-tune database queries and operations.
Leverage performance optimization techniques by implementing proper indexes, including B-Tree, Geospatial, and Text indexes, to enhance query performance and overall system efficiency.
Implement the most effective backup and recovery methods to safeguard critical data and ensure business continuity in case of data loss or system failures.
Collaborate with cross-functional teams to understand business requirements.
Stay updated with the latest MongoDB advancements, features, and best practices.
Qualifications and Skills:
Bachelor’s/Master’s degree in software engineering or Information Technology.
3+ years of experience in executing data solutions using MongoDB.
Recommend and implement best practices for Rest API integration framework/model.
Analytical and problem-solving skills.
Configuring schema and MongoDB data modelling.
Experience with database security management.
Administration of MongoDB in Atlas.
Thorough understanding of MongoDB architecture.
Excellent communication skills.
This job description is intended to convey information essential to understanding the scope of the job and the general nature and level of work performed by job holders within this job. However, this job description is not intended to be an exhaustive list of qualifications, skills, efforts, duties, responsibilities or working conditions associated with the position.
CrossCountry Mortgage, LLC offers MORE than a job, we offer a career. Apply now to begin your path to success! https://crosscountrymortgage.com/about-us/careers/apply/
California residents: Please see CrossCountry’s privacy statement for information about how CrossCountry collects and uses personal information about California consumers.
CrossCountry Mortgage, LLC. (NMLS3029) is an Equal Opportunity Employer committed to workforce diversity. Qualified applicants will receive consideration without regard to race, religion, creed, color, orientation, gender, age, national origin, veteran status, disability status, marital status, sexual orientation, gender identity, or gender expression, or any other protected status in accordance with all applicable federal, state and local laws. The collective sum of the individual differences, life experiences, knowledge, inventiveness, innovation, self-expression, unique capabilities and talent that our employees invest in their work represents a significant part of not only our culture, but our reputation. The Company is committed to fostering, cultivating and preserving a culture that welcomes diversity and inclusion. Employment is contingent upon successful completion of a background investigation. CrossCountry Mortgage, LLC. is an FHA Approved Lending Institution and is not acting on behalf of or at the direction of HUD/FHA or the Federal government. To verify licensing, please visit www.NMLSConsumerAccess.org.",3.3,"CrossCountry Mortgage, LLC.
3.3","Charlotte, NC",1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
22,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
23,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
24,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
25,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1.0,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
26,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
27,Data Engineer,$76K - $101K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
The Data Governance/Data Quality Subject Matter Expert (SME) is a professional responsible for designing, implementing, and managing the client’s data governance strategy and ensuring high data quality standards across data-centric operations and decision-making.
The SME is in charge of developing and maintaining robust data pipelines, ensuring data observability, managing data within the Microsoft Azure environment, and implementing data governance principles in alignment with industry best practices and regulatory standards.
Primary Responsibilities and Accountabilities:
Data Governance & Quality: This includes developing and implementing data governance policies and procedures, monitoring data quality, managing data within the Microsoft Azure environment, and ensuring compliance with data privacy regulations.
Data Pipeline & Workflow Development: Designing and implementing efficient and secure data pipelines in Azure Data Factory and workflows, ensuring optimal performance and incorporating data quality principals.
Data Integration & Observability: Designing data integration strategies, integrating data from multiple sources, ensuring data quality and observability through tools like Azure Synapse.
Data Security: Defining and implementing data security policies and procedures, including access control and data encryption within the Azure environment.
Performance Tuning: Optimizing data access, indexing, and query performance within Azure Data Factory to ensure scalable and reliable data infrastructure.
Monitoring & Troubleshooting: Monitoring data systems, troubleshooting issues, identifying performance bottlenecks, and collaborating with team members to resolve problems.
Collaboration & Communication: Working closely with stakeholders to understand their data needs and develop solutions adhering to industry best practices and the needs of the business.
Automation & Tooling: Automating repetitive tasks and maintaining tools necessary for data processing and storage within the Azure environment.
Qualifications
A Bachelor's degree in a related field such as computer science, mathematics, engineering, or information systems.
3+ years of hands-on experience programming in SQL.
2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes within the Microsoft Azure environment.
Experience in Data Governance OR Data Quality Management is a must.
Skills
Competencies:
Expertise in Microsoft Azure data tools, with a focus on Azure Data Factory and Azure Synapse for data processing, storage, and analysis.
Strong SQL skills and knowledge of data warehousing concepts and ETL (Extract, Transform, Load) processes within the Azure environment.
Proficiency in programming languages such as Python, and experience with big data technologies such as Hadoop, Spark, and Kafka.
Ability to design, build, and maintain data pipelines, with a particular focus on Azure Data Factory.
Strong analytical and problem-solving skills, with the ability to identify and resolve data-related issues effectively.
Experience with data security and privacy regulations, and understanding of HIPAA compliance.
Strong experience with data governance, data quality, and data observability principles.
Excellent written and verbal communication skills, ability to explain complex technical concepts to non-technical stakeholders.
Excellent collaboration and teamwork skills, with the ability to work in a team involving client data engineers, data scientists, and data analysts.
Strong attention to detail and commitment to ensuring high data quality.
Continuous learning mindset and ability to stay up-to-date with new developments and technologies in data governance and data quality
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
28,GCP Data Engineer,Employer Provided Salary:$50.00 - $56.00 Per Hour,"JOB Description: -
ABOUT CONGLOMERATE
Conglomerate IT is a certified and a pioneer in providing premium end-to-end Global Workforce Solutions and IT Services to diverse clients across various domains. Visit us at ConglomerateIThttp://www.conglomerateit.comConglomerateIT
Conglomerate IT mission is to establish global cross culture human connections that further the careers of our employees and strengthen the businesses of our clients. We are driven to use the power of global network to connect business with the right people without bias. We provide Global Workforce Solutions with affability.
We offer a gamut of services across diverse domains, categories, skill sets with varying lengths of assignments /core capabilities included:
· Application Development Services
· Business Intelligence Services
· Enterprise Resource Planning
· Infrastructure Support Services
· Testing Services
· Program / Project Management
· Temporary & Permanent Staffing
Conglomerate IT is headquartered in Addison, Texas with branch offices across Canada, India, Sri Lanka, Argentina to further serve our clients.
JOB ROLE: GCP Data Engineer
Location: Job Type:Hartford, CT (Onsite Role)
Contract : 12+ Months
Work Authorization: w2 ,1099
Must have Skills: - Skills: Python/PySpark, GCP Cloud , HDFS, Hive, Big data
About Role: -
As a GCP Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and data architecture on the Google Cloud Platform. You will work closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and storage. Your expertise in GCP services and data engineering best practices will play a crucial role in maintaining a scalable, reliable, and high-performance data infrastructure.
Roles and Responsibilities:
· Design and implement data pipelines: Develop ETL (Extract, Transform, Load) processes to move data from various sources into GCP storage solutions such as BigQuery, Cloud Storage, and Datastore.
· Data modeling: Design and optimize data models to support data analytics, reporting, and machine learning initiatives. Ensure data accuracy, integrity, and appropriate access controls.
· Performance optimization: Fine-tune data pipelines and query performance to maximize efficiency and reduce latency, utilizing techniques like partitioning, indexing, and caching.
· GCP services utilization: Leverage GCP services like Dataflow, Dataproc, Pub/Sub, and Composer to create, schedule, and manage data processing and orchestration tasks.
· Data integration: Collaborate with cross-functional teams to integrate data from various sources, both internal and external, to enable comprehensive analysis and reporting.
· Data quality and governance: Implement data quality checks, monitoring, and validation processes to ensure data accuracy and consistency across the platform.
· Security and compliance: Implement security measures and access controls to protect sensitive data. Ensure compliance with industry standards and regulations.
· Documentation: Create and maintain technical documentation for data pipelines, workflows, and processes.
· Troubleshooting and support: Diagnose and resolve data-related issues, performance bottlenecks, and operational challenges in a timely manner.
Ideal Profile
· You have experience with Python Hive, Big data
· You have experience withYou GCP Cloud
· have strong experience with HDFS and other relevant tools/technologies.
· You have advanced python skills.
· You have PySparkknowledge.
· You have excellent communication and collaboration skills.
· You have attention to details.
· You are self-directed.
What's on Offer?
· Join a team of world class IT professionals
· Comprehensive salary package
· Great opportunity for growth and development
Job Type: Contract
Salary: $50.00 - $56.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Experience:
google cloud platform: 4 years (Required)
Work Location: In person",-1.0,CONGLOMERATE IT LLC,"Novi, MI",-1,-1,-1,-1,-1,-1
29,Snowflake Data Engineer,Employer Provided Salary:$70.00 - $80.00 Per Hour,"Job: Snowflake Data Engineer – Contract (6+ Months) – Stillwater, MN
REMOTE - HAVE TO WORK ONISTE WHEN EVER CLIENT CALL. Preferably local to MN NEEDED
EAD – GC – H4- EAD G C - CITIZEN
W2 Contract
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Expected hours: 40 per week
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 7 years (Required)
Snowflake API: 4 years (Required)
Snowflake administration: 6 years (Required)
Snowflake RBAC, RLS, CLS: 5 years (Required)
Willingness to travel:
50% (Required)
Work Location: Remote",-1.0,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
30,Data Engineer I,Employer Provided Salary:$90K - $120K,"Interested in joining a growing company where you will work with talented colleagues, enhance a supportive and energetic culture, and be part of the climate solution? At Energy Solutions, we focus on the big impacts. And we believe that market-based programs can be a powerful force to deliver large-scale energy, carbon, and water-use savings. For 28 years, we've harnessed that power to offer proven, performance-based solutions for our utility, government, and institutional customers.
Energy Solutions (ES) is currently seeking a full-time Data Engineer I to join our Business Operations Team. The Data Engineer role is part of the Business Operations Team and will work on critical projects such as the Corporate Operational Data Warehouse (CODW) and the new ERP system. The CODW project is one of ES's strategic goals, to build and maintain a system that enables ES to centralize, integrate, and analyze internal data from various sources. The goal is to provide a single source of truth for ES internal data and enable data-driven decision-making across the organization. This role will potentially lead the design and implementation of the infrastructure for internal reporting needs, driving operational efficiencies and helping our company's transition to operating internationally with 1000+ employees.
Energy Solutions has a remote-friendly work environment for staff located throughout the United States. We also have offices in Oakland and Orange, California, Boston, Massachusetts, New York, and Chicago, IL, for those that wish to work from one of our offices.

Daily responsibilities include but are not limited to:
Collaborate with stakeholders to understand data requirements and develop data-driven solutions that support business goals
Analyze data sources and pipelines needed for the CODW project
Design and implement scalable data pipelines, ETL processes, and data integration solutions for the CODW project using Python, SQL, and AWS technologies
Develop and maintain a data warehouse and optimize data storage, processing, and retrieval
Build schema, populate the database, and automate reporting
Document and maintain technical specifications, ETL processes, data mappings, and dictionaries
Support the Data Science Center of Excellence (DSCOE) with other data engineering needs

Minimum Qualifications:
BS/BA in Computer Science, Physics/Engineering, Business or Mathematics
Experience in building ETL data pipelines, real-time pipelines are a plus
Strong programming skills in Python and SQL
Minimum 2 years of data warehouse development and strong fundamentals in dimensional data modeling
Minimum 2 years of experience creating SQL queries and ETL design, implementation, and maintenance
Minimum 2 years of experience developing data pipelines using Python
Minimum 2 years of experience with AWS, AWS Redshift, AWS Data Engineering and ML Ops tools and services (ie: S3, Redshift, RDS)
Proven ability to aggregate, normalize, munge, analyze, and summarize value from disparate datasets
Strong oral, written, and interpersonal communication skills
High degree of accuracy and attention to detail
Ability to establish priorities, work independently with minimum supervision
Preferred Qualifications:
Ability to manage and communicate data warehouse plans to a non-technical audience
Expertise in cloud services architecting and designing secure AWS environments
Experience using BI tools like Tableau and Looker
Experienced in analyzing and troubleshooting data quality issues
Self-driven, highly motivated, and able to learn quickly
Familiarity with large data sets, cloud-based development and deployment, open-source practices and frameworks, and intelligent applications is desirable
Proven success in migrating data from legacy systems

Salary DOE: $90k - $120k / Annually
Compensation is commensurate with experience and includes a generous retirement package. Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP).
To see this and our other open positions, please visit https://energy-solution.com/company/careers/.
We encourage candidates from diverse backgrounds to apply. For more information about Energy Solutions and our Diversity, Equity and Inclusion efforts, please visit us on our website at www.energy-solution.com.
Background Check Information
Information will be requested to perform the compulsory background check. A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment. Energy Solutions is an equal opportunity employer.
Reasonable Accommodations
Energy Solutions is committed to providing access and reasonable accommodation for individuals with disabilities. If you require an accommodation in completing this application, interviewing, and/or completing any pre-employment testing, or otherwise participating in the employee selection process, please email accommodation@energy-solution.com.
Privacy Notice for Job Applicants",3.9,"Energy Solutions
3.9",Remote,10000+ Employees,2008,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$25 to $100 million (USD)
31,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
32,Sr. Azure Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"LeadStack Inc. is an award-winning, one of the nation's fastest-growing, certified minority-owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Position Details:
Title: Azure Data Engineer
Location: Remote
Duration: 6+ Months Contract
Top 3 skills: SQL, Snowflake, and Data Warehousing/Design
Its a W2 Role. No C2C.
Client is seeking a talented and motivated Data Engineer to join our dynamic team. As a Data Engineer, you will play a critical role in developing and maintaining our data infrastructure, ensuring the smooth flow of data for our analytics and insights projects. The ideal candidate will have a strong background in SQL, data warehousing, ETL processes, and a preference for Snowflake experience. Additional experience with dbt or semantic layer tools will be a bonus, and proficiency in Python will also be advantageous.
Bachelor's degree in Computer Science, Engineering, or a related field.
Solid experience with SQL and data warehousing concepts.
Strong understanding of ETL processes and tools.
Preferred experience with Snowflake data warehouse platform.
Bonus for experience with dbt or other semantic layer tools.
Proficiency in Python is a plus.
Excellent problem-solving and analytical skills.
Strong attention to detail and ability to work in a fast-paced environment.
Effective communication skills and ability to collaborate with cross-functional teams.
Key Responsibilities:
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.
Optimize data ingestion and transformation processes to ensure efficient and scalable data flows.
Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.
Develop and maintain data models, schemas, and data dictionaries for efficient data storage and retrieval.
Perform data validation, quality checks, and troubleshooting to identify and resolve data-related issues.
Monitor and optimize database performance, including query tuning and index optimization.
Collaborate with data scientists and analysts to implement data models and support their data requirements.
Stay up-to-date with industry trends and best practices in data engineering, and proactively recommend and implement improvements to our data infrastructure
To know more about current opportunities at LeadStack, please visit us at https://leadstackinc.com/careers/
Should you have any questions, feel free to call me on 510-480-1006 or send an email on tarique.aziz@leadstackinc.com
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Data Warehousing/Design: 8 years (Required)
Snowflake: 5 years (Required)
Azure: 5 years (Required)
Work Location: Remote",4.3,"Leadstack Inc
4.3",Remote,51 to 200 Employees,2016,Company - Private,HR Consulting,Human Resources & Staffing,Unknown / Non-Applicable
33,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
34,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1.0,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
35,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
36,Energy Data Engineer - Mid Level,$81K - $121K (Glassdoor est.),"About the Organization
Now is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.

The Data Engineer will support Redhorse in its execution of specific task areas related to the Federal Government Carbon Pollution Free Electricity (CFE) Technical and Strategy Support contract. The program will establish technical frameworks and analytical tools and methods to inform CFE procurement decisions for the entirety of the federal government. Work will be conducted primarily at Redhorse office locations (hybrid remote work) and on government client site as required. The ideal candidate will live in the DC/Virginia/Maryland area and willing to undergo a background check to obtain a Public Trust.
Primary Duties and Responsibilities for this position include
Review and analyze the US Federal Government Executive Agencies electric load data
Develop and test modeling techniques to understand an agency’s hourly load profile to inform the agency’s hourly CFE procurement requirement as an off taker from a new CFE technology source.
Support the configuration and ingestion of designated structured, unstructured, and semi-structured data repositories into capabilities that satisfy client requirements and support a data analytics pipeline to drive rapid delivery of functionality to the client.
Maintain all operational aspects of data transfers while accounting for the security posture of the underlying infrastructure and the systems and applications that are supported and monitoring the health of the environment through a variety of health tracking capabilities.
Automate configuration management, leverage tools, and stay current on data extract, transfer, and load (ETL) technologies and services.
Work under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.
Apply comprehension of data engineering-specific technologies and services, leverage expertise in databases and a variety of approaches to structuring and retrieving of data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically using vendor consoles.
Engage with multiple functional groups to comprehend client challenges, prototype new ideas and new technologies, help to create solutions to drive the next wave of innovation, and design, implement, schedule, test, and deploy full features and components of solutions.
Identify and implement scalable and efficient coding solutions.
Minimum Requirements for skills, Experience, and Credentials
Master’s Degree in Engineering, Computer Science, Mathematics or Statistics
Four (4) years of experience in enterprise data analysis with a preferred focus on utility data, to include advanced metering infrastructure (AMI) electricity data
Local to Washington D.C. metro area preferred, remote work is an option
Willingness to travel in support of on-site customer meetings
Willingness to obtain a Public Trust
Desired Basic Requirements for Skills, Experience, and Credentials include
Knowledge of U.S. Federal Government Energy Legislation and Executive Orders (especially EO 14057)
Experience in or knowledge of utility interval data and electricity pricing
Executive Order Requirement:
While currently on HOLD, Redhorse recognizes that we may need to comply with Executive Order 14042 which requires Federal Contractors to ensure all U.S. new hires be fully vaccinated for COVID-19. As required by the Executive Order, Redhorse will work in coordination with applicable contract agencies to consider requests for Reasonable Accommodations.
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.
EOE/M/F/Vet/Disabled",3.5,"Redhorse
3.5","Arlington, VA",51 to 200 Employees,2008,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
37,2024 Data Engineer I,-1,"2024 Data Engineer I
316114

Textron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.

Responsibilities:
Analyze, manipulate, or process large sets of data using statistical software.
Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces
Identify business problems or management objectives that can be addressed through data analysis.
Document data standards, semantic layer mapping, data flows and conceptual flows.
Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.
Perform statistical analysis to provide recommendations on best courses of action to business leaders.
Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.
Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.
Collaborate with business users to ensure data is available for self-service analytics.
Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.


Qualifications:
Education: Bachelor’s degree in computer science, engineering, mathematics, or similar field required
Years of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience required
Software Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferred
Experience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)
EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

Recruiting Company Textron Specialized Vehicles
Primary Location US-Georgia-Augusta
Job Field Information Technology
Schedule Full-time
Job Level Individual Contributor
Shift First Shift
Job Posting 08/24/2023, 8:20:05 AM",3.8,"Textron Specialized Vehicles
3.8","Augusta, GA",10000+ Employees,1923,Company - Public,Aerospace & Defense,Aerospace & Defense,$10+ billion (USD)
38,AWS Data Engineer,Employer Provided Salary:$60.00 - $75.00 Per Hour,"Title: AWS Data Engineer
Location: Columbus, OH (Day 1 onsite / Hybrid Model)
Duration: Contract
Job Description:
Data Engineer: Open positions – 2
Skills:
Must Have:
BS/BA degree or equivalent experience
General: Strong organizational, problem-solving, and critical thinking skills; Strong documentation skills
Coding: Proficiency in Java
• Cluster Computing frameworks: Proficiency in Spark and Spark SQL
• AWS Data Services: Proficiency in Lake formation, Glue ETL (or) EMR, S3, Glue Catalog, Athena, Kinesis (or) MSK, Airflow (or) Lambda + Step Functions + Event Bridge
Data De/Serialization: Expertise in at least 2 of the formats: Parquet, Iceberg, AVRO, JSON-LD
AWS Data Security: Good Understanding of security concepts such as: Lake formation, IAM, Service roles, Encryption, KMS, Secrets Manager
Good to Have:
• Linux Scripting, Jenkins
• DevOps: Git, CI/CD, JIRA, TDD
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Ability to commute/relocate:
Columbus, OH 43085: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Required)
Java: 4 years (Required)
Data warehouse, Glue and SparkSQL: 8 years (Required)
Work Location: In person",4.5,"Synkriom Technology Pvt. Ltd.
4.5","Columbus, OH",201 to 500 Employees,2015,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
39,Senior Data Center Engineer,-1,"Marco is a one-stop shop for all things business tech. Our employees are “movers and shakers” and our company is always striving to do what’s right. Does this sound like a culture you want to be a part of? We’re hiring a new team member to help take Marco’s technology further – working full-time, Monday - Friday, 8am-5pm.
More about us. We do it all – from copy and print solutions to IT and managed services. We are an organization led by salespeople with 650+ engineers ready to fix any and all issues. We have offices in 12 states and service nationally.
Join our growing team. You won’t regret it.

The Senior Data Center Engineer is responsible for providing quality services and solutions to our internal clients while maintaining a high level of client satisfaction. This position works cooperatively with other teams in a coordinated effort to develop, design, implement and support Marco’s Data Centers, along with the IT environment that depends upon them. While not a leadership position, the Senior datacenter engineer displays leadership qualities and may be called upon to lead others in certain situations.

ESSENTIAL FUNCTIONS:
Lead the overall design and implementation of the technology stack within the Marco Data Centers
Maintain physical access to the Marco Data Centers and be a primary contact for essential services
Perform proactive maintenance and reactive support to the equipment within the Marco Data Centers
Create and maintain documentation for all equipment within the Marco Data Center. Review and assist others with their documentation, offering constructive feedback and pointing out deviations from our policies and standards where necessary.
Report on the health of the Marco Data Centers, perform capacity planning and proactively provide recommendations for future growth
Monitor and provide proactive recommendations for backup systems and disaster recovery processes
Review all systems and make technical/process recommendations for improving efficiency
Assist in defining standards for all systems and processes provided by the Data Center Services group
Administer and maintain a reliable, stable, secure and cost-effective hosting environment to deliver 24x7x365 operational service
Provide escalated remote technical support to Marco Data Center clients. Handle requests with due urgency, empathy, and a spirit of cooperation.
Document your work, including troubleshooting steps, time, and resolution in the service ticketing system.
Ensure your work and the work of others adheres to Marco internal policies and applicable regulatory requirements.
Create and maintain standard templates, documentation, images, policy, and automation that will aid in a consistent and rapid deployment of Marco Data Centers
Research, test and recommend new tools and products to enhance the offering and efficiency of Marco’s Data Centers
Document the procedures, infrastructure and environments and keep updated with changes to the environment
Implement and maintain Data Center system builds that are consistent, optimized for client experience, overall performance and supportability
Set deadlines for projects, then organize your time to meet these deadlines; be willing to put in extra time and effort to meet a time-bound goal when necessary
Mentor other staff while you assist with implementation or utilization within your area of technical specialty
Work multiple projects simultaneously and provide consistent, high-quality results
Assist with implementation and or migration of clients’ existing data and applications
Perform system and application testing to provide functional environment
Provide onsite and or remote technical support to Marco clients during onboarding process
Implement changes and work orders for existing Marco clients
Review customer hosted systems and make technical/process recommendations for improving efficiency
Stay up to date with relevant state-of-the-art technology, equipment, and/or systems
QUALIFICATIONS:
Bachelor’s Degree and 4+ years of experience or equivalent experience
Experience and current certifications or specializations aligning with the technologies used in Marco’s Data Center to include Cisco NX-OS, SAN-OS, IOS, UCS-B, ASAv, EMC VNX, Meraki MX, VMWare vSphere, Citrix XenApp, Zscaler and XenDesktop preferred.
Current high level industry recognized certifications including one or more of the following: CCNP, CCE, VCDX, EMCTA
Solid understanding of the administration of network operating systems and services found in a Data Center environment
Benefits:
We’re not just competitive when it comes to business tech – we’re also pretty proud of what we offer our employees. Our benefits include medical, dental, and vision insurance. We also have paid holidays and vacation, 401k with generous company match, flexible spending accounts, employee purchase program, employer-paid life insurance, voluntary-term life insurance, short and long-term disability, critical illness and accident benefits, and pet insurance. Yes, we care about your furry family too.

all benefits are dependent on employment status
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities
Applicant Labor Law Posters",4.0,"Marco Technologies LLC
4.0",Remote,1001 to 5000 Employees,1973,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
40,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Data Engineer
Candidate must have AWS (Redshift, Glue, S3), Spark, Phython, Scala and Should be expert in SQL
Basic qualifications
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
Design, implementation and management of data processing systems dealing with large data sets.
Proficiency in, at least, one modern programming language such as Python, Java or Scala
Hands-on experience and advanced knowledge of SQL.
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and cloud computing
Experience working with Open-Source Big Data tools (Hadoop, Spark, Hive, Presto and etc.)
Preferred qualifications
Experience in working and delivering end-to-end projects independently
Relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics
Experience working with AWS Big Data Technologies (AWS Glue, EMR, Athena, Ec2, Redshift and S3 etc.)
Experience working with Oracle, PostgreSQL, SQL Server or MySQL.
Proven track record of delivering a big data solution
Experience working with both Batch and Real Time data processing systems
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Expected hours: 40 per week
Compensation package:
1099 contract
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Austin, TX 78703: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 8 years (Required)
Python: 8 years (Required)
AWS: 5 years (Preferred)
Work Location: Hybrid remote in Austin, TX 78703",-1.0,PRISM IT LLC,"Austin, TX",-1,-1,-1,-1,-1,-1
41,Data Engineer,Employer Provided Salary:$75.00 Per Hour,"8+ years of experience with Oracle and SQL Server databases
• 8+ years of experience with Informatica Power Center.
8+ years software development experience in one or more of the following areas: ETL, BI and DW
• 8+ years of experience in DW Multi-Dimensional modelling.
Understanding of the Project Delivery Framework and SDLC
Understanding of Waterfall and Agile Project Management Methodology
Strong analytical, communication and interpersonal skills.
Excellent and proven collaboration skills
Job Type: Full-time
Pay: $75.00 per hour
Expected hours: 40 per week
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Monday to Friday
Experience:
Informatica PowerCenter: 5 years (Required)
Multi-Dimensional modelling: 3 years (Required)
Work Location: Hybrid remote in New York, NY 10001",-1.0,Cybotic System,"New York, NY",-1,-1,-1,-1,-1,-1
42,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
43,Data Engineer,$73K - $110K (Glassdoor est.),"Data Engineer

Posting Date: Aug 23, 2023
Location: Houston, TX, US
Lake Charles, LA, US
Company: Sasol
A job at Sasol is a career made by you, with purpose, development opportunities, benefits to support every stage of life and a working culture that embraces flexibility, diversity, and inclusion. Learn more.

Purpose of Job
The Data Engineer works as part of an agile team, interacts with data scientists/translators and industry experts to understand how data needs to be converted, loaded and presented. Collects, structures, analyses data and ensures quality. Responsible for the definition, conception, construction, and ongoing support of big data platforms and solutions.
Recruitment Description / Key Accountabilities
Integrate IT (Information Technology) and OT (Operational Technology) Systems.
Use programming languages and big data technologies following best practices and standards
Understands latest analytics tools and technologies (open source/ proprietary/ vendor supported) and determines fit with current architecture
Building and maintaining data pipelines that move data between various systems.
Ensuring data quality, integrity and security
Monitoring and optimizing data performance
Collaborate with technical teams to develop and maintain a flexible and sustainable workflow.
Collaborate with Product Owner, Data Scientists, Data Translators, and other Business Stakeholders to implement and deploy scalable solutions in an agile way of working; actively participate in scrum process and meetings with team where required
Assist with communication, improve transparency, radiate information, and address challenges effectively
Formal Education
University Bachelors Degree in Computer Science & Inf Systems or Engineering: Computer
Min Experience
Experience: 5+ relevant years
Long Description
Excellent communication and written skills.
Experience in building complex data pipelines using Cloud Services, REST APIs, SQL Databases. Python and SQL skills are are required. Experience in Azure ADF, Synapse or Databricks are beneficial.
Knowlege of Cloud Computing and Infrastructure, Data Models and Standards, Database Administration, Plant Information Management Systems, ERP Systems, Networking and Protocols, Security and Compliance, Change Managemnt, Problem Solving and Trouble Shooting are a plus.

Sasol (USA) Corporation is an Equal Opportunity Employer and gives consideration for employment to qualified applicants without regard to race, color, religion, creed, age, sex, sexual orientation, gender identity, pregnancy, national origin, disability or protected veteran status, as well as any other characteristic protected by applicable law, regulation or local ordinance. For more information about your rights under the law, see http://www.eeoc.gov/employers/upload/poster_screen_reader_optimized.pdf

Sasol treats work-authorized job applicants equally in recruiting and hiring without regard to their citizenship status or national origin unless required by law, regulation, executive order, government contract, or determination by the Attorney General. Please note Sasol will not sponsor, obtain, and/or petition for temporary visa status (for example, E, F-1, H-1, H-2, L, B, J, or TN) for candidates for this position and/or personnel hired for this position.


Nearest Major Market: Houston",3.9,"Sasol
3.9","Houston, TX",10000+ Employees,1950,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$10+ billion (USD)
44,Data Engineer,-1,"SalesIntel is the top revenue intelligence platform on the market. Our unique approach to data collection, enhancement, verification and growth distinguishes us from the market, solidifying our position as the best B2B data partner.

The Data Engineer will be responsible for building and extending our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys working with big data and building systems from the ground up.

You will collaborate with our software engineers our software engineers, database architects, data analysts and data scientists to ensure our data delivery architecture is consistent throughout the platform. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What You’ll Be Doing:
Design and build parts of our data pipeline architecture for extraction, transformation, and loading of data from a wide variety of data sources using the latest Big Data technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with machine learning, data, and analytics experts to drive innovation, accuracy and greater functionality in our data system.
Qualifications:
Bachelor's degree in Engineering, Computer Science, or relevant field.
5+ years of experience in a Data Engineer role.
3+ years experience with Apache Spark and solid understanding of the fundamentals.
Deep understanding of Big Data concepts and distributed systems.
Strong coding skills with Scala, Python, Java and other languages and the ability to quickly switch between them with ease.
Advanced working SQL knowledge and experience working with a variety of relational databases such as Postgres and MySQL.
Experience working with data stored in many formats including TSV, CSV, JSON, Parquet.
Cloud Experience with DataBricks or similar service: S3, Athena, EC2, EMR, RDS, Redshift.
Comfortable working in a linux shell environment and writing scripts as needed.
Comfortable working in an Agile environment
Machine Learning knowledge is a plus.
Must be capable of working independently and delivering stable, efficient and reliable software.
Excellent written and verbal communication skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

This is a remote position.",-1.0,"SalesIntel Research, Inc.",Remote,51 to 200 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
45,Apple Music - Data Engineer,-1,"Summary
Posted: Aug 18, 2023
Weekly Hours: 40
Role Number:200496819
This team is more than a group of engineers — it’s a group of music lovers. That passion has made Apple Music the world’s most complete music experience, with over 60 million songs, thousands of playlists, and daily selections from music experts for 115 countries. The team’s data-driven engineers focus relentlessly on the customer experience by running worldwide experiments and analyzing usage and latency, while collaborating with Apple’s product groups. As a result, someone can use the Shazam app to identify an intriguing song in a café in the morning, add it to their playlist from Apple Watch, listen to it through their AirPods on their commute, and share it with their family on HomePod at dinnertime. And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love. Apple Music is a huge part of Apple’s business because it’s a big part of people’s lives.
Key Qualifications
Expertise in designing, implementing and supporting highly scalable data systems and services in Scala
Experience with distributed computing technologies such as Hadoop MapReduce, Spark/Spark-SQL, YARN/MR2
Expertise building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Solid understanding of data-modeling and data-architecture optimized for big data patterns, such as efficient storage and query on HDFS Experience with distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues
Experience with low-latency NoSQL datastores, and traditional relational databases is a plus
Description
We are seeking a hardworking, performance-savvy, engineer to build out the big data platform and services, which power many of these customer features — existing and new. As a core member of the Data Engineering team you will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability. This will include crafting systems to model, ingest, process and compute large-scale, mission-critical data across Apple Music. High-throughput and reliability are essential. The computed datasets are produced for internal reporting used by executives and they are also shared across many teams within the ASE organization, such as the Search team, Recommendation team, AB team, Marketing team. You will also enjoy the benefits of working in a fast growing business where you are encouraged to “Think Different” to solve very interesting technical challenges and where your efforts play a key role in the success of Apple’s business. This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users. Come join us to help deliver the next amazing Apple product!
Education & Experience
BS CS or equivalent work experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $131,500 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",4.2,"Apple
4.2","Seattle, WA",10000+ Employees,1976,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
46,Big Data Engineer,-1,"Job Title: Big Data Engineer
Job Location: Remote
Job Type: Full Time

Job Requirements:
5-7 years of experience and Bachelor's Degree.
Experience with Product Development including designing products, developing product roadmaps, translating design requirements, prototyping, etc.
Working on integration of data into readily available formats while maintaining existing structures and govern their use according to business requirements.
Development of database requirements and support the planning and engineering of enterprise databases.
Experience on tools:
ETL
Java and/or Python
JSON
APIs for developing or programming software
SQL
Amazon Web Services (AWS) offerings, development, and networking platforms

Interested candidates can send their updated resumes to jobs@global-itech.com",3.9,"Global Information Technology
3.9",Remote,51 to 200 Employees,1995,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
47,GCP Data Engineer,$84K - $113K (Glassdoor est.),"Infosys is seeking an GCP Data Engineer with experience working in a Big Data ecosystem. The position will primarily be responsible interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle including Requirements Elicitation and Design. You will play an important role in creating the high-level design artifacts. You will also deliver high quality code deliverables for a module, lead validation for all types of testing and support activities related to implementation, transition and warranty. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued.

Required Qualifications:
Candidate must be located within commuting distance of Richardson, Tx or be willing to relocate to the area. This position may require travel in the US and Canada.
Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education.
US citizens and those authorized to work in the Canada are encouraged to apply. We are unable to sponsor at this time.
At least 4 years of experience with Information Technology
Minimum 2 years of hands on-experience working with technologies like – GCP with data engineering – data flow / air flow, pub sub/ kafta, data proc/Hadoop, Big Query.
At least 3 years of ETL development experience with strong SQL background
Consultant should have hands-on experience of building and operationalizing data processing systems
Prior experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka is required.
Candidate must have experience in working with data platforms (Data warehouse, Data Lake, ODS)
Minimum of 2 years of experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT, Control-M)

Preferred Qualifications:
GCP (google cloud platform) experience.
Data analysis / Data mapping skills
Experience in data manipulation JSON and XM.
CI / CD exposure
Ability to work in team in diverse/ multiple stakeholder environment
Ability to communicate complex technology solutions to diverse teams namely, technical, business and management teams

The job may entail extensive travel. The job may also entail sitting as well as working at a computer for extended periods of time. Candidates should be able to effectively communicate by telephone, email, and face to face.

About Us
Infosys is a global leader in next-generation digital services and consulting. We enable clients in more than 50 countries to navigate their digital transformation. With over four decades of experience in managing the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver unprecedented levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.

Infosys is an equal opportunity employer and all qualified applicants will receive consideration without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, spouse of protected veteran, or disability.",3.3,"Infosys
3.3","Richardson, TX",10000+ Employees,1981,Company - Public,Information Technology Support Services,Information Technology,$10+ billion (USD)
48,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
49,Azure Data Engineer,Employer Provided Salary:$70.00 Per Hour,"Job Description
Seeking an experienced Azure Data Engineer to design and implement data solutions using Azure technologies. The ideal candidate will have a strong background in data modeling, ETL, and data warehousing, and ability in SQL, Python, and Azure data services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.
Essential Functions
Design and implement data pipelines using Azure Data Factory to move data from various sources to Azure Data Lake Storage and Azure SQL Database.
Create and keep data models in Azure Synapse Analytics for reporting and analytics with Power BI.
Work with data scientists and analysts to implement data solutions.
Collaborate with the DevOps team to automate the deployment of data pipelines using Azure DevOps.
Participate in data governance efforts to ensure data quality and compliance.
Qualifications
2-5+ years of experience as a data engineer or related role.
Strong background in data modeling, ETL, and data warehousing.
Proficient in SQL, Python, and Azure data services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.
Experience using version control software, preferably git.
Experience with Visual Studio, Azure DevOps, and Power BI is a plus.
Microsoft Certified: Azure Data Engineer Associate or higher is a plus.
Education: bachelor’s degree in computer science, data science, or a related field.
Strong diligence.
Excellent troubleshooting and communication skills
Able to work well in a team setting and mentor others.
Job Types: Full-time, Contract
Pay: Up to $70.00 per hour
Benefits:
Health insurance
Ability to commute/relocate:
Brentwood, TN: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
SQL: 2 years (Required)
Data warehouse: 2 years (Required)
Azure Data Engineering: 2 years (Required)
design and implement data solutions: 2 years (Required)
Azure technologies: 2 years (Required)
data modeling, ETL, and data warehousing: 2 years (Required)
SQL, Python, and Azure data services: 2 years (Required)
Azure Databricks, and Azure Synapse Analytics: 2 years (Required)
Azure Data Factory: 2 years (Required)
version control software: 2 years (Required)
Visual Studio, Azure DevOps, and Power BI: 2 years (Required)
License/Certification:
Microsoft Certified: Azure Data Engineer Associate or higher (Required)
Work Location: In person",-1.0,Vision,"Brentwood, TN",-1,-1,-1,-1,-1,-1
50,Data Engineer,$79K - $116K (Glassdoor est.),"Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
Sr. Consultant /Data Engineer – Data Architecture & Engineering
Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
A Senior Consultant in the Data and Analytics Practice is someone that looks forward to being challenged and continuously grow their technical and team collaboration skills. You'll provide technical guidance, as needed, to other team members for quality assurance, mentoring and growth opportunities. If this is a path that excites you, let's chat!
Responsibilities
Technical delivery in both Agile and Waterfall methodologies
Implement solutions using the Microsoft Power BI tool suite and related technologies
Leverage advanced database systems including data warehouses, data marts, and models needed for business, financial and operational analysis and/or reporting
Develop high performing, reliable and scalable solutions
Ability to clearly communicate technical details to business and management personnel
Perform data analysis that will support and enhance Information Management systems
Author, oversee and gain approval of design documents for projects assigned
Develop reports, dashboards & scorecards, data exploration and visual analytics using existing and emerging tools and technologies.
Work independently or as part of a team to design and develop solutions
Assist business development team with pre-sales activities and RFPs
Provide meaningful feedback and coaching of other team members to successfully overcome technological challenges
Leading and demonstrating emerging technologies and concepts to teams
Requirements
Bachelor's Degree desired in Computer Science, Information Technology, or related field
2+ years of experience with data visualization
Expert knowledge of Data Management, Business Intelligence and Analytics concepts including:
Extract, Transform, Load (ETL)
Data Warehousing, Data Lakes, Data Marts, Data Stores, and Cubes
Data Quality and Profiling
Data Governance, Master Data Management, and Metadata Management
Data Visualization, Dashboard, Reporting, Self Service Business Intelligence (BI), Key Performance Indicators (KPI), and Scorecards
Predictive, Prescription, and Descriptive Analytics
Expert knowledge of Microsoft Business Intelligence stack including Power BI, Excel and SQL Server (SSAS, SSRS, SSIS)
Minimum of 3-5 years of experience (of which 2-3 years in consulting)
Experience with Azure Data Estate (Azure SQL DB, Cosmos DB, Azure SQL DW, Azure Data Lake) is a plus
Azure ML Studio/Services experience a plus
Certifications are a plus
Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and tuning is a plus
Knowledge of relational and dimensional database structures, theories, principles, and practice is a plus
Experience with Data Science tools, technologies and techniques (R, Python, algorithms) is a plus
Passionate about learning new technologies
Ability to learn new concepts and software quickly
Analytical approach to problem-solving; ability to use technology to solve business problems
Familiarity with database-centric applications
Ability to communicate effectively in both a technical and non-technical manner
Ability to work in a fast-paced environment
Ability to travel up to 30%
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways.
At this time, we cannot sponsor applicants for work visas. Candidates should be based in the United States.
About 3Cloud
3Cloud is a ""born in the cloud"" Gold-Certified Microsoft Azure technology consulting firm and Azure Expert Managed Services Provider that provides cloud strategy, design, implementation, acquisition and ongoing managed services to its clients across various industries. To meet the needs of its clients, 3Cloud offers comprehensive services across Cloud Infrastructure, DevOps & Automation, Cloud Application Development and Data, Analytics and AI. Founded by former Microsoft technology leaders, 3Cloud uniquely offers: (1) highly experienced and proven cloud architects and technologists, (2) strong business strategy, financial acumen and operational proficiency and (3) deep relationship and network into Microsoft engineering and field leadership.
3Cloud Total Rewards Highlights Include:
Flexible work location with a virtual first approach to work!
401(K) with match up to 50% of your 6% contributions of eligible pay
Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days
Three medical plan options to allow you the choice to elect what works best for you!
Option for vision and dental coverage
100% employer premium coverage for STD and LTD
Paid leave for birth parents and non-birth parents
Option for FSA, HSA, HRA and Dependent Care
$67.00 monthly tech and home office allowance
Utilization and/or discretionary bonus eligibility based on role
Robust Employee Assistance Program to help with everyday challenges
3Cloud offers competitive compensation. In addition to base pay employees are eligible to receive an annual discretionary/utilization bonus. If you are hired at 3Cloud your final base salary is based on factors such as skills, education, experience and/or geographic location. Please keep in mind that the range mentioned above includes the full base salary range for the role. It is not typical for offers to be made at or near the top of the range.
Base Salary Range
$90,000—$183,000 USD
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway.
At this time, we cannot sponsor applicants for work visas.",4.3,"3Cloud
4.3","Downers Grove, IL",501 to 1000 Employees,2016,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
51,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
52,Data Engineer,$79K - $115K (Glassdoor est.),"Job Title :- Data Engineer
Location:- San Antonio, TX
Required:- Active Top Secret Clearance
We are looking for a talented Data Engineer to support the acquisition of mission-critical and mission-support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is performed mostly on customer site in Columbia, MD.
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.
Minimum Qualifications
Security Clearance - Must have a current Secret level security clearance and therefore all candidates must be a U.S. Citizen with a willingness to go to TS/SCI and take the CI poly after starting the position.
5 years experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.
Experience with programming languages such as Python and Java.
Proficiency with acquisition and understanding of network data and the associated metadata.
Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
Experience with Kibana and Elasticsearch.
Familiarity with various log formats such as JSON, XML, and others.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must have a Security+ or similar certification or the ability to obtain it immediately.
Must be able to work on customer site in Columbia, MD at least 4 days a week. (Subject to change)
Desired Skills (Optional)
Experience with NOSQL databases such as Accumulo desired
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
Job Type: Full-time
Schedule:
8 hour shift
Security clearance:
Top Secret (Preferred)
Work Location: In person",3.9,"Helm360
3.9","San Antonio, TX",201 to 500 Employees,1997,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
53,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
54,Senior Data Engineer,$115K - $151K (Glassdoor est.),"Senior Data Engineer
Location: Various
Raleigh NC; Atlanta GA; Philadelphia PA; Charlottesville VA; others considered;
Hybrid, Remote, Travel required
Elder Research, Inc. (ERI) is a Data Science consulting firm specialized in providing analytic solutions to clients in Commercial and Government industries. The Commercial business unit is seeking to hire a Senior Data Engineer with strong engineering skills who will provide technical support across multiple project teams by leading, designing, and implementing the software and data architectures necessary to deliver analytics to our clients, as well as providing consulting and training support to client teams in the areas of architecture, data engineering, ML engineering and/or related areas. We are trusted advisors to our clients, building lasting relationships and partnering as preferred analytics providers. We use a variety of programming languages and tools to create analytic solutions, often fitting within our clients’ environment and needs.
We are looking to hire a Senior Data Engineer who can support robust and repeatable data manipulation, large-scale infrastructure for data ingestion, and data visualization for custom client applications. A Senior Data Engineer works collaboratively with Business Analysts, Data Scientists, Data Analysts, other Software/Data Engineers, and business stakeholders to create and deploy dynamic data applications that help our customers make meaningful business decisions.
Some of your responsibilities will include leading and implementing software engineering best practices, mentoring junior engineers, designing software architectures, wrangling and fusing large and disparate data sets, assisting in the deployment of models and algorithms, automating data pipelines in their entirety, and communicating model results through user-focused data visualizations. Candidates should have the ability and the willingness to tailor applications to a clients’ business goals using an iterative methodology. Candidates should also be able to consider both long-term stability and scalability while taking a user-focused approach to development and deployment. Excellent decision making skills are a must.
Desired Skills
Candidates should have a mix of the following experiences:
Set or contributed to data engineering and governance standards
Reviewed code/design/architecture and coached on adjustments (as needed)
Mentored junior engineers
Designed and implemented software architectures
Implemented a data pipeline that includes data transformations such as aggregations, joins, and cleaning.
Ingested data into SQL or NoSQL databases.
Interacted with SQL or NoSQL databases via Python, Java, or similar.
Provided access to transformed data for downstream applications or visualizations via REST APIs or similar.
Worked with visualization software such as Power BI, Tableau, or similar.
Skilled in both applying and teaching others to apply Git (or other distributed version control) for collaboration, record-keeping, and deployment scenarios
Skilled in both using and guiding others in using cloud platforms (AWS, Azure, Google Cloud) for building, deploying, evolving, and maintaining data pipelines
Skilled in both applying and guiding others in applying continuous integration and continuous deployment (CI/CD) techniques for surfacing situations where new work breaks existing functionality
Had exposure to modeling and/or data analysis.
Configured a technical service such as a database, version control system, or operating system.
Led and collaborated with others to accomplish a technical task.
Partnered on the development or deployment of a data strategy
Presented technical work to a non-technical audience via writing or presentation.
Desired Education and Experience
Education:
Master’s in a technical field and 4-5 years’ experience OR
Bachelor’s and 6+ years’ experience
Certifications:
AWS, Azure, or Google Cloud
Minimum Requirements
Bachelors/Master’s degree in Computer Science or related field requiring software development
4+ years’ of professional experience with Python, Java, or similar
Excellent written and verbal communication skills
Ability to parse project requirements, including asking questions to ensure full and correct understanding
Willingness and ability to learn new concepts and tools quickly
Willingness and ability to provide technical support across multiple projects
What You Would Do
Work on small teams in a highly collaborative environment.
Participate in both existing and new projects.
Contribute to consulting projects to solve interesting problems for multiple clients in various industries.
Communicate details of the technical architecture to fellow team members and clients in both technical and non-technical terms.
Work with Data Scientists to design and/or implement the technical architecture necessary to support analytics.
Work with Software/Data Engineers to improve the robustness and scalability of software products.
Participate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance.
Work on documentation of the technical architecture for internal and client use.
Design, create, and provision data stores.
Build trusted and lasting relationships with clients.
Provide value to our clients through analytics, software tools, and leadership.
Manage and monitor pipelines to extract, transform, and load data (ETL).
About Elder Research, Inc.
Headquartered in Charlottesville, VA with offices in Arlington, VA, Baltimore, MD, and Raleigh, NC, Elder Research, Inc. (ERI) is a fast-growing solutions and consulting firm specializing in predictive analytics. At ERI, you’ll be part of a fun, friendly community. In keeping with our entrepreneurial spirit, we want candidates that are self-motivated with an innate curiosity and strong team work ethic. We work hard to provide the best value to our clients and allow each person to contribute their ideas and put their skills to use immediately.
ERI believes in continuous learning; each week the entire company attends a “Tech Talk” and an office lunch. Individuals often share what they have learned at informal bag lunch talks or afternoon chats. ERI provides a supportive work environment with established parental, bereavement, and PTO policies. By prioritizing a healthy work-life balance - with reasonable hours, solid pay, low travel, and flexible time off - ERI enables and encourages its employees to serve others.
ERI provides analytic solutions to hundreds of companies across numerous industries. Our team enjoys great variety in the type of work we do and exposure to a wide range of techniques and tools. If you are passionate about integrating data, technology, and analytics in a team-based environment to solve problems, then ERI may be a good fit for you.",4.5,"Elder Research Inc
4.5","Charlottesville, VA",51 to 200 Employees,1995,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
55,Lead Azure Data Engineer,Employer Provided Salary:$135K - $140K,"Lead Data Engineer.
Location : New York, NY ( Hybrid)
Full Time
10+ years of experience in data engineering
5+ years of experience managing a team of data engineers
Strong understanding of SQL Server SSIS
Excellent communication and interpersonal skills
Ability to work independently and as part of a team
Ability to meet deadlines and work under pressure
Strong desire to be hands-on with the development and implementation of data pipelines and data warehouses
Experience working with stakeholders to develop product backlog grooming, sprint planning data engineering and
QA Testing
Migration of SQL Server to Azure is desired
Experience with Azure Data Factory, ADLS, and CI/CD highly desired
The Hands-On Data Engineering Lead is responsible for leading a team of data engineers in the design, development, and
implementation of data pipelines and data warehouses. The ideal candidate will have a strong understanding of data
engineering principles and practices, as well as experience with SQL Server SSIS. The Hands-On Data Engineering Lead will
also be responsible for managing the team's workload, mentoring junior engineers, and communicating with stakeholders.
Additionally, the Hands-On Data Engineering Lead will be expected to be hands-on with the development and
implementation of data pipelines and data warehouses, and to have experience working with stakeholders to develop
product backlog grooming, sprint planning data engineering and QA Testing.
Responsibilities:
Lead the design, development, and implementation of data pipelines and data warehouses
Develop and implement data pipelines and data warehouses using SQL Server SSIS
Troubleshoot data pipelines and data warehouses
Communicate with stakeholders to understand their needs and ensure that projects meet their requirements
Manage the team's workload and ensure that projects are completed on time and within budget
Mentor junior data engineers and help them develop their skills
Stay up-to-date on the latest data engineering technologies and best practices
Provide support to production teams and QA teams
Work with stakeholders to develop product backlog grooming, sprint planning data engineering and QA Testing
Working with other engineers to design and implement data pipelines and data warehouses using Azure Data
Factory, ADLS, and CI/CD
Experience in migrating SQL server to Azure
30% team management and 70% hands-on
Job Type: Full-time
Salary: $135,000.00 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 10 years (Required)
SQL: 10 years (Required)
Data warehouse: 10 years (Required)
SSIS: 10 years (Required)
Work Location: In person",-1.0,Rivago Infotech Inc,"New York, NY",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
56,Data Engineer (Contract),$77K - $111K (Glassdoor est.),"About RevOpsforce:
At RevOpsforce, our mission is to drive sustainable revenue growth for our clients by leveraging data-driven insights, technology, and process optimization. We strive to deliver measurable results that exceed expectations, while building strong partnerships with our clients and continuously improving our own capabilities. Our Expert Network is composed of the highest skilled and certified professionals in revenue operations. We leverage this network to solve our clients' most complex operational challenges.
Type: Contract
Job Description:
We are seeking a talented and experienced Data Engineer to join our growing team. As a Data Engineer at our company, you will be responsible for designing, building, and maintaining the data pipelines that power our data-driven applications. In this role, you will work closely with our data science and engineering teams to develop high-quality, efficient data pipelines that enable us to derive insights from vast amounts of data. You should be comfortable working with large volumes of structured and unstructured data, and have a strong understanding of SQL, Python, and other programming languages.
Responsibilities:
Design, build, and maintain data pipelines to support data-driven applications and analytics
Analyze data to identify trends and patterns
Collaborate with data scientists and engineers to develop data-driven solutions
Write and maintain documentation for data pipelines
Monitor and optimize data pipelines for performance and efficiency
Qualifications:
Bachelor's or Master's degree in a related field (e.g. Computer Science, Data Science, Engineering)
3+ years of experience in a data engineering role
Strong skills in SQL and at least one programming language (e.g. Python, Java, C++, etc.)
Experience with big data technologies (e.g. Hadoop, Spark, etc.) and cloud platforms (e.g. AWS, Azure, GCP)
Strong problem-solving and analytical skills
Excellent communication skills and ability to work in a team environment
Join our dynamic and forward-thinking team to make a significant impact on our clients' revenue growth journey!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Website is coming soon and will be located at www.revopsforce.com",-1.0,RevOpsforce,"Austin, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
57,Data Engineer (Informatica Power Center),Employer Provided Salary:$75.00 Per Hour,"ONLY ON W2
NOTE FROM HM : The Candidate needs to be strong enough in Informatica Power center development and Data Warehouse Multi-Dimensional modelling along with good SQL and PL/SQL skills.
The Data Engineer is part of a Corporate Analytics team responsible for supporting data and analytics solutions for Client's Corporate Functions. This individual will collaborate with analytics team to design and implement Client corporate data strategy, ensuring reliable data infrastructure and creating data solutions for variety of business use cases. This individual will primarily support on enterprise financial data warehouse and multiple data marts working with various stakeholders and reporting applications team, successfully delivering the data requirement solutions.
What you need to have:
8+ years of experience with Oracle and SQL Server databases
8+ years of experience with Informatica Power Center.
8+ years software development experience in one or more of the following areas: ETL, BI and DW
8+ years of experience in DW Multi-Dimensional modelling.
Understanding of the Project Delivery Framework and SDLC
Understanding of Waterfall and Agile Project Management Methodology
Strong analytical, communication and interpersonal skills.
Excellent and proven collaboration skills
We will count on you to:
Designs, develops, automates, and supports complex applications to extract, transform, and load data.
Ensures data quality of the warehouse and data marts.
Plans and conducts ETL unit and development tests; monitors result and takes corrective action.
Work on basic and advanced transformations in informatica power center independently.
Performance Tuning of SQLs and handles huge volumes of data.
Ability to quickly diagnose the problem areas and come up with solutions and/or workarounds
Build, maintain, and enhance all objects packages/functions in PL/SQL to support application process.
Translates transformation and movement requirements into functional requirements and mapping designs.
Designs automation processes to control data access, transformation, and movement.
Ensures source data availability and update accessibility, data integrity, restart ability, and error handling.
Participates in the overall development process via architecture guidance, design and code reviews, and estimation and planning assistance
Performs other related duties as required Knowledge, Skills, And Abilities
Documents and troubleshoot problems and effectively communicate with business and technical team members at all levels
What makes you stand out?
Excellent communication and presentation skills
Domain knowledge in one of more of corporate functions such as HR, Finance, Real Estate is preferred
Excellent Problem-solving skills with innovative and proactive approach
Ability to recommend and implement best practices and processes
Job Type: Contract
Salary: $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Informatica Power Center: 1 year (Required)
Ability to Commute:
New York, NY 10001 (Preferred)
Ability to Relocate:
New York, NY 10001: Relocate before starting work (Required)
Work Location: Hybrid remote in New York, NY 10001",4.5,"TalentMovers
4.5","New York, NY",1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
58,Data Scientist / ML Engineer (REMOTE /Contract),-1,"Role: Data Scientist / Machine Learning Engineer
Location: REMOTE
Duration: 6+ months
Scheduling Interviews
Skills/Experience:
6+ years of experience working in Data Science and Machine Learning field.
Extensive experience in applying data preprocessing, statistical analysis, data analytics tools, predictive modelling, model deployment and evidence-based approaches to find lean, actionable solutions to various real-world enterprise business problems.
Experience in the application of Supervised and Unsupervised Learning algorithms, Naïve Bayes, Regression Analysis, Neural Networks/Deep Learning, Support Vector Machines (SVM), Random Forest, K-Means, Hierarchical, Spectral clustering, DBSCAN, Collaborative Filtering and other advanced machine learning techniques.
Used AWS Cloud Services Sagemaker, ECS, EKS, S3, Redshift, QuickSight
Experience with Google Cloud Platform (GCP) AutoML, Vertex AI, BigQuery, Colab, DataProc, Data Studio,
Exposure to Azure ML, PowerBI, Azure SQL and DataBricks
Design custom BI reporting dashboards or interactive data visualizations and widgets in R and Python using Shiny, Tableau, Ggplot2, Plotly, Matplotlib, and Seaborn.
Produce custom BI reporting dashboards in R and Python using Shiny, and Plotly for rapid dissemination of actionable, data driven insights.
Strong experience in Software Development Life Cycle (SDLC), MLOPS
Experience in working with relational databases (Teradata, Oracle) with advanced SQL skills.",3.5,"Glow Networks
3.5",Remote,51 to 200 Employees,2003,Subsidiary or Business Segment,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
59,Data Engineer (Informatica Power Center),Employer Provided Salary:$70.00 - $75.00 Per Hour,"This is a 6 months contract role
Skills: Informatica Power center, Data Warehouse Multi-Dimensional modelling, SQL and PL/SQL skills.
Should be strong in Informatica Power center Development and Data Warehouse Multi-Dimensional modelling along with good SQL and PL/SQL skills.
Job Summary
The Data Engineer is part of a Corporate Analytics team responsible for supporting data and analytics solutions for Client's Corporate Functions. This individual will collaborate with analytics team to design and implement Client corporate data strategy, ensuring reliable data infrastructure and creating data solutions for variety of business use cases. This individual will primarily support on enterprise financial data warehouse and multiple data marts working with various stakeholders and reporting applications team, successfully delivering the data requirement solutions.
What you need to have:
8+ years of experience with Oracle and SQL Server databases
• 8+ years of experience with Informatica Power Center.
8+ years software development experience in one or more of the following areas: ETL, BI and DW
• 8+ years of experience in DW Multi-Dimensional modelling.
Understanding of the Project Delivery Framework and SDLC
Understanding of Waterfall and Agile Project Management Methodology
Strong analytical, communication and interpersonal skills.
Excellent and proven collaboration skills
We will count on you to:
Designs, develops, automates, and supports complex applications to extract, transform, and load data.
Ensures data quality of the warehouse and data marts.
Plans and conducts ETL unit and development tests; monitors result and takes corrective action.
Work on basic and advanced transformations in informatica power center independently.
Performance Tuning of SQLs and handles huge volumes of data.
Ability to quickly diagnose the problem areas and come up with solutions and/or workarounds
Build, maintain, and enhance all objects packages/functions in PL/SQL to support application process.
Translates transformation and movement requirements into functional requirements and mapping designs.
Designs automation processes to control data access, transformation, and movement.
Ensures source data availability and update accessibility, data integrity, restart ability, and error handling.
Participates in the overall development process via architecture guidance, design and code reviews, and estimation and planning assistance
Performs other related duties as required Knowledge, Skills, And Abilities
Documents and troubleshoot problems and effectively communicate with business and technical team members at all levels
What makes you stand out?
Excellent communication and presentation skills
Domain knowledge in one of more of corporate functions such as HR, Finance, Real Estate is preferred
Excellent Problem-solving skills with innovative and proactive approach
Ability to recommend and implement best practices and processes
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Vision insurance
Experience level:
8 years
Application Question(s):
Please provide your LinkedIn profile link
Elaborate your experience in Datawarehouse Multi-Dimensional modelling.
Please describe your background in Informatica Power center development.
Please provide date of birth in MMDD format (Month and Date only)
What is your current location with the Zip code?
What is your desired hourly rate?
Have you applied or been interviewed for any role with Marsh McLennan in the past? If so, please provide details.
What is your work authorization status? US citizens, Green Card, Visa...
Elaborate on your SQL and PL/SQL's skills
Work Location: In person",2.9,"Sun Spread
2.9","New York, NY",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
60,"Sr. Data Engineer (AWS, Python)",Employer Provided Salary:$60.00 - $65.00 Per Hour,"IT Software Engineer
Location - Remote hybrid in Peoria ideally. 100% Remote is also fine.
CST hours – if in Peoria, 1-3 days a week onsite
Typical task breakdown:- dDaily standup, collaborate with peers, work independently to complete sprint commitments, participate in agile ceremonies. C
Interaction with team:- Daily stand up meetings with team – working mainly with Helios Platform adoption team
Education & Experience Required:
- Bachelor's in computer science required plus 10+ years of Working experience .
- Masters degree with 10+ years working experience acceptable as well.
- Preferred: AWS cloud certifications
Technical Skills
(Required)
- *exp with Object oriented programming - Python (6+ years)
- *AWS (5+ years overall exp) lambda and/or glue, EMR, S3
- *SQL (4+ years)
- * 2+ years exp in test driven development, unit test, integration testing, regression testing
(Desired)
- *Event driven framework exp *Kinesis or Kafka exp *familiar with working in Agile methodology
Soft Skills
(Required)
- strong written and verbal communication, experience working as a member of agile engineering team, ability to commit to and deliver work within allocated sprint window.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Schedule:
Monday to Friday
Experience:
AWS: 6 years (Preferred)
Python: 8 years (Preferred)
Work Location: Remote",5.0,"DSMH LLC
5.0",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
61,Lead Azure Data Engineer,Employer Provided Salary:$135K - $140K,"Lead Data Engineer.
Location : New York, NY ( Hybrid)
Full Time
10+ years of experience in data engineering
5+ years of experience managing a team of data engineers
Strong understanding of SQL Server SSIS
Excellent communication and interpersonal skills
Ability to work independently and as part of a team
Ability to meet deadlines and work under pressure
Strong desire to be hands-on with the development and implementation of data pipelines and data warehouses
Experience working with stakeholders to develop product backlog grooming, sprint planning data engineering and
QA Testing
Migration of SQL Server to Azure is desired
Experience with Azure Data Factory, ADLS, and CI/CD highly desired
The Hands-On Data Engineering Lead is responsible for leading a team of data engineers in the design, development, and
implementation of data pipelines and data warehouses. The ideal candidate will have a strong understanding of data
engineering principles and practices, as well as experience with SQL Server SSIS. The Hands-On Data Engineering Lead will
also be responsible for managing the team's workload, mentoring junior engineers, and communicating with stakeholders.
Additionally, the Hands-On Data Engineering Lead will be expected to be hands-on with the development and
implementation of data pipelines and data warehouses, and to have experience working with stakeholders to develop
product backlog grooming, sprint planning data engineering and QA Testing.
Responsibilities:
Lead the design, development, and implementation of data pipelines and data warehouses
Develop and implement data pipelines and data warehouses using SQL Server SSIS
Troubleshoot data pipelines and data warehouses
Communicate with stakeholders to understand their needs and ensure that projects meet their requirements
Manage the team's workload and ensure that projects are completed on time and within budget
Mentor junior data engineers and help them develop their skills
Stay up-to-date on the latest data engineering technologies and best practices
Provide support to production teams and QA teams
Work with stakeholders to develop product backlog grooming, sprint planning data engineering and QA Testing
Working with other engineers to design and implement data pipelines and data warehouses using Azure Data
Factory, ADLS, and CI/CD
Experience in migrating SQL server to Azure
30% team management and 70% hands-on
Job Type: Full-time
Salary: $135,000.00 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 10 years (Required)
SQL: 10 years (Required)
Data warehouse: 10 years (Required)
SSIS: 10 years (Required)
Work Location: In person",-1.0,Rivago Infotech Inc,"New York, NY",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
62,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
63,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Job Role: Data Engineer with Snowflake
Location: REMOTE, Stillwater, MN.
HAVE TO WORK ONISTE WHEN EVER CLIENT CALL.
Duration: 6+ Months Contract
Visas: USC, GC, EAD-GC, H4-EAD
W2 Requirement
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Expected hours: 40 per week
Experience level:
7 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
Snowflake: 4 years (Required)
Work Location: Remote",-1.0,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
64,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
65,USA - Infrastructure Data Engineer (AWS),$104K - $149K (Glassdoor est.),"Job Title: Infrastructure Data Engineer (AWS)
Location: San Jose, California, United States
Type: Fulltime

The candidate has to be in San Jose, CA and it is 100% onsite job”.

Roles & Responsibilities:
Design, build and maintain data platform infrastructure on AWS environment.
Oversee design, build, and maintain data platform infrastructure on AWS environment.
Develop data pipelines to collect the metrics that is required to monitor data refreshes, reports deliveries and track SLAs.
Work with the DWH development team and business users in establishing SLAs for data refreshes and reports.
Build continuous integration/deployment (CI/CD) pipelines to accelerate development and improve team agility.
Oversee project.
Monitor all aspects of data platform system security, performance, storage, incidents, and usage for databases, data pipelines, applications, and infrastructure on AWS. Escalate to respective teams for fixes.
Ensure data pipelines meet intraday and daily SLAs, as per documented SLA definitions and escalate accordingly.
develop appropriate instrumentation to collect metrics on system performance, cost, data ingress/egress /storage processes.
Have a clear understanding of the reports/analyses/insights to be driven by data and build data driven solutions to optimally support the operational analytics needs.
Documents user stories, epics, and reports
Coordinate infrastructure enhancements and maintenance with the system/network engineering teams
Work with DWH development team and analytics team to do manual releases where required.
Onboard users to data analytics systems with appropriate approvals
Conduct system performance tests and collect metrics. Tune/add capacity.
Complete knowledge management processes
Own strategy and communicate potential major shifts in expected workload based on business, market, or operational changes.
Operate ongoing business relationship management sessions to review operational metrics, understand pain points, identify upcoming projects and engagement efforts.",-1.0,Avestacs,"San Jose, CA",51 to 200 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
66,Senior Data Analytics Engineer,$96K - $130K (Glassdoor est.),"At Wistia our goal is to help every business to thrive with video. That's a big task, and we're super excited to help the world get there faster through simple and delightful video-focused products.
Wistia's Business Intelligence team is charged with leveraging data to empower strategic, insights-driven decision making across the organization. The Analytics Engineer role is a hybrid role with responsibilities that typically fall to Data Analysts, Data Engineers and Data Scientists. We are a small team and are looking for someone who is comfortable wearing different ""data"" hats. A successful candidate will combine strong technical and communication skills to manage the full data lifecycle at Wistia by creating and maintaining ETL pipelines, automating critical workflows, analyzing data to surface insights and translating those insights into clear recommendations for stakeholders.
The other members of the Business Intelligence team are curious, attentive to detail, impact-focused and approach problem solving with an engineering mindset. The team is centralized (not embedded) with each team member working with data from across the business. We strive to adopt the tools and best practices of the ""modern data stack"". For us that looks like: Redshift, dbt, Fivetran, Mode Analytics, Census, Snowplow Analytics. The Redshift warehouse we maintain serves as the single source of truth for the business.
Key Responsibilities
Maintain our 3rd-Party and custom ETL/reverse-ETL jobs and create new ones as required
Develop new predictive models as required to meet business outcomes
Generate easily interpretable analyses of complex datasets using SQL or python
Create persuasive narratives with data visualizations that enable self-serve exploration
Assist in the design and analysis of A/B testing to optimize business outcomes
Define version-controlled, modular business logic and datasets to be reused across the team
Document your findings, insights and recommendations for stakeholders
Meet business stakeholders ""where they live"" by becoming an expert in the data models and usage of their preferred SaaS tools, eg, Salesforce, HubSpot, Google Analytics, Amplitude, Stripe
Collaborate with:
Team members to share learnings, brainstorm new approaches and define best-practices
Stakeholders to scope, shape and prioritize their data requests
Qualifications
This section should talk about the competencies needed for the role, including qualities. Be sure to seek the input of key stakeholders and make sure we distinguish between must-haves and nice-to-haves because we will be using this to build our job positing and interview plan.
Technical
Highly proficient in SQL and python (or comparable general purpose programming language)
Facile with statistics and hypothesis testing
Skilled at constructing meaningful data visualizations
Comfortable with the command line and bash scripting
Knowledgeable about database architecture, data warehousing principles, REST APIs and basic ML algorithms
Non-technical
Experienced at scoping broad, loosely-defined requests into well-structured plans
Adept at translating data into actionable insights for stakeholders
Strong in written communication
Able to work independently with minimal oversight
Familiar with SaaS metrics and business practices
Values fit (for Wistia at large and BI specifically)
collaborative
learning oriented
attention to detail
simplicity as the starting point starts simple and add complexity only as needed
Key Metrics
This is a great place for more detail on the metrics we expect this person to own, and any that we measure success by.
Product, marketing, sales, and customer success teams have the data and insights they need to make decisions
Product, marketing, sales, and customer success teams have confidence in the data they use
Product, marketing, sales, and customer success teams are able to monitor business performance and spot problem areas and new opportunities
Responses to data requests are timely
Success Looks Like
This section should outline what success looks like. Knowing this helps inform the level and type of candidates we need, interview questions, onboarding plan, and coaching our new hires to success. It also makes it easier to spot performance issues early and hold ourselves accountable in hiring (and learn how we can improve).
30 Days
Complete Wistia onboarding program
Ramp up and learn the tools of the role (Mode, dbt, github, Fivetran, Redshift, Google Analytics, Salesforce, Pendo)
Ramp up and learn Wistia's data architecture, starting with Wistia app database
Ramp up and learn data analysis best practices and procedures
Complete ~5 basic data requests
90 Days
Confident in use of Mode, dbt, and GitHub for data analysis process
Confident in data architecture and able to self-source most data tables
Regularly handling basic data requests
Completed 1 complex data investigation
1 Year
Regularly initiating new data investigations and unearthing new insights
Regularly making improvements to our approach to data presentation
Regularly making improvements to our data models and data cleanliness
Working at Wistia
We try to ensure Wistia is an inclusive and diverse place where everyone feels happy, fulfilled, respected, comfortable, and welcome. We're proud to be an equal-opportunity workplace. We care a lot, so our benefits are actually benefits, not just the fun stuff like swag and snacks in the office (though we also have lots of those too!).
We know the biggest investment we can make is in our employees, so we provide:
A competitive compensation package that includes internal equity stock options
401k with 3% company contribution, regardless of whether you contribute
Fully paid healthcare, dental, and vision insurance (family plans included)
Pre-Tax FSA and Dependent Care Account
Fitness reimbursement
Flexible working hours – work at the times when you operate best, or set aside time for child and/or elderly care responsibilities during traditional ""office hours""
16 weeks paid parental leave for all new and expecting parents
Unlimited PTO (pretty common for people to take 4+ weeks off throughout the year)
Remote-first culture (work from anywhere in the U.S.)
Annual professional development stipend (courses, conferences, and more)
New hire bonus to enhance your home office set up
Pet insurance discount
Location/Remote Opportunities
Wistia is a remote-first company. Employees can work from our beautiful office in Cambridge, MA, or anywhere in the continental US",4.8,"Wistia
4.8","Cambridge, MA",51 to 200 Employees,2006,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
67,Senior Data Center Engineer,-1,"Marco is a one-stop shop for all things business tech. Our employees are “movers and shakers” and our company is always striving to do what’s right. Does this sound like a culture you want to be a part of? We’re hiring a new team member to help take Marco’s technology further – working full-time, Monday - Friday, 8am-5pm.
More about us. We do it all – from copy and print solutions to IT and managed services. We are an organization led by salespeople with 650+ engineers ready to fix any and all issues. We have offices in 12 states and service nationally.
Join our growing team. You won’t regret it.

The Senior Data Center Engineer is responsible for providing quality services and solutions to our internal clients while maintaining a high level of client satisfaction. This position works cooperatively with other teams in a coordinated effort to develop, design, implement and support Marco’s Data Centers, along with the IT environment that depends upon them. While not a leadership position, the Senior datacenter engineer displays leadership qualities and may be called upon to lead others in certain situations.

ESSENTIAL FUNCTIONS:
Lead the overall design and implementation of the technology stack within the Marco Data Centers
Maintain physical access to the Marco Data Centers and be a primary contact for essential services
Perform proactive maintenance and reactive support to the equipment within the Marco Data Centers
Create and maintain documentation for all equipment within the Marco Data Center. Review and assist others with their documentation, offering constructive feedback and pointing out deviations from our policies and standards where necessary.
Report on the health of the Marco Data Centers, perform capacity planning and proactively provide recommendations for future growth
Monitor and provide proactive recommendations for backup systems and disaster recovery processes
Review all systems and make technical/process recommendations for improving efficiency
Assist in defining standards for all systems and processes provided by the Data Center Services group
Administer and maintain a reliable, stable, secure and cost-effective hosting environment to deliver 24x7x365 operational service
Provide escalated remote technical support to Marco Data Center clients. Handle requests with due urgency, empathy, and a spirit of cooperation.
Document your work, including troubleshooting steps, time, and resolution in the service ticketing system.
Ensure your work and the work of others adheres to Marco internal policies and applicable regulatory requirements.
Create and maintain standard templates, documentation, images, policy, and automation that will aid in a consistent and rapid deployment of Marco Data Centers
Research, test and recommend new tools and products to enhance the offering and efficiency of Marco’s Data Centers
Document the procedures, infrastructure and environments and keep updated with changes to the environment
Implement and maintain Data Center system builds that are consistent, optimized for client experience, overall performance and supportability
Set deadlines for projects, then organize your time to meet these deadlines; be willing to put in extra time and effort to meet a time-bound goal when necessary
Mentor other staff while you assist with implementation or utilization within your area of technical specialty
Work multiple projects simultaneously and provide consistent, high-quality results
Assist with implementation and or migration of clients’ existing data and applications
Perform system and application testing to provide functional environment
Provide onsite and or remote technical support to Marco clients during onboarding process
Implement changes and work orders for existing Marco clients
Review customer hosted systems and make technical/process recommendations for improving efficiency
Stay up to date with relevant state-of-the-art technology, equipment, and/or systems
QUALIFICATIONS:
Bachelor’s Degree and 4+ years of experience or equivalent experience
Experience and current certifications or specializations aligning with the technologies used in Marco’s Data Center to include Cisco NX-OS, SAN-OS, IOS, UCS-B, ASAv, EMC VNX, Meraki MX, VMWare vSphere, Citrix XenApp, Zscaler and XenDesktop preferred.
Current high level industry recognized certifications including one or more of the following: CCNP, CCE, VCDX, EMCTA
Solid understanding of the administration of network operating systems and services found in a Data Center environment
Benefits:
We’re not just competitive when it comes to business tech – we’re also pretty proud of what we offer our employees. Our benefits include medical, dental, and vision insurance. We also have paid holidays and vacation, 401k with generous company match, flexible spending accounts, employee purchase program, employer-paid life insurance, voluntary-term life insurance, short and long-term disability, critical illness and accident benefits, and pet insurance. Yes, we care about your furry family too.

all benefits are dependent on employment status
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities
Applicant Labor Law Posters",4.0,"Marco Technologies LLC
4.0",Remote,1001 to 5000 Employees,1973,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
68,AWS Data Engineer,Employer Provided Salary:$60.00 - $75.00 Per Hour,"Title: AWS Data Engineer
Location: Columbus, OH (Day 1 onsite / Hybrid Model)
Duration: Contract
Job Description:
Data Engineer: Open positions – 2
Skills:
Must Have:
BS/BA degree or equivalent experience
General: Strong organizational, problem-solving, and critical thinking skills; Strong documentation skills
Coding: Proficiency in Java
• Cluster Computing frameworks: Proficiency in Spark and Spark SQL
• AWS Data Services: Proficiency in Lake formation, Glue ETL (or) EMR, S3, Glue Catalog, Athena, Kinesis (or) MSK, Airflow (or) Lambda + Step Functions + Event Bridge
Data De/Serialization: Expertise in at least 2 of the formats: Parquet, Iceberg, AVRO, JSON-LD
AWS Data Security: Good Understanding of security concepts such as: Lake formation, IAM, Service roles, Encryption, KMS, Secrets Manager
Good to Have:
• Linux Scripting, Jenkins
• DevOps: Git, CI/CD, JIRA, TDD
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Ability to commute/relocate:
Columbus, OH 43085: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Required)
Java: 4 years (Required)
Data warehouse, Glue and SparkSQL: 8 years (Required)
Work Location: In person",4.5,"Synkriom Technology Pvt. Ltd.
4.5","Columbus, OH",201 to 500 Employees,2015,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
69,Senior Data Analytics Engineer,Employer Provided Salary:$75.00 Per Hour,"JOB PROFILE: NCDHHS-Senior Data Analytics Engineer (714485)
JOB LOCATION: 820 s Boylan Ave ,Raleigh, NC, 27603
JOB TYPE: Hybrid
CONTRACT TENNURE: (1 YEAR WITH POSSIBLE EXTENTION)
PAY RATE: $75.00/hr on W2
Candidate must have US Citizenship or Green Card/Permanent Residency in the US to be considered for this position.
Responsibilities:
Provide strategic insights using SAS/SQL, Tableau, and Congas to analyze complex data and assist DHB management in informed decision-making.
Engage directly with clients to understand data needs, translating requirements into effective solutions.
Expertly manage technical aspects, ensuring data quality control and accurate implementation of specifications.
Propose and implement operational healthcare reporting solutions for improved decision-making.
Aggregate, analyze, and report on complex healthcare data from sources like MMIS and claims data.
Develop operational reports and dashboards aligned with healthcare management goals.
Collaborate with staff to assess needs, design solutions, and conduct statistical analysis for insights.
Create advanced analytics based on technical specifications, aiding healthcare program oversight.
Efficiently manage client data requests, ensuring timely delivery and effective communication.
Skills Required:
SQL Proficiency.
Cognos Proficiency.
Tableau Expertise.
Data Quality Control.
Data Aggregation and Analysis.
Technical Specifications.
Local Candidate Consideration.
Job Type: Full-time
Pay: $75.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 5 years (Required)
Tableau: 5 years (Required)
Cognos: 5 years (Required)
data quality control: 5 years (Required)
Data aggregation with MMIS: 5 years (Required)
SAS and SQL coding: 3 years (Required)
technical specifications to develop reports: 5 years (Required)
Work Location: In person",-1.0,"Changing Technologies, Inc.","Raleigh, NC",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
70,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
71,Senior Data Engineer,Employer Provided Salary:$87K - $180K,"Oakland, California; Atlanta, Georgia; Irving, Texas
Regular Employee Full-Time
R-2023-08-81

Who We Are:
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners - in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
Pandora
Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. Pandora provides consumers with a uniquely-personalized music and podcast listening experience with its proprietary Music Genome Project® and Podcast Genome Project® technology. Pandora is available through its mobile app, the web, and integrations with more than 2,000 connected products.
How you’ll make an impact:
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What you’ll do:
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What you’ll need:
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to pay attention to details and be organized.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $87,200 to $180,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.

Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-08-81",3.6,"Adswizz
3.6","Oakland, CA",51 to 200 Employees,2007,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
72,Senior Software Engineer (Data),Employer Provided Salary:$160K - $200K,"About Juniper Square
Our mission is to unlock the full potential of private markets. Privately owned assets like commercial real estate, private equity, and venture capital make up half of our financial ecosystem yet remain inaccessible to most people. We are digitizing these markets, and as a result, bringing efficiency, transparency, and access to one of the most productive corners of our financial ecosystem. If you care about making the world a better place by making markets work better through technology – all while contributing as a member of a values-driven organization – we want to hear from you.
Juniper Square offers employees a variety of ways to work, ranging from a fully remote experience to working full-time in one of our physical offices. We invest heavily in digital-first operations, allowing our teams to collaborate effectively across most US states, 2 Canadian Provinces, and Mexico. We also have physical offices in San Francisco, CA and Austin, TX, for employees who prefer to work in an office some or all of the time.
GP Experience
Juniper Square serves two sides of the private capital markets, the investment managers (GPs) and the investors (LPs). The GP eXperience team (i.e., GPX) is responsible for Juniper Square’s product offering for General Partners (GPs). This is our core product that enables all other innovation at Juniper Square as we unlock and improve the world’s private capital markets. Our platform handles billions of dollars of transactions each month and we are actively expanding into additional private asset classes such as Venture Capital & Private Equity. Come help us innovate in fundraising, reporting, asset-ownership mapping, and more.
The Team
The Data Engineering team is responsible for Data pipelines that serve multiple types of customers including internal Juniper Square users for Business Intelligence and GPs for Analytics on their data. We support the ability for these customers to create and manage their custom dashboards. We also support the ability for other Product Engineering teams to add metrics to track product usage for the features they launch into production.
About your role
Juniper Square is growing rapidly, and our data needs are growing even faster, so we’re growing our Data Engineering Team. As a Senior Data Engineer your role will be pivotal to evolving our existing data and reporting experiences. You’ll build out pipelines to gather data from multiple sources and make it available for analysis. You will shape both internal and external analytics products to help guide business-critical decisions, enhance their workflows, and improve decision-making.
What you’ll do
Design and implement sophisticated data models in SQL.
Work closely with the other Software Engineers to ensure sound, scalable implementation.
Act as a technical expert on our team regarding all things data, especially as the data team grows and evolves.
Introduce new technologies to evolve and enhance our data pipeline capabilities.
Document data models, architectural decisions and data dictionaries to enable collaboration, maintainability and usability of our analytics platforms and code.
Assist with governance, guidance, code reviews, and access controls so that we maintain consistency, quality, and business confidentiality as we scale analytics access across the company and to customers.
Externally: learn our application data schema, and develop a fluency in how to transform it to enhance customer’s decision-making with data.
Internally: guide product and development teams, advising on instrumentation and laying development foundations for product usage reporting.
Fulfill projects with minimal guidance but with an appropriate sense of when and how to collaborate with others.
Build scalable, highly performant infrastructure for delivering clear business insights from a variety of raw data sources.
Qualifications
Bachelor's degree in Computer Science, or equivalent work experience
4+ years of experience building ETL (Extraction Transform Load) or ELT (Extraction Load Transform) pipelines from scratch
Strong command of relational databases (Postgresql preferred), data modeling and database design
Strong command of Python and experience building production web applications using Python
Experience with cloud based services (AWS RDS preferred)
Experience developing on (or administering) BI / data visualization platforms (ex. Looker, Tableau, PowerBI, Mode, Data Studio, Domo, QlikView etc.).
Basic understanding of data warehouses such as Amazon Redshift, Google BigQuery, Snowflake etc.
Demonstrated history of translating data into clear and actionable narratives and communicating opportunities and challenges relevant to stakeholders.
You must be flexible and adaptable—you will be operating in a fast-paced startup environment.
At Juniper Square, we believe building a diverse workforce and an inclusive culture makes us a better company. If you think this job sounds like a fit, we encourage you to apply even if you don’t meet all the qualifications.
Benefits
Compensation for this position includes a base salary, equity, and a variety of benefits. The U.S. base salary range for this role is $160,000 - $200,000. Actual base salaries will be based on candidate-specific factors, including experience, skillset, and location, and local minimum pay requirements as applicable. We are actively hiring for this role in Canada, and offer competitive local pay and benefits. Your recruiter can provide further details.
Competitive salary and meaningful equity
Health, dental, and vision care for you and your family
Unlimited vacation policy and paid holidays
Generous paid family leave, medical leave, and bereavement leave policies
401k retirement savings plan
Healthcare FSA and commuter benefits programs
Freedom to customize your work and technology setup as you see fit
Professional development stipend
Monthly work from home wellness stipend while we're all remote
Mental wellness coverage including live coaching and therapy sessions
Home office productivity allowance to help create an ideal work from home setup
#LI-AD1
#LI-Remote",4.0,"Juniper Square
4.0",Illinois,201 to 500 Employees,2014,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
73,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
74,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
75,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
76,Data Engineer (Data Platform),Employer Provided Salary:$120K - $165K,"About Atlas Health:
Atlas Health automates philanthropic aid to improve access, affordability, outcomes, and health equity for vulnerable populations. Through intelligent matching and patient-friendly digital enrollment to 20,000+ philanthropic aid programs, healthcare organizations can improve patient outcomes, advance health equity, reduce the total cost of care and improve the patient experience. Join us on our mission of saving and improving lives by helping patients access and afford healthcare.

Data Engineers on our data platform team develop systems that manage data flow throughout the Atlas infrastructure and support downstream data applications. These systems are responsible for handling sensitive patient data, and members of the team are technical champions of ensuring HIPAA standards for confidentiality and compliance. Broadly, this role focuses on all elements of data engineering, such as ingestion, transformation, and distribution of data, and works alongside security engineering and business implementation & operations teams as a custodian of patient and company data.
Responsibilities:
Engineer scalable, reliable, and performant systems that manage data
Consume data from a variety of sources and formats, such as flat files, streaming systems, or APIs
Leverage cloud infrastructure to develop scalable data solutions
Collaborate with platform engineers on creating reliable and secure systems
Collaborate closely with team members and product stakeholders
Create trustworthy, secure, governable, and standardized data components for consumption
Develop readable, well-tested applications, APIs, and libraries
Implement application observability in the form of metrics, logging, and monitoring
Requirements:
Professional experience with cloud-based systems
Experience with data architectures and tools in support of streaming and batch driven data processing
Solid understanding of distributed task orchestration
Demonstrated ability in developing and testing systems that manage data reliability, efficiency, and quality
Understanding of multiple software development paradigms
First-nature comfort in working with containerization tools
Computer Science or related technical degree in a related field or equivalent technical experience
Bonus points:
Depth of knowledge in Google Cloud Platform and experience working in an ePHI environment
Familiarity with domain driven design concepts
Preferred Qualifications:
Fluency in Python
Experience using Apache Airflow or similar orchestration systems
Experience with IaC tools
The salary range for this position is $120,000 - $165,000.
Why Join Our Team:
Because you're motivated by a combination of success, working alongside incredible people, and have a passion for helping clients and patients. Atlas helps people access essential medical treatment, and avoid financial ruin from medical debt. You care about being a part of the journey and wish to play a key role in our organization's success.
Benefits:
We offer a comprehensive benefit plan for our U.S. based employees which includes:
Health, dental and vision insurance
401K
Flexible time off
Paid holidays
Atlas values diversity of all kinds, and we're committed to building a diverse and inclusive workplace where we learn from each other. We are an equal opportunity employer and welcome people of all different backgrounds, experiences, abilities, and perspectives.",-1.0,Atlas Health,Oregon,51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
77,Data Engineer - Mid Level,-1,"Why USAA?
At USAA, we have an important mission: facilitating the financial security of millions of U.S. military members and their families. Not all of our employees served in our nation’s military, but we all share in the mission to give back to those who did. We’re working as one to build a great experience and make a real impact for our members.

We believe in our core values of honesty, integrity, loyalty and service. They’re what guides everything we do – from how we treat our members to how we treat each other. Come be a part of what makes us so special!
The Opportunity
We are seeking a dedicated Data Engineer – Mid Level for our Plano, TX Office. The candidate selected for this position is going to work with the Enterprise Data Engineering Services Certified Metrics team. They will work with data engineering technologies and support the team responsible for developing and maintaining USAA’s digital reporting and analytical environment in Snowflake. The resource should be experienced in building analytical applications in the cloud. SQL skills are a must and knowledge of Snowflake, dbt, and other cloud-based data movement tools would be highly beneficial.
This position is a hybrid work type and will be based in Plano, TX. Hybrid roles help employees gain the best of both worlds – collaborating in-person in the office and working from home when needed to achieve focused results.
What you'll do:
Independently conducts work on the full life cycle of data engineering to include analysis, solution design, data pipeline engineering, testing, deployment, scheduling, and production support.
Designs and implements complex technical solutions for data engineering and analytic systems.
Identifies and solves significant technical problems and architecture deficiencies to include design, security, and performance.
Collaborates on design reviews by providing feedback on trends and makes recommendations for solutions.
Breaks down business features and into technical stories and approaches. Influences stakeholders in technical adoption for best solutions.
Creates proof of concepts and prototypes that drive the vision and the outcome for complex initiatives to be delivered.
Collaborates with the team and other engineers on new technologies and alternatives to plan and execute complex assignments and tasks.
Helps on-board entry level engineers. May begin mentoring junior engineers.
Acquires data from multiple data sources and maintains resulting databases, data warehouses, and/or data lakes.
Assists to develop, maintain, and enforce the company’s data development tools and standards. Conducts code reviews on a regular basis to improve quality and ensure compliance.
Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled in accordance with risk and compliance policies and procedures.
What you have:
Bachelor’s degree; OR 4 years of related experience (in addition to the minimum years of experience required) may be substituted in lieu of degree.
4 years of data engineering, data analysis or software development experience implementing data solutions with at least 1 year of data engineering or data management experience.
Extensive knowledge and working experience in SQL and Relational Databases.
Strong analytical and problem-solving skills.
Basic understanding of cloud technologies and tools.
What sets you apart:
Experience with Snowflake
Experience with AWS
Visualization experience with Tableau or similar tool
Experience with DBT
Experience with Python
Experience providing technical leadership, guidance, or mentorship to other engineers
The above description reflects the details considered necessary to describe the principal functions of the job and should not be construed as a detailed description of all the work requirements that may be performed in the job.
What we offer:
Compensation: USAA has an effective process for assessing market data and establishing ranges to ensure we remain competitive. You are paid within the salary range based on your experience and market data of the position. The actual salary for this role may vary by location. The salary range for this position is: $86,520.00 – $165,340.00.
Employees may be eligible for pay incentives based on overall corporate and individual performance and at the discretion of the USAA Board of Directors.
Benefits: At USAA our employees enjoy best-in-class benefits to support their physical, financial, and emotional wellness. These benefits include comprehensive medical, dental and vision plans, 401(k), pension, life insurance, parental benefits, adoption assistance, paid time off program with paid holidays plus 16 paid volunteer hours, and various wellness programs. Additionally, our career path planning and continuing education assists employees with their professional goals.
For more details on our outstanding benefits, please visit our benefits page on USAAjobs.com.
Relocation assistance is not available for this position.
USAA is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",3.4,"USAA
3.4","Plano, TX",10000+ Employees,1922,Company - Private,Insurance Carriers,Insurance,$25 to $100 million (USD)
78,Senior Data Engineer,-1,"CoreTrust is the market leading commercial Group Purchasing Organization (GPO), leveraging the combined purchasing volume of its 3,000+ members to negotiate preferential pricing and terms across more than 80 indirect spend categories. Strategically aligned with private equity portfolios and large independent companies, we complement sourcing bandwidth and improve supply chain efforts with our industry-leading national contracts.
Recently acquired by Blackstone Private Equity, CoreTrust is growing rapidly and we’re looking for a passionate Senior Data Engineer.
Reporting to the Director of Data, you will be part of the data solutions team and be responsible for building our data platforms, enabling the use of advanced analytics to drive continued evolution and growth.
You will create and manage data pipelines to feed and curate our data lake solution and help develop our data roadmap. The ideal candidate has a great understanding of various data / tech solutions (e.g., data modeling tools, data pipeline, data catalogs, cloud databases) and a record of using them to bring tangible dollar impact. You should be excited to seek out and capitalize on a wide variety of opportunities to use data to create value across the organization.

Responsibilities
Lead data projects to build innovative and highly available solutions while ensuring adherence to budget, schedule, and scope of project
Mentor other members in the data solutions team
Develop and assist with oversight on the data tech infrastructure
Drive data & analytics solutions from conception to deployment/delivery with clear ROI impact
Develop and maintain relationships with all relevant business and tech stakeholders and functions
Provide input to proposals for assigned projects including project objectives, technologies, systems, information specifications, timelines, and staffing
Communicate timely status updates to affected internal or external customers and stakeholders
Collect, analyze, and summarize information and trends as needed to prepare project status reports
Assist in developing a culture of data-driven decision-making, including adoption of business intelligence, analysis, and advanced analytics globally
Perform other related duties as assigned

Qualifications
Bachelor’s degree in computer or information science or relevant experience
9+ years of relevant experience in a data-driven professional setting
Ability to assist with the vision of the team (e.g., mission, priorities, engagement model, tooling)
A record of accomplishment of successfully managing complex cross-functional projects under tight deadlines
Strong technical background – familiarity with Python, SQL, cloud technologies like Azure and AWS, statistics / machine learning, Snowflake, DBT, FiveTran, Data Visualization Software
Exceptional communication and presentation skills, particularly in the context of engaging senior management teams
A successful history of manipulating, processing, and extracting value from large, disconnected datasets
Build processes supporting data transformation, data structures, metadata, dependency, and workload management
Working knowledge of creating and leveraging API or Stream based data extraction processes such as Salesforce API
Strong command of databases and SQL
Proficiency with Python or R, especially for data manipulation and analysis, and ability to build, maintain and deploy sequences of automated processes with these tools
Ability to motivate groups of people to complete a project in a timely manner
Excellent analytical, logical thinking, and problem-solving skills
Thorough understanding of project management principles and planning
Thorough understanding of information technology procedures and practices
Proficient with, or able to quickly become proficient with, a range of general and specialized applications, software, and hardware used in the organization and the industry

Benefits
Competitive compensation package
Free individual employee medical coverage
Company subsidized dental and vision coverage
Dollar for dollar 401(k) match up to 6% of your salary with immediate vesting
Company-paid Short-Term and Long-Term Disability coverage
Employee Assistance Program to support your wellbeing and mental health
$1500 annual stipend for undergraduate/graduate college courses; $500 annual stipend for continuing education courses/certifications
Free snacks and beverages on-site
Brand new, state-of-the-art, tech-enabled work environment in downtown Nashville
Flexible/hybrid work culture",2.8,"CoreTrust
2.8",Remote,Unknown,-1,Subsidiary or Business Segment,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
79,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1.0,Radiant System,Remote,-1,-1,-1,-1,-1,-1
80,Senior Data Engineer,$87K - $114K (Glassdoor est.),"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.

This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration.
How will you make an impact?
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
What are we looking for?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework
You might be a good fit if you like to:
Leverage technology to deliver data solutions to drive business outcomes
Drive for continuous improvement of processes and solutions
Bring energy and commitment to excellence to drive delivery of high-quality solutions
Share knowledge and mentor team members on technologies and solutions
What you can learn on the job:
Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers
Grow practical understanding of agile work practices through application of the scrum framework to deliver work products
Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science
What does success look like?
First 30 days:
Understand the existing analytics data platform and technologies
Understand team’s work management processes
First 6 months:
Develop data solutions for the business including shared components and specific data sets
Learn business processes and develop necessary contacts throughout the organization
First year:
Continue to develop data solutions for the business
Become expert on the team for other data engineers to go to for mentoring in completing their work
Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,"AE Business Solutions
4.2","Milwaukee, WI",51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
81,Azure Data Engineer,Employer Provided Salary:$50.00 Per Hour,"7+ years of relevant experience
At least two years of experience building and leading highly complex, technical engineering teams.
Strong hands-on experience in Databricks
Experience managing distributed teams preferred.
Strong technical experience in large distributed systems, Data Warehousing, Data Lake at scale Project management skills: financial/budget management, scheduling and resource management experience with medium and large-scale projects
Comfortable working with ambiguity and multiple stakeholders.
Comfortable working cross functionality with product management and directly with customers; ability to deeply understand product and customer personas.
Architecture Design Experience for Cloud and Non-cloud platforms
Expertise on Azure Cloud platform
Knowledge on orchestrating workloads on cloud
Ability to set and lead the technical vision while balancing business drivers
Strong experience with PySpark, Python programming
Proficiency with APIs, containerization and orchestration is a plus.
Job Type: Contract
Salary: $50.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Work Location: Remote",3.8,"Visvak Solutions
3.8",Remote,51 to 200 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,Less than $1 million (USD)
82,Data Engineer,$77K - $107K (Glassdoor est.),"Position Summary:
Reporting to the Director of Data Science, the Data Engineer will play a crucial role in designing, developing, and maintaining data pipelines and workflows to facilitate data integration, storage, and analysis. With the support of the Data Science team, this position will utilize proven expertise in Microsoft Azure cloud computing to implement and automate processes, ensure the health and efficiency of the data ecosystem, and bring unmatched value to our +1.5 million members across the country and across the world.

Duties and Responsibilities:
Lead architectural efforts for implementing the Data Science team's models and solutions.
Design, implement, and manage data pipelines using Azure Data Factory to extract, transform, and load data from various sources efficiently and securely.
Develop and optimize storage solutions on Microsoft Azure, ensuring data availability, scalability, and reliability.
Automate data-related jobs, processes, and workflows to streamline data delivery
Collaborate closely with cross-functional teams, including data scientists, analysts, and developers, to understand data requirements and support their data-related needs.
Design and maintain data models and architecture, enabling efficient data consumption by business applications and analytical tools.
Ensure data quality, integrity, and security throughout the data lifecycle.
Troubleshoot and promptly resolve data-related issues, performance bottlenecks, and system failures.
Research new data sources, third-party platforms, and data engineering best practices.
Stay current with the latest data engineering and cloud computing trends, particularly in the Azure ecosystem.
Assist Data Scientists with general analysis and dashboards, as needed.
Qualifications:
3-5+ years of relevant Data Engineering, Software Engineering, and/or Analytics experience.
Proved experience with Azure cloud solutions, preferably through your work portfolio and Azure certifications.
Strong proficiency in Azure Data Factory, Azure Storage, and other relevant Azure services.
Extensive experience designing and implementing automated data pipelines, ETL processes, and data workflows.
Advanced skills in SQL and Python for data manipulation, querying, and scripting tasks.
Familiarity with data warehousing concepts, modeling, and integration techniques.
Expert in integrating cloud solutions with external API sources and knowledgeable in creating in-house API solutions that meet our needs.
Excellent problem-solving abilities and a track record of handling complex data engineering challenges.
Strong communication skills and the ability to collaborate effectively with multidisciplinary teams.
Demonstrated ability to prioritize tasks, meet deadlines, and adapt to evolving project requirements.
Azure certifications (e.g., Azure Data Engineer Associate) will be a plus.
Willingness to travel up to 10%.
Organizational Overview:
The National Association of REALTORS® (NAR) is a team of professionals dedicated to providing world-class service to over 1.5 million REALTORS® working in the United States and around the world. The real estate industry is fast-paced and fast-changing--each year, our members participate in the sale, lease, and management of billions of dollars in real estate. As in every industry, our members' value proposition is constantly being challenged by innovation.

It is our mission to empower REALTORS® as they preserve, protect and advance the right to real property for all. We cannot do that without the ideas, passion, and commitment from our talented employees. As our greatest assets, employees are offered their pick of competitive benefits/perks and flexible work options. NAR is certified as a Great Place to Work®. Our most successful employees are committed to our Core Values, which are:

Putting members first
Leading change
Advancing Diversity and Inclusion
Giving respect
Collaborating
Communicating",3.7,"National Association of REALTORS®
3.7","Washington, DC",201 to 500 Employees,1908,Nonprofit Organization,Real Estate,Real Estate,$100 to $500 million (USD)
83,GA/GCP Data Engineer,Employer Provided Salary:$65.00 - $85.00 Per Hour,"A Billion dollar women's clothier with over 1300 stores across North America is seeking a highly skilled Google Analytics / GCP Data Engineer to join our team for a 6 month plus project. As the Google Analytics / GCP Data Engineer, you will be responsible for building BQ tables in GCP, mapping GA data between GA4 and UA, owning, maintaining, and providing recommendations around the GA Schemas to leverage reporting, and utilizing DBT and Git for data transformation and version control. You will also implement ELT pipelines to extract, load, and transform data from various sources.
Responsibilities:
Build and maintain BigQuery tables in GCP
Map GA data between GA4 and UA
Own and maintain the GA Schemas to leverage reporting
Utilize DBT and Git for data transformation and version control
Implement ELT pipelines to extract, load and transform data from various sources
Provide recommendations for optimizing Big Query tables and data flows
Communicate effectively with stakeholders and team members
Requirements:
In-depth experience with Google Analytics (GA4 & UA)
Strong experience in Google Analytics Schema Mapping between Versions
Very strong GCP and BigQuery Skills
Experience using DBT and Git for data transformation and version control
Experience with implementing ELT pipelines to extract, load and transform data from various sources
Strong problem-solving and analytical skills
Excellent communication and collaboration skills
Job Type: Contract
Pay: $65.00 - $85.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Compensation package:
Hourly pay
Experience level:
7 years
Schedule:
Monday to Friday
Experience:
GCP: 5 years (Preferred)
Big Query: 5 years (Preferred)
Google Analytics: 5 years (Preferred)
Tableau: 4 years (Preferred)
Work Location: Remote",4.1,"V Soft Consulting
4.1",Remote,501 to 1000 Employees,1997,Company - Private,HR Consulting,Human Resources & Staffing,$100 to $500 million (USD)
84,Data Engineer,Employer Provided Salary:$150K - $300K,"What we're looking for
Stytch is the platform for user authentication. We build infrastructure that sits in the critical path of our customer's applications. As a data engineer, you'll work on designing and building event-driven architecture systems to drive analytics insights and observability tooling for our customers.

What Excites You
Championing data-driven insights - you see data analytics and observability as a product critical to success
Solving problems with pragmatic solutions — you know when to make trade-offs between completeness and utility and you know when to cut scope to ship something good enough quickly
Building products that make developers lives easier — as a data engineer for a developer infrastructure company, what you build will have an immediate impact on our customers
Shaping the culture and growing the team through recruiting, mentorship, and establishing best practices
Learning new skills and technologies in a fast paced environment

What Excites Us
Comfort working in a modern data stack using tools like Snowflake, Redshift, DBT, Fivetran, ElasticSearch, and Kinesis
Appreciation for schema design and architecture that balance flexibility and simplicity
Experience designing and building highly reliable back-end and ETL systems
3+ years as a data or backend engineer

What Success Looks Like
Technical — build new, highly reliable services that our customers can depend on
Ownership — advocate for projects and solutions that you believe in and ship them to production
Leadership — level up your teammates by providing mentorship and guidance

Our Tech Stack
Data moves through Snowflake, ElasticSearch, MySQL, and Kinesis
Go and Node for application services
We run on AWS with Kubernetes for containerization
gRPC and protobufs for internal service communication

Expected base salary $150,000-$300,000. The anticipated base salary range is not inclusive of full benefits including equity, health care insurance, time off, paid parental leave, etc. This base salary is accurate based on information at the time of posting. Actual compensation for hired candidates will be determined using a number of factors including experience, skills, and qualifications.
We're looking to hire a GREAT team and that means hiring people who are highly empathetic, ambitious, and excited about building the future of user authentication. You should feel empowered to apply for this role even if your experience doesn't exactly match up to our job description (our job descriptions are directional and not perfect recipes for exactly what we need). We are committed to building a diverse, inclusive, and equitable workspace where everyone (regardless of age, education, ethnicity, gender, sexual orientation, or any personal characteristics) feels like they belong. We look forward to hearing from you!

Learn more about our team and culture here!",5.0,"Stytch
5.0",California,1 to 50 Employees,2020,Company - Public,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
85,Senior Data Integration Engineer,-1,"If you are passionate about energy efficiency & carbon footprint reduction, growing your technical skills, working with clients, or love working in a cross functional team setup, this is the role for you!
The implementation engineering team helps our utility partners integrate with the Opower platform and plays an essential technical role in implementing Opower’s SaaS-based energy efficiency and customer engagement programs. You will work directly with our utility partners’ technical teams to integrate their data with our platform and to configure our applications to meet their program requirements. Additionally, you will work with a broad group of other teams at Opower, including project managers and product engineering teams.
Implementation engineers have a diverse set of technical skills, with a focus on delivering high-quality products customized for our utility partners' needs. A typical week might involve ingesting and analyzing utility data; running Unix command-line text manipulation tools; writing python scripts to automate work; adding documentation; working with a utility partner's technical team; and collaborating with R&D teams on future roadmap projects.
You will be an ideal candidate for this job if you have solid understanding of relational databases, are comfortable with advanced UNIX commands, have done some basic programming, and do not hesitate to ask questions. You will work with a variety of teams in different contexts, making the role a good opportunity if you are looking to develop new skills while helping us accomplish our mission of energy efficiency & carbon footprint reduction for our utility partners.
There is flexibility to work remotely full-time, otherwise we have teams located in Oracle’s San Francisco, CA and Arlington, VA offices. You will join a team of extremely helpful engineers with different cultural and professional backgrounds. Besides standard company benefits like 401k matches and unlimited PTO, you will have an excellent work-life balance, the ability to direct your career, and access to an abundance of educational material and professional trainings to help you grow

Responsibilities:
Work with utility project teams on data integrations and energy efficiency product implementations
Explain technical specifications of Opower data integration and products - including highlighting risks with customer experience when requirements are not met
Analyze, transform, and load utility provided data to meet Opower's data requirements for a successful delivery of downstream end-user communications and web experiences
Develop new or maintain tooling (in ruby or python) to make our data integration and product implementations more cost & time effective
Configure and customize Opower's energy efficiency SaaS platform to meet the specific needs of each client
Develop a deep understanding of our products with the ability to explain them to others with non-technical backgrounds
Improve our ability to customize and deliver energy efficiency products to our customers by optimizing delivery processes and writing useful documentation
About You:
Experience writing effective SQL or Hive queries to analyze large relational datasets
Have experience writing data transformations scripts using ETL tools
Experience performing advanced file searches and text manipulation using the Unix/Linux command-line
Comfortable working directly with client teams
Experience writing software tools using object-oriented programming
You understand code versioning concepts and have experience with tools like git
Can connect dots among difference pieces of information gained from multiple sources
Experience troubleshooting software applications that involve APIs, databases, and frontend
Can effectively prioritize multiple tasks at one time
Enjoy being part of a team, helping and learning from others.
Have 8+ years of professional experience. We are open to hiring at different levels too if there’s a better fit.",3.9,"Oracle
3.9",United States,10000+ Employees,1977,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
86,AI/ML Data Automation Engineer,$88K - $125K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
87,Senior Data Engineer,$115K - $151K (Glassdoor est.),"Senior Data Engineer
Location: Various
Raleigh NC; Atlanta GA; Philadelphia PA; Charlottesville VA; others considered;
Hybrid, Remote, Travel required
Elder Research, Inc. (ERI) is a Data Science consulting firm specialized in providing analytic solutions to clients in Commercial and Government industries. The Commercial business unit is seeking to hire a Senior Data Engineer with strong engineering skills who will provide technical support across multiple project teams by leading, designing, and implementing the software and data architectures necessary to deliver analytics to our clients, as well as providing consulting and training support to client teams in the areas of architecture, data engineering, ML engineering and/or related areas. We are trusted advisors to our clients, building lasting relationships and partnering as preferred analytics providers. We use a variety of programming languages and tools to create analytic solutions, often fitting within our clients’ environment and needs.
We are looking to hire a Senior Data Engineer who can support robust and repeatable data manipulation, large-scale infrastructure for data ingestion, and data visualization for custom client applications. A Senior Data Engineer works collaboratively with Business Analysts, Data Scientists, Data Analysts, other Software/Data Engineers, and business stakeholders to create and deploy dynamic data applications that help our customers make meaningful business decisions.
Some of your responsibilities will include leading and implementing software engineering best practices, mentoring junior engineers, designing software architectures, wrangling and fusing large and disparate data sets, assisting in the deployment of models and algorithms, automating data pipelines in their entirety, and communicating model results through user-focused data visualizations. Candidates should have the ability and the willingness to tailor applications to a clients’ business goals using an iterative methodology. Candidates should also be able to consider both long-term stability and scalability while taking a user-focused approach to development and deployment. Excellent decision making skills are a must.
Desired Skills
Candidates should have a mix of the following experiences:
Set or contributed to data engineering and governance standards
Reviewed code/design/architecture and coached on adjustments (as needed)
Mentored junior engineers
Designed and implemented software architectures
Implemented a data pipeline that includes data transformations such as aggregations, joins, and cleaning.
Ingested data into SQL or NoSQL databases.
Interacted with SQL or NoSQL databases via Python, Java, or similar.
Provided access to transformed data for downstream applications or visualizations via REST APIs or similar.
Worked with visualization software such as Power BI, Tableau, or similar.
Skilled in both applying and teaching others to apply Git (or other distributed version control) for collaboration, record-keeping, and deployment scenarios
Skilled in both using and guiding others in using cloud platforms (AWS, Azure, Google Cloud) for building, deploying, evolving, and maintaining data pipelines
Skilled in both applying and guiding others in applying continuous integration and continuous deployment (CI/CD) techniques for surfacing situations where new work breaks existing functionality
Had exposure to modeling and/or data analysis.
Configured a technical service such as a database, version control system, or operating system.
Led and collaborated with others to accomplish a technical task.
Partnered on the development or deployment of a data strategy
Presented technical work to a non-technical audience via writing or presentation.
Desired Education and Experience
Education:
Master’s in a technical field and 4-5 years’ experience OR
Bachelor’s and 6+ years’ experience
Certifications:
AWS, Azure, or Google Cloud
Minimum Requirements
Bachelors/Master’s degree in Computer Science or related field requiring software development
4+ years’ of professional experience with Python, Java, or similar
Excellent written and verbal communication skills
Ability to parse project requirements, including asking questions to ensure full and correct understanding
Willingness and ability to learn new concepts and tools quickly
Willingness and ability to provide technical support across multiple projects
What You Would Do
Work on small teams in a highly collaborative environment.
Participate in both existing and new projects.
Contribute to consulting projects to solve interesting problems for multiple clients in various industries.
Communicate details of the technical architecture to fellow team members and clients in both technical and non-technical terms.
Work with Data Scientists to design and/or implement the technical architecture necessary to support analytics.
Work with Software/Data Engineers to improve the robustness and scalability of software products.
Participate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance.
Work on documentation of the technical architecture for internal and client use.
Design, create, and provision data stores.
Build trusted and lasting relationships with clients.
Provide value to our clients through analytics, software tools, and leadership.
Manage and monitor pipelines to extract, transform, and load data (ETL).
About Elder Research, Inc.
Headquartered in Charlottesville, VA with offices in Arlington, VA, Baltimore, MD, and Raleigh, NC, Elder Research, Inc. (ERI) is a fast-growing solutions and consulting firm specializing in predictive analytics. At ERI, you’ll be part of a fun, friendly community. In keeping with our entrepreneurial spirit, we want candidates that are self-motivated with an innate curiosity and strong team work ethic. We work hard to provide the best value to our clients and allow each person to contribute their ideas and put their skills to use immediately.
ERI believes in continuous learning; each week the entire company attends a “Tech Talk” and an office lunch. Individuals often share what they have learned at informal bag lunch talks or afternoon chats. ERI provides a supportive work environment with established parental, bereavement, and PTO policies. By prioritizing a healthy work-life balance - with reasonable hours, solid pay, low travel, and flexible time off - ERI enables and encourages its employees to serve others.
ERI provides analytic solutions to hundreds of companies across numerous industries. Our team enjoys great variety in the type of work we do and exposure to a wide range of techniques and tools. If you are passionate about integrating data, technology, and analytics in a team-based environment to solve problems, then ERI may be a good fit for you.",4.5,"Elder Research Inc
4.5","Charlottesville, VA",51 to 200 Employees,1995,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
88,Data Engineer Azure databricks,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer - Databricks
For: CDW (Direct Client)
W2, Corp to Corp, W2 Contract Okay
Fully Remote
`
DESIRED SKILLS AND ABILITIES
Key Responsibilities
You will design, build and test cross cloud and on-premise data pipelines and leverage Azure Databricks for data processing.
Collaborate with cloud architects, tech leads to facilitate on-premise to Databricks migration
You should understand data pipelines and modern ways of automating data using cloud-based and on-premise technologies
Develop reference architectures and design pattern library for typical Cloud based solutions implementations
Advise on Cloud project set up, security and role based access implementation, and network optimizations
Qualifications:
2+ years with SQL and database table design – able to write structured and efficient queries on large data sets
1-2 years on Databricks on any cloud platform (AWS, Azure)
1-2 years working on AWS/Azure cloud
1-2 year’s design and/or implementation of enterprise applications
Experience in “migrating” on premise workloads to one or more industry leading public cloud(s)
Experience in AGILE development, SCRUM and Application Lifecycle Management (ALM) with scripting experience in Python and shell.
Hands-on experience in the full life-cycle of software development or methodology using Agile Scrum/Kanban etc.
Able to work with agile teams as they perform feature level design, development, testing, and performance analysis
Databricks certification is a plus.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Schedule:
8 hour shift
Work Location: Remote",-1.0,Data Ninjas Inc.,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
89,Senior Software Engineer - Data Engineering,$120K - $153K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: The merchant Reporting and Reconciliation team is looking for a senior Back-end engineer responsible for providing technical leadership to the team and executing the unification projects end to end. Your primary focus will be developing massively scalable, distributed software systems that require high availability to our business
Job Description:
Merchant reporting is crucial for our Merchants so that they can close their account books timely and accurately with complete payment data in core markets. We are looking for people who have a passion for developing massively scalable, distributed software systems that require high availability to our business. As a member on the Merchant Reporting and Reconciliation team, you thrive in a fast-paced environment and enjoy driving innovation through rapid prototyping and iterative development. You will work directly with our Product Owners and Domain Technical Leads to create outstanding solutions and deliver incredible reporting products. You will be involved from ideation to rollout.
Your day-to-day:
Work with Product Managers and other business partners to identify opportunities for improvement
Analyze data based on product requirements
Create reports for internal teams and/or external clients
Use graphs, infographics, and other methods to visualize data
Structure large data sets to find usable information
Work with a team of analysts and other associates to process information
Create presentations and reports based on recommendations and findings
Define validation queries when needed and how to identify discrepancies in the data as they arise
Write queries for runbooks that automate the discrepancy identification process
Implement the reporting data model
Deliver within schedule in an Agile software development using test-driven development methodologies.
Participate in development life cycle activities like design, coding, testing, and production release.
Be proactive with identifying areas for improvement and innovation to improve development productivity
What do you need to bring:
BS in EE/CS or equivalent work experience and successful completion of major projects for which you can show code examples.
5+ years of hands-on data/software engineering experience
Experience working with coding languages—preferably SQL, Java, Spark-SQL, Pyspark, Python
Experience working with SQL and NoSQL DataBase
High proficiency in MS Excel, MS PowerPoint, GIT, Apache Airflow
Have a passion for quality and writing clean and solid code that scales and performs well.
Strong desire to learn, push the envelope and share knowledge with others.
Excellent analytical and time management skills
Teamwork skills with a problem-solving attitude
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
90,Senior Data Engineer (Remote),Employer Provided Salary:$50.00 - $70.00 Per Hour,"Company Overview: IT Giants INC is a dynamic and innovative technology company at the forefront of delivering advanced data solutions across various industries. We are seeking an experienced Data Engineer with expertise in cloud platforms, programming languages, database technologies, and analytics tools. If you also have a strong background in the airline domain, this is an opportunity to join our dedicated team and shape the future of data-driven decision-making in the aviation sector.
Position Overview: As a Data Engineer at IT Giants INC, you will play a pivotal role in designing, building, and maintaining sophisticated data pipelines and analytical solutions for our airline industry clients. Leveraging your deep understanding of both data engineering and the airline domain, you will empower our clients to harness the power of their data to improve operations and customer experiences.
Key Responsibilities:
Data Pipeline Development: Architect, develop, and maintain end-to-end data pipelines, integrating data from diverse sources, including cloud platforms (GCP, Azure, AWS), APIs, and various databases (Teradata, Oracle, PostgreSQL, NoSQL).
Data Transformation and Modeling: Apply your expertise in SQL, Python, and Alteryx to transform raw data into meaningful insights, while ensuring data integrity and accuracy.
Cloud Expertise: Utilize your proficiency in GCP, Azure, and AWS to design and implement scalable and cost-efficient cloud-based solutions that meet the unique demands of the airline industry.
Database Management: Manage relational and NoSQL databases to store and retrieve data efficiently, ensuring optimal performance and security.
Analytics and Reporting: Collaborate with data scientists and analysts to develop interactive dashboards and reports using tools like Tableau, QuickSight, and CloudWatch, enabling stakeholders to make informed decisions.
Data Quality and Governance: Implement data quality checks, validation processes, and adherence to industry regulations to ensure data accuracy and compliance.
Version Control and Collaboration: Utilize GitLab and other version control tools to collaborate with cross-functional teams on code management and deployment.
Real-time Monitoring: Develop real-time data processing solutions using technologies like CloudWatch to enable timely insights into operational metrics.
Linux Environment: Leverage your proficiency in Linux to optimize system performance and troubleshoot issues as they arise.
Domain Knowledge: Apply your deep understanding of the airline industry to structure data effectively and drive actionable insights from the data.
Qualifications:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
years of proven experience in data engineering, with a focus on cloud platforms (GCP, Azure, AWS), Python, SQL, and relevant database technologies.
Experience with Alteryx, Tableau, and data visualization tools for creating impactful dashboards.
Proficiency in managing databases such as Teradata, Oracle, PostgreSQL, and NoSQL databases.
Strong command of Linux environments and version control using GitLab.
Familiarity with CloudWatch, QuickSight, and real-time data processing is advantageous.
Domain expertise in the airline industry, including a solid understanding of aviation data, standards, and operational challenges.
Excellent problem-solving skills and the ability to troubleshoot complex data issues.
Effective communication skills to collaborate with technical and non-technical teams.
Proven ability to work independently and in a team, delivering high-quality solutions.
Join Our Team: If you are a driven and knowledgeable Data Engineer with a passion for both technology and the airline industry, IT Giants INC offers an exciting opportunity to contribute to transformative data solutions. Join us in our mission to revolutionize data analytics within the aviation sector and drive meaningful impact.
Job Types: Full-time, Contract
Pay: $50.00 - $70.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Work Location: Remote",5.0,"IT Giants
5.0",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
91,BI Data Engineer (Remote),Employer Provided Salary:$75.00 - $85.00 Per Hour,"Stefanini Group is hiring!
Exciting opportunity awaits, let us help you get started!
Click Apply now or you may call: (248) 728-2611 / email: palak.agrawal@stefanini.com for faster processing!

Job Description:
We require a reliable, talented, highly motivated BI/Data Engineer with excellent SQL, PowerBI, Python skills, The firm is putting a huge focus on data and reporting, with an increased demand of data related tasks.
As a global leader in Asset management, you will be working as part of an innovative, fast paced technology driven culture who are always looking at ways to enhance how technology is used within their organization.
You will be a valued member of an effective data team working to deliver growth against ambitious targets.
You will play a key role in extracting useful information from data, providing important insights to global technology teams.
The role will focus on creating best practice, high quality data solutions in collaboration with numerous business stakeholders and technology partners.
We are looking for someone who enjoys problem solving, who has an inquisitive nature, a team player and can work in a collaborative environment with good communication skills.
Ability to help extract and analyze data is required for the job.



Requirements:
We are looking for a good mix of the following skills:
Excellent experience in the Microsoft BI stack, Excel.
Excellent experience in building data pipelines and integrations (ETL/ELT).
Excellent knowledge of data warehousing and analysis concepts, data normalization, dimensional modelling, and associated practices.
Knowledge and experience of data manipulation languages such as DAX or MDX, Visualization or dashboarding experience.
Python experience is a must. Azure cloud experience would be a bonus.
We believe in building a culture supported by strong values, trust and independence. Within the team, we are highly collaborative, supportive, open and have high standards in the work we produce.

Education:
Bachelor's degree or equivalent experience.

*Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives*

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",3.3,"Stefanini, Inc
3.3","New York, NY",10000+ Employees,1987,Company - Private,Enterprise Software & Network Solutions,Information Technology,$1 to $5 billion (USD)
92,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1.0,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
93,"Data Engineer (AWS, Python)",Employer Provided Salary:$50.00 - $52.00 Per Hour,"Title: Data Engineer (AWS, Python)
Location: Remote
Duration: 12+ Months
Job Description:
Typical task breakdown:-
Daily standup, collaborate with peers, work independently to complete sprint commitments, participate in agile ceremonies.
Interaction with team:-
Daily stand up meetings with team – working mainly with Helios Platform adoption team.
Work environment:
Education & Experience Required:
Bachelor's in computer science required plus 7+ Working experience .
Masters degree with 6+ years working expeirence acceptable as well.
Preferred: AWS cloud certifications
Technical Skills
(Required)
Exp with Object oriented programming - Python (4+ years)
AWS (3+ years overall exp) lambda and/or glue, EMR, S3
SQL (3+ years)
2+ years exp in test driven development, unit test, integration testing, regression testing
(Desired)
Event driven framework experience.
Kinesis or Kafka experience
Familiar with working in Agile methodology
Soft Skills
(Required)
Strong written and verbal communication, experience working as a member of agile engineering team, ability to commit to and deliver work within allocated sprint window.
Job Type: Contract
Pay: $50.00 - $52.00 per hour
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",5.0,"DSMH LLC
5.0",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
94,Data Engineer,Employer Provided Salary:$45.00 Per Hour,"Job Title: GCP Data Engineer
Duration: 12+ Months
Location: Hartford, CT (Onsite Role)
Tax term : W2 & 1099
Job Description:
As a GCP Data Engineer, you will play a pivotal role in designing, implementing, and optimizing data solutions on the Google Cloud Platform (GCP). Your expertise will be essential in leveraging various technologies to build robust data pipelines and contribute to the overall success of data-driven initiatives.
Roles and Responsibilities:
Design, develop, and maintain efficient data pipelines using Python or PySpark on the Google Cloud Platform.
Seamlessly integrate data solutions using GCP services like Google Cloud Storage, BigQuery, Dataflow, and Pub/Sub.
Utilize expertise in the Hadoop ecosystem, including HDFS and Hive, to craft seamless data workflows.
Transform raw data into insights by creating complex SQL queries for data transformation and analysis.
Implement data quality checks and validation procedures to ensure accuracy and completeness.
Optimize data pipelines and processes for maximum performance and efficiency.
Collaborate with cross-functional teams to understand data requirements and deliver tailored solutions.
Create comprehensive documentation for pipelines, workflows, and processes.
Stay updated with the latest data engineering and cloud technology trends.
Tackle complex challenges with a proactive problem-solving mindset.
Required Qualifications:
Expertise in Python/PySpark and hands-on experience with Google Cloud Platform (GCP) for data engineering tasks.
Strong understanding of the Hadoop ecosystem, including HDFS and Hive.
Proficiency in SQL for effective data manipulation and analysis.
Experience working with cloud platforms, preferably Google Cloud Platform (GCP).
Strong problem-solving skills and analytical thinking.
Effective communication and collaboration abilities.
Adaptability to evolving data engineering technologies and practices.
Job Type: Contract
Salary: From $45.00 per hour
Experience level:
9 years
Application Question(s):
What is your work authorization?
Experience:
Data engineer: 9 years (Required)
Google Cloud Platform: 5 years (Required)
Hadoop: 5 years (Required)
Python: 4 years (Preferred)
SQL: 4 years (Preferred)
Work Location: On the road",-1.0,Conglomerate IT,"Hartford, CT",-1,-1,-1,-1,-1,-1
95,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
96,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
97,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
98,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
99,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
100,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
101,Sr. Data Engineer,Employer Provided Salary:$105K - $110K,"Role : Data Engineer
Location : Remote
Note : We are looking Someone who can be technical and also be able to
structure and manage transformation projects as well.
JD :
* Data Engineer Skill: Spark, Python and Databricks
* 12+ years of experience in Data pipeline engineering for both batch
and streaming applications.
* Experience with data ingestion process, creating data pipelines
(should be able to move data between stages in data lake, apply
transformation rules) and performance tuning. Must be hands-on coding using
Python with Spark.
* Exposure working with complex data types like XML or JSON, CI/CD
pipeline and scheduling tools like Airflow.
* Proven experience in DWH (type-2 dimensions, aggregations, star
schema, etc.) design and implementation
* Proficient in writing complex SQL queries
* Effective communication and collaboration skills to work with
cross-functional teams and stakeholders.
* Oversee and monitor the work of offshore according to projects
timelines and, if necessary, resolve any issues as they arise
Job Type: Full-time
Salary: $105,000.00 - $110,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
technical and able to structure and manage projects: 5 years (Required)
Data pipeline engineering for both batch and streaming apps.: 10 years (Required)
data ingestion process, creating data pipelines: 5 years (Required)
Python and Spark: 4 years (Required)
Exposure of XML or JSON, CI/CD: 4 years (Required)
scheduling tools like Airflow: 5 years (Required)
writing complex SQL queries: 5 years (Required)
Oversee and monitor the work of offshore: 3 years (Required)
working on fulltime role: 2 years (Required)
Work Location: Remote",4.3,"keasis Inc
4.3",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
102,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
103,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
104,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
105,Azure Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Required Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 9 years (Required)
Python: 8 years (Required)
Data lake: 7 years (Required)
Data Bricks: 7 years (Required)
Work Location: Remote",-1.0,TekValue IT Solutions,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
106,Data Engineer,-1,"Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore.

Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office.
The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus.
Position Responsibilities
Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities
Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization
Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process
Qualifications & Desired Skills
Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance
Minimum 2 years' of development experience using either Python, C#/Java or C++
Minimum 2 years' years of database/SQL experience
Familiarity with object-oriented programming concepts
Must be comfortable with development across the application stack
Ability to complete complex projects independently
Detail-oriented with strong verbal and written communication skills
Ability to work effectively in a high-energy, time sensitive team environment
Interest or prior experience in the financial trading industry (particularly in fixed income) a plus
This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education.
Base Salary for this role is expected to be between:
$110,000—$150,000 USD",4.4,"Garda Capital Partners
4.4","New York, NY",51 to 200 Employees,-1,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
107,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
108,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department.
The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
109,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1.0,Percept Health,Remote,-1,-1,-1,-1,-1,-1
110,Data Engineer (MongoDB),$80K - $114K (Glassdoor est.),"CrossCountry Mortgage (CCM) is one of the nation’s top 3 retail mortgage lenders with more than 7,000 employees operating nearly 600 branches across all 50 states. We have been recognized nine times on the Inc. 5000 list of America’s fastest-growing private businesses and have received countless awards for our standout culture. Through our dedication to getting it done, we make every mortgage feel like a win.

A culture where you can grow! CCM has created an exceptional culture driving employee engagement, exceeding employee expectations, and directly impacting company success. At our core, our entrepreneurial spirit empowers every employee to be who they are to help us move forward together. You’ll get unwavering support from all departments and total transparency from the top down.

CCM offers eligible employees a competitive compensation plan and a robust benefits package, including medical, dental, vision, as well as a 401K with an employer match. We also offer company-provided short-term disability, an employee assistance program, and a wellness program.
Position Overview:
We are looking for an experienced MongoDB developer to work with our development teams to integrate innovative features on our new products and applications, as well as maintain existing applications. The MongoDB developer will engineer scripts that automate daily tasks and ensure the security of data sets.
Job Responsibilities:
Upgrade databases through patches.
Understand and convert company needs into technical specifications and creating scalable solutions using MongoDB.
Optimize database schema designs, ensuring data integrity, and promoting best practices to enhance the overall performance and scalability of MongoDB solutions.
Assist developers in identifying and resolving performance issues by utilizing Mongo Profiler to profile and fine-tune database queries and operations.
Leverage performance optimization techniques by implementing proper indexes, including B-Tree, Geospatial, and Text indexes, to enhance query performance and overall system efficiency.
Implement the most effective backup and recovery methods to safeguard critical data and ensure business continuity in case of data loss or system failures.
Collaborate with cross-functional teams to understand business requirements.
Stay updated with the latest MongoDB advancements, features, and best practices.
Qualifications and Skills:
Bachelor’s/Master’s degree in software engineering or Information Technology.
3+ years of experience in executing data solutions using MongoDB.
Recommend and implement best practices for Rest API integration framework/model.
Analytical and problem-solving skills.
Configuring schema and MongoDB data modelling.
Experience with database security management.
Administration of MongoDB in Atlas.
Thorough understanding of MongoDB architecture.
Excellent communication skills.
This job description is intended to convey information essential to understanding the scope of the job and the general nature and level of work performed by job holders within this job. However, this job description is not intended to be an exhaustive list of qualifications, skills, efforts, duties, responsibilities or working conditions associated with the position.
CrossCountry Mortgage, LLC offers MORE than a job, we offer a career. Apply now to begin your path to success! https://crosscountrymortgage.com/about-us/careers/apply/
California residents: Please see CrossCountry’s privacy statement for information about how CrossCountry collects and uses personal information about California consumers.
CrossCountry Mortgage, LLC. (NMLS3029) is an Equal Opportunity Employer committed to workforce diversity. Qualified applicants will receive consideration without regard to race, religion, creed, color, orientation, gender, age, national origin, veteran status, disability status, marital status, sexual orientation, gender identity, or gender expression, or any other protected status in accordance with all applicable federal, state and local laws. The collective sum of the individual differences, life experiences, knowledge, inventiveness, innovation, self-expression, unique capabilities and talent that our employees invest in their work represents a significant part of not only our culture, but our reputation. The Company is committed to fostering, cultivating and preserving a culture that welcomes diversity and inclusion. Employment is contingent upon successful completion of a background investigation. CrossCountry Mortgage, LLC. is an FHA Approved Lending Institution and is not acting on behalf of or at the direction of HUD/FHA or the Federal government. To verify licensing, please visit www.NMLSConsumerAccess.org.",3.3,"CrossCountry Mortgage, LLC.
3.3","Charlotte, NC",1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
111,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
112,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
113,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
114,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1.0,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
115,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
116,Data Engineer,$76K - $101K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
The Data Governance/Data Quality Subject Matter Expert (SME) is a professional responsible for designing, implementing, and managing the client’s data governance strategy and ensuring high data quality standards across data-centric operations and decision-making.
The SME is in charge of developing and maintaining robust data pipelines, ensuring data observability, managing data within the Microsoft Azure environment, and implementing data governance principles in alignment with industry best practices and regulatory standards.
Primary Responsibilities and Accountabilities:
Data Governance & Quality: This includes developing and implementing data governance policies and procedures, monitoring data quality, managing data within the Microsoft Azure environment, and ensuring compliance with data privacy regulations.
Data Pipeline & Workflow Development: Designing and implementing efficient and secure data pipelines in Azure Data Factory and workflows, ensuring optimal performance and incorporating data quality principals.
Data Integration & Observability: Designing data integration strategies, integrating data from multiple sources, ensuring data quality and observability through tools like Azure Synapse.
Data Security: Defining and implementing data security policies and procedures, including access control and data encryption within the Azure environment.
Performance Tuning: Optimizing data access, indexing, and query performance within Azure Data Factory to ensure scalable and reliable data infrastructure.
Monitoring & Troubleshooting: Monitoring data systems, troubleshooting issues, identifying performance bottlenecks, and collaborating with team members to resolve problems.
Collaboration & Communication: Working closely with stakeholders to understand their data needs and develop solutions adhering to industry best practices and the needs of the business.
Automation & Tooling: Automating repetitive tasks and maintaining tools necessary for data processing and storage within the Azure environment.
Qualifications
A Bachelor's degree in a related field such as computer science, mathematics, engineering, or information systems.
3+ years of hands-on experience programming in SQL.
2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes within the Microsoft Azure environment.
Experience in Data Governance OR Data Quality Management is a must.
Skills
Competencies:
Expertise in Microsoft Azure data tools, with a focus on Azure Data Factory and Azure Synapse for data processing, storage, and analysis.
Strong SQL skills and knowledge of data warehousing concepts and ETL (Extract, Transform, Load) processes within the Azure environment.
Proficiency in programming languages such as Python, and experience with big data technologies such as Hadoop, Spark, and Kafka.
Ability to design, build, and maintain data pipelines, with a particular focus on Azure Data Factory.
Strong analytical and problem-solving skills, with the ability to identify and resolve data-related issues effectively.
Experience with data security and privacy regulations, and understanding of HIPAA compliance.
Strong experience with data governance, data quality, and data observability principles.
Excellent written and verbal communication skills, ability to explain complex technical concepts to non-technical stakeholders.
Excellent collaboration and teamwork skills, with the ability to work in a team involving client data engineers, data scientists, and data analysts.
Strong attention to detail and commitment to ensuring high data quality.
Continuous learning mindset and ability to stay up-to-date with new developments and technologies in data governance and data quality
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
117,GCP Data Engineer,Employer Provided Salary:$50.00 - $56.00 Per Hour,"JOB Description: -
ABOUT CONGLOMERATE
Conglomerate IT is a certified and a pioneer in providing premium end-to-end Global Workforce Solutions and IT Services to diverse clients across various domains. Visit us at ConglomerateIThttp://www.conglomerateit.comConglomerateIT
Conglomerate IT mission is to establish global cross culture human connections that further the careers of our employees and strengthen the businesses of our clients. We are driven to use the power of global network to connect business with the right people without bias. We provide Global Workforce Solutions with affability.
We offer a gamut of services across diverse domains, categories, skill sets with varying lengths of assignments /core capabilities included:
· Application Development Services
· Business Intelligence Services
· Enterprise Resource Planning
· Infrastructure Support Services
· Testing Services
· Program / Project Management
· Temporary & Permanent Staffing
Conglomerate IT is headquartered in Addison, Texas with branch offices across Canada, India, Sri Lanka, Argentina to further serve our clients.
JOB ROLE: GCP Data Engineer
Location: Job Type:Hartford, CT (Onsite Role)
Contract : 12+ Months
Work Authorization: w2 ,1099
Must have Skills: - Skills: Python/PySpark, GCP Cloud , HDFS, Hive, Big data
About Role: -
As a GCP Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and data architecture on the Google Cloud Platform. You will work closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and storage. Your expertise in GCP services and data engineering best practices will play a crucial role in maintaining a scalable, reliable, and high-performance data infrastructure.
Roles and Responsibilities:
· Design and implement data pipelines: Develop ETL (Extract, Transform, Load) processes to move data from various sources into GCP storage solutions such as BigQuery, Cloud Storage, and Datastore.
· Data modeling: Design and optimize data models to support data analytics, reporting, and machine learning initiatives. Ensure data accuracy, integrity, and appropriate access controls.
· Performance optimization: Fine-tune data pipelines and query performance to maximize efficiency and reduce latency, utilizing techniques like partitioning, indexing, and caching.
· GCP services utilization: Leverage GCP services like Dataflow, Dataproc, Pub/Sub, and Composer to create, schedule, and manage data processing and orchestration tasks.
· Data integration: Collaborate with cross-functional teams to integrate data from various sources, both internal and external, to enable comprehensive analysis and reporting.
· Data quality and governance: Implement data quality checks, monitoring, and validation processes to ensure data accuracy and consistency across the platform.
· Security and compliance: Implement security measures and access controls to protect sensitive data. Ensure compliance with industry standards and regulations.
· Documentation: Create and maintain technical documentation for data pipelines, workflows, and processes.
· Troubleshooting and support: Diagnose and resolve data-related issues, performance bottlenecks, and operational challenges in a timely manner.
Ideal Profile
· You have experience with Python Hive, Big data
· You have experience withYou GCP Cloud
· have strong experience with HDFS and other relevant tools/technologies.
· You have advanced python skills.
· You have PySparkknowledge.
· You have excellent communication and collaboration skills.
· You have attention to details.
· You are self-directed.
What's on Offer?
· Join a team of world class IT professionals
· Comprehensive salary package
· Great opportunity for growth and development
Job Type: Contract
Salary: $50.00 - $56.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Experience:
google cloud platform: 4 years (Required)
Work Location: In person",-1.0,CONGLOMERATE IT LLC,"Novi, MI",-1,-1,-1,-1,-1,-1
118,Snowflake Data Engineer,Employer Provided Salary:$70.00 - $80.00 Per Hour,"Job: Snowflake Data Engineer – Contract (6+ Months) – Stillwater, MN
REMOTE - HAVE TO WORK ONISTE WHEN EVER CLIENT CALL. Preferably local to MN NEEDED
EAD – GC – H4- EAD G C - CITIZEN
W2 Contract
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Expected hours: 40 per week
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 7 years (Required)
Snowflake API: 4 years (Required)
Snowflake administration: 6 years (Required)
Snowflake RBAC, RLS, CLS: 5 years (Required)
Willingness to travel:
50% (Required)
Work Location: Remote",-1.0,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
119,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1.0,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
120,Data Engineer I,Employer Provided Salary:$90K - $120K,"Interested in joining a growing company where you will work with talented colleagues, enhance a supportive and energetic culture, and be part of the climate solution? At Energy Solutions, we focus on the big impacts. And we believe that market-based programs can be a powerful force to deliver large-scale energy, carbon, and water-use savings. For 28 years, we've harnessed that power to offer proven, performance-based solutions for our utility, government, and institutional customers.
Energy Solutions (ES) is currently seeking a full-time Data Engineer I to join our Business Operations Team. The Data Engineer role is part of the Business Operations Team and will work on critical projects such as the Corporate Operational Data Warehouse (CODW) and the new ERP system. The CODW project is one of ES's strategic goals, to build and maintain a system that enables ES to centralize, integrate, and analyze internal data from various sources. The goal is to provide a single source of truth for ES internal data and enable data-driven decision-making across the organization. This role will potentially lead the design and implementation of the infrastructure for internal reporting needs, driving operational efficiencies and helping our company's transition to operating internationally with 1000+ employees.
Energy Solutions has a remote-friendly work environment for staff located throughout the United States. We also have offices in Oakland and Orange, California, Boston, Massachusetts, New York, and Chicago, IL, for those that wish to work from one of our offices.

Daily responsibilities include but are not limited to:
Collaborate with stakeholders to understand data requirements and develop data-driven solutions that support business goals
Analyze data sources and pipelines needed for the CODW project
Design and implement scalable data pipelines, ETL processes, and data integration solutions for the CODW project using Python, SQL, and AWS technologies
Develop and maintain a data warehouse and optimize data storage, processing, and retrieval
Build schema, populate the database, and automate reporting
Document and maintain technical specifications, ETL processes, data mappings, and dictionaries
Support the Data Science Center of Excellence (DSCOE) with other data engineering needs

Minimum Qualifications:
BS/BA in Computer Science, Physics/Engineering, Business or Mathematics
Experience in building ETL data pipelines, real-time pipelines are a plus
Strong programming skills in Python and SQL
Minimum 2 years of data warehouse development and strong fundamentals in dimensional data modeling
Minimum 2 years of experience creating SQL queries and ETL design, implementation, and maintenance
Minimum 2 years of experience developing data pipelines using Python
Minimum 2 years of experience with AWS, AWS Redshift, AWS Data Engineering and ML Ops tools and services (ie: S3, Redshift, RDS)
Proven ability to aggregate, normalize, munge, analyze, and summarize value from disparate datasets
Strong oral, written, and interpersonal communication skills
High degree of accuracy and attention to detail
Ability to establish priorities, work independently with minimum supervision
Preferred Qualifications:
Ability to manage and communicate data warehouse plans to a non-technical audience
Expertise in cloud services architecting and designing secure AWS environments
Experience using BI tools like Tableau and Looker
Experienced in analyzing and troubleshooting data quality issues
Self-driven, highly motivated, and able to learn quickly
Familiarity with large data sets, cloud-based development and deployment, open-source practices and frameworks, and intelligent applications is desirable
Proven success in migrating data from legacy systems

Salary DOE: $90k - $120k / Annually
Compensation is commensurate with experience and includes a generous retirement package. Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP).
To see this and our other open positions, please visit https://energy-solution.com/company/careers/.
We encourage candidates from diverse backgrounds to apply. For more information about Energy Solutions and our Diversity, Equity and Inclusion efforts, please visit us on our website at www.energy-solution.com.
Background Check Information
Information will be requested to perform the compulsory background check. A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment. Energy Solutions is an equal opportunity employer.
Reasonable Accommodations
Energy Solutions is committed to providing access and reasonable accommodation for individuals with disabilities. If you require an accommodation in completing this application, interviewing, and/or completing any pre-employment testing, or otherwise participating in the employee selection process, please email accommodation@energy-solution.com.
Privacy Notice for Job Applicants",3.9,"Energy Solutions
3.9",Remote,10000+ Employees,2008,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$25 to $100 million (USD)
121,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
122,"Data Engineer (Snowflake, PowerBi)-9583",Employer Provided Salary:$40.00 - $45.00 Per Hour,"Typical Day: -
Manage, and aggregate data for analytic consumption
Create and maintain data tables in Snowflake
Utilize data from multiple cloud sources and develop/maintain dashboards
Data movement and Extract, Transformation, Load (ETL) development
Education Requirements:
Associate’s degree in computer programming or a relevant field required.
Bachelor's degree preferred.
0-2 years experience required.
Technical Skills:
**Required: Basic knowledge of logical data modeling and physical data modeling. Data movement and Extract, Transformation, Load (ETL) development skills. Basic knowledge of computer software, such as SQL, Visual Basic, Scripting language - Python, Snowflake, etc. Experience with Microsoft Office tools. **Desired: Experience with other Microsoft tools, Teams, SharePoint (Office 360).
Soft Skills: Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. Basic ability to work independently and manage one’s time.
Job Type: Contract
Pay: $40.00 - $45.00 per hour
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Aurora, IL 60502: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 2 years (Required)
SQL: 2 years (Required)
Work Location: Hybrid remote in Aurora, IL 60502",5.0,"DSMH LLC
5.0","Aurora, IL",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
123,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
124,Data Movement Engineer C2H,Employer Provided Salary:$55.00 Per Hour,"Client required information:
***They need Someone who has strong Talend Experience minimum of 5 years***.
Talend API is Desirable, But any API is also fine like snowflake as a last option.
They are working on Data Base Movement from Talend to ETL/ELT.
Formally they need ETL/ELT Developers or Talend Developers with ETL/ELT data movement experience.
Implementation and Configuration Talend is Mandate.
They also need to take a an on call support after work hours for ongoing production Support team, this might happen on weekends too.
They are going to implement more API’s, so having extra API experience is plus.
They need to understand the baseline of Insurance( The Basic work or Knowledge on Insurance Modules).
They can Work Remote, But need only from EST Zones.
Manager is not going to accept any profiles without Talend Experience.
Skill Qualifications Required:
· Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
· Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
· Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
· Some experience with cloud-based database technologies required
· Working knowledge of data warehousing concepts, structures and ETL best practices
· Experience using query tools (e.g. AQT, MS Query)
· Ability to problem solve using analytical thinking skills
· Must work well independently - must be inquisitive and seek answers to complex questions without being prompted
· Strong organizational and time management skills
· Strong communication skills including verbal and written to communicate effectively with clients and management
· Strong project management skills to ensure that projects get done on time and within budget
· Effectively participates in teams and moves the team toward completion of goals
Job Types: Full-time, Contract
Salary: From $55.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Experience:
Talend: 5 years (Required)
ETL: 5 years (Required)
SQL query: 3 years (Required)
data warehousing: 3 years (Required)
business intelligence tools: 3 years (Required)
Work Location: Remote",-1.0,Technovant inc,"New Haven, CT",-1,-1,-1,-1,-1,-1
125,Lead Azure Data Engineer,Employer Provided Salary:$135K - $140K,"Lead Data Engineer.
Location : New York, NY ( Hybrid)
Full Time
10+ years of experience in data engineering
5+ years of experience managing a team of data engineers
Strong understanding of SQL Server SSIS
Excellent communication and interpersonal skills
Ability to work independently and as part of a team
Ability to meet deadlines and work under pressure
Strong desire to be hands-on with the development and implementation of data pipelines and data warehouses
Experience working with stakeholders to develop product backlog grooming, sprint planning data engineering and
QA Testing
Migration of SQL Server to Azure is desired
Experience with Azure Data Factory, ADLS, and CI/CD highly desired
The Hands-On Data Engineering Lead is responsible for leading a team of data engineers in the design, development, and
implementation of data pipelines and data warehouses. The ideal candidate will have a strong understanding of data
engineering principles and practices, as well as experience with SQL Server SSIS. The Hands-On Data Engineering Lead will
also be responsible for managing the team's workload, mentoring junior engineers, and communicating with stakeholders.
Additionally, the Hands-On Data Engineering Lead will be expected to be hands-on with the development and
implementation of data pipelines and data warehouses, and to have experience working with stakeholders to develop
product backlog grooming, sprint planning data engineering and QA Testing.
Responsibilities:
Lead the design, development, and implementation of data pipelines and data warehouses
Develop and implement data pipelines and data warehouses using SQL Server SSIS
Troubleshoot data pipelines and data warehouses
Communicate with stakeholders to understand their needs and ensure that projects meet their requirements
Manage the team's workload and ensure that projects are completed on time and within budget
Mentor junior data engineers and help them develop their skills
Stay up-to-date on the latest data engineering technologies and best practices
Provide support to production teams and QA teams
Work with stakeholders to develop product backlog grooming, sprint planning data engineering and QA Testing
Working with other engineers to design and implement data pipelines and data warehouses using Azure Data
Factory, ADLS, and CI/CD
Experience in migrating SQL server to Azure
30% team management and 70% hands-on
Job Type: Full-time
Salary: $135,000.00 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 10 years (Required)
SQL: 10 years (Required)
Data warehouse: 10 years (Required)
SSIS: 10 years (Required)
Work Location: In person",-1.0,Rivago Infotech Inc,"New York, NY",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
126,2024 Data Engineer I,-1,"2024 Data Engineer I
316114

Textron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.

Responsibilities:
Analyze, manipulate, or process large sets of data using statistical software.
Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces
Identify business problems or management objectives that can be addressed through data analysis.
Document data standards, semantic layer mapping, data flows and conceptual flows.
Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.
Perform statistical analysis to provide recommendations on best courses of action to business leaders.
Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.
Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.
Collaborate with business users to ensure data is available for self-service analytics.
Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.


Qualifications:
Education: Bachelor’s degree in computer science, engineering, mathematics, or similar field required
Years of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience required
Software Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferred
Experience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)
EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

Recruiting Company Textron Specialized Vehicles
Primary Location US-Georgia-Augusta
Job Field Information Technology
Schedule Full-time
Job Level Individual Contributor
Shift First Shift
Job Posting 08/24/2023, 8:20:05 AM",3.8,"Textron Specialized Vehicles
3.8","Augusta, GA",10000+ Employees,1923,Company - Public,Aerospace & Defense,Aerospace & Defense,$10+ billion (USD)
127,Big Data Engineer,Employer Provided Salary:$48.00 - $52.00 Per Hour,"Greetings from KonnectingTree Inc.,
We are actively looking for a candidate with below mentioned experience,
Role : Bigdata Engineer
Location : Hybrid ( Bloomington, MN)
Experience : 3-5 years
Job Description:
* Good experience in build and deployment of Big data applications using Pyspark
* Good experience in Python.
* Experience in Hadoop file structure.
* Good Communication and team player
Job Type: Contract
Salary: $48.00 - $52.00 per hour
Experience level:
3 years
4 years
Schedule:
8 hour shift
Ability to commute/relocate:
Bloomington, MN 55425: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",-1.0,KONNECTINGTREE INC,"Bloomington, MN",-1,-1,-1,-1,-1,-1
128,Senior Data Analytics Engineer,Employer Provided Salary:$75.00 Per Hour,"JOB PROFILE: NCDHHS-Senior Data Analytics Engineer (714485)
JOB LOCATION: 820 s Boylan Ave ,Raleigh, NC, 27603
JOB TYPE: Hybrid
CONTRACT TENNURE: (1 YEAR WITH POSSIBLE EXTENTION)
PAY RATE: $75.00/hr on W2
Candidate must have US Citizenship or Green Card/Permanent Residency in the US to be considered for this position.
Responsibilities:
Provide strategic insights using SAS/SQL, Tableau, and Congas to analyze complex data and assist DHB management in informed decision-making.
Engage directly with clients to understand data needs, translating requirements into effective solutions.
Expertly manage technical aspects, ensuring data quality control and accurate implementation of specifications.
Propose and implement operational healthcare reporting solutions for improved decision-making.
Aggregate, analyze, and report on complex healthcare data from sources like MMIS and claims data.
Develop operational reports and dashboards aligned with healthcare management goals.
Collaborate with staff to assess needs, design solutions, and conduct statistical analysis for insights.
Create advanced analytics based on technical specifications, aiding healthcare program oversight.
Efficiently manage client data requests, ensuring timely delivery and effective communication.
Skills Required:
SQL Proficiency.
Cognos Proficiency.
Tableau Expertise.
Data Quality Control.
Data Aggregation and Analysis.
Technical Specifications.
Local Candidate Consideration.
Job Type: Full-time
Pay: $75.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 5 years (Required)
Tableau: 5 years (Required)
Cognos: 5 years (Required)
data quality control: 5 years (Required)
Data aggregation with MMIS: 5 years (Required)
SAS and SQL coding: 3 years (Required)
technical specifications to develop reports: 5 years (Required)
Work Location: In person",-1.0,"Changing Technologies, Inc.","Raleigh, NC",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
129,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
130,AWS Data Engineer,Employer Provided Salary:$60.00 - $75.00 Per Hour,"Title: AWS Data Engineer
Location: Columbus, OH (Day 1 onsite / Hybrid Model)
Duration: Contract
Job Description:
Data Engineer: Open positions – 2
Skills:
Must Have:
BS/BA degree or equivalent experience
General: Strong organizational, problem-solving, and critical thinking skills; Strong documentation skills
Coding: Proficiency in Java
• Cluster Computing frameworks: Proficiency in Spark and Spark SQL
• AWS Data Services: Proficiency in Lake formation, Glue ETL (or) EMR, S3, Glue Catalog, Athena, Kinesis (or) MSK, Airflow (or) Lambda + Step Functions + Event Bridge
Data De/Serialization: Expertise in at least 2 of the formats: Parquet, Iceberg, AVRO, JSON-LD
AWS Data Security: Good Understanding of security concepts such as: Lake formation, IAM, Service roles, Encryption, KMS, Secrets Manager
Good to Have:
• Linux Scripting, Jenkins
• DevOps: Git, CI/CD, JIRA, TDD
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Ability to commute/relocate:
Columbus, OH 43085: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Required)
Java: 4 years (Required)
Data warehouse, Glue and SparkSQL: 8 years (Required)
Work Location: In person",4.5,"Synkriom Technology Pvt. Ltd.
4.5","Columbus, OH",201 to 500 Employees,2015,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
131,Data center infrastructure Engineer,Employer Provided Salary:$60.00 - $68.00 Per Hour,"Requirements and Responsibilities:
Must have Bachelor’s degree in IS or equivalent.
Must have experience with VMWARE, DATA CENTER OPERATIONS, IT INFRASTRUCTURE, and VIRTUALIZATION.
Experience working with Linux (CentOS/RHEL/Rocky) .
Experience working with VMWare 6.x / 7.x and storage area network (SAN).
Manage and support Microsoft domain / Active Directory Services, RBAC, Group Policy, Windows Servers, desktops and services automate tasks through scripting technologies Support execution of Information
Support and manage the VMWare virtualization and storage (SAN - Dell Extreme IO & Pure Storage) infrastructure to meet and exceed development operations growth expectations.
Job Type: Contract
Pay: $60.00 - $68.00 per hour
Benefits:
Vision insurance
Experience:
Data center operations: 4 years (Required)
IT infrastructure: 3 years (Required)
Work Location: In person",3.0,"BJ Services
3.0","Exton, PA",1001 to 5000 Employees,1872,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",$1 to $5 billion (USD)
132,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
133,Senior Data Engineer,Employer Provided Salary:$87K - $180K,"Oakland, California; Atlanta, Georgia; Irving, Texas
Regular Employee Full-Time
R-2023-08-81

Who We Are:
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners - in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
Pandora
Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. Pandora provides consumers with a uniquely-personalized music and podcast listening experience with its proprietary Music Genome Project® and Podcast Genome Project® technology. Pandora is available through its mobile app, the web, and integrations with more than 2,000 connected products.
How you’ll make an impact:
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What you’ll do:
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What you’ll need:
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to pay attention to details and be organized.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $87,200 to $180,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.

Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-08-81",3.6,"Adswizz
3.6","Oakland, CA",51 to 200 Employees,2007,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
134,Senior Data Engineer,$87K - $114K (Glassdoor est.),"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.

This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration.
How will you make an impact?
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
What are we looking for?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework
You might be a good fit if you like to:
Leverage technology to deliver data solutions to drive business outcomes
Drive for continuous improvement of processes and solutions
Bring energy and commitment to excellence to drive delivery of high-quality solutions
Share knowledge and mentor team members on technologies and solutions
What you can learn on the job:
Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers
Grow practical understanding of agile work practices through application of the scrum framework to deliver work products
Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science
What does success look like?
First 30 days:
Understand the existing analytics data platform and technologies
Understand team’s work management processes
First 6 months:
Develop data solutions for the business including shared components and specific data sets
Learn business processes and develop necessary contacts throughout the organization
First year:
Continue to develop data solutions for the business
Become expert on the team for other data engineers to go to for mentoring in completing their work
Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,"AE Business Solutions
4.2","Milwaukee, WI",51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
135,Apple Music - Data Engineer,-1,"Summary
Posted: Aug 18, 2023
Weekly Hours: 40
Role Number:200496819
This team is more than a group of engineers — it’s a group of music lovers. That passion has made Apple Music the world’s most complete music experience, with over 60 million songs, thousands of playlists, and daily selections from music experts for 115 countries. The team’s data-driven engineers focus relentlessly on the customer experience by running worldwide experiments and analyzing usage and latency, while collaborating with Apple’s product groups. As a result, someone can use the Shazam app to identify an intriguing song in a café in the morning, add it to their playlist from Apple Watch, listen to it through their AirPods on their commute, and share it with their family on HomePod at dinnertime. And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love. Apple Music is a huge part of Apple’s business because it’s a big part of people’s lives.
Key Qualifications
Expertise in designing, implementing and supporting highly scalable data systems and services in Scala
Experience with distributed computing technologies such as Hadoop MapReduce, Spark/Spark-SQL, YARN/MR2
Expertise building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Solid understanding of data-modeling and data-architecture optimized for big data patterns, such as efficient storage and query on HDFS Experience with distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues
Experience with low-latency NoSQL datastores, and traditional relational databases is a plus
Description
We are seeking a hardworking, performance-savvy, engineer to build out the big data platform and services, which power many of these customer features — existing and new. As a core member of the Data Engineering team you will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability. This will include crafting systems to model, ingest, process and compute large-scale, mission-critical data across Apple Music. High-throughput and reliability are essential. The computed datasets are produced for internal reporting used by executives and they are also shared across many teams within the ASE organization, such as the Search team, Recommendation team, AB team, Marketing team. You will also enjoy the benefits of working in a fast growing business where you are encouraged to “Think Different” to solve very interesting technical challenges and where your efforts play a key role in the success of Apple’s business. This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users. Come join us to help deliver the next amazing Apple product!
Education & Experience
BS CS or equivalent work experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $131,500 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",4.2,"Apple
4.2","Seattle, WA",10000+ Employees,1976,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
136,Sr. Azure Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"LeadStack Inc. is an award-winning, one of the nation's fastest-growing, certified minority-owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Position Details:
Title: Azure Data Engineer
Location: Remote
Duration: 6+ Months Contract
Top 3 skills: SQL, Snowflake, and Data Warehousing/Design
Its a W2 Role. No C2C.
Client is seeking a talented and motivated Data Engineer to join our dynamic team. As a Data Engineer, you will play a critical role in developing and maintaining our data infrastructure, ensuring the smooth flow of data for our analytics and insights projects. The ideal candidate will have a strong background in SQL, data warehousing, ETL processes, and a preference for Snowflake experience. Additional experience with dbt or semantic layer tools will be a bonus, and proficiency in Python will also be advantageous.
Bachelor's degree in Computer Science, Engineering, or a related field.
Solid experience with SQL and data warehousing concepts.
Strong understanding of ETL processes and tools.
Preferred experience with Snowflake data warehouse platform.
Bonus for experience with dbt or other semantic layer tools.
Proficiency in Python is a plus.
Excellent problem-solving and analytical skills.
Strong attention to detail and ability to work in a fast-paced environment.
Effective communication skills and ability to collaborate with cross-functional teams.
Key Responsibilities:
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.
Optimize data ingestion and transformation processes to ensure efficient and scalable data flows.
Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.
Develop and maintain data models, schemas, and data dictionaries for efficient data storage and retrieval.
Perform data validation, quality checks, and troubleshooting to identify and resolve data-related issues.
Monitor and optimize database performance, including query tuning and index optimization.
Collaborate with data scientists and analysts to implement data models and support their data requirements.
Stay up-to-date with industry trends and best practices in data engineering, and proactively recommend and implement improvements to our data infrastructure
To know more about current opportunities at LeadStack, please visit us at https://leadstackinc.com/careers/
Should you have any questions, feel free to call me on 510-480-1006 or send an email on tarique.aziz@leadstackinc.com
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Data Warehousing/Design: 8 years (Required)
Snowflake: 5 years (Required)
Azure: 5 years (Required)
Work Location: Remote",4.3,"Leadstack Inc
4.3",Remote,51 to 200 Employees,2016,Company - Private,HR Consulting,Human Resources & Staffing,Unknown / Non-Applicable
137,Data Engineer,Employer Provided Salary:$75.00 Per Hour,"8+ years of experience with Oracle and SQL Server databases
• 8+ years of experience with Informatica Power Center.
8+ years software development experience in one or more of the following areas: ETL, BI and DW
• 8+ years of experience in DW Multi-Dimensional modelling.
Understanding of the Project Delivery Framework and SDLC
Understanding of Waterfall and Agile Project Management Methodology
Strong analytical, communication and interpersonal skills.
Excellent and proven collaboration skills
Job Type: Full-time
Pay: $75.00 per hour
Expected hours: 40 per week
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Monday to Friday
Experience:
Informatica PowerCenter: 5 years (Required)
Multi-Dimensional modelling: 3 years (Required)
Work Location: Hybrid remote in New York, NY 10001",-1.0,Cybotic System,"New York, NY",-1,-1,-1,-1,-1,-1
138,Senior Data Engineer,$115K - $151K (Glassdoor est.),"Senior Data Engineer
Location: Various
Raleigh NC; Atlanta GA; Philadelphia PA; Charlottesville VA; others considered;
Hybrid, Remote, Travel required
Elder Research, Inc. (ERI) is a Data Science consulting firm specialized in providing analytic solutions to clients in Commercial and Government industries. The Commercial business unit is seeking to hire a Senior Data Engineer with strong engineering skills who will provide technical support across multiple project teams by leading, designing, and implementing the software and data architectures necessary to deliver analytics to our clients, as well as providing consulting and training support to client teams in the areas of architecture, data engineering, ML engineering and/or related areas. We are trusted advisors to our clients, building lasting relationships and partnering as preferred analytics providers. We use a variety of programming languages and tools to create analytic solutions, often fitting within our clients’ environment and needs.
We are looking to hire a Senior Data Engineer who can support robust and repeatable data manipulation, large-scale infrastructure for data ingestion, and data visualization for custom client applications. A Senior Data Engineer works collaboratively with Business Analysts, Data Scientists, Data Analysts, other Software/Data Engineers, and business stakeholders to create and deploy dynamic data applications that help our customers make meaningful business decisions.
Some of your responsibilities will include leading and implementing software engineering best practices, mentoring junior engineers, designing software architectures, wrangling and fusing large and disparate data sets, assisting in the deployment of models and algorithms, automating data pipelines in their entirety, and communicating model results through user-focused data visualizations. Candidates should have the ability and the willingness to tailor applications to a clients’ business goals using an iterative methodology. Candidates should also be able to consider both long-term stability and scalability while taking a user-focused approach to development and deployment. Excellent decision making skills are a must.
Desired Skills
Candidates should have a mix of the following experiences:
Set or contributed to data engineering and governance standards
Reviewed code/design/architecture and coached on adjustments (as needed)
Mentored junior engineers
Designed and implemented software architectures
Implemented a data pipeline that includes data transformations such as aggregations, joins, and cleaning.
Ingested data into SQL or NoSQL databases.
Interacted with SQL or NoSQL databases via Python, Java, or similar.
Provided access to transformed data for downstream applications or visualizations via REST APIs or similar.
Worked with visualization software such as Power BI, Tableau, or similar.
Skilled in both applying and teaching others to apply Git (or other distributed version control) for collaboration, record-keeping, and deployment scenarios
Skilled in both using and guiding others in using cloud platforms (AWS, Azure, Google Cloud) for building, deploying, evolving, and maintaining data pipelines
Skilled in both applying and guiding others in applying continuous integration and continuous deployment (CI/CD) techniques for surfacing situations where new work breaks existing functionality
Had exposure to modeling and/or data analysis.
Configured a technical service such as a database, version control system, or operating system.
Led and collaborated with others to accomplish a technical task.
Partnered on the development or deployment of a data strategy
Presented technical work to a non-technical audience via writing or presentation.
Desired Education and Experience
Education:
Master’s in a technical field and 4-5 years’ experience OR
Bachelor’s and 6+ years’ experience
Certifications:
AWS, Azure, or Google Cloud
Minimum Requirements
Bachelors/Master’s degree in Computer Science or related field requiring software development
4+ years’ of professional experience with Python, Java, or similar
Excellent written and verbal communication skills
Ability to parse project requirements, including asking questions to ensure full and correct understanding
Willingness and ability to learn new concepts and tools quickly
Willingness and ability to provide technical support across multiple projects
What You Would Do
Work on small teams in a highly collaborative environment.
Participate in both existing and new projects.
Contribute to consulting projects to solve interesting problems for multiple clients in various industries.
Communicate details of the technical architecture to fellow team members and clients in both technical and non-technical terms.
Work with Data Scientists to design and/or implement the technical architecture necessary to support analytics.
Work with Software/Data Engineers to improve the robustness and scalability of software products.
Participate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance.
Work on documentation of the technical architecture for internal and client use.
Design, create, and provision data stores.
Build trusted and lasting relationships with clients.
Provide value to our clients through analytics, software tools, and leadership.
Manage and monitor pipelines to extract, transform, and load data (ETL).
About Elder Research, Inc.
Headquartered in Charlottesville, VA with offices in Arlington, VA, Baltimore, MD, and Raleigh, NC, Elder Research, Inc. (ERI) is a fast-growing solutions and consulting firm specializing in predictive analytics. At ERI, you’ll be part of a fun, friendly community. In keeping with our entrepreneurial spirit, we want candidates that are self-motivated with an innate curiosity and strong team work ethic. We work hard to provide the best value to our clients and allow each person to contribute their ideas and put their skills to use immediately.
ERI believes in continuous learning; each week the entire company attends a “Tech Talk” and an office lunch. Individuals often share what they have learned at informal bag lunch talks or afternoon chats. ERI provides a supportive work environment with established parental, bereavement, and PTO policies. By prioritizing a healthy work-life balance - with reasonable hours, solid pay, low travel, and flexible time off - ERI enables and encourages its employees to serve others.
ERI provides analytic solutions to hundreds of companies across numerous industries. Our team enjoys great variety in the type of work we do and exposure to a wide range of techniques and tools. If you are passionate about integrating data, technology, and analytics in a team-based environment to solve problems, then ERI may be a good fit for you.",4.5,"Elder Research Inc
4.5","Charlottesville, VA",51 to 200 Employees,1995,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
139,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
140,Data Engineer,$73K - $110K (Glassdoor est.),"Data Engineer

Posting Date: Aug 23, 2023
Location: Houston, TX, US
Lake Charles, LA, US
Company: Sasol
A job at Sasol is a career made by you, with purpose, development opportunities, benefits to support every stage of life and a working culture that embraces flexibility, diversity, and inclusion. Learn more.

Purpose of Job
The Data Engineer works as part of an agile team, interacts with data scientists/translators and industry experts to understand how data needs to be converted, loaded and presented. Collects, structures, analyses data and ensures quality. Responsible for the definition, conception, construction, and ongoing support of big data platforms and solutions.
Recruitment Description / Key Accountabilities
Integrate IT (Information Technology) and OT (Operational Technology) Systems.
Use programming languages and big data technologies following best practices and standards
Understands latest analytics tools and technologies (open source/ proprietary/ vendor supported) and determines fit with current architecture
Building and maintaining data pipelines that move data between various systems.
Ensuring data quality, integrity and security
Monitoring and optimizing data performance
Collaborate with technical teams to develop and maintain a flexible and sustainable workflow.
Collaborate with Product Owner, Data Scientists, Data Translators, and other Business Stakeholders to implement and deploy scalable solutions in an agile way of working; actively participate in scrum process and meetings with team where required
Assist with communication, improve transparency, radiate information, and address challenges effectively
Formal Education
University Bachelors Degree in Computer Science & Inf Systems or Engineering: Computer
Min Experience
Experience: 5+ relevant years
Long Description
Excellent communication and written skills.
Experience in building complex data pipelines using Cloud Services, REST APIs, SQL Databases. Python and SQL skills are are required. Experience in Azure ADF, Synapse or Databricks are beneficial.
Knowlege of Cloud Computing and Infrastructure, Data Models and Standards, Database Administration, Plant Information Management Systems, ERP Systems, Networking and Protocols, Security and Compliance, Change Managemnt, Problem Solving and Trouble Shooting are a plus.

Sasol (USA) Corporation is an Equal Opportunity Employer and gives consideration for employment to qualified applicants without regard to race, color, religion, creed, age, sex, sexual orientation, gender identity, pregnancy, national origin, disability or protected veteran status, as well as any other characteristic protected by applicable law, regulation or local ordinance. For more information about your rights under the law, see http://www.eeoc.gov/employers/upload/poster_screen_reader_optimized.pdf

Sasol treats work-authorized job applicants equally in recruiting and hiring without regard to their citizenship status or national origin unless required by law, regulation, executive order, government contract, or determination by the Attorney General. Please note Sasol will not sponsor, obtain, and/or petition for temporary visa status (for example, E, F-1, H-1, H-2, L, B, J, or TN) for candidates for this position and/or personnel hired for this position.


Nearest Major Market: Houston",3.9,"Sasol
3.9","Houston, TX",10000+ Employees,1950,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$10+ billion (USD)
141,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
142,Senior Data Center Engineer,-1,"Marco is a one-stop shop for all things business tech. Our employees are “movers and shakers” and our company is always striving to do what’s right. Does this sound like a culture you want to be a part of? We’re hiring a new team member to help take Marco’s technology further – working full-time, Monday - Friday, 8am-5pm.
More about us. We do it all – from copy and print solutions to IT and managed services. We are an organization led by salespeople with 650+ engineers ready to fix any and all issues. We have offices in 12 states and service nationally.
Join our growing team. You won’t regret it.

The Senior Data Center Engineer is responsible for providing quality services and solutions to our internal clients while maintaining a high level of client satisfaction. This position works cooperatively with other teams in a coordinated effort to develop, design, implement and support Marco’s Data Centers, along with the IT environment that depends upon them. While not a leadership position, the Senior datacenter engineer displays leadership qualities and may be called upon to lead others in certain situations.

ESSENTIAL FUNCTIONS:
Lead the overall design and implementation of the technology stack within the Marco Data Centers
Maintain physical access to the Marco Data Centers and be a primary contact for essential services
Perform proactive maintenance and reactive support to the equipment within the Marco Data Centers
Create and maintain documentation for all equipment within the Marco Data Center. Review and assist others with their documentation, offering constructive feedback and pointing out deviations from our policies and standards where necessary.
Report on the health of the Marco Data Centers, perform capacity planning and proactively provide recommendations for future growth
Monitor and provide proactive recommendations for backup systems and disaster recovery processes
Review all systems and make technical/process recommendations for improving efficiency
Assist in defining standards for all systems and processes provided by the Data Center Services group
Administer and maintain a reliable, stable, secure and cost-effective hosting environment to deliver 24x7x365 operational service
Provide escalated remote technical support to Marco Data Center clients. Handle requests with due urgency, empathy, and a spirit of cooperation.
Document your work, including troubleshooting steps, time, and resolution in the service ticketing system.
Ensure your work and the work of others adheres to Marco internal policies and applicable regulatory requirements.
Create and maintain standard templates, documentation, images, policy, and automation that will aid in a consistent and rapid deployment of Marco Data Centers
Research, test and recommend new tools and products to enhance the offering and efficiency of Marco’s Data Centers
Document the procedures, infrastructure and environments and keep updated with changes to the environment
Implement and maintain Data Center system builds that are consistent, optimized for client experience, overall performance and supportability
Set deadlines for projects, then organize your time to meet these deadlines; be willing to put in extra time and effort to meet a time-bound goal when necessary
Mentor other staff while you assist with implementation or utilization within your area of technical specialty
Work multiple projects simultaneously and provide consistent, high-quality results
Assist with implementation and or migration of clients’ existing data and applications
Perform system and application testing to provide functional environment
Provide onsite and or remote technical support to Marco clients during onboarding process
Implement changes and work orders for existing Marco clients
Review customer hosted systems and make technical/process recommendations for improving efficiency
Stay up to date with relevant state-of-the-art technology, equipment, and/or systems
QUALIFICATIONS:
Bachelor’s Degree and 4+ years of experience or equivalent experience
Experience and current certifications or specializations aligning with the technologies used in Marco’s Data Center to include Cisco NX-OS, SAN-OS, IOS, UCS-B, ASAv, EMC VNX, Meraki MX, VMWare vSphere, Citrix XenApp, Zscaler and XenDesktop preferred.
Current high level industry recognized certifications including one or more of the following: CCNP, CCE, VCDX, EMCTA
Solid understanding of the administration of network operating systems and services found in a Data Center environment
Benefits:
We’re not just competitive when it comes to business tech – we’re also pretty proud of what we offer our employees. Our benefits include medical, dental, and vision insurance. We also have paid holidays and vacation, 401k with generous company match, flexible spending accounts, employee purchase program, employer-paid life insurance, voluntary-term life insurance, short and long-term disability, critical illness and accident benefits, and pet insurance. Yes, we care about your furry family too.

all benefits are dependent on employment status
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities
Applicant Labor Law Posters",4.0,"Marco Technologies LLC
4.0",Remote,1001 to 5000 Employees,1973,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
143,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
144,Cloud Data Engineer,Employer Provided Salary:$55.00 - $65.00 Per Hour,"Note: This is a W2 - contract to hire role
Azure Data Engineer
12+ months (contract to hire)
100% remote

Job Description:
As a Senior Engineer - DBA, the Cloud Data Engineer candidate will be responsible for using the candidate’s technical knowledge to solve business problems.
The client is looking for a talented individual who can serve as a subject matter expert in their area of focus and represent their department on complex assignments.
Candidates will be responsible for evaluating elements of technology effectiveness through requirements gathering, testing, research, and investigation and making recommendations for improvements that result in increased quality and effectiveness.
Candidates will be required to listen to and evaluate customer needs to determine and provide high-quality solutions that align with customer expectations.
In this role on the Cloud Data Engineering team, the candidate will be a part of a team responsible for migrating existing data platform solutions to the cloud.
Candidates will also collaborate with others to develop new data pipelines.
As a data engineer in the Cloud Data Engineering team, the candidate must work in a data-first mindset.
The candidate will use Microsoft Azure Cloud PaaS technologies to engineer client data solutions in a way that allows us to optimize client information, make better decisions, and meet client customer’s needs.
If the Candidate has a passion for engineering, working with massive amounts of data, and empowering smart business decisions, this is the role for you.
Equal Opportunity Employer/Disability/Veterans

Required Qualifications:
Must be presently authorized to work in the U.S. without a requirement for work authorization sponsorship by the company for this position now or
in the future
Must be committed to incorporating security into all decisions and daily job responsibilities
3 plus of related experience in Cosmos DB/Similar DB technology
Experience with configuration management and building automation capabilities such as Git/Jenkins/Bitbucket
Experience with Microsoft Azure Cloud workflows
Experience in T-SQL and scripting skills.
Knowledge of Microsoft cloud-managed DBs/Systems, e.g. Managed SQL Instance, Cosmos DB, Databricks Delta Lake
Experience with Big Data solutions such as Delta Lake by Databricks and SQL DBMSs
Independently analyze, solve, and correct issues in real-time, providing problem resolution end-to-end
Identify new opportunities and help refine automation of regular processes, track issues, and document changes
Solve/Assist in complex query tuning and schema refinement
Expert in troubleshooting performance issues
Experience rightsizing Database object workflow for cost management
Ability to multi-task and context-switch effectively between different activities and teams
Join the on-call rotation with other Engineers
Must be able to both collaborate in a team-oriented environment and work independently with direction
Must be able to work in a fast-paced environment with the ability to handle multiple tasks

Pay Range: $55/hr - $65/hr
The specific compensation for this position will be determined by a number of factors, including the scope, complexity and location of the role as well as the cost of labor in the market; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. Our full-time consultants have access to benefits including medical, dental, vision and 401K contributions as well as any other PTO, sick leave, and other benefits mandated by appliable state or localities where you reside or work.",3.7,"Pinnacle Technical Resources
3.7",Missouri,1 to 50 Employees,-1,Company - Public,-1,-1,$1 to $5 million (USD)
145,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
146,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
147,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1.0,Radiant System,Remote,-1,-1,-1,-1,-1,-1
148,Energy Data Engineer - Mid Level,$81K - $121K (Glassdoor est.),"About the Organization
Now is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.

The Data Engineer will support Redhorse in its execution of specific task areas related to the Federal Government Carbon Pollution Free Electricity (CFE) Technical and Strategy Support contract. The program will establish technical frameworks and analytical tools and methods to inform CFE procurement decisions for the entirety of the federal government. Work will be conducted primarily at Redhorse office locations (hybrid remote work) and on government client site as required. The ideal candidate will live in the DC/Virginia/Maryland area and willing to undergo a background check to obtain a Public Trust.
Primary Duties and Responsibilities for this position include
Review and analyze the US Federal Government Executive Agencies electric load data
Develop and test modeling techniques to understand an agency’s hourly load profile to inform the agency’s hourly CFE procurement requirement as an off taker from a new CFE technology source.
Support the configuration and ingestion of designated structured, unstructured, and semi-structured data repositories into capabilities that satisfy client requirements and support a data analytics pipeline to drive rapid delivery of functionality to the client.
Maintain all operational aspects of data transfers while accounting for the security posture of the underlying infrastructure and the systems and applications that are supported and monitoring the health of the environment through a variety of health tracking capabilities.
Automate configuration management, leverage tools, and stay current on data extract, transfer, and load (ETL) technologies and services.
Work under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.
Apply comprehension of data engineering-specific technologies and services, leverage expertise in databases and a variety of approaches to structuring and retrieving of data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically using vendor consoles.
Engage with multiple functional groups to comprehend client challenges, prototype new ideas and new technologies, help to create solutions to drive the next wave of innovation, and design, implement, schedule, test, and deploy full features and components of solutions.
Identify and implement scalable and efficient coding solutions.
Minimum Requirements for skills, Experience, and Credentials
Master’s Degree in Engineering, Computer Science, Mathematics or Statistics
Four (4) years of experience in enterprise data analysis with a preferred focus on utility data, to include advanced metering infrastructure (AMI) electricity data
Local to Washington D.C. metro area preferred, remote work is an option
Willingness to travel in support of on-site customer meetings
Willingness to obtain a Public Trust
Desired Basic Requirements for Skills, Experience, and Credentials include
Knowledge of U.S. Federal Government Energy Legislation and Executive Orders (especially EO 14057)
Experience in or knowledge of utility interval data and electricity pricing
Executive Order Requirement:
While currently on HOLD, Redhorse recognizes that we may need to comply with Executive Order 14042 which requires Federal Contractors to ensure all U.S. new hires be fully vaccinated for COVID-19. As required by the Executive Order, Redhorse will work in coordination with applicable contract agencies to consider requests for Reasonable Accommodations.
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.
EOE/M/F/Vet/Disabled",3.5,"Redhorse
3.5","Arlington, VA",51 to 200 Employees,2008,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
149,Data Engineer II,$90K - $119K (Glassdoor est.),"Data Engineer II
Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What’s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.
Join us to do the best work of your career and make a profound social impact as a Data Engineer on our Growth Marketing team in Round Rock, Texas, Hopkinton, Massachusetts, or Remote United States
What you’ll achieve
As a Data Engineer II on a growing team, you will bring in your industry experience to develop, deploy, and maintain productionized data pipelines that support the training and productionization of machine-learning models.
You will:
Build, test, and implement data pipelines using tools like Apache Airflow, Spark, and Python for the training and inferencing of machine-learning models
Ensure data quality, monitor data pipelines, and troubleshoot production issues
Document metadata, data models, data pipelines and APIs to help data science teams understand and leverage data assets.
Work with data scientists and business analysts to understand data requirements
Take the first step toward your dream career
Every Dell Technologies team member brings something unique to the table. Here’s what we are looking for in this role:
Essential Requirements
U.S. citizen residing in the U.S.
Bachelor’s Degree in Computer Science, Engineering, IT, or equivalent professional experience
1 to 2 years experience developing production-quality Python and performant SQL, working with data warehouses, JSON, Apache Hive, Hadoop or Spark, or related technologies
1 to 2 years experience with orchestration tools, such as Apache AirFlow
Experience with monitoring tools and observability
Who we are

We believe that each of us has the power to make an impact. That’s why we put our team members at the center of everything we do. If you’re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we’re looking for you.

Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us.

Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. Read the full Equal Employment Opportunity Policy here.
Job ID:R227305

Dell’s Flexible & Hybrid Work Culture

At Dell Technologies, we believe our best work is done when flexibility is offered.

We know that freedom and flexibility are crucial to all our employees no matter where you are located and our flexible and hybrid work style allows team members to have the freedom to ideate, be innovative, and drive results their way. To learn more about our work culture, please visit our locations page.",4.2,"Dell Technologies
4.2","Round Rock, TX",10000+ Employees,1984,Company - Public,Information Technology Support Services,Information Technology,$10+ billion (USD)
150,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
151,Senior Data Center Engineer,-1,"Marco is a one-stop shop for all things business tech. Our employees are “movers and shakers” and our company is always striving to do what’s right. Does this sound like a culture you want to be a part of? We’re hiring a new team member to help take Marco’s technology further – working full-time, Monday - Friday, 8am-5pm.
More about us. We do it all – from copy and print solutions to IT and managed services. We are an organization led by salespeople with 650+ engineers ready to fix any and all issues. We have offices in 12 states and service nationally.
Join our growing team. You won’t regret it.

The Senior Data Center Engineer is responsible for providing quality services and solutions to our internal clients while maintaining a high level of client satisfaction. This position works cooperatively with other teams in a coordinated effort to develop, design, implement and support Marco’s Data Centers, along with the IT environment that depends upon them. While not a leadership position, the Senior datacenter engineer displays leadership qualities and may be called upon to lead others in certain situations.

ESSENTIAL FUNCTIONS:
Lead the overall design and implementation of the technology stack within the Marco Data Centers
Maintain physical access to the Marco Data Centers and be a primary contact for essential services
Perform proactive maintenance and reactive support to the equipment within the Marco Data Centers
Create and maintain documentation for all equipment within the Marco Data Center. Review and assist others with their documentation, offering constructive feedback and pointing out deviations from our policies and standards where necessary.
Report on the health of the Marco Data Centers, perform capacity planning and proactively provide recommendations for future growth
Monitor and provide proactive recommendations for backup systems and disaster recovery processes
Review all systems and make technical/process recommendations for improving efficiency
Assist in defining standards for all systems and processes provided by the Data Center Services group
Administer and maintain a reliable, stable, secure and cost-effective hosting environment to deliver 24x7x365 operational service
Provide escalated remote technical support to Marco Data Center clients. Handle requests with due urgency, empathy, and a spirit of cooperation.
Document your work, including troubleshooting steps, time, and resolution in the service ticketing system.
Ensure your work and the work of others adheres to Marco internal policies and applicable regulatory requirements.
Create and maintain standard templates, documentation, images, policy, and automation that will aid in a consistent and rapid deployment of Marco Data Centers
Research, test and recommend new tools and products to enhance the offering and efficiency of Marco’s Data Centers
Document the procedures, infrastructure and environments and keep updated with changes to the environment
Implement and maintain Data Center system builds that are consistent, optimized for client experience, overall performance and supportability
Set deadlines for projects, then organize your time to meet these deadlines; be willing to put in extra time and effort to meet a time-bound goal when necessary
Mentor other staff while you assist with implementation or utilization within your area of technical specialty
Work multiple projects simultaneously and provide consistent, high-quality results
Assist with implementation and or migration of clients’ existing data and applications
Perform system and application testing to provide functional environment
Provide onsite and or remote technical support to Marco clients during onboarding process
Implement changes and work orders for existing Marco clients
Review customer hosted systems and make technical/process recommendations for improving efficiency
Stay up to date with relevant state-of-the-art technology, equipment, and/or systems
QUALIFICATIONS:
Bachelor’s Degree and 4+ years of experience or equivalent experience
Experience and current certifications or specializations aligning with the technologies used in Marco’s Data Center to include Cisco NX-OS, SAN-OS, IOS, UCS-B, ASAv, EMC VNX, Meraki MX, VMWare vSphere, Citrix XenApp, Zscaler and XenDesktop preferred.
Current high level industry recognized certifications including one or more of the following: CCNP, CCE, VCDX, EMCTA
Solid understanding of the administration of network operating systems and services found in a Data Center environment
Benefits:
We’re not just competitive when it comes to business tech – we’re also pretty proud of what we offer our employees. Our benefits include medical, dental, and vision insurance. We also have paid holidays and vacation, 401k with generous company match, flexible spending accounts, employee purchase program, employer-paid life insurance, voluntary-term life insurance, short and long-term disability, critical illness and accident benefits, and pet insurance. Yes, we care about your furry family too.

all benefits are dependent on employment status
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities
Applicant Labor Law Posters",4.0,"Marco Technologies LLC
4.0",Remote,1001 to 5000 Employees,1973,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
152,Data Engineer (multiple openings) - IHM,Employer Provided Salary:$88K - $157K,"Discover. A brighter future.
With us, you’ll do meaningful work from Day 1. Our collaborative culture is built on three core behaviors: We Play to Win, We Get Better Every Day & We Succeed Together. And we mean it — we want you to grow and make a difference at one of the world's leading digital banking and payments companies. We value what makes you unique so that you have an opportunity to shine.

Come build your future, while being the reason millions of people find a brighter financial future with Discover.

Job Description:
Employer: DFS Corporate Services LLC

Job Title: Data Engineer (multiple openings)

Job Location: Riverwoods, Illinois

Job Type: Full Time

Duties: Responsible for designing, developing, testing, and maintaining complex data solutions for the product. Mentor and influence peers to achieve commitments on data solutions on time and with quality. Telecommuting and/or working from home may be permissible pursuant to company policies.

Requirements: Employer will accept a Bachelor's degree in Computer Science, Computer Engineering, or a related field and 3 years of experience in Senior Data Engineer; ODI/DW/Batch Developer or related occupation.

Position required skills: Three (3) years of experience in the job offered or related occupation: working with Agile software development methodology and Agile management tools, including Jira; utilizing modern engineering practices, including the design and development of ETL data integration solutions in data warehouse environments; working with relational databases and Cloud-based technologies; utilizing design and architecture experience on large-scale ETL solutions; and working with Enterprise Data warehouse data models and dimensional modeling concepts, source to target mapping, and data integration architecture.

Position eligible for incentives under Employee Referral Program.

Rate of Pay: The base pay for this position generally ranges between $88,150.00 to $157,000.00. Additional incentives may be provided as part of a market competitive total compensation package. Factors, such as but not limited to, geographical location, relevant experience, education, and skill level may impact the pay for this position. We also offer a range of benefits and programs based on eligibility. Learn more at MyDiscoverBenefits.com .

QUALIFIED APPLICANTS: Please apply directly through our website by clicking on “Apply Now.” No calls. Equal Opportunity Employer/disability/vet.

What are you waiting for? Apply today!

All Discover employees place our customers at the very center of our work. To deliver on our promises to our customers, each of us contribute every day to a culture that values compliance and risk management.

Discover is committed to a diverse and inclusive workplace. Discover is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or other legally protected status. (Know Your Rights)",3.9,"Discover Financial Services
3.9","Riverwoods, IL",10000+ Employees,1985,Company - Public,Banking & Lending,Financial Services,$1 to $5 billion (USD)
153,"Staff Engineer, Data Infrastructure",Employer Provided Salary:$180K - $240K,"The Mission:
This is a critical and exciting time at Enigma. We're transforming the small business financing ecosystem, and our product is gaining adoption even faster than we anticipated. This creates a new set of technical challenges for us as we continue to scale.
Over the past year we've made significant investments in our data infrastructure to allow engineers and data scientists to quickly deliver customer value by reliably testing and shipping changes to our data pipeline. The challenges ahead are to:
Reduce overall runtime of our data pipeline
Improve computational efficiency to reduce costs
The Role:
This is a role responsible for developing and implementing our strategy for driving data processing efficiency. Your impact will be measured by:
The increase in the team's capability to optimize data transformations (combination of methodology and tooling)
The specific improvements we achieve in processing time and computational cost
This role has an important hands-on component - you'll be showing the team what good looks like. However, your greatest impact will be in the technical strategy you'll formulate and the close coaching and mentorship you'll provide to team members.
We're looking for someone who:
Is motivated by leading engineers to increase the efficiency and speed of complicated data processing systems
Thrives as a coach and mentor and measures their impact by the increase in capabilities of the team
Operates with a bias for action and knows how to deliver value in the short, medium and long term
Adopts a principled approach to difficult data processing problems and demonstrates expertise and excellence in their engineering craft
Operates transparently, collaboratively and with low ego—loves learning from others and having their ideas questioned and challenged
What makes this job interesting?
Impact: Your decisions will determine the speed at which we can scale to take on new customers and impact the developer experience of dozens of teammates.
Technical Challenge: You will take on some of our thorniest technical problems and render them tractable.
Leadership: Success in this role will achieved through building the team's capabilities and influencing our engineering culture.
Our ideal candidate:
Knows Databricks and Spark inside and out
Brings a clear point of view on data processing optimization, data modeling, cluster management and Databricks performance techniques
Has a strong track record of technical leadership to grow and scale teams
About Us:
At Enigma, we're building the single, most reliable source of data on businesses to power the future of financial services. By engineering better data from hundreds of public and third-party sources, we aim to tell the complete story of every business, so that companies of every size can access the financial services they need to grow and thrive. Our core values – generosity, curiosity, ingenuity, & drive – guide everything we do, from how we make our most important product decisions to how we work with and support one another on a daily basis. We're a team of curious, driven individuals with diverse backgrounds and skills, but we're all passionate about engineering deeper understanding through data—together. If this resonates, we would love to hear from you!
We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.
Salary Range: $180,000-$240,000
A note on salary ranges: we pride ourselves on paying competitively for our size and industry. Salary is one piece of a total Enigma compensation package that includes additional benefits and opportunities. All of our compensation packages include equity because we believe 100% of Enigma employees should have the option to purchase ownership in the company and benefit from the value we're creating together.",-1.0,Enigma,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
154,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
155,USA - Infrastructure Data Engineer (AWS),$104K - $149K (Glassdoor est.),"Job Title: Infrastructure Data Engineer (AWS)
Location: San Jose, California, United States
Type: Fulltime

The candidate has to be in San Jose, CA and it is 100% onsite job”.

Roles & Responsibilities:
Design, build and maintain data platform infrastructure on AWS environment.
Oversee design, build, and maintain data platform infrastructure on AWS environment.
Develop data pipelines to collect the metrics that is required to monitor data refreshes, reports deliveries and track SLAs.
Work with the DWH development team and business users in establishing SLAs for data refreshes and reports.
Build continuous integration/deployment (CI/CD) pipelines to accelerate development and improve team agility.
Oversee project.
Monitor all aspects of data platform system security, performance, storage, incidents, and usage for databases, data pipelines, applications, and infrastructure on AWS. Escalate to respective teams for fixes.
Ensure data pipelines meet intraday and daily SLAs, as per documented SLA definitions and escalate accordingly.
develop appropriate instrumentation to collect metrics on system performance, cost, data ingress/egress /storage processes.
Have a clear understanding of the reports/analyses/insights to be driven by data and build data driven solutions to optimally support the operational analytics needs.
Documents user stories, epics, and reports
Coordinate infrastructure enhancements and maintenance with the system/network engineering teams
Work with DWH development team and analytics team to do manual releases where required.
Onboard users to data analytics systems with appropriate approvals
Conduct system performance tests and collect metrics. Tune/add capacity.
Complete knowledge management processes
Own strategy and communicate potential major shifts in expected workload based on business, market, or operational changes.
Operate ongoing business relationship management sessions to review operational metrics, understand pain points, identify upcoming projects and engagement efforts.",-1.0,Avestacs,"San Jose, CA",51 to 200 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
156,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
157,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
158,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
159,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
160,"Staff Software Engineer, Data Infrastructure",Employer Provided Salary:$190K - $230K,"At Runway, we believe everyone has a story to tell. Our mission is to make professional video and content creation accessible to all. We are taking recent advancements in computer graphics, the web, and machine learning to push the boundaries of creativity and in turn, lower the barriers of content creation; unfastening a new wave of storytelling
Over the last four years, we’ve raised funding from top-tier investors and partners including Amplify, Coatue, Compound, Felicis, Google, Lux, NVIDIA, and Salesforce. Our team consists of creative, open minded, caring, and entrepreneurial individuals from all walks of life. We aspire to build incredible things which starts with building an incredible team, so we’d love to hear from you!
About the role
We’re looking for a strong Staff Software Engineer to help us build scalable and robust data infrastructure to power Runway’s applied research. You will be the first hire in a new team that’s focused on building data pipelines to support all of Runway’s generative model training and continuous model improvement. The best fit for this role has experience with building large-scale systems for analysis, curation, and retrieval of multimodal data, as well as strong system design and collaboration skills.
What you’ll do
Build out pipelines for the creation, curation, and processing of large-scale multimodal datasets
Create internal tooling to enable Runway’s applied research team to build workflows and run experiments on that infrastructure
What you’ll need
Strong knowledge of Python
Experience with one or more frameworks for large-scale data processing (e.g. Spark, Ray, etc) and one or more ML frameworks (e.g. PyTorch, JAX)
Experience building out data pipelines from scratch and optimizing those pipelines for high performance and great developer experience
Familiarity and experience with AWS and/or GCP is a bonus
Runway strives to recruit and retain exceptional talent from diverse backgrounds while ensuring pay equity for our team. Our salary ranges are based on competitive market rates for our size, stage and industry, and salary is just one part of the overall compensation package we provide.
There are many factors that go into salary determinations, including relevant experience, skill level and qualifications assessed during the interview process, and maintaining internal equity with peers on the team. The range shared below is a general expectation for the function as posted, but we are also open to considering candidates who may be more or less experienced than outlined in the job description. In this case, we will communicate any updates in the expected salary range.
Lastly, the provided range is the expected salary for candidates in the U.S. Outside of those regions, there may be a change in the range, which again, will be communicated to candidates.
Salary range: $190,000-230,000

Working at Runway
We are a small and growing team of artists, engineers, researchers, and dreamers working together to reimagine creativity. And we’re building a unique team of talented individuals from diverse backgrounds. We believe that this will allow us to continue to up-level each other, our company, and our product. We’re looking for people that will add to our culture, not just fit in.
We’re committed to creating a space where our employees can bring their full selves to work and have equal opportunity to succeed. So regardless of race, gender identity or expression, sexual orientation, religion, origin, ability, age, veteran status, if joining this mission speaks to you, we encourage you to apply!",5.0,"Runway AI
5.0",Remote,51 to 200 Employees,2018,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
161,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
162,Cloud Data Engineer,Employer Provided Salary:$55.00 - $65.00 Per Hour,"Note: This is a W2 - contract to hire role
Azure Data Engineer
12+ months (contract to hire)
100% remote

Job Description:
As a Senior Engineer - DBA, the Cloud Data Engineer candidate will be responsible for using the candidate’s technical knowledge to solve business problems.
The client is looking for a talented individual who can serve as a subject matter expert in their area of focus and represent their department on complex assignments.
Candidates will be responsible for evaluating elements of technology effectiveness through requirements gathering, testing, research, and investigation and making recommendations for improvements that result in increased quality and effectiveness.
Candidates will be required to listen to and evaluate customer needs to determine and provide high-quality solutions that align with customer expectations.
In this role on the Cloud Data Engineering team, the candidate will be a part of a team responsible for migrating existing data platform solutions to the cloud.
Candidates will also collaborate with others to develop new data pipelines.
As a data engineer in the Cloud Data Engineering team, the candidate must work in a data-first mindset.
The candidate will use Microsoft Azure Cloud PaaS technologies to engineer client data solutions in a way that allows us to optimize client information, make better decisions, and meet client customer’s needs.
If the Candidate has a passion for engineering, working with massive amounts of data, and empowering smart business decisions, this is the role for you.
Equal Opportunity Employer/Disability/Veterans

Required Qualifications:
Must be presently authorized to work in the U.S. without a requirement for work authorization sponsorship by the company for this position now or
in the future
Must be committed to incorporating security into all decisions and daily job responsibilities
3 plus of related experience in Cosmos DB/Similar DB technology
Experience with configuration management and building automation capabilities such as Git/Jenkins/Bitbucket
Experience with Microsoft Azure Cloud workflows
Experience in T-SQL and scripting skills.
Knowledge of Microsoft cloud-managed DBs/Systems, e.g. Managed SQL Instance, Cosmos DB, Databricks Delta Lake
Experience with Big Data solutions such as Delta Lake by Databricks and SQL DBMSs
Independently analyze, solve, and correct issues in real-time, providing problem resolution end-to-end
Identify new opportunities and help refine automation of regular processes, track issues, and document changes
Solve/Assist in complex query tuning and schema refinement
Expert in troubleshooting performance issues
Experience rightsizing Database object workflow for cost management
Ability to multi-task and context-switch effectively between different activities and teams
Join the on-call rotation with other Engineers
Must be able to both collaborate in a team-oriented environment and work independently with direction
Must be able to work in a fast-paced environment with the ability to handle multiple tasks

Pay Range: $55/hr - $65/hr
The specific compensation for this position will be determined by a number of factors, including the scope, complexity and location of the role as well as the cost of labor in the market; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. Our full-time consultants have access to benefits including medical, dental, vision and 401K contributions as well as any other PTO, sick leave, and other benefits mandated by appliable state or localities where you reside or work.",3.7,"Pinnacle Technical Resources
3.7",Missouri,1 to 50 Employees,-1,Company - Public,-1,-1,$1 to $5 million (USD)
163,"Software Engineer, Infrastructure Data",-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
Software Engineer, Infrastructure Data
As an Infrastructure Data Engineer, you are a full stack data engineer that loves solving business problems with data. You work with key business and product leads, analysts and data scientists to understand the business domain and how data can empower them. You engage with fellow engineers to develop better data platforms to make the process of producing data and deriving insights easy and efficient. You are passionate about the quality of the data you produce and take pride in having your data drive our business.
What you'll do:
Understand the business drivers and analytical use-cases and translate these to data products
Explore new technologies and learn new techniques to solve business problems creatively
Think big and drive the strategy for better data quality within Pinterest
Design, implement and maintain pipelines that produce business critical data reliably and efficiently using cloud technology
Become the voice of business within engineering, and of engineering within business
Create data visualizations that allow easy consumption of the data learnings and insights
Collaborate with many teams from Product, Engineering and Business to produce relevant data solutions that can be used across multiple use cases
What we're looking for:
2+ years of experience with big data (Presto/Trino, Spark, Snowflake, Databricks).
Proficiency with scripting language (Python) and data visualization (Tableau, Superset) technologies
Hands-on experience in principled data warehouse design, data visualization and data pipeline design and development
Prior experience working with business stakeholders in the large-scale cloud infrastructure space, preferably in cost attribution
Great collaboration and communication skills
Experience in working independently and driving projects end to end
Strong analytical skills
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AG8
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$96,900—$200,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.

Not Specified
0",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
164,Lead Data Engineer - CCRD,Employer Provided Salary:$76K - $152K,"Make banking a Fifth Third better®

We connect great people to great opportunities. Are you ready to take the next step? Discover a career in banking at Fifth Third Bank.
GENERAL FUNCTION:
The data engineer designs and builds platforms, tools, and solutions that help the bank manage, secure, and generate value from its data. The person in this role creates scalable and reusable solutions for gathering, collecting, storing, processing, and serving data on both small and very large (i.e. Big Data) scales. These solutions can include on-premise and cloud-based data platforms, and solutions in any of the following domains ETL, business intelligence, analytics, persistence (relational, NoSQL, data lakes), search, messaging, data warehousing, stream processing, and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions, elevating concerns, and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues, and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite, achieves results by consistently identifying, assessing, managing, monitoring, and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design, Development, and Support of data solutions, APIs, tools, and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams, Enterprise architecture, infrastructure, information security, and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design, performance, integration, security, etc.
Conduct research and Development based on current trends and technologies related to the banking industry, data engineering and architecture, data security, and related topics.
Work with developers to Build CI/CD pipelines, Self-service Build tools, and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed, including legacy system replacement, Monitoring and analytics improvements, tool Development, and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE, SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management, business and IT sponsors, and technical resources in language that is appropriate for each group.
At least 6 years of related experience, including at least 4 years in a hands-on software development role.
Significant experience with at least one major RDBMS product.
Experience working with and supporting Unix/Linux and Windows systems.
Proficient in relational database modeling concepts and techniques.
Solid conceptual understanding of distributed computing principles.
Working knowledge of application and data security concepts, best practices, and common vulnerabilities.
Experience in one or more of the following disciplines preferred: big data technologies and distributions, metadata management products, commercial ETL tools, Bi and reporting tools, messaging systems, data warehousing, Java (language and run time environment), major version control systems, continuous integration/delivery tools, infrastructure automation and virtualization tools, major cloud, or rest API design and development.
Previous experience working with offshore teams desired.
Financial industry experience is a plus.
Lead Data Engineer - CCRD
Total Base Pay Range 76,000.00 - 152,400.00 USD Annual
LOCATION - Virtual, Florida 00000
Fifth Third Bank, National Association is proud to have an engaged and inclusive culture and to promote and ensure equal employment opportunity in all employment decisions regardless of race, color, gender, national origin, religion, age, disability, sexual orientation, gender identity, military status, veteran status or any other legally protected status.",3.7,"Fifth Third Bank
3.7",Remote,10000+ Employees,1989,Company - Public,Investment & Asset Management,Financial Services,$5 to $10 billion (USD)
165,Senior Data Engineer,Employer Provided Salary:$87K - $180K,"Oakland, California; Atlanta, Georgia; Irving, Texas
Regular Employee Full-Time
R-2023-08-81

Who We Are:
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners - in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
Pandora
Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. Pandora provides consumers with a uniquely-personalized music and podcast listening experience with its proprietary Music Genome Project® and Podcast Genome Project® technology. Pandora is available through its mobile app, the web, and integrations with more than 2,000 connected products.
How you’ll make an impact:
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What you’ll do:
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What you’ll need:
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to pay attention to details and be organized.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $87,200 to $180,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.

Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-08-81",3.6,"Adswizz
3.6","Oakland, CA",51 to 200 Employees,2007,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
166,Data Engineer,-1,"Hummingbird is a remote-first, fully distributed team united by the shared mission of helping fight financial crime. Since our launch in 2017, we’ve helped major financial institutions and tech-savvy trailblazers alike (e.g. Stripe, Affirm, etc.) orchestrate their compliance programs through our thoughtfully designed, intuitive SaaS product. We believe finding and stopping financial crime is a problem rooted in code, language and design, so we built the product that the heroes doing this work deserve.
We are customer-obsessed, and we love building and shipping great products. We set a high bar, challenge our assumptions, seek diverse opinions, and support each other to do our best work.
We do our best to write inclusive, descriptive and accurate job descriptions, but we’re not always perfect. If you’re interested in the role, we’d love to hear from you even if you don’t feel like you meet everything we’re looking for. We’re always iterating and improving, and it’s possible that your experience is even more impactful than we could have imagined.

About the Role
We are looking for a driven data engineer to join our team and champion the use of data at Hummingbird. Data plays a crucial role in our mission to fight financial crime and you will help us find new and innovative ways to leverage it to provide powerful tools for our users, and allow us to better understand the usage of our product. Hummingbird is uniquely positioned at the intersection of financial technology, security, policy, and law enforcement and as such we have built up a one of a kind data set that we can now use to give our customers the edge in their efforts to stay ahead of criminals. As an employee at a small startup, you will have the opportunity to wear many hats, working from product discovery through implementation.
What you’re looking to do:
Level up our use of data to make better decisions, build more powerful features, and fight financial crime
Leverage Infrastructure as Code (IaC) to manage and deploy infrastructure that supports a variety of different projects, such as data replication and orchestration for machine learning workflows
Build new data pipelines for ingesting data into our data warehouse via both batch and streaming architectures
Work closely with data science to enable us to build products that benefit our customers while keeping compliance and security at the forefront
Achieve goals through a combination of independent building, educating your peers, and influencing others to contribute towards your vision
What we’re looking for:
A data engineer with a history of taking projects from the earliest stages through successful rollout to production
Someone who is excited by the prospect of pioneering data as a practice at a fast growing startup and who is unafraid to dig in to discover what is possible
A flexible self starter that will cut across organizational lines to understand the business and identify the most valuable work
An engineer who brings a pragmatic approach to problem solving, favoring simplicity and shortening delivery cycles
Experience building data pipelines for sensitive data, including best practices for de-identification and data security
Experience deploying infrastructure via terraform or a similar infrastructure as code tool
Expertise in SQL and one or more programming languages, especially python
What’s in it for you:
The chance to help build from the ground up. The hires we’re making now are foundational to our growth as a company, so you will have an opportunity to help shape the future of Hummingbird.
Competitive compensation including cash and equity.
Remote-first, fully distributed company with flexible working hours.
Awesome health, vision & dental benefits, and 401k.
Safe, respectful & comfortable work environment with colleagues and leadership who prioritize diversity, equity, inclusion, and belonging.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please don't hesitate to contact us to request accommodation.",-1.0,Hummingbird,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
167,Senior Data Engineer,-1,"CoreTrust is the market leading commercial Group Purchasing Organization (GPO), leveraging the combined purchasing volume of its 3,000+ members to negotiate preferential pricing and terms across more than 80 indirect spend categories. Strategically aligned with private equity portfolios and large independent companies, we complement sourcing bandwidth and improve supply chain efforts with our industry-leading national contracts.
Recently acquired by Blackstone Private Equity, CoreTrust is growing rapidly and we’re looking for a passionate Senior Data Engineer.
Reporting to the Director of Data, you will be part of the data solutions team and be responsible for building our data platforms, enabling the use of advanced analytics to drive continued evolution and growth.
You will create and manage data pipelines to feed and curate our data lake solution and help develop our data roadmap. The ideal candidate has a great understanding of various data / tech solutions (e.g., data modeling tools, data pipeline, data catalogs, cloud databases) and a record of using them to bring tangible dollar impact. You should be excited to seek out and capitalize on a wide variety of opportunities to use data to create value across the organization.

Responsibilities
Lead data projects to build innovative and highly available solutions while ensuring adherence to budget, schedule, and scope of project
Mentor other members in the data solutions team
Develop and assist with oversight on the data tech infrastructure
Drive data & analytics solutions from conception to deployment/delivery with clear ROI impact
Develop and maintain relationships with all relevant business and tech stakeholders and functions
Provide input to proposals for assigned projects including project objectives, technologies, systems, information specifications, timelines, and staffing
Communicate timely status updates to affected internal or external customers and stakeholders
Collect, analyze, and summarize information and trends as needed to prepare project status reports
Assist in developing a culture of data-driven decision-making, including adoption of business intelligence, analysis, and advanced analytics globally
Perform other related duties as assigned

Qualifications
Bachelor’s degree in computer or information science or relevant experience
9+ years of relevant experience in a data-driven professional setting
Ability to assist with the vision of the team (e.g., mission, priorities, engagement model, tooling)
A record of accomplishment of successfully managing complex cross-functional projects under tight deadlines
Strong technical background – familiarity with Python, SQL, cloud technologies like Azure and AWS, statistics / machine learning, Snowflake, DBT, FiveTran, Data Visualization Software
Exceptional communication and presentation skills, particularly in the context of engaging senior management teams
A successful history of manipulating, processing, and extracting value from large, disconnected datasets
Build processes supporting data transformation, data structures, metadata, dependency, and workload management
Working knowledge of creating and leveraging API or Stream based data extraction processes such as Salesforce API
Strong command of databases and SQL
Proficiency with Python or R, especially for data manipulation and analysis, and ability to build, maintain and deploy sequences of automated processes with these tools
Ability to motivate groups of people to complete a project in a timely manner
Excellent analytical, logical thinking, and problem-solving skills
Thorough understanding of project management principles and planning
Thorough understanding of information technology procedures and practices
Proficient with, or able to quickly become proficient with, a range of general and specialized applications, software, and hardware used in the organization and the industry

Benefits
Competitive compensation package
Free individual employee medical coverage
Company subsidized dental and vision coverage
Dollar for dollar 401(k) match up to 6% of your salary with immediate vesting
Company-paid Short-Term and Long-Term Disability coverage
Employee Assistance Program to support your wellbeing and mental health
$1500 annual stipend for undergraduate/graduate college courses; $500 annual stipend for continuing education courses/certifications
Free snacks and beverages on-site
Brand new, state-of-the-art, tech-enabled work environment in downtown Nashville
Flexible/hybrid work culture",2.8,"CoreTrust
2.8",Remote,Unknown,-1,Subsidiary or Business Segment,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
168,Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Title: Data Engineer
Experience in Retail merchandising analytics is a must,
Duration: 12 months
Location: Remote
Interview Process:
1*_st_ round – Hirevue Video Call*
2*_nd_ round – Hiring Manager*
Skills Required:
Data engineering, analytics, and data modeling experience
Python, Spark, Databricks, Azure, Power BI
Experience in retail merchandising analytics is a must
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
retail merchandising analytics: 8 years (Required)
Python: 8 years (Required)
Power BI: 8 years (Required)
Work Location: Remote",-1.0,zettalogix.Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
169,Data Engineer,-1,"ArchWell Health is a new, innovative healthcare provider devoted to improving the lives of our senior members. We deliver best-in-class care at comfortable, accessible neighborhood clinics where seniors can feel at home and become part of a vibrant, wellness-focused community. Our members experience greater continuity of care, as well as the comfort of knowing they will be treated with respect by people who genuinely care about them, their families, and their communities.

Duties/Responsibilities:
Build data integrations from internal and external sources to centralize data into a Data Warehouse environment.
Monitor data integration operations, data quality, troubleshoot, and resolve problems.
Profile data sources and map to target table formats.
Develop and monitor data quality processes and address problems.
Develop, unit test and system test integration components.
Create support documentation describing the functionality of the integrations.
Participating in technical design & requirements gathering meetings.
Participate in planning and implementing data integration and data migration activities.
Perform QA tests to ensure data integrity and quality.
Research data issues between source systems and the data warehouse.
Required Skills/Experience:
Bachelor’s degree required; Master's degree (in data science, computer science or MIS, mathematics, engineering, or related field) preferred.
5+ years of prior experience in Data Management / ETL / ELT / Data Warehousing
Experience in writing Data Quality routines for cleansing of data and capturing confidence score
Experience with master data management
Strong knowledge of Structured Query Language (SQL) and Transact-SQL (T-SQL)
Experience using scripting languages such as JavaScript or Python
Experience Healthcare data models, datasets, and source systems (e.g. EHR, claims, labs, etc.)
Experience with healthcare reference data (ICD, CPT etc.)
Experience with agile delivery methodologies
Data Modeling experience preferred.
Strong organizational, administrative, and analytical skills required.
Experience managing and working in cloud environments such as Amazon Web Services or Azure
Knowledge of HIPAA; ability to implement systems and processes in accordance with regulations
Excellent interpersonal communication skills, both written and verbal
ArchWell Health is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to their race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected classification.",4.0,"ArchWell Health
4.0",Remote,Unknown,-1,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
170,Senior Data Engineer (Remote),Employer Provided Salary:$91K - $155K,"** This role is remote; if in area near an ICF office, hybrid may be recommended.**
Our Public Sector Group (PSG) is growing and we are looking for a talented, highly motivated Senior Data Engineer to join our Engineering and Emerging Technologies (EET) line of business for supporting a large government contract in the DC metro area.
Key Responsibilities:
Responsible for the data engineering on a large-scale technical modernization project in accordance with contract requirements and company policies, procedures and guidelines. Strong experience in I.T. modernization projects. Has a good mix of data requirements and data engineering, and knowledge of I.T. modernization efforts.
What you’ll be doing:
Support technical Data Warehouse/BI tasks for a major federal initiative, working as part of an extended system development team on project execution.
Develop extract, transform, and load (ETL) processing routines and data feeds, creating necessary data structures and data models to support data at all stages.
Perform extensive data profiling and analysis based on client data.
Work with UI and business analysis team members and the client to define BI and reporting requirements.
Design and implement custom data analytic and BI/reporting products, custom reports, and data visualization products.
What you must have:
Bachelor’s degree (e.g., Computer Science, Engineering or related discipline)
8+ years of experience in SQL and/or Python coding language.
8+ year of experience with procedural, functional, and object-oriented programming
3+ years of experience developing database ETL environments with business intelligence applications
3+ years of experience with AWS database, analytics, and compute services such as RDS/Aurora, AWS Glue, and Lambda
3+ years of experience with data warehouse design and development with Amazon Redshift
3+ years of experience working with BI tools such as Tableau, PowerBI or AWS Quicksight
3+ years of development experience in a DevSecOps environment, with programming languages such as Java, Node, or Python
Employment must be compliant with eligibility for Public Trust Clearance due to Government Contract.
Candidate must reside in the US and be a US Citizen
What we’d like you to know:
Understand ETL concepts of data flow, data enrichment, data consolidation, change data capture and transformation
Understand database concepts of referential integrity, indexes and keys and table metadata
Demonstrated experience showing strong critical thinking and problem-solving skills paired with a desire to take initiative
Knowledge of Big Data integration tools such as Hive, Airflow, Storm, Spark, AWS Kinesis, and Kafka
Experience building CI/CD pipelines with tools such as Jenkins and CodeBuild
Experience with containerization platforms including Docker
Experience with Agile development process
Technologies you’ll use in this role:
SQL, Node, Python, Tableau, Quicksight, Aurora, Lambda, Sequelize
Aurora, AWS Glue, S3, RedShift, AWS Kinesis
Git, Terraform, CodeBuild
Professional Skills:
Must have excellent written and oral communications skills.
Must be comfortable working in a fast-paced, matrixed team environment, with a client-centric culture, and an environment of high performers.
Must be able to multi-task and shift priorities as needed.
Why you’ll love working here:
Comprehensive health benefits
Generous vacation and retirement plans
Employee support program
Participation in charity initiatives
About ICF International:
ICF International (NASDAQ:ICFI) partners with government and commercial clients to deliver professional services and technology solutions in the energy and climate change; environment and infrastructure; health, human services, and social programs; and homeland security and defense markets. The firm combines passion for its work with industry expertise and innovative analytics to produce compelling results throughout the entire program life cycle, from research and analysis through implementation and improvement. Since 1969, ICF has been serving government at all levels, major corporations, and multilateral institutions. More than 4,000 employees serve these clients worldwide. ICF's Web site is
www.icf.com
ICF offers an excellent benefits package, an award-winning talent development project, and fosters a highly skilled, energized and empowered workforce.
ICF is an equal opportunity employer that values diversity at all levels. (EEO/AA – Minorities/Females/Veterans/Individuals with Disabilities)
For a listing of other career opportunities at ICF, please visit our Career Center at
www.icf.com/careers
Working at ICF
ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our
EEO & AA policy
.
Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email
icfcareercenter@icf.com
and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination:
Know Your Rights
and
Pay Transparency Statement.

Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
$90,940.00 - $154,598.00
Nationwide Remote Office (US99)",3.8,"ICF
3.8","Reston, VA",5001 to 10000 Employees,1969,Company - Public,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
171,Data center & infra Engineer,Employer Provided Salary:$50.00 - $55.00 Per Hour,"Job Title: Data Center Infra Engineer
Location: Remote
Contract Duration: 3 month to start
Qualifications:
• Someone with a deep understanding of software-hardware interactions that obsesses about low-level Linux configurations and optimizations
• Deep experience and fluency with Linux environments
• Experience building with modern infrastructure tools such as Docker, Kubernetes, Ansible, and Packer and TerraformExperience with TCP/IP configurations and troubleshooting on Linux systems
• Experience writing code to automate infrastructure management tasks with at least one language (Go, Python, Bash, or similar)
• 5+ years of infrastructure management experience including bare metal server management and operating system image creation and deployments Responsibilities:
1. Design and Implementation:
Collaborate with IT teams to design and plan data center infrastructure, including server racks, networking equipment, power distribution, cooling systems, and security measures.
Implement and deploy hardware and software solutions in the data center environment.
Ensure compliance with industry standards, best practices, and security protocols.
2. Maintenance and Troubleshooting:
Perform regular maintenance tasks such as server hardware upgrades, firmware updates, and system patches.
Monitor data center systems for performance, availability, and security, and proactively address issues as they arise.
Troubleshoot hardware, software, and network problems to minimize downtime and maintain optimal performance.
3. Capacity Planning and Optimization:
Monitor data center resources, including power consumption, cooling efficiency, and server utilization, to ensure scalability and efficient resource allocation.
Analyze performance metrics and recommend upgrades or optimizations to meet growing business needs.
4. Networking and Connectivity:
Configure and maintain network switches, routers, and firewalls to ensure seamless data flow and secure communication within the data center.
Collaborate with network engineers to design and implement network architecture that supports high availability and redundancy.
5. Security and Compliance:
Implement and maintain security measures to protect data center assets from physical and cyber threats.
Ensure compliance with industry standards and regulations such as ISO 27001, NIST, and HIPAA, depending on the industry.
6. Documentation:
Create and maintain documentation for data center layouts, configurations, and procedures.
Keep accurate records of hardware and software inventory, licenses, and maintenance schedules.
7. Vendor Management:
Collaborate with vendors and suppliers to procure data center equipment, negotiate contracts, and ensure timely delivery of hardware and services.
Evaluate and recommend new technologies and solutions that enhance data center efficiency and performance. Requirements:
Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent experience).
Proven experience in data center operations, infrastructure design, and implementation.
Proficiency in server hardware, operating systems (Linux, Windows), virtualization technologies, and networking protocols.
Familiarity with data center management tools and monitoring solutions.
Strong troubleshooting and problem-solving skills.
Knowledge of security best practices and experience implementing security measures in a data center environment.
Excellent communication and collaboration skills for working with cross-functional teams and vendors.
Relevant certifications such as CompTIA Server+, Cisco CCNA, VMware VCP, or equivalent, are a plus.
You should have deep experience and nuanced opinions on existing cloud services and developer tools
5+ years of professional SRE experience
5+ years of experience contributing to architecture and design (architecture, design patterns, reliability and scaling) of new and current systems
Bachelor's Degree in Computer Science or related field, or 8+ years relevant work experience
Solid understanding of infrastructure design, including the operational trade-offs of various designs
Experience writing high quality code with at least one programming language (Python, Go, or similar)
Experience building with modern infrastructure tools such as Docker, Kubernetes, Ansible, Cloud Formation, Terraform
Experience building with modern CI/CD practices and build systems, such as GitLab CI/CD, CircleCI, GitHub Actions
Experience with logging, monitoring and alerting systems and tools
Experience with Unix/Linux environments
Experience with TCP/IP and network programming
Thanks & Regards,
Job Type: Full-time
Pay: $50.00 - $55.00 per hour
Schedule:
8 hour shift
Experience:
Data center: 5 years (Required)
Infra Engineer: 5 years (Required)
Linux: 3 years (Required)
(Docker or Kubernetes or Ansible or Packer or Terraform: 3 years (Required)
Work Location: Remote",-1.0,Blue X Technologies,Remote,-1,-1,-1,-1,-1,-1
172,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
173,Engineer II - Imaging Data and Solutions,$73K - $101K (Glassdoor est.),"About the Department
Novo Nordisk Data Management and Informatics within the Digital Science and Innovation Organization provides informatics solutions, data products and analysis support to the research organization in Novo Nordisk. Data Management and Informatics is establishing a data products organization across our research sites. Staff will be co-located to one of our global sites in Seattle, WA, Fremont, CA, Lexington, MA, Oxford, UK and Denmark.

The Position
The Imaging Data and Solutions Engineer has a subject matter knowledge on imaging systems and image processing. They will set directions and deliver well curated imaging data and software products via appropriately architected data pipelines to solve complex scientific problems. This role focuses on providing findable, accessible, interoperable, and reusable imaging data within our diabetes, obesity, chronic and rare disease therapeutic areas. They will provide support to data consumers including computational scientists, citizen data scientists, bots and laboratory scientists and contribute through application of expertise. They will use their knowledge of digital to speed the ability of scientists and data scientists to access and work with imaging data, and to use scientific applications
They will use their hands on expertise to provide solutions using preferred tooling and technologies, and will work with other data engineers, product owners and specialist across Data Management & Informatics and Global IT to address larger needs. This position will also enable the visualization needs for imaging data and be well versed in best practices required to streamline data handling, and represent the local needs at the sites in the context of the global data management and informatics organization.
Do you believe that the digitalization journey in Research and Early Development (R&ED) is crucial for the success of pharmaceutical companies in the future? Then apply to become part of the next wave of scientific discovery by joining Digital Science & Innovation (DSI).

Relationships
The Imaging Data and Solutions Engineer reports to the Director, Imaging Data and Solutions Engineering. Internal partners include therapeutic area scientists, computational biologists, data and software engineers, software developers, platform and compute engineers in Research and Development and Information Technology.
External relationships include relationships with commercial and academic collaboration partners.

Essential Functions
Strong grasp of the image processing specifically DICOM headers and image transforms
Gather/organize large, complex data sets and develop ETL/ELT systems to manage and utilize this data. This will involve profiling, cleansing, transforming and developing data structures, schema and dictionaries to create more efficient workflows
Build automated monitoring mechanisms to ensure compliance and integrity of the pipelines and database
Being a core contributor to agile product delivery teams with a focus on publishing data products into the research and enterprise data catalog, as well as platform engineering and delivering clinical data for research capability
Ensure scientists and data scientists are aware of available imaging data and can access, integrate and query it in a performant manner
Enable streamlined sharing of rich data with collaborators through Cloud, accelerated compute and AI/ML approaches
Assist with provisioning of compute and data pipelines to deliver performant imaging data products via the research and enterprise data ecosystem
Optimize workflows and exchange of research imaging data within and to the global organization
Ensure researchers are familiar with and can use preferred applications
Develop or acquire new systems and software in collaboration with internal and external partners
Conduct end-user training and technical support of informatics systems
Advocate for the use of data and data science including computational and machine learning approaches in research projects
Participate in sustaining a suite of tools and application such as Python, R, Jupyter Hub, Domino, DataLab, Omero.
This role is an individual contributor role that must leverage agile software development practices. They will have a proven track record of creating business results with impact at the VP level.

Physical Requirements
0-10% overnight travel required.

Qualifications
Bachelor’s degree preferred. Degree within insert subject mater expertise preferred
Bachelor’s degree with 3+ years’ and Master’s degree with 1+ years’ relevant experience can be considered
Relevant experience includes:
Experience with imaging data sets including the formulation of scientific hypotheses and analysis plans that require the integration of multi-modal data into integrated data products
Basic understanding of the ontologies, vocabularies and standards for data representation across the research and development value chain.
Proficiency in one or more programming languages such as Python, Matlab, C/C++ and Java.
Experience adapting, developing and adding functionality to programming tools such as ImageIO, python imaging library, scikit-image, Flask, FastAPI, Jinja preferred.
Experience on machine learning tools (Pytorch and Tensorflow) is desirable.
Experience open-source contributions and publications demonstrating value to life sciences and drug discovery projects preferred.
Proficiency in Amazon, Azure or Google Cloud is desirable.
2+ years imaging data and image processing expertise and experience designing, developing, implementing and maintaining data pipelines and using cloud research data platforms.
Experience with entire data and solution engineering lifecycle from design, deployment, implementing and maintenance of proprietary and commercial software
Preferred:
2+ years experience in life sciences, medical device or pharmaceutical industry
Broad expertise spanning data and digital, including the ability to perform hands on technical work
Strong analytical skills, the ability to plan and realize robust and scalable solutions with a structured approach and detail oriented
Ability to work independently or need occasional guidance from manager/senior colleagues
Automated testing skills preferred
Excellent communication skills at bench scientist/end-user level and with middle management
Excellent written and oral communication skills is required
We commit to an inclusive recruitment process and equality of opportunity for all our job applicants.

At Novo Nordisk we recognize that it is no longer good enough to aspire to be the best company in the world. We need to aspire to be the best company for the world and we know that this is only possible with talented employees with diverse perspectives, backgrounds and cultures. We are therefore committed to creating an inclusive culture that celebrates the diversity of our employees, the patients we serve and communities we operate in. Together, we’re life changing.

Novo Nordisk is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, ethnicity, color, religion, sex, gender identity, sexual orientation, national origin, disability, protected veteran status or any other characteristic protected by local, state or federal laws, rules or regulations.

If you are interested in applying to Novo Nordisk and need special assistance or an accommodation to apply, please call us at 1-855-411-5290. This contact is for accommodation requests only and cannot be used to inquire about the status of applications.",4.3,"Novo Nordisk
4.3","Lexington, MA",10000+ Employees,1923,Company - Public,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,$10+ billion (USD)
174,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1.0,Radiant System,Remote,-1,-1,-1,-1,-1,-1
175,Data Engineer (Contract),$77K - $111K (Glassdoor est.),"About RevOpsforce:
At RevOpsforce, our mission is to drive sustainable revenue growth for our clients by leveraging data-driven insights, technology, and process optimization. We strive to deliver measurable results that exceed expectations, while building strong partnerships with our clients and continuously improving our own capabilities. Our Expert Network is composed of the highest skilled and certified professionals in revenue operations. We leverage this network to solve our clients' most complex operational challenges.
Type: Contract
Job Description:
We are seeking a talented and experienced Data Engineer to join our growing team. As a Data Engineer at our company, you will be responsible for designing, building, and maintaining the data pipelines that power our data-driven applications. In this role, you will work closely with our data science and engineering teams to develop high-quality, efficient data pipelines that enable us to derive insights from vast amounts of data. You should be comfortable working with large volumes of structured and unstructured data, and have a strong understanding of SQL, Python, and other programming languages.
Responsibilities:
Design, build, and maintain data pipelines to support data-driven applications and analytics
Analyze data to identify trends and patterns
Collaborate with data scientists and engineers to develop data-driven solutions
Write and maintain documentation for data pipelines
Monitor and optimize data pipelines for performance and efficiency
Qualifications:
Bachelor's or Master's degree in a related field (e.g. Computer Science, Data Science, Engineering)
3+ years of experience in a data engineering role
Strong skills in SQL and at least one programming language (e.g. Python, Java, C++, etc.)
Experience with big data technologies (e.g. Hadoop, Spark, etc.) and cloud platforms (e.g. AWS, Azure, GCP)
Strong problem-solving and analytical skills
Excellent communication skills and ability to work in a team environment
Join our dynamic and forward-thinking team to make a significant impact on our clients' revenue growth journey!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Website is coming soon and will be located at www.revopsforce.com",-1.0,RevOpsforce,"Austin, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
176,Data Engineer (Data Platform),Employer Provided Salary:$120K - $165K,"About Atlas Health:
Atlas Health automates philanthropic aid to improve access, affordability, outcomes, and health equity for vulnerable populations. Through intelligent matching and patient-friendly digital enrollment to 20,000+ philanthropic aid programs, healthcare organizations can improve patient outcomes, advance health equity, reduce the total cost of care and improve the patient experience. Join us on our mission of saving and improving lives by helping patients access and afford healthcare.

Data Engineers on our data platform team develop systems that manage data flow throughout the Atlas infrastructure and support downstream data applications. These systems are responsible for handling sensitive patient data, and members of the team are technical champions of ensuring HIPAA standards for confidentiality and compliance. Broadly, this role focuses on all elements of data engineering, such as ingestion, transformation, and distribution of data, and works alongside security engineering and business implementation & operations teams as a custodian of patient and company data.
Responsibilities:
Engineer scalable, reliable, and performant systems that manage data
Consume data from a variety of sources and formats, such as flat files, streaming systems, or APIs
Leverage cloud infrastructure to develop scalable data solutions
Collaborate with platform engineers on creating reliable and secure systems
Collaborate closely with team members and product stakeholders
Create trustworthy, secure, governable, and standardized data components for consumption
Develop readable, well-tested applications, APIs, and libraries
Implement application observability in the form of metrics, logging, and monitoring
Requirements:
Professional experience with cloud-based systems
Experience with data architectures and tools in support of streaming and batch driven data processing
Solid understanding of distributed task orchestration
Demonstrated ability in developing and testing systems that manage data reliability, efficiency, and quality
Understanding of multiple software development paradigms
First-nature comfort in working with containerization tools
Computer Science or related technical degree in a related field or equivalent technical experience
Bonus points:
Depth of knowledge in Google Cloud Platform and experience working in an ePHI environment
Familiarity with domain driven design concepts
Preferred Qualifications:
Fluency in Python
Experience using Apache Airflow or similar orchestration systems
Experience with IaC tools
The salary range for this position is $120,000 - $165,000.
Why Join Our Team:
Because you’re motivated by a combination of success, working alongside incredible people, and have a passion for helping clients and patients. Atlas helps people access essential medical treatment, and avoid financial ruin from medical debt. You care about being a part of the journey and wish to play a key role in our organization’s success.
Benefits:
We offer a comprehensive benefit plan for our U.S. based employees which includes:
Health, dental and vision insurance
401K
Flexible time off
Paid holidays
Atlas values diversity of all kinds, and we’re committed to building a diverse and inclusive workplace where we learn from each other. We are an equal opportunity employer and welcome people of all different backgrounds, experiences, abilities, and perspectives.",4.6,"Atlas Labs
4.6",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
177,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
178,Data Engineer (Fully Remote),$56K - $91K (Glassdoor est.),"Position Description: Who We Are
NIP Group www.nipgroup.com is a rapidly growing insurance service provider of specialty programs for commercial insurance brokers and carriers providing underwriting, distribution, product management, administration, and risk management services primarily by acting as a managing underwriter (MGA) and a Reciprocal Services Manager (RSM).
Our culture is one that empowers and encourages employees to be innovative, collaborative, and forward-thinking. If you are interested in being a part of a growing, entrepreneurial spirited organization, wed love to hear from you!

About the Position
Reporting into the Senior Actuary, you will own the continuous improvement culture for designing, building, and maintaining the infrastructure and systems required for collecting, storing, processing, and analyzing large volumes of data. As a Data Engineer, you will play a crucial role in ensuring that data is readily available, accessible, and usable by other data professionals, analysts, and stakeholders within an organization. You will be responsible for managing a data Lake / warehouse and implementing efficient data integration processes with internal and external systems.

What Youll Do
Design and maintain a scalable and secure Azure data warehouse.
Build data pipelines: Create and manage data integration processes, including data extraction, transformation, and loading (ETL) from various sources into the data warehouse.
Develop and implement data management strategies to ensure data quality, consistency, and accuracy.
Collaborate with cross-functional teams, including to identify data requirements and develop data models.
Develop, adhere, and enforce data governance policies and procedures to ensure compliance.
Monitor data quality and develop response mechanisms to address problems.
Collaborate with IT teams to ensure the availability, reliability, and performance of data systems and infrastructure.
Develop documentation related to data management processes, procedures, and data dictionaries.
Stay up to date with industry trends and advancements in data management technologies and techniques.
What Were Looking For
Education/Experience
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Proven experience working as Data Engineer or similar role, preferably in the insurance or financial industry.
Strong knowledge of data management principles, data quality assurance, and data governance practices.
Proficiency in data warehousing concepts, and ETL processes.
Experience with data integration tools and technologies (e.g., SQL, Python, ETL frameworks).
Solid understanding of regulatory requirements related to data privacy and security (e.g., GDPR, HIPAA).
Excellent problem-solving and analytical skills, with the ability to analyze complex data sets.
Strong communication and interpersonal skills to collaborate with cross-functional teams and communicate complex ideas effectively.
Technical Competencies
Proficiency in Microsoft services in Azure, Data Factory, Data Lake/warehouse storage.
Ability to design and implement data integration solutions using Azure Data Factory.
Expertise in designing and optimizing data warehousing solutions Azure SQL Data Warehouse.
Familiarity with Azure Data Lake Storage and data processing using Azure Data Lake Analytics.
Data Governance and Security: Understanding of data governance principles and compliance requirements within Azure, including access control and data privacy.
Monitoring and Troubleshooting: Skills in monitoring, optimizing, and troubleshooting data pipelines within Azure Data Factory.
Proficiency in scripting languages like PowerShell or Python for automation tasks in Azure.
Strong communication and collaboration skills to work effectively with cross-functional teams.
Proactive in staying updated with the latest Azure data services and advancements.
What You'll Receive
At NIP Group, we recognize there are many factors that contribute to your overall satisfaction both at work, and in your personal life. For that reason, we provide a perfect mix of compensation, benefits, company culture, and resources to ensure your everyday happiness. Below are some benefits that youll receive.
Competitive compensation to reward you for your hard work every day.
Progressive Paid-Time Off program for you to enjoy time out of the office, including time off for volunteering and life events.
Group Medical, Dental, Vision and Life insurance to encourage a healthy lifestyle.
Pretax Health and Dependent Care Spending Accounts to ease taxes on spending.
Discounts in retail and entertainment.",3.0,"NIP Group, Inc.
3.0","Woodbridge, NJ",51 to 200 Employees,-1,Company - Private,Insurance Agencies & Brokerages,Insurance,Unknown / Non-Applicable
179,Senior Software Engineer - Data Engineering,$120K - $153K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: The merchant Reporting and Reconciliation team is looking for a senior Back-end engineer responsible for providing technical leadership to the team and executing the unification projects end to end. Your primary focus will be developing massively scalable, distributed software systems that require high availability to our business
Job Description:
Merchant reporting is crucial for our Merchants so that they can close their account books timely and accurately with complete payment data in core markets. We are looking for people who have a passion for developing massively scalable, distributed software systems that require high availability to our business. As a member on the Merchant Reporting and Reconciliation team, you thrive in a fast-paced environment and enjoy driving innovation through rapid prototyping and iterative development. You will work directly with our Product Owners and Domain Technical Leads to create outstanding solutions and deliver incredible reporting products. You will be involved from ideation to rollout.
Your day-to-day:
Work with Product Managers and other business partners to identify opportunities for improvement
Analyze data based on product requirements
Create reports for internal teams and/or external clients
Use graphs, infographics, and other methods to visualize data
Structure large data sets to find usable information
Work with a team of analysts and other associates to process information
Create presentations and reports based on recommendations and findings
Define validation queries when needed and how to identify discrepancies in the data as they arise
Write queries for runbooks that automate the discrepancy identification process
Implement the reporting data model
Deliver within schedule in an Agile software development using test-driven development methodologies.
Participate in development life cycle activities like design, coding, testing, and production release.
Be proactive with identifying areas for improvement and innovation to improve development productivity
What do you need to bring:
BS in EE/CS or equivalent work experience and successful completion of major projects for which you can show code examples.
5+ years of hands-on data/software engineering experience
Experience working with coding languages—preferably SQL, Java, Spark-SQL, Pyspark, Python
Experience working with SQL and NoSQL DataBase
High proficiency in MS Excel, MS PowerPoint, GIT, Apache Airflow
Have a passion for quality and writing clean and solid code that scales and performs well.
Strong desire to learn, push the envelope and share knowledge with others.
Excellent analytical and time management skills
Teamwork skills with a problem-solving attitude
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
180,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
181,Data Engineer,$76K - $101K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
The Data Governance/Data Quality Subject Matter Expert (SME) is a professional responsible for designing, implementing, and managing the client’s data governance strategy and ensuring high data quality standards across data-centric operations and decision-making.
The SME is in charge of developing and maintaining robust data pipelines, ensuring data observability, managing data within the Microsoft Azure environment, and implementing data governance principles in alignment with industry best practices and regulatory standards.
Primary Responsibilities and Accountabilities:
Data Governance & Quality: This includes developing and implementing data governance policies and procedures, monitoring data quality, managing data within the Microsoft Azure environment, and ensuring compliance with data privacy regulations.
Data Pipeline & Workflow Development: Designing and implementing efficient and secure data pipelines in Azure Data Factory and workflows, ensuring optimal performance and incorporating data quality principals.
Data Integration & Observability: Designing data integration strategies, integrating data from multiple sources, ensuring data quality and observability through tools like Azure Synapse.
Data Security: Defining and implementing data security policies and procedures, including access control and data encryption within the Azure environment.
Performance Tuning: Optimizing data access, indexing, and query performance within Azure Data Factory to ensure scalable and reliable data infrastructure.
Monitoring & Troubleshooting: Monitoring data systems, troubleshooting issues, identifying performance bottlenecks, and collaborating with team members to resolve problems.
Collaboration & Communication: Working closely with stakeholders to understand their data needs and develop solutions adhering to industry best practices and the needs of the business.
Automation & Tooling: Automating repetitive tasks and maintaining tools necessary for data processing and storage within the Azure environment.
Qualifications
A Bachelor's degree in a related field such as computer science, mathematics, engineering, or information systems.
3+ years of hands-on experience programming in SQL.
2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes within the Microsoft Azure environment.
Experience in Data Governance OR Data Quality Management is a must.
Skills
Competencies:
Expertise in Microsoft Azure data tools, with a focus on Azure Data Factory and Azure Synapse for data processing, storage, and analysis.
Strong SQL skills and knowledge of data warehousing concepts and ETL (Extract, Transform, Load) processes within the Azure environment.
Proficiency in programming languages such as Python, and experience with big data technologies such as Hadoop, Spark, and Kafka.
Ability to design, build, and maintain data pipelines, with a particular focus on Azure Data Factory.
Strong analytical and problem-solving skills, with the ability to identify and resolve data-related issues effectively.
Experience with data security and privacy regulations, and understanding of HIPAA compliance.
Strong experience with data governance, data quality, and data observability principles.
Excellent written and verbal communication skills, ability to explain complex technical concepts to non-technical stakeholders.
Excellent collaboration and teamwork skills, with the ability to work in a team involving client data engineers, data scientists, and data analysts.
Strong attention to detail and commitment to ensuring high data quality.
Continuous learning mindset and ability to stay up-to-date with new developments and technologies in data governance and data quality
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
182,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
183,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
184,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
185,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1.0,Percept Health,Remote,-1,-1,-1,-1,-1,-1
186,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
187,Cloud Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"We are seeking a highly experienced Principal Data Engineer with a deep understanding of PySpark using Databricks or AWS Glue or AWS EMR and cloud-based databases such as Snowflake. Proficiency in workflow management tools like Airflow is essential. Healthcare industry experience is a significant advantage. The ideal candidate will be responsible for designing, implementing, and maintaining data pipelines while ensuring the highest performance, security, and data quality.
Responsibilities:
Design, develop, and maintain scalable, reliable, and secure data pipelines to process large volumes of structured and unstructured healthcare data using PySpark and cloud-based databases.
Collaborate with data architects, data scientists, and analysts to understand data requirements and implement solutions that meet business and technical objectives.
Leverage AWS or Azure cloud services for data storage, processing, and analytics, optimizing cost and performance.
Utilize tools like Airflow for workflow management and Kubernetes for container orchestration to ensure seamless deployment, scaling, and management of data processing applications.
Develop and implement data ingestion, transformation, and validation processes to ensure data quality, consistency, and reliability across various healthcare datasets.
Monitor and troubleshoot data pipelines, proactively identifying and resolving issues to minimize downtime and ensure optimal performance.
Establish and enforce data engineering best practices, ensuring compliance with data privacy and security regulations specific to the healthcare industry.
Continuously evaluate and adopt new tools, technologies, and frameworks to improve the data infrastructure and drive innovation.
Mentor and guide junior data engineers, fostering a culture of collaboration, learning, and growth within the team.
Collaborate with cross-functional teams to align data engineering efforts with broader organizational goals and strategies.
Is familiar with SOC 2 compliance and its impact on company policies and processes.
Understands the importance of adhering to SOC 2 requirements and maintains an effort to do so.
Reviews and understands the Employee Handbook, and internal policies that define individual security responsibilities, and maintains segregation of duties in accordance with their role requirements.
Requirements:
Bachelor's or Master’s degree in Computer Science, Engineering, or a related field.
8+ years of experience in data engineering, with a strong background in Apache Spark and cloud-based databases such as Snowflake.
Strong Knowledge in Big Data Technologies, Databricks, AWS Services, and PySpark, thorough in one or more programming languages like Python.
Proven experience with AWS or Azure cloud services for data storage, processing, and analytics.
Expertise in workflow management tools like Airflow and container orchestration systems such as Kubernetes.
Strong knowledge of SQL and NoSQL databases, as well as data modeling and schema design principles.
Familiarity with healthcare data standards, terminologies, and regulations, such as HIPAA and GDPR, is highly desirable.
Excellent problem-solving, communication, and collaboration skills, with the ability to work effectively in cross-functional teams.
Demonstrated ability to manage multiple projects, prioritize tasks, and meet deadlines in a fast-paced environment.
A strong desire to learn, adapt, and contribute to a rapidly evolving data landscape.
Benefits:
401(k).
Dental Insurance.
Health insurance.
Vision insurance.
We are an equal-opportunity employer and value diversity, equality, inclusion, and respect for people.
The salary will be determined based on several factors including, but not limited to, location, relevant education, qualifications, experience, technical skills, and business needs.
Additional Responsibilities:
Participate in OrangePeople monthly team meetings, and participate in team-building efforts.
Contribute to OrangePeople technical discussions, peer reviews, etc.
Contribute content and collaborate via the OP-Wiki/Knowledge Base.
Provide status reports to OP Account Management as requested.
About us:
OrangePeople is an Enterprise Architecture and Project Management solutions company. Our most valuable asset is our people: dynamic, creative thinkers, who are passionate about doing quality work. As a member of the OrangePeople team, you will have access to industry-leading consulting practices, strategies & technologies, innovative training & education. An ideal Orange Person is a technology leader with a proven track record of technical achievements and a strong process/methodology orientation.
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Application Question(s):
Do you require sponsorship for this job?
Work Location: Remote",4.1,"OrangePeople
4.1",Remote,51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
188,Data Engineer,$100K - $144K (Glassdoor est.),"Position: Data Engineer
Duration: Contract
Location: Fremont, California
Skills:
Strong in SQL and Python
Data warehousing concepts , Data Modeling and RDBMS concepts
Good to have exposure to big data technologies – Apache Spark, Kafka, Airflow, NoSQL
Good to have exposure on Insurance & Finance business processes
Regards
Tejash Gupta
Recruiter
tejash@swifttechinc.com
Job Type: Contract
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Python: 7 years (Required)
SQL: 7 years (Required)
Data modeling: 6 years (Required)
RDBMS: 6 years (Required)
Work Location: In person",4.1,"Swift Technologies Inc
4.1","Fremont, CA",1 to 50 Employees,-1,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
189,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
190,AWS Data Engineer II,$77K - $105K (Glassdoor est.),"Overview:
Shift4 (NYSE: FOUR) is boldly redefining commerce by simplifying complex payments ecosystems across the world. As the leader in commerce-enabling technology, Shift4 powers billions of transactions annually for hundreds of thousands of businesses in virtually every industry. For more information, visit www.shift4.com.

Shift4 Payments is seeking a Business Intelligence Engineer with an AWS Focus to join our growing internal Information Technology team. As a BI Engineer, you will help design and maintain the flow of information and analytics to the entire organization utilizing AWS capabilities to store, move and report on data. The position requires at least 3 years of prior experience as a Business Intelligence AWS engineer. We are looking for individuals that are extremely self-sufficient, available to work flexible hours and hold themselves to the highest standards of professionalism.Maintain and expand BI AWS architecture and content.
Responsibilities:
Support of BI AWS architecture and content up to and including off hours support when necessary.
Work on necessary bugs, enhancements and new system developments utilizing our Software Development Life Cycle (SDLC) standards.
Complete root cause analysis and provide recommendations to leadership to address opportunities.
Contribute to and enforce the use of the organization's Business Intelligence solution.
Develop software solutions that will provide solid foundations for the growth of the company.
Support key metrics and shared goals that determine success for a particular area.
Qualifications:
3-5+ years of professional experience in business intelligence AWS development or a related field.
Advanced experience working with AWS: S3, Glue, Lambda, Step Functions, Redshift, Athena.
Advanced experience working with various data sources such as AWS S3 and Redshift, Postgres, SQL Server, Amazon Athena, Excel, Flat Files, etc.
Advanced experience working with large data sets from sources such as AWS S3 and Redshift, Postgres, SQL Server, etc.
Strong PySpark, Python, CLI proficiency
Strong SQL proficiency (T-SQL, MySQL, and PostgreSQL).
Experience working with an Enterprise Data Warehouse and Dimensional Databases is a plus.
Advanced experience of the SDLC and how Development processes fit into KANBAN and Scrum Frameworks.
Experience with Jira and Confluence is a plus.
Ability to prioritize multiple tasks and easily adjust to changing priorities.
Ability to identify problems, initiate solutions, and effectively collaborate and communicate with team members.
Have excellent verbal and written communication skills.

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.",3.5,"Shift4 Payments
3.5","Allentown, PA",1001 to 5000 Employees,1999,Company - Public,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
191,Senior Data Engineer,-1,"JOB DESCRIPTION:
Need to be expertise Coding on PySpark.
Supply Chain and Azure.
Experience in working with end-to-end technology implementation project, preferable those related to Data warehousing/Analytics/Reporting
Strong communication skills with the ability to collaborate effectively with business owners/users and technical teams.
Proven ability to gather product requirements from stakeholders and prioritize feature implementations based on business needs and product strategy.
Strong problem-solving skills and attention to detail.
Good to have experience with CPG/ Retail industry (preferable CPG companies such as Nestle, Unilever, P&G, Coca coal, PepsiCo etc)
Bachelor's/master's degree in business administration, computer science, engineering, or a related field.
Job Type: Contract
Experience:
PySpark: 5 years (Preferred)
SQL: 10 years (Preferred)
Data analytics: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Azure: 5 years (Preferred)
Data Bricks: 5 years (Preferred)
Data Lake: 5 years (Preferred)
Synapse: 5 years (Preferred)
Retail Domain: 8 years (Preferred)
CPG: 7 years (Preferred)
Work Location: Remote",4.0,"Quadrant Resource
4.0",Remote,1001 to 5000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
192,Data Engineer,Employer Provided Salary:$120K - $130K,"Role – Data Engineer- DataBricks -
Location – O Fallon, Missouri (Initially Remote)
Yrs. of experience – 8+
Full-Time
Job Description :
o 5+ years in a customer facing Technical architecture, consulting role with the expertise in The following technology.
o Developing modern Data warehouse solutions using Databricks and AWS/Azure stack.
o Drive technical discussion with client architect and team members.
o Knowledge in Databricks DELTA lake for the Analytical data lake use case
o Hand on experience in create MLOPS data pipeline creation.
o AIML Models develop, train and implement for the AIML use cases.
o Knowledge in Banking domain on card and payment areas.
o Any relation database – Data classification/ Data profiling for MLOPS use cases
o Good experience in offshore onsite coordination.
o Experience in translating a customer’s business needs.
Job Type: Full-time
Salary: $120,000.00 - $130,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
O'Fallon, MO 63366: Reliably commute or planning to relocate before starting work (Required)
Experience:
DataBricks: 1 year (Required)
MLOPS: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: In person",-1.0,Sbase Technologies,"O Fallon, MO",-1,-1,-1,-1,-1,-1
193,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
194,ETL Data Pipeline Engineer,Employer Provided Salary:$60.00 Per Hour,"ETL Data Pipeline Engineer
We do not work with 3rd party employers. Visa Sponsorship NOT available.
We are seeking a ETL DATA Pipeline Engineer for a consulting engagement with a major entertainment and media company. This person will be hands-on-date engineering development across multiple projects.
Required Skills:
10+ years of experience as Data Engineer with Large Data Pipelines
Strong SQL skills
Distributed Systems (Spark, Hadoop)
Cloud experience
STRONG ETL Experience
Python/Bash
Agile/Scrum
----------------------------------------
ABOUT MOORECROFT
A quality oriented national consulting firm. For more than two decades, providing expert IT professionals to our clients around the nation. Our core values are professionalism, honesty and integrity. We are only interested in working with professionals that subscribe to the same values and are driven to truly serve clients.
Job Type: Contract
Pay: From $60.00 per hour
Experience level:
10 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Santa Monica, CA 90404: Reliably commute or planning to relocate before starting work (Required)
Work Location: Hybrid remote in Santa Monica, CA 90404",-1.0,Moorecroft Systems,"Santa Monica, CA",Unknown,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
195,Data Engineer,Employer Provided Salary:$115K - $135K,"**This position is a majority Remote role with occasional in-office meetings on an as-needed basis. Because of this, interested applicants must live within a reasonable driving distance of Symmetry Lending's office in Anaheim, CA.**
Job Description
This role will play a pivotal role in assisting the IT team with the evolution of the company data architecture. In addition to supporting extensions to the data warehouse design, this role will design and develop ETL required to onboard new types of business data. Finally, this role will also provide support for Analytics assignments as needed.
Responsibilities
Display sense of ownership over assigned work, requiring minimal direction and driving to completion in a sometimes fuzzy and uncharted environment.
Designing and building new data pipelines that support business requirements.
Work with IT and other business partners to support the evolution of the enterprise data architecture.
Work with IT to build new ETLs to take data from various operational systems and extend existing data warehouse data model for analytics and reporting.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business constituents.
Support the development of the company BI infrastructure, including the construction of scalable analytic solutions, and on-premises reporting tools.
Utilize Power BI development to build scalable reporting models to serve BI reporting needs.
Demonstrate the ability to gather requirements, extract and manipulate data residing in multiple disparate databases, and articulate solutions to support the business.
Design, develop, and maintain performant data models in Power BI utilizing best practices.
Serve as a subject matter expert for all data warehouse and enterprise systems
Use new or existing technologies to produce analytics solutions (in the form of excel spreadsheets, dashboards, etc.).
Work with IT and Analytics team to architect and build data pipelines to optimize for performance, data quality, scalability, ease of future development, and cost.
Gather requirements, assess gaps and build roadmaps to help the analytics driven organization achieve its goals.
Develop data-related Proof of Concepts in order to demonstrate feasibility and value to Symmetry business constituents.
Qualifications / Requirements
Bachelor’s degree in Information Systems, Computer Science, Finance, or similar education from an accredited college
2-3 years Accounting and Finance background preferred
Strong skillset in Microsoft Excel (2-3 years, financial formulas & VBA is a plus) and familiarity with Microsoft Office Productivity Suite (Excel, Word, etc.)
5+years’ experience with advanced SQL concepts and writing SQL statements (SQL Server or similar).
Prefer 2 years’ experience with Microsoft BI Suite, PowerBI, Power Query, PowerPivot, Reporting Service (SSRS), SharePoint
Experience in mentoring other team members in development best practice, and methodologies.
You are passionate about data quality control and know how and where to anticipate potential errors.
Knowledge of the software development lifecycle, agile methodologies, and structured software development methodologies.
Experience performing analysis with large datasets in a cloud-based environment.
Ability to work effectively with stakeholders at all levels within the organization
Strong communication and time management skills and a self-motivated approach
Ability to work independently, detail-oriented, and execution focused
Highly collaborative and team oriented
Tenacious (doesn’t give up easily)
Genuine passion for clean and reliable data
At least 5 years of work experience
About Symmetry
Symmetry Lending, specializes in providing mortgage fulfillment services to include origination, servicing, and capital markets needs to various Lenders across the country with whom we partner. We have offices in Atlanta, GA, Eden Prairie, MN, Denver, CO, Orlando, FL, and Anaheim, CA, and we do business from coast to coast. We take great pride in building a diverse team of motivated professionals that contribute to an exciting work atmosphere. We provide a competitive benefits package including medical, dental, and vision plan options, paid time off, and more.
California Disclosure - Employee Notice at Collection
This disclosure is intended to comply with the California Consumer Privacy Act (CCPA), which gives California residents who are applicants, employees, or contractors of Symmetry Lending (“Symmetry”) the right to know what categories of personal information Symmetry collects about them and the purposes for which Symmetry uses that information. As used in this Privacy Notice, “Personal Information” means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a particular individual or household. Personal Information includes, but is not limited to, the categories of personal information identified below if such information identifies, relates to, describes, is reasonably capable of being associated with, or could be reasonably linked, directly or indirectly, with a particular individual or household.
The following is a list of the categories of Personal Information that we may collect about consumers:
Identifiers. This may include a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, Social Security number, driver's license number, passport number, or other similar identifiers.
Personal information described in the California Customer Records Statute (Cal. Civ. Code § 1798.80(e)). This may include a name, signature, Social Security number, physical characteristics or description, address, telephone number, passport number, driver's license or state identification card number, insurance policy number, education, employment, employment history, bank account number, or any other financial information, medical information, or health insurance information.
Characteristics of Protected Classification under California or Federal Law. This may include age, race, color, ancestry, national or ethnic origin, citizenship status, religion or belief, marital status, a childbirth or related medical condition, physical or mental disability, sex (including gender, gender identity, gender expression, pregnancy or childbirth, and related medical conditions), sexual orientation, veteran or military status.
Biometric information. This may include voice and video recordings.
Sensory data. This may include audio, electronic, visual, or similar information, including photos.
Professional or employment-related information. This may include current or past job history, compensation data, performance evaluations, or employee benefits.
Beneficiaries, dependents, and emergency contact information. This may include the name, gender, phone number, and relationship of beneficiaries, dependents, and emergency contacts.
We may use the categories of Personal Information for the following business or commercial purposes:
To perform background checks necessary to comply with licensing requirements, to perform reference checks, to verify eligibility to work in the United States, for contact purposes, to assess your qualification for employment, to conduct performance evaluations, for payrolls and budgeting purposes, for implementation of employee benefits, for internal organizational purposes to establish proper accommodations for sick time, PTO, leaves of absences, or emergency situations, and to conduct health screenings as allowed by OSHA and the CDC to protect the safety of our employee during pandemic situations.
For internal use, such as tracking access into buildings, timekeeping, activity logs, etc.
To comply with laws and regulations, including but not limited to applicable tax, health and safety, anti-discrimination, immigration, labor and employment, and social welfare laws.
For security or the prevention, detection, or investigation of fraud, suspected or actual illegal activity, violations of company policy or rules, or other misconduct.
To comply with civil, criminal, judicial, or regulatory inquiries, investigations, subpoenas, or summons.
To comply with all licensing requirements necessary for our business operations, including state exam audits.
To exercise or defend the legal rights of Symmetry and its employees, affiliates, customers, contractors, and agents.
To seek advice from lawyers, auditors, or other professional advisors.
If Symmetry uses Personal Information of an applicant, employee, or contractor for a purpose materially different than those disclosed in this notice, Symmetry will notify the employee and obtain explicit consent from the employee to use the Personal Information for this new purpose.
Job Type: Full-time
Pay: $115,000.00 - $135,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Paid time off
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
Accounting and Finance: 2 years (Preferred)
advanced SQL concepts and writing SQL statements: 5 years (Required)
Work Location: Hybrid remote in Anaheim, CA 92807",3.9,"Symmetry Lending
3.9","Anaheim, CA",51 to 200 Employees,-1,Company - Public,Banking & Lending,Financial Services,$5 to $25 million (USD)
196,Data Engineer,-1,"Clear Demand Company Overview
Clear Demand is the leader in Intelligent Price Management and Optimization (IPMO) for retail. We were the first company to deliver an omni-channel lifecycle pricing solution that synchronizes prices, promotions, and markdowns online and in-store to produce a consistent brand and shopping experience. Clear Demand is the leading innovator in retail pricing solutions with patented science that analyzes historical sales to understand shoppers’ sensitivity to price and generate price and promotion strategies that account for pricing rules, cost changes, and competitor prices to achieve profit and revenue goals. Architected on big data and delivered through Software-as-a-Service (SaaS), Clear Demand’s Intelligent IPMO solution can be administered from a public or private cloud. Clear Demand’s innovations in retail science simplify adoption and use, while allowing retailers to see value in just weeks with more transparency and minimal disruption to existing business.
Clear Demand offers a competitive salary with stock options and a benefits package, including medical, dental, 401k.
Job Description – Data Engineer
This is a permanent position with tremendous potential for growth. The successful candidate will be exceptionally talented and hardworking---a self-starter able to multi-task and deliver results in a fast-paced environment. We are looking for a team player with experience developing high-performance applications for large enterprises. The software developer will be experienced in agile product-development methodologies. The ideal candidate will have a proven track record showing commitment to and sense of urgency for project timelines. This position will report to the Director of Engineering.
Primary Responsibilities
Designing, developing, testing, deploying, and maintaining applications to support business requirements.
Develop and improve data solutions for
- Designing relational schemas for persisting complex business objects
- ETL of customer data into our solution.
- Performing validation and mitigation strategies to handle invalid incoming data.
- Exchanging data between CDI’s user facing application and backend pricing optimization science solutions.
- Versioning database schemas, stored procedures
- Visualizing analytics and reporting
Working closely with both the Engineering and Science departments to build cohesive data-centric solutions.
Providing development expertise on migrating from a document store to a relational database.
Resolving technical issues through debugging and troubleshooting is also required.
Estimating level of effort for user stories and tasks.
Participate in Agile/SCRUM processes and ceremonies.
Required Skills
5+ years of experience in software development
Team player and effective communicator
Knowledge, experience, and proficiency with:
- Agile Development Methodology
- Relational Databases (SQL Server, Postgres)
- Document stores (i.e., MongoDB)
- Postgres
- Python
- Database query performance optimization
- Database versioning and migration
- ETL
- Git
- Object-Relational Mapping (ORM)
Performance optimization and debugging
Takes initiative to identify and address technology issues and opportunities, and proactively contributes to the business.
Good interpersonal, written, and oral communication skills.
Experience working in a team-oriented, collaborative environment.
Technical documentation skills.
Desired Skills
Google Cloud
CI/CD build and release pipeline
Jira
C#
HTML, JavaScript
Security/SSO/SSL
SaaS (Software-as-a-Service)
Experience with the Retail industry
Education
College degree in Computer Science or equivalent.
Clear Demand offers a competitive salary with stock options and a benefits package, including medical, dental, 401(k).
To apply, please your send resume to HumanResources@ClearDemand.com.
To learn more about Clear Demand, visit http://ClearDemand.com.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Employee discount
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Retirement plan
Vision insurance
Experience level:
5 years
Application Question(s):
Would you be willing to do a coding exercise as part of this application process?
Experience:
Agile: 3 years (Preferred)
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Scottsdale, AZ 85258",-1.0,Clear Demand,"Scottsdale, AZ",-1,-1,-1,-1,-1,-1
197,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1.0,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
198,Data Engineer,Employer Provided Salary:$55.00 - $75.00 Per Hour,"Senior Data Engineer - 10+ Years of Total Experience Required
Location: Dallas, TX and Remote
Job Description:
Slesha inc is looking for a Data Engineer to join our team in our new location in Dallas, TX. This role will be responsible for the following:
Data Engineer
Responsibilities
· Writes ETL (Extract / Transform / Load) processes, designs database systems and, develops tools for real-time and offline analytic processing.
· Troubleshoots software and processes for data consistency and integrity. Integrates complex and large scale data from a variety of sources for business partners to generate insight and make decisions.
· Translates business specifications into design specifications and code. Responsible for writing complex programs, ad hoc queries, and reports. Ensures that all code is well structured, includes sufficient documentation, and is easy to maintain and reuse.
· Partners with internal clients to gain an expert understanding of business functions and informational needs. Works closely with other technical and data analytics experts across the business to implement data solutions.
· Leads all phases of solution development. Explains technical considerations at related meetings, including those with internal clients and less experienced team members.
· Assesses data quality and tests code thoroughly for accuracy of intended purpose. Provides data analysis guidance and serves as a technical consultant for the client.
· Educates and develops junior data engineers on the team while applying quality control to their work. Develops data engineering standards and contributes expertise to other data expert teams across Vanguard.
· Tests and implements new software releases through regression testing. Identifies issues and engages with vendors to resolve and elevate software into production.
· Participates in special projects and performs other duties as assigned.
Qualifications
· Deep technical knowledge – including proficiency in at least two of Python, SQL, Hive, Spark, Amazon Web Services / cloud computing (e.g., Elastic MapReduce, EC2, S3), Bash shell scripting
· Experience writing production quality code to create data products
· Ability to effectively communicate technical concepts to non-technical audiences
Job Type: Contract
Pay: $55.00 - $75.00 per hour
Compensation package:
Hourly pay
Experience level:
10 years
Schedule:
8 hour shift
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
AWS: 3 years (Required)
ETL: 3 years (Required)
Data warehouse: 3 years (Required)
10 key typing: 9 years (Required)
Work Location: Remote",-1.0,Slesha inc,"Dallas, TX",-1,-1,-1,-1,-1,-1
199,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
200,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
201,Data Engineer,-1,"At MNTN, we've built a culture based on quality, trust, ambition, and accountability – but most importantly, we really enjoy working here. We pride ourselves on our self-service platform, originally coded by our President and CEO, and are constantly seeking to improve the user experience for our customers and scale for efficiency. Our startup spirit powers our growth mindset and supports our teammates as they build the future of ConnectedTV. We're looking for people who naturally want to do more, own more, and make an impact in their careers – and we're seeking someone to be part of our next stage of growth.
As a Senior Data Engineer on the Data team, you will help build the platform to generate, track, manage and triage key business and client success metrics. The goal is to have rapid insights across all available information to mitigate issues and identify opportunities for a smooth marketing experience.
You will:
Become the expert on the MNTN platforms, UI, data infrastructure, and data processes
Extract meaningful business metrics from raw data using SQL and other tools
Create and manage ETL/ELT workflows that transform our billions of raw data points daily into quickly accessible information across our databases and data warehouses
Organize data and metrics for measurable and trackable confidence in reporting and client performance to fulfill agreed-upon quality standards
Organize visualizations, reporting, and alerting necessary to rapidly illustrate performance, data quality, trends and opportunities
Investigate critical incidents and otherwise ensure that any issues reach resolution by the relevant parties
You have:
5+ years of experience related to data engineering, analysis and modeling complex data
Strong experience in SQL, data modeling, and manipulating and extracting large data sets.
Hands-on experience working with data warehouse technologies. Familiarity with building data pipelines and architectures and designing ETL flows.
Experience with programming languages such as Python, Java, or shell scripting. Familiarity with algorithms.
Familiarity with software processes and tools such as Git, CI/CD pipelines, Linux, and Airflow
Experience with working in a cloud computing environment such as AWS, Azure, or GCP
Familiarity in a business intelligence tool such as Domo, Looker or Tableau
Written and verbal communication skills to convey complex technical topics to non-technical audiences across the organization
MNTN Perks:
100% remote
Open-ended vacation policy with an annual vacation allowance
Three-day weekend every month of the year
Competitive compensation
100% healthcare coverage
401k plan
Flexible Spending Account (FSA) for dependent, medical, and dental care
Access to coaching, therapy, and professional development
About MNTN:
MNTN provides advertising software for brands to reach their audience across Connected TV, web, and mobile. MNTN Performance TV has redefined what it means to advertise on television, transforming Connected TV into a direct-response, performance marketing channel. Our web retargeting has been leveraged by thousands of top brands for over a decade, driving billions of dollars in revenue.
Our solutions give advertisers total transparency and complete control over their campaigns – all with the fastest go-live in the industry. As a result, thousands of top brands have partnered with MNTN, including, Petsmart, Build with Ferguson Master, Simplisafe, Yieldstreet and National University.
#Li-Remote",3.9,"MNTN
3.9",Remote,201 to 500 Employees,2009,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
202,Data Engineer,Employer Provided Salary:$91K - $116K,"A reasonable salary expectation is between $91,000.00 and $116,022.40, based upon experience and internal equity.
This position is on a hybrid work schedule. (Mon & Fri - remote, Tues - Thurs onsite in Rancho Cucamonga, CA)
Position Summary/Position
The Data Engineer II assists in the implementation of methods to improve data reliability and quality. This role is responsible for combining raw information from different sources to create consistent and machine-readable formats. The Data Engineer II must also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling. The Data Engineer II will focus on data accessibility, which will enable the organization to utilize data for performance evaluation and optimization. The data Engineer II is also responsible for managing the entire back-end development life cycle for the company's enterprise data warehouse. In this role the incumbent will handle tasks associated with the implementation of ETL procedures, building warehouse databases, database performance management, and dimensional modeling and design of the table structures.
Major Functions (Duties and Responsibilities)
1. Design and develop data warehouse Extraction, Transformation and Loading (ETL) solutions using Microsoft SQL Server Integration Services (SSIS), Azure Data Factory, Synapse Analytics, Az Data Bricks, PySpark ETL.
2. Develop and implement data collection processes in conjunction with the data warehouse. Source data from legacy systems supporting a centralized data warehouse and reporting platform.
3. Develop technical solutions to meet the requirements for Data Warehouse, BI & Analytics
4. Work closely with the data engineering and BI & Analytics teams to design data and analytics solutions to increase the usability, completeness, and accuracy of enterprise data
5. Analyze user requirements and translate into database requirements and implement in database code
6. Create and maintain the optimal data pipeline architectures based on micro services based on platform and application requirements
7. Assemble large, complex data sets that meet functional / non-functional business requirements
8. Identify, design, and implement process improvements: automating manual pipeline processes, optimizing data ingestion and consumption, re-designing infrastructure for greater scalability, micro services, etc
9. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
10. Work closely with Data Warehouse Architect and Data Systems Architect to design data and analytics solutions to increase the usability, completeness, and accuracy of enterprise data
11. Create, maintain, and optimize SQL queries and routines
12. Analyze potential data quality issues to determine the root cause and create effective solutions.
13. Develop, adopt, and enforce Data Warehouse and ETL standards and architecture
14. Monitor and support ETL processes ensuring integrity and proper integration of all data sources
15. Create high throughput historical and incremental ETL jobs
16. Facilitate problem management, and communication among data architects, managers, informaticists and analysts
17. Provide detailed analysis of data issues; data mapping; and the process for automation and enhancement of data quality
18. Perform development activities such as source to target mapping validations, identify, document and execute unit test cases/scripts, peer and lead code reviews per code review checklist and document test and review results.
19. Collaborate and contribute to data integration strategies and visions
20. Provide ongoing proactive technical support for ETL and data warehouse system to ensure business continuity.
21. Work with Informaticists and Analysts to translate analytic requirements into technical solutions.
Experience Qualifications
Four (4) years of relevant work experience. Experience and knowledge in logical, rational, dimensional, and physical data modeling. Background in database systems along with a strong knowledge of SQL. Experience with Orchestration tools, Azure DevOps, and CI/CD. Intermediate experience with the following tools and technologies:
a. Azure Data Catalogue / Purview
b. Azure Cloud
c. Databricks
d. Power BI Dataflows
e. Power Query
f. Azure Cosmos
g. Azure Monitor
h. PowerShell
i. Python
Preferred Experience
Development experience using PySpark, Spark, Hadoop, Kubernetes, and RDMIS is highly desired.
Education Qualifications
Bachelor's degree from an accredited institution required.
Preferred Education
Master’s degree from an accredited institution preferred.
Professional Certification
Azure Data Engineering Certification is preferred.
Knowledge Requirement
Multi-server environment knowledge such as linked servers, data replication, backup/restore with MS SQL Server 2008+. Knowledge of applicable data privacy practices and laws.
Skills Requirement
Highly skilled in developing and optimizing T-SQL (DDL, DML, DCL) queries, stored procedures, functions, and views for various applications that involve numerous database tables and complex business logic. Good written and oral communication skills. Strong technical documentation skills. Good interpersonal skills.
Abilities Requirement
Highly self-motivated and directed. Keen attention to detail. Proven analytical and problem-solving abilities. Ability to effectively prioritize and execute tasks in a high-pressure environment.
Commitment to Team Culture
The IEHP Team environment requires a Team Member to participate in the IEHP Team Culture. A Team Member demonstrates support of the Culture by developing professional and effective working relationships that include elements of respect and cooperation with Team Members, Members and associates outside of our organization.
Working Conditions
Word processing and programming involving computer keyboard and screens.
Position is eligible for Hybrid work location upon completing the necessary steps and receiving HR approval. All IEHP positions approved for telecommute or hybrid work locations may periodically be required to report to IEHP’s main campus for mandatory in-person meetings or for other business needs as determined by IEHP leadership.
Inland Empire Health Plan (IEHP) is the largest not-for-profit Medi-Cal and Medicare health plan in the Inland Empire. We are also one of the largest employers in the region, designated as “Great Place to Work.” With a provider network of more than 5,000 and a team of more than 3,000 employees, IEHP provides quality, accessible healthcare services to more than 1.5 million members. And our Mission, Vision, and Values help guide us in the development of innovative programs and the creation of an award-winning workplace. As the healthcare landscape is transformed, we’re ready to make a difference today and in the years to come. Join our Team and make a difference with us! IEHP offers a competitive salary and stellar benefit package with a value estimated at 35% of the annual salary, including medical, dental, vision, team bonus, and state pension plan.
Job Types: Full-time, Permanent
Pay: $91,000.00 - $116,022.40 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Parental leave
Retirement plan
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Rancho Cucamonga, CA 91730: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
data engineering: 4 years (Required)
Work Location: Hybrid remote in Rancho Cucamonga, CA 91730",3.7,"Inland Empire Health Plan
3.7","Rancho Cucamonga, CA",1001 to 5000 Employees,1996,Company - Public,Health Care Services & Hospitals,Healthcare,$1 to $5 billion (USD)
203,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
204,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
205,Data Engineer I - People Analytics,$73K - $111K (Glassdoor est.),"POSITION SUMMARY Our People Analytics team is expanding! The Data Engineer I is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.

This position can work in a flexible, hybrid model with 3 days on-site in Appleton, WI and 2 days remote.

JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.

QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming. Preferred experience in Python applied to data engineering and transformation.
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated

#LI-Hybid

Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101 .",4.1,"US Venture
4.1","Appleton, WI",1001 to 5000 Employees,1951,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
206,Senior Data Engineer,Employer Provided Salary:$190K,"Welcome to the MOMENTUM Family!
MOMENTUM is not just our company name; it is the highest value we deliver to our customers. We are a rapidly growing technology solutions company delivering innovative technology, engineering, and intelligence solutions across the DoD sector. The efforts of our high-capacity team ultimately strengthen our Nation and the warfighter.

Our team is dispersed throughout the US, which means we value the diversity and unique collaboration fostered throughout our team. We work incredibly hard for our customers and believe deeply in our core values. We're a high-energy, high-growth team and we love to win.

Data Engineer
The Data Engineer provides engineering support to the data science and software engineering team members. Includes augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments. Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, to include hardware and software support to existing servers.

In this role, you will:
Support ad-hoc data analysis requirements defined by the client's Leadership.
Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives.
Knowledge of business intelligence reporting tools and data visualization software including Tableau.
Will work on Data cleaning and transformation efforts in delivery of CDRLs.
Experienced in extracting and aggregating structured and unstructured data.
Experienced in data programming languages and tools such as Python and R.
Experience with SQL or similar database language.
Experience designing and implementing data models to enable, sustain and enhance the value of information they contain.
Strong analytical and critical thinking skills.
Ability to work collaboratively and effectively in a team environment.
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers
Make sure pedigree and provenance of the data is maintained such that the access to data is protected
Clean and preprocess data to enable analytic access
Collaborate with enterprise working groups to advance the state of data standards
Collaborate with the engineering team, data stewards, and mission partners to aid in processes getting actionable value out of the data holdings architects complex, repeatable ETL
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic and others
Supports Experience with Targeting using Sponsor Tools, Reverse Engineering
Develop API connectors to enable ingest of new data catalog entries from databases and files to ensure that data mappings will provide the best performance for expected user experience
Supports Deliverables and Reports

If you're suitable for this role, you have:
Top Secret SCI with FULL SCOPE POLY REQUIRED
9+ Years of verifiable experience


To learn more about us, check out our website at www.gomomentum.tech!

MOMENTUM is an EEO/M/F/Veteran/Disabled Employer:
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The qualifications listed above are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions.

Accommodations:
Consistent with the Americans with Disabilities Act (ADA) and Alabama civil rights law, it is the policy of Momentum to provide reasonable accommodation when requested by a qualified applicant or employee with a disability, unless such accommodation would cause an undue hardship. The policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If reasonable accommodation is needed, please include a request when applying.",3.6,"Momentum
3.6","Chantilly, VA",501 to 1000 Employees,1987,Company - Public,Advertising & Public Relations,Media & Communication,$100 to $500 million (USD)
207,Data Engineer,Employer Provided Salary:$150K - $300K,"What we're looking for
Stytch is the platform for user authentication. We build infrastructure that sits in the critical path of our customer's applications. As a data engineer, you'll work on designing and building event-driven architecture systems to drive analytics insights and observability tooling for our customers.

What Excites You
Championing data-driven insights - you see data analytics and observability as a product critical to success
Solving problems with pragmatic solutions — you know when to make trade-offs between completeness and utility and you know when to cut scope to ship something good enough quickly
Building products that make developers lives easier — as a data engineer for a developer infrastructure company, what you build will have an immediate impact on our customers
Shaping the culture and growing the team through recruiting, mentorship, and establishing best practices
Learning new skills and technologies in a fast paced environment

What Excites Us
Comfort working in a modern data stack using tools like Snowflake, Redshift, DBT, Fivetran, ElasticSearch, and Kinesis
Appreciation for schema design and architecture that balance flexibility and simplicity
Experience designing and building highly reliable back-end and ETL systems
3+ years as a data or backend engineer

What Success Looks Like
Technical — build new, highly reliable services that our customers can depend on
Ownership — advocate for projects and solutions that you believe in and ship them to production
Leadership — level up your teammates by providing mentorship and guidance

Our Tech Stack
Data moves through Snowflake, ElasticSearch, MySQL, and Kinesis
Go and Node for application services
We run on AWS with Kubernetes for containerization
gRPC and protobufs for internal service communication

Expected base salary $150,000-$300,000. The anticipated base salary range is not inclusive of full benefits including equity, health care insurance, time off, paid parental leave, etc. This base salary is accurate based on information at the time of posting. Actual compensation for hired candidates will be determined using a number of factors including experience, skills, and qualifications.
We're looking to hire a GREAT team and that means hiring people who are highly empathetic, ambitious, and excited about building the future of user authentication. You should feel empowered to apply for this role even if your experience doesn't exactly match up to our job description (our job descriptions are directional and not perfect recipes for exactly what we need). We are committed to building a diverse, inclusive, and equitable workspace where everyone (regardless of age, education, ethnicity, gender, sexual orientation, or any personal characteristics) feels like they belong. We look forward to hearing from you!

Learn more about our team and culture here!",5.0,"Stytch
5.0","San Francisco, CA",1 to 50 Employees,2020,Company - Public,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
208,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
209,Azure Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Required Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 9 years (Required)
Python: 8 years (Required)
Data lake: 7 years (Required)
Data Bricks: 7 years (Required)
Work Location: Remote",-1.0,TekValue IT Solutions,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
210,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
211,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
212,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
213,"Staff Engineer, Data Infrastructure",Employer Provided Salary:$180K - $240K,"The Mission:
This is a critical and exciting time at Enigma. We're transforming the small business financing ecosystem, and our product is gaining adoption even faster than we anticipated. This creates a new set of technical challenges for us as we continue to scale.
Over the past year we've made significant investments in our data infrastructure to allow engineers and data scientists to quickly deliver customer value by reliably testing and shipping changes to our data pipeline. The challenges ahead are to:
Reduce overall runtime of our data pipeline
Improve computational efficiency to reduce costs
The Role:
This is a role responsible for developing and implementing our strategy for driving data processing efficiency. Your impact will be measured by:
The increase in the team's capability to optimize data transformations (combination of methodology and tooling)
The specific improvements we achieve in processing time and computational cost
This role has an important hands-on component - you'll be showing the team what good looks like. However, your greatest impact will be in the technical strategy you'll formulate and the close coaching and mentorship you'll provide to team members.
We're looking for someone who:
Is motivated by leading engineers to increase the efficiency and speed of complicated data processing systems
Thrives as a coach and mentor and measures their impact by the increase in capabilities of the team
Operates with a bias for action and knows how to deliver value in the short, medium and long term
Adopts a principled approach to difficult data processing problems and demonstrates expertise and excellence in their engineering craft
Operates transparently, collaboratively and with low ego—loves learning from others and having their ideas questioned and challenged
What makes this job interesting?
Impact: Your decisions will determine the speed at which we can scale to take on new customers and impact the developer experience of dozens of teammates.
Technical Challenge: You will take on some of our thorniest technical problems and render them tractable.
Leadership: Success in this role will achieved through building the team's capabilities and influencing our engineering culture.
Our ideal candidate:
Knows Databricks and Spark inside and out
Brings a clear point of view on data processing optimization, data modeling, cluster management and Databricks performance techniques
Has a strong track record of technical leadership to grow and scale teams
About Us:
At Enigma, we're building the single, most reliable source of data on businesses to power the future of financial services. By engineering better data from hundreds of public and third-party sources, we aim to tell the complete story of every business, so that companies of every size can access the financial services they need to grow and thrive. Our core values – generosity, curiosity, ingenuity, & drive – guide everything we do, from how we make our most important product decisions to how we work with and support one another on a daily basis. We're a team of curious, driven individuals with diverse backgrounds and skills, but we're all passionate about engineering deeper understanding through data—together. If this resonates, we would love to hear from you!
We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.
Salary Range: $180,000-$240,000
A note on salary ranges: we pride ourselves on paying competitively for our size and industry. Salary is one piece of a total Enigma compensation package that includes additional benefits and opportunities. All of our compensation packages include equity because we believe 100% of Enigma employees should have the option to purchase ownership in the company and benefit from the value we're creating together.",-1.0,Enigma,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
214,Senior Data Engineer,$87K - $114K (Glassdoor est.),"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.

This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration.
How will you make an impact?
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
What are we looking for?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework
You might be a good fit if you like to:
Leverage technology to deliver data solutions to drive business outcomes
Drive for continuous improvement of processes and solutions
Bring energy and commitment to excellence to drive delivery of high-quality solutions
Share knowledge and mentor team members on technologies and solutions
What you can learn on the job:
Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers
Grow practical understanding of agile work practices through application of the scrum framework to deliver work products
Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science
What does success look like?
First 30 days:
Understand the existing analytics data platform and technologies
Understand team’s work management processes
First 6 months:
Develop data solutions for the business including shared components and specific data sets
Learn business processes and develop necessary contacts throughout the organization
First year:
Continue to develop data solutions for the business
Become expert on the team for other data engineers to go to for mentoring in completing their work
Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,"AE Business Solutions
4.2","Milwaukee, WI",51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
215,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
216,"Manager, Data Engineer & ETL Processing",$83K - $118K (Glassdoor est.),"Company Description

About Spark Foundry:

Spark Foundry is a global media agency that exists to bring HEAT – Higher Engagement, Affinity, and Transactions – to brands. By combining flawless media fundamentals with aggressive innovation, Spark inspires consumers to pay more attention, to care more about our clients’ brands, and to buy more products and services from them.
Balancing the nimble spirit of a startup with the powerhouse soul of Publicis Media, Spark Foundry delivers the best of both worlds to a client roster that spans some of the world’s best and most beloved brands and companies. We combine boutique-caliber insights and service with the buying clout and first-look access of a global leader, bringing the heat to challenger brands that want to act like giants, and to giant brands that want to act like challengers.
With a bottom-up culture that celebrates diversity and aims for all voices to be heard, Spark has become a magnet for the industry’s best talent, with one of the best retention rates in the industry. And by applying a whole-person approach to professional and personal development, Spark develops a workforce that is well prepared for today’s challenges, and also poised to create meaningful careers in the years to come.
Because we know that heat arises the intersection of complementary forces, our professionals come from myriad disciplines and backgrounds: data, analytics, and insights, content and creative production, communications and strategy, finance and marketing, and sociology, psychology, and other liberal arts disciplines.

Job Description

Overview:
The Manager, Data Engineering & ETL Development is a key driver to build data platform leveraging best in class ETL practice and is also a strategic thinker and a talented data visualization expert with emphasis on dashboard reporting. The primary responsibility will be developing business intelligence platform to support media and marketing data for our clients. This position will allow you to be a significant contributor as part of the team to support easy access of data and visualizations to fuel stronger insights and media optimizations.
This position requires strong technical and tactical skill sets with an eye for numbers, intellectual curiosity, proficiency at problem solving, and a critical understanding of online media. The candidate must have a proven track record in managing ETL’s, APIs and business intelligence platforms. Candidate should be a team player. A “roll up the sleeves” approach is mandatory and a “get it done” attitude is a must. Specific responsibilities include coordination between the research, analytics & media teams ensuring high quality data projects are effectively delivered.
Successful candidates will be multi-dimensional ‘rising-stars’ who are able to employ complex problem solving skills, and are able to communicate these succinctly to a broad client and media stakeholder audience.
Role Objectives:
Responsible for loading and validating data into the data warehouse from various source systems
Analyze, develop, fix, test, review and deploy functionality, and bug fixes in ETL data pipelines
Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required
Building Data mappings between Source to Target systems
Provides support for technical issues and ensuring system availability
Work with business customers to identify and develop additional data and reporting needs
Understand how business intelligence platform/data technologies work and offer the ability to explain technical concepts in ordinary terms (be technically savvy - understand the opportunities and limitations)
Establish and manage data integrations utilizing a taxonomy nomenclature to make recommendations for data visualization to showcase media performance through the use of Datorama or Tableau
Perform regular quality assurance/quality control checks on assigned client campaigns to ensure the data is processing accurately
Design and build backend data streams and processes to automate reporting capability with data visualization tools
Contribute to client status and reporting calls, including presentation of reporting as required
Develop subject matter expertise in ETL, API development, and Business Intelligence platforms
Clearly define project deliverables, timelines, and dependencies for junior team members, internal stakeholders and clients
Collaborate on an inclusive team, where members openly communicate and collectively problem-solve
Strong ability to evaluate new technologies and present findings to team
Contribute to knowledge sharing efforts and mentorships
Complete other duties as assigned.

Qualifications
Bachelor’s degree or combination of education, and equivalent work experience is preferred in the field of computer science, management information systems or Information Technology
3+ years’ related experience ETL development and business intelligence platform management
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
3+ plus years’ development experience with database engines including Presto, Mongo db and Hadoop.
3+ years’ experience in ETL development, Strong Database (Modeling, SQL), SQL Server, Hadoop, AWS Redshift, Qubole.
Experience managing team of 2 or more associates/analysts preferred
Strong database modeling and SQL skills. Ability to write complex SQL statement, Procedures and data automation programs
Knowledge of ETL tools like Airflow, AWS Glue, Alteryx, SSIS, Qubole/Spark is a must
Knowledge of Python or similar programming languages required
Proficiency with Datorama, Tableau, or other data visualization tools is preferred
Experience working with AWS or other cloud technologies
Advanced user Microsoft Office Suite and reporting tools
Demonstrated expertise in core MS Excel functions (vlookup, pivot tables, data visualization)
Experience in designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.
Strong analytical and problem-solving skills
Strong verbal/written communication and interpersonal skills is required
Self-starter with ability to thrive in a fast-paced environment and able to function independently while providing status updates to a team of analysts
Cooperative, flexible, conscientious, dependable, resourceful, self-motivated, and team-oriented
Problem solving, time management, and critical thinking skills with a professional and positive attitude
Ability to work independently and as part of an agile team, participating in daily stand-ups, sprint planning and sprint review

Character:
The following qualities help drive success as member of the Spark Data and Analytics team:
Entrepreneurial, engaging, resourceful, curious, and self-directed spirit
Willing and easily roll sleeves up or down; love the nitty-gritty and the strategy
Collaborative approach to building cohesive, strong teams
Loving and living the intersections between brands, people, media, communications
Relentlessly passionate and resolute
Planning and time management excellence.
Embrace challenges
Proactive, especially in pushing for new opportunities, approaches, and ideas.
Keenly focused on action and solutions; thrives with deep critical thinking and analysis.
Pioneering insight attitude and research in-the-know.
Resourcefulness, flexibility and adaptability, strong ability to pivot when the need arises.
Inspired to be part of the insight journey/revolution with a growing, dedicated team

Additional Information

All your information will be kept confidential according to EEO guidelines.
23-2795",3.9,"Spark Foundry
3.9","Dallas, TX",51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
217,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
218,Senior Data Engineer,Employer Provided Salary:$166K - $232K,"Patreon is the best place for creators to build memberships by providing exclusive access to their work and a deeper connection with their communities. We're building a content and community platform where creators can engage directly with their fans and monetize their creativity, while maintaining full ownership over the work they make and the communities they create.
We're leaders in the membership space with 250,000+ active creators and over $3.5 billion paid directly to creators on our platform. Our team is building tools to optimize the creator-to-fan relationship, including native video, enhanced podcasting features, improved creation tools, and new community experiences. We're continuing to invest heavily in building the most talented team in the Creator Economy and are looking for a Data Engineer to support our mission.
This role is available to those wishing to work in our SF and NY offices on a hybrid work model or those wishing to be fully remote in the United States.
About the Role
Work on a tight-knit team of highly motivated and experienced data engineers with frequent collaboration with data scientists, product managers and product engineers.
Work on both ""data analytics"" and ""data infrastructure"" type projects in a fast-paced, high-growth startup environment.
Build core data sets and metrics to power analytics, reports and experimentation.
Write real-time and batch data pipelines to support a wide range of projects and features including our creator-facing analytics product, executive reporting, FP&A, marketing initiatives, model training, data science analytics, A/B testing, etc.
Help manage and build out our data platform and suite of data tools.
Be a driver of a data-centric culture at Patreon. Work autonomously on large green field initiatives and help define data best practices at the company.
About You
Expert in SQL, Spark and Python or Scala
Significant experience modeling data and developing core data sets and metrics to support analytics, reports and experimentation. Solid understanding of how to use data to inform the product roadmap.
Enjoy collaborating with Data Scientists, Product Managers and Product Engineers. Comfortable playing the role of a Project Manager in order to drive results.
Have previously built real-time and batch data pipelines using tooling such as Airflow, Spark, Kafka, S3, Fivetran, Census, etc. Experience working with event tracking frameworks, data observability frameworks and experimentation frameworks.
Experience managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc.
Highly motivated self-starter that is keen to make an impact and is unafraid of tackling large, complicated problems and putting in the work to ensure high craft deliverables.
About Patreon
Patreon powers creators to do what they love and get paid by the people who love what they do. Our team is passionate about making this mission and our core values come to life every day in our work. Through this work, our Patronauts:
Put Creators First | They're the reason we're here. When creators win, we win.****
Build with Craft | We sign our name to every deliverable, just like the creators we serve.
Make it Happen | We don't quit. We learn and deliver.
Win Together | We grow as individuals. We win as a team.
We hire talented and passionate people from different backgrounds because we want our company to represent the vast diversity of our creators. If you're excited about a role but your prior experience doesn't match with every bullet point outlined above, we strongly encourage you to apply anyway. If you're a creator at heart, are energized by our mission, and share our company values, we'd love to hear from you.
Patreon is proud to be an equal-opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.
Patreon offers a competitive benefits package including and not limited to salary, equity plans, healthcare, unlimited paid time off, company holidays and recharge days, commuter benefits, lifestyle stipends, learning and development stipends, patronage, parental leave, and 401k plan with matching.
The posted range represents the expected salary range for this job requisition and does not include any other potential components of the compensation package, benefits and perks previously outlined. Ultimately, in determining pay, we'll consider your experience, leveling, location and other job-related factors.
San Francisco Pay Range
$166,000—$231,500 USD",3.2,"Patreon
3.2","San Francisco, CA",51 to 200 Employees,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
219,Sr. Azure Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"LeadStack Inc. is an award-winning, one of the nation's fastest-growing, certified minority-owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Position Details:
Title: Azure Data Engineer
Location: Remote
Duration: 6+ Months Contract
Top 3 skills: SQL, Snowflake, and Data Warehousing/Design
Its a W2 Role. No C2C.
Client is seeking a talented and motivated Data Engineer to join our dynamic team. As a Data Engineer, you will play a critical role in developing and maintaining our data infrastructure, ensuring the smooth flow of data for our analytics and insights projects. The ideal candidate will have a strong background in SQL, data warehousing, ETL processes, and a preference for Snowflake experience. Additional experience with dbt or semantic layer tools will be a bonus, and proficiency in Python will also be advantageous.
Bachelor's degree in Computer Science, Engineering, or a related field.
Solid experience with SQL and data warehousing concepts.
Strong understanding of ETL processes and tools.
Preferred experience with Snowflake data warehouse platform.
Bonus for experience with dbt or other semantic layer tools.
Proficiency in Python is a plus.
Excellent problem-solving and analytical skills.
Strong attention to detail and ability to work in a fast-paced environment.
Effective communication skills and ability to collaborate with cross-functional teams.
Key Responsibilities:
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.
Optimize data ingestion and transformation processes to ensure efficient and scalable data flows.
Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.
Develop and maintain data models, schemas, and data dictionaries for efficient data storage and retrieval.
Perform data validation, quality checks, and troubleshooting to identify and resolve data-related issues.
Monitor and optimize database performance, including query tuning and index optimization.
Collaborate with data scientists and analysts to implement data models and support their data requirements.
Stay up-to-date with industry trends and best practices in data engineering, and proactively recommend and implement improvements to our data infrastructure
To know more about current opportunities at LeadStack, please visit us at https://leadstackinc.com/careers/
Should you have any questions, feel free to call me on 510-480-1006 or send an email on tarique.aziz@leadstackinc.com
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Data Warehousing/Design: 8 years (Required)
Snowflake: 5 years (Required)
Azure: 5 years (Required)
Work Location: Remote",4.3,"Leadstack Inc
4.3",Remote,51 to 200 Employees,2016,Company - Private,HR Consulting,Human Resources & Staffing,Unknown / Non-Applicable
220,Data Communications Engineer - III,Employer Provided Salary:$91K - $181K,"Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","San Francisco, CA",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
221,Sr. Data Engineer( ETL testing experience),Employer Provided Salary:$50.00 - $60.00 Per Hour,"Key Skills to evaluate – Python (advanced level), Pyspark, data flow pipeline in AWS, distributed system, Snowflake, Redshift, ETL testing, QE knowledge
JD:
· Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Job Type: Full-time
Pay: $50.00 - $60.00 per hour
Experience level:
8 years
Experience:
python advanced: 10 years (Preferred)
pyspark: 10 years (Preferred)
aws: 10 years (Preferred)
snowflake: 10 years (Preferred)
Work Location: Remote",-1.0,Sana Pivot Inc,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
222,Data Engineer,$88K - $119K (Glassdoor est.),"About the Job

Loopback is hiring an innovative and team-oriented Data Engineer/Operation who will be responsible for building and maintaining optimized and highly available data pipelines that facilitate deeper analysis and reporting across clinical sites in areas of Life Sciences industry analytics.

This engineer’s role will be to manage the data flow processes, analyze data, and lead partnerships with other Data and Analytics teams to identify and implement systems and process improvements. This engineer also designs, architects, implements, and supports key datasets.

Duties to Include

Manage ongoing shifts in data ingestion formats across Health Systems, Life Science, and Enterprise Partner types
Own code base, documentation, and roadmap for all data transformations via Data Lake and underlying Tables
Develop and sequence jobs and processes to transform data into data lake and data warehouse
Own integral Loopback data model, code base, and software to deliver against client/application SLA's
Design and develop highly scalable and reliable data engineering pipelines to process large volumes of data across diverse data sources and analytics use cases
Identify, design, and implement internal process improvements by automating manual processes and optimizing data delivery
Plan, coordinate and implement security measures to safeguard information in computer files against accidental or unauthorized damage, modification or disclosure.
Identify and implement ways to improve data reliability, efficiency, and quality
Develop and promote and implement best practices in data engineering
Business meetings to understand use cases and questions, capture agreement on business rules, understand analyst and stakeholder objectives, and support usage of data to solve business problems
Data profiling, data documentation, and measuring data quality with manual verification and
development of automated data quality tests

Requirements

You will thrive if you:
Exhibit a “self-starter” mindset in taking ownership over delegated responsibilities
Have excellent program/task organizational skills
Are detail and results oriented

You bring a toolkit of your past experiences of:
3-5 Years of experience as a Data Engineer/Operations
Implementing and designing data curation, and data analysis
Orchestrating data transformation through cloud native analytics platforms (Snowflake, Databricks) across cloud environments (Azure, AWS, GCP)
Building and modeling data in relational and non-relational data storage technologies including schema design, stored procedure development and performance and optimization techniques (e.g. SQL & NoSQL, C#, Python, etc.)
Effective collaboration, experienced in creating technical partnerships across teams
Learning and understanding the various technical domains across the enterprise and able to communicate complex technical and business concepts across the enterprise and various business stakeholders
Effectively delivering results in a fast-paced environment while managing multiple priorities
Documenting and testing your designed solutions, engaging with QA and DevOps teams
Contact

Founded in 2009, Loopback was rated as one of the best places to work in Dallas by the DBJ, Loopback Analytics is a leading provider of data-driven solutions for hospitals and health systems. The company’s comprehensive analytics platform drives growth for specialty and
ambulatory pharmacy programs while connecting pharmacy activities with clinical and economic outcomes. Loopback’s clients include leading Academic Medical Centers, health systems, and Life Sciences companies. For more information about our company and services please visit our website at www.loopbackanalytics.com.

This employer will not sponsor applicants for employment visa status (e.g., H1-B) for this position. All applicants must be currently authorized to work in the United States on a full-time basis.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.",4.4,"Loopback Analytics
4.4","Dallas, TX",51 to 200 Employees,2009,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
223,Data Engineer,$80K - $108K (Glassdoor est.),"True Homes, one of the Charlotte Observer's Top Workplaces, is growing! Because of this tremendous growth, we are searching for a Data Engineer to join our Enterprise Technology Services team! If you are looking to join a team that values you and want to be part of a company that takes a new approach to home building and workplace culture, please apply now!
This is an on-site position located at our Corporate Office in Monroe, NC.
The Data Engineer will play a key role in developing and maintaining the company’s data infrastructure. The Data Engineer will be accountable for ensuring that data remains readily available and of high quality to cater to the organization's analytics and business intelligence needs. The Data Engineer is responsible for developing data integration, governance, and solutions in a way that leverages modern technology and design so that our enterprise data can provide value to business strategy through accessible and trusted data and to enhance our associate and stakeholder experiences.
Responsibilities:
Collaborates with the BI and Application Development teams to design and implement data architectures, Extract, Transform, and Load (ETL) processes, and data models that support our analytics and reporting needs
Develops and maintains scalable and efficient data pipelines to ingest, transform, and integrate data from diverse sources into our data warehouse or data lake
Ensures data quality and consistency by implementing data validation and cleansing processes
Monitors and troubleshoots data pipelines, proactively identifying and resolving any issues to maintain the reliability and accuracy of our data assets
Optimizes data workflows and performance, identifies opportunities for improvement and implements enhancements to maximize efficiency
Monitors emerging trends and technologies in data engineering and updates and makes recommendations for process improvements and tool selection
Documents data engineering processes, data lineage, and data flows to facilitate knowledge sharing and maintains a high level of data governance
Results/Accountability:
Delivers high-quality and accurate data pipelines, ensuring reliable and timely availability of data to the BI team and other stakeholders by assigned deadline
Optimizes data workflows to improve efficiency, reducing processing time and enhancing overall system performance
Ensures data accuracy and integrity, with a focus on reducing data quality issues through proactive data validation and cleansing techniques
Collaborates effectively with the BI team, enabling seamless data integration and supporting the timely delivery of analytics and reporting projects by assigned deadline
Contributes to the growth and success of the BI team by actively participating in team discussions, sharing knowledge, and driving process improvements
Demonstrates accountability for data engineering projects, meeting project milestones and delivering results within agreed timelines
Remains current with advancements in data engineering technologies and proactively identifies opportunities for innovation and recommends their implementation
Qualifications:
Bachelor’s Degree in Computer Science, Computer Engineering, Data Science, or a related field.
1 to 3 years of experience in data integration, data pipelines, ETL processes, data modeling, data warehousing, and data governance.
Proficiency in programming languages such as Spark, Python, Java, Scala, or SQL.
Strong understanding of databases, relational database management systems (RDBMS), and SQL querying.
Understanding of data quality principles and experience with implementing data quality checks and validation processes.
Possess strong problem-solving and analytical abilities to identify and resolve data-related issues.
Knowledge of Microsoft Power Platform or Microsoft Fabric, preferred
General Requirements:
Excellent attention to detail
Strong communication skills, written, and verbal skills
Strong organizational skills
Ability to work in a fast-paced environment
Comply with all company policies and procedures
Demonstrate the qualities and character traits as defined in the Sustainable Competitive Advantage
Physical Requirements:
Must be able to remain in a stationary position 75% of the time
Associate needs to occasionally move about the office to access file cabinets, office equipment, etc.
Constantly operates a computer and other office equipment
True Homes offers a highly competitive salary in addition to benefits including health and vision program and company paid dental, Life & AD&D, short-term and long term disability and 18 days of paid time off in your first year.
Employment with True Homes is contingent on the ability to successfully pass a drug screen and background check.
True Homes is an equal opportunity employer.
To learn more about True Homes, visit www.truehomesusa.com",4.4,"True Homes
4.4","Monroe, NC",201 to 500 Employees,2007,Company - Private,Construction,"Construction, Repair & Maintenance Services",Unknown / Non-Applicable
224,Data Engineer,$78K - $112K (Glassdoor est.),"Translate business requirements and functional specifications into logical program designs and to deliver code modules, stable application systems, and software solutions.
Develop, configure, or modify complex integrated business and/or enterprise application solutions within various computing environments.
Implement and maintain complex business and enterprise software solutions to ensure successful deployment of released applications.
Translate complex cross-functional business requirements and functional specifications into logical program designs, code modules, stable application systems, and software solutions.
Partner with Product Team to understand business needs and functional specifications. .
Collaborate with cross-functional teams to ensure specifications are converted into flexible, scalable, and maintainable solution designs.
Evaluate project deliverables to ensure they meet specifications and architectural standards.
Coordinate, execute, and participate in component integration (CIT) scenarios, systems integration testing (SIT), and user acceptance testing (UAT) to identify application errors and to ensure quality software deployment.
Participate in all software development end-to-end product lifecycle phases by applying and sharing an in-depth understanding of complex company and industry methodologies, policies, standards, and controls.
Architect software design patterns and approaches, application level software architecture and make technical trade-off decisions at application level.
Automate and simplify team development, test, and operations processes.
Develop detailed architecture plans for large scale enterprise architecture projects and drives the plans to fruition.
Solve complex architecture/design and business problems; solutions are extensible; work to simplify, optimize, remove bottlenecks, etc.
Minimum Education Required:- All the responsibilities mentioned above are in line with the professional background and requires an absolute minimum of bachelor’s degree in computer science, computer information systems, information technology or a combination of education and experience equating to the U.S. equivalent of a bachelor's degree in one of the aforementioned subjects.",4.9,"Wise Equation Solutions
4.9","Mooresville, NC",51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
225,Data Engineer ll,Employer Provided Salary:$130K - $160K,"The Company
Have you ever found yourself or a loved one waiting hours and hours in a hospital Emergency Room to get care? Or have you ever had a surgery scheduled for months in the future that needed to happen sooner? Unfortunately, our healthcare system is full of these types of operational problems. Our work saves lives and helps hospitals cut tens of millions of dollars in operational costs, while improving the quality of care they’re able to deliver.
Qventus is a real-time decision making platform for hospital operations. Our mission is to simplify how healthcare operates, so that hospitals and caregivers can focus on delivering the best possible care to patients. We use artificial intelligence and machine learning to create products that help nurses, doctors, and hospital staff anticipate issues and make operational decisions proactively.
Qventus works with leading public, academic and community hospitals across the United States. The company was recognized by the 2019 Black Book Awards in healthcare for patient flow and by CB Insights as a 2019 top 100 Most Promising Company in Artificial Intelligence. Recently, Qventus won the Robert Wood Johnson Foundation Emergency Response for the Healthcare System Innovation Challenge through its work helping health systems across the country plan for and operate in the COVID pandemic.
The role
Qventus is looking for a Data Platform Engineer II to help scale our solutions, focusing on our analytical and data science needs. The Data Platform team acts as stewards for Qventus’ data. We stream hospital EMR to our core warehouses in real time, build out curated data layers to power our Healthcare AI & Analytical insights and overall ensure Qventus data users have the tools they need to explore and power the Qventus product at scale and cost to improve the lives of patients and doctors across the country.
As a Data Platform Engineer II, you will build and own significant components of the Qventus solution pipelines. You will be comfortable designing, building, and leading cross-functional initiatives with analytical and data science partners - from schema design, to pipeline design, to scaling services to support company expansion within the healthcare space (and HIPAA restrictions). You will have a strong passion for well designed data models and be motivated and excited to have an impact on the team and in the company and to improve the quality of healthcare operations.
Key Responsibilities
Work closely with core data users to understand product needs and design, build, tune and improve our core data assets and the overall end-to-end workflow of data users at Qventus (incl. designing data structures, building and scheduling data transformation pipelines, improving transparency etc.).
Automate & manage the lifecycle of data sets (schema development, deprecation, and iteration).
Improve the data quality and transparency of the pipelines (defining data requirements, identifying and implementing data observability tooling - lineage, sources, transformations).
Work closely with core team members to develop, test, deploy, and operate high quality, scalable software and raise engineering standards.
Key Qualifications
Demonstrated experience in data modeling / schema design and transformation pipeline implementation in collaboration with data science and analytics partners
Experience developing & coordinating execution in a fast paced dynamic environment across multiple technologies
Strong cross-functional communication - ability to break down complex technical components for technical and non-technical partners alike
Interest in mentoring and supporting new developers particularly in data modeling and analytical collaboration
3+ years of professional experience working with modern programming languages such as Java, C/C++, Python with a dedication to high code quality.
Nice to Have Skills
Degree in Computer Science, Engineering, or related field, or equivalent training / experience
Competence participating in technical architecture discussions to help drive high quality technical development within your team
Practical hands on experience with:
building large-scale, high complexity metrics and monitoring
ELK, DBT, Snowflake, AWS, Terraform, Looker, Ansible experience
Experience building and maintaining robust and efficient backend data systems with functional proficiency with AWS cloud services & modern data warehouse services (Snowflake)
We consider several factors when determining compensation, including location, experience, and other job-related factors.
Salary Range: $130,000 to $160,000 annually + equity + benefits- Qventus expects to hire for this position near the middle of the range. Only in truly rare or exceptional circumstances where a candidate's experience, credentials, or expertise far exceed those required or expected will we consider and offer at the top of the salary range.
Qventus offers a competitive benefits package including medical, dental, vision, paid time off, company holidays, and a stock option plan.
Qventus is an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
Candidate information will be treated in accordance with our candidate privacy notice which can be found here: https://qventus.com/ccpa-privacy-notice/
This position does not provide visa sponsorship.
Employment is contingent upon the satisfactory completion of our pre-employment background investigation and drug test.
#LI-REMOTE",4.3,"Qventus
4.3",Remote,51 to 200 Employees,2012,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
226,Data Engineer,$77K - $107K (Glassdoor est.),"Position Summary:
Reporting to the Director of Data Science, the Data Engineer will play a crucial role in designing, developing, and maintaining data pipelines and workflows to facilitate data integration, storage, and analysis. With the support of the Data Science team, this position will utilize proven expertise in Microsoft Azure cloud computing to implement and automate processes, ensure the health and efficiency of the data ecosystem, and bring unmatched value to our +1.5 million members across the country and across the world.

Duties and Responsibilities:
Lead architectural efforts for implementing the Data Science team's models and solutions.
Design, implement, and manage data pipelines using Azure Data Factory to extract, transform, and load data from various sources efficiently and securely.
Develop and optimize storage solutions on Microsoft Azure, ensuring data availability, scalability, and reliability.
Automate data-related jobs, processes, and workflows to streamline data delivery
Collaborate closely with cross-functional teams, including data scientists, analysts, and developers, to understand data requirements and support their data-related needs.
Design and maintain data models and architecture, enabling efficient data consumption by business applications and analytical tools.
Ensure data quality, integrity, and security throughout the data lifecycle.
Troubleshoot and promptly resolve data-related issues, performance bottlenecks, and system failures.
Research new data sources, third-party platforms, and data engineering best practices.
Stay current with the latest data engineering and cloud computing trends, particularly in the Azure ecosystem.
Assist Data Scientists with general analysis and dashboards, as needed.
Qualifications:
3-5+ years of relevant Data Engineering, Software Engineering, and/or Analytics experience.
Proved experience with Azure cloud solutions, preferably through your work portfolio and Azure certifications.
Strong proficiency in Azure Data Factory, Azure Storage, and other relevant Azure services.
Extensive experience designing and implementing automated data pipelines, ETL processes, and data workflows.
Advanced skills in SQL and Python for data manipulation, querying, and scripting tasks.
Familiarity with data warehousing concepts, modeling, and integration techniques.
Expert in integrating cloud solutions with external API sources and knowledgeable in creating in-house API solutions that meet our needs.
Excellent problem-solving abilities and a track record of handling complex data engineering challenges.
Strong communication skills and the ability to collaborate effectively with multidisciplinary teams.
Demonstrated ability to prioritize tasks, meet deadlines, and adapt to evolving project requirements.
Azure certifications (e.g., Azure Data Engineer Associate) will be a plus.
Willingness to travel up to 10%.
Organizational Overview:
The National Association of REALTORS® (NAR) is a team of professionals dedicated to providing world-class service to over 1.5 million REALTORS® working in the United States and around the world. The real estate industry is fast-paced and fast-changing--each year, our members participate in the sale, lease, and management of billions of dollars in real estate. As in every industry, our members' value proposition is constantly being challenged by innovation.

It is our mission to empower REALTORS® as they preserve, protect and advance the right to real property for all. We cannot do that without the ideas, passion, and commitment from our talented employees. As our greatest assets, employees are offered their pick of competitive benefits/perks and flexible work options. NAR is certified as a Great Place to Work®. Our most successful employees are committed to our Core Values, which are:

Putting members first
Leading change
Advancing Diversity and Inclusion
Giving respect
Collaborating
Communicating",3.7,"National Association of REALTORS®
3.7","Washington, DC",201 to 500 Employees,1908,Nonprofit Organization,Real Estate,Real Estate,$100 to $500 million (USD)
227,Sr. Data Engineer I,$82K - $109K (Glassdoor est.),"Pax8 is the leading value-added cloud-based SaaS distributor, simplifying the cloud journey for our partners by integrating technology, business intelligence and proactive service to deliver an unparalleled experience. Serving thousands of partners through the indirect sales channel, our mission is to be the world's favorite place to buy cloud products. We are a fast-growing, dynamic and high-energy startup organization, allowing you to make a meaningful impact on the business. Culture is important to us, and at Pax8, it's business, and it IS personal. We are passionate, creative and humorously offbeat. We work hard, keep it fun, and expect the best.
We Elev8 each other. We Advoc8 for our partners. We Innov8 continuously. We Celebr8 life.
No matter who you are, Pax8 is a place you can call home. We know there's no such thing as a ""perfect"" candidate, so we don't look for the right ""fit"" – instead, we look for the add. We encourage you to apply for a role at Pax8 even if you don't meet 100% of the bullet points. We believe in cultivating an environment with a diversity of perspectives, in hopes that we can all thrive in an inclusive environment.
We are only as great as our people. And we have great people all over the world. No matter where you live and work, you're a part of the Pax8 team. This means embracing hybrid- and remote-work whenever possible.
Position Summary:
The Sr. Data Engineer I designs, develops, tests, deploys, maintains, and improves systems that collect, transform, store, and manage data for end users. They deliver individual projects based upon deadlines and required deliverables. The Sr. Engineer builds complex features independently and collaborates with other teams to conduct design and code reviews. They develop and/or provide technical leadership in the development of data systems involving the application of new technologies with significant technical risk. The Sr. Engineer prepares detailed plans, which may span over a year for complex projects. They determine test philosophy, goals, and objectives, and participate in the formation of project goals, scope, and schedule.
Essential Responsibilities:
Builds pipelines to ingest new data sources
Transforms data to support varied use cases
Includes testing in all aspects of the development process
Mentors junior and mid-level Engineers
Optimizes existing data pipelines and improves existing code quality
Makes updates and improvements to deployment processes
Participates in project planning and architecture discussions
Analyzes potential problems and finds solutions to pressing data issues
Participates in on-call rotation
Ideal Skills, Experience, and Competencies:
At least Four (4) years of relevant data engineering experience
Advanced experience with Python
Expert experience with SQL
Intermediate experience with a JVM language
Exposure to other software development languages
Advanced experience with Apache Spark or other distributed processing engines
Advanced experience with Apache Kafka or other stream processing frameworks
Intermediate experience with Terraform, Docker, Kubernetes, or other similar infrastructure tooling
Advanced experience with cloud data tools such as S3, Glue, and Athena
Intermediate experience with building CI/CD pipelines
Effective problem solving and troubleshooting abilities
Ability to consistently achieve results, even under tough circumstances
Effective technical leadership abilities
Excellent verbal and written communication skills
Experience with innovative application design and implementation
Ability to make sense of complex, high quantity, and sometimes contradictory information to effectively solve problems
Required Education & Certifications:
B.A./B.S. in related field or equivalent work experience
M.S./M.A. in related field or equivalent work experience
Compensation:
Qualified candidates can expect a salary beginning at $140,000 or more depending on experience
#LI-Remote #LI-JF1 #Dice-J #BI-Remote

Note: Compensation is benchmarked on local Denver Metro area market rates. Qualified candidates in other locations can expect a salary package that may be adjusted based off applicable cost of wages in their respective location.
At Pax8 we believe that your Total Rewards should include a benefits package that shows how much we value our greatest assets. All FTE Pax8 people enjoy the following benefits:
Non-Commissioned Bonus Plans or Variable Commission
401(k) plan with employer match
Medical, Dental & Vision Insurance
Employee Assistance Program
Employer Paid Short & Long Term Disability, Life and AD&D Insurance
Flexible, Open Vacation
Paid Sick Time Off
Extended Leave for Life events
RTD Eco Pass (For local Colorado Employees)
Career Development Programs
Stock Option Eligibility
Employee-led Resource Groups
Pax8 is an EEOC Employer.",4.1,"Pax8
4.1","Greenwood Village, CO",501 to 1000 Employees,2012,Company - Private,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
228,Junior Data Engineer,$83K - $119K (Glassdoor est.),"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a junior data engineer, you will be part of a team that is responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.
The position:
Develop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Support various components of the data pipelines, including ingestion, validation, cleansing and curation
Manage and ensure the success of ongoing data pipeline routines
Collaborate with our implementation team to assist with the identification and reconciliation of data anomalies
Create and maintain documentation on data pipelines
Engage with our software engineering team to ensure precise data points per application specifications
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering, or applicable experience
Expertise with SQL, database design and data manipulation methodologies
Expertise with ETL/ELT and the development of automated validation and data pipelines
Strong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW, master data management and other database design principles
Comfortable working with high volume data in a variety of formats
Excellent verbal and written communication
Self-motivated
Passionate at learning
Familiarity with healthcare data is a plus
Experience with CI/CD and version control tools is a plus
Experience working within hybrid cloud environment such as AWS is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation
Social outings",5.0,"Decision Point Healthcare
5.0","Boston, MA",1 to 50 Employees,2013,Company - Public,-1,-1,Less than $1 million (USD)
229,Big Data Engineer,Employer Provided Salary:$40.00 - $50.00 Per Hour,"This is NOT a C2C ROLE.
This is the Full-Time/ Contract W2 opportunity with Amazee Global Ventures Inc.
Title: Big Data Engineer
Location: Phoenix AZ (Onsite) ONLY LOCALS
Technical Skills:
Big Data Engineering – PySpark, Hive, ETL automation, Data Pipeline optimization.
Must have 7+ years (preferably more) in working with these technologies and understand the details of how PySpark and Hive are used to optimize SQL queries and data pipelines.
Entry-level experience will not work for these roles.
Other Key Skills:
Must be able to work autonomously.
Collect requirements from business stakeholders, driving solution design and implementation.
Can potentially lead other team members.
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Full-time
Salary: $40.00 - $50.00 per hour
Ability to commute/relocate:
Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",5.0,"Amazee Global Ventures Inc
5.0","Phoenix, AZ",1 to 50 Employees,2019,Company - Public,-1,-1,Less than $1 million (USD)
230,Data Engineer,$88K - $124K (Glassdoor est.),"Recently awarded one of Crain's Best Places to Work in Chicago®, Premier International is a privately held and private equity backed software and technology consulting firm headquartered in downtown Chicago, serving large enterprise consulting and Fortune 500 firms deploying large-scale systems implementations.
You will work in a fast-paced environment that exposes you to diverse project experiences as we collaborate to solve our clients' biggest data challenges.
The Opportunity:
Premier International is hiring an experienced Data Engineer to join our growing Data Governance Practice. You will work in a fast-paced environment that exposes you to diverse project experiences, leading-edge technologies, and continuous learning experiences that will grow your career while solving clients' biggest challenges.
Our Data Governance Practice delivers end-to-end business advisory services, implementation, and technical solutions for the Data Governance Lifecycle including Consulting, Metadata Integration, Reference Data Management, Sensitive Data Management, Tool Evaluation, and Product Implementation.
What You'll Be Doing:
Designing, implementing, and maintaining Data Warehouse environments
Creating and maintaining comprehensive documentation of data engineering processes, pipelines, and workflows
Collaborating effectively with cross-functional teams, including data scientists and analysts, to understand their data needs and support data-related initiatives
What You'll Bring to the Team:
Bachelor's degree in Data Science, Computer Science, Statistics, or a related field
8+ years of relevant experience in data migration
Proficient in Python and Spark development/programming with a focus on performance and scalability
Experience with version control systems, GitHub, for code integrity
Strong analytical mindset and the ability to derive actionable insights from data
Excellent communication and presentation skills, capable of conveying complex information in a clear and concise manner
Ability to work independently and collaboratively in a fast-paced and dynamic environment
Premier Perks & Benefits:
Highly competitive compensation with annual bonus incentive
401K plan with company match
Company paid individual health, dental, vision, disability, and life insurance coverage
Four weeks of paid time off
Nine company paid holidays
Employee referral bonuses
Much more at one of Chicago's Best and Brightest Companies to Work For®!
Premier has been named one of The Best and Brightest Companies to Work For® in Chicago (2019, 2020 & 2021), one of Crain's top 100 Best Places to Work in Chicago (2020 & 2021) and recently made the 2021 Inc. 5000 list of America's Fastest-Growing Private Companies. While we are relentlessly client-focused, we are proud to have our culture and company recognized by others.
Premier is an EEO Employer and provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",4.2,"Premier International
4.2","Chicago, IL",1 to 50 Employees,1985,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
231,Sr. Data Engineer,$80K - $112K (Glassdoor est.),"Object Technology Solutions, Inc (OTSI) has an immediate opening for Sr. Data Engineer
This is an on-site position in Dallas, TX OR Kansas City, MO (Long term)
Skills & abilities required:
Minimum 5+ years of experience as Data Engineer.
Prefer to have Databricks experience
Prefer to have understanding of embedding Python into Azure environment (containerization)
Expectation is that this position works with Data Scientist to rapidly deploy models into Azure DW and/or pipeline DW required data to Data Scientist requirements to build and test new models.
About us:
OTSI is a leading global technology company offering solutions, consulting, and managed services for businesses worldwide since 1999. OTSI serves clients from its 15 offices across 6 countries around the globe with a “Follow-the-Sun” model. Headquartered in Overland Park, Kansas, we have a strong presence in North America, Central America, and Asia-Pacific with a Global Delivery Center based in India. These strategic locations offer our customers the competitive advantages of onshore, nearshore, and offshore engagement and delivery options, with 24/7 support. OTSI works with 100+ enterprise customers, of which many are Fortune ranked, OTSI focuses on industry segments such as Banking, Financial Services & Insurance, Healthcare & Life Sciences, Energy & Utilities, Communications & Media Entertainment, Engineering & Telecom, Retail & Consumer Services, Hi-tech, Manufacturing, Engineering, transport logistics, Government, Defense & PSUs.
Our Center of Excellence:
Data & Analytics
Digital Transformation
QA & Automation
Enterprise Applications
Disruptive Technologies
Job Type: Contract
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: In person",4.6,"OTSi
4.6","Dallas, TX",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
232,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
233,Staff Data Engineer,Employer Provided Salary:$165K - $278K,"USA (Remote)
Affinity stitches together billions of data points from massive datasets to create a powerful, accurate representation of the world's professional relationship graph. Based on this data, we offer our users the insights and visibility they need to nurture and tap into their team's network of opportunities.
Reporting to the Director of Engineering, you'll support creating the magic that underlies Affinity's industry-leading relationship intelligence model as the key technical leader of Affinity’s Data Enrichment team.
In this role, you’ll leverage your past experiences and deep understanding of back-end technologies to help shape and execute Affinity's roadmap for dataflow and system architecture, champion engineering best practices, delivery velocity, and act as a technical mentor for other engineers on the team. You’ll play a significant role in defining the future of how businesses around the world use their relationships.
What you’ll be doing:
Drive complex technical, architecture, design, and product discussions
Lead data domain, technical and business discussions in relation to future architecture direction
Design, implement, and build data solutions that deliver data with measurable quality using Spark, Python, Databricks, and the AWS ecosystem (S3, Redshift, EMR, Athena, Glue)
Help define our data roadmap. You'll collaborate with our fast-growing team of data engineering, machine learning engineering, product, and business leaders to help to answer these questions and more
Mentoring, coaching, and inspiring the engineers on the team
Identify and fill gaps in the team, and create the processes necessary for the teams’ success

Qualifications
Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every qualification. At Affinity, we are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about this role, but your past experience doesn’t perfectly align with the qualifications above, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Required:
You have 10+ years of experience working in data engineering, with at least 3+ years of acting as a senior team lead or staff engineer, leading complex, sometimes ambiguous engineering projects across team boundaries
You have extensive hands-on experience in building scalable data platforms and reliable data pipelines using technologies such as Spark, Hadoop, DataBricks, AWS SQS, AWS Kinesis, and/or Kafka
You have experience working with large, multi-terabyte datasets and are comfortable with high-scale data ingestion, transformation, and distributed processing tools such as Apache Spark (Scala or Python)
Experience with AWS, DBX or related cloud technologies
You're comfortable with the building blocks of modern back-end systems, such as horizontally scalable data infrastructure, event-driven architecture, and beyond and can clearly articulate the pros/cons of different approaches, while also providing a recommended solution based on the current context
You have familiarity with databases and analytics technologies in the industry, including Data Warehousing, Data Lakes, ETL and Relational Databases
You have experience mentoring and helping the engineers around you grow
You have experience partnering with product and machine learning teams on large, strategic data projects and routine partner work
You take pride in delivering exceptionally high quality work in terms of data accuracy, performance, and reliability
You’re eager to contribute your ideas and experiences to help Affinity continuously improve as a product and company
Nice to have:
Experience leveraging machine learning to improve the quality of ingested data.
You have worked with multiple third party data vendors and have experience in conflict resolution approaches.

How we work:
Our culture is a key part of how we operate as well as our hiring process:
We iterate quickly. As such, you must be comfortable embracing ambiguity, be able to cut through it, and deliver incremental value to our customers each sprint
We are candid, transparent, and speak our minds while simultaneously caring personally with each person we interact with
We make data driven decisions and make the best decision for the moment based on the information available
Join us in enabling every professional on the planet to succeed by harnessing the power of their relationships.

What you'll enjoy at Affinity:
We live our values as playmakers, obsessed with learning, caring personally about our colleagues and clients, are radically open-minded, and take pride in everything we do.
We pay your medical, dental, and vision insurance with comprehensive PPO and HMO plans. And provide flexible personal & sick days. We want our team to be happy and healthy :)
We offer a 401k plan to help you plan for retirement.
We provide an annual budget for you to spend on education and offer a comprehensive L&D program – after all, one of our core values is that we're #obsessedwithlearning!
We support our employee's overall health and well-being and reimburse monthly for things such as; transportation, Home Internet, Meals, and Wellness memberships/equipment.
Virtual team building and socials. Keeping people connected is essential.
Please note that the role compensation details below reflect the base salary only and do not include any variable pay, equity, or benefits. This represents the salary range that Affinity believes, in good faith, at the time of this posting, that it will pay for the posted job.
A reasonable estimate of the current range is $165,000 to $278,000 USD. Within the range, individual pay is determined by factors such as job-related skills, experience, and relevant education or training.
About Affinity
We have raised over $120M and are backed by some of Silicon Valley’s best firms, with over 2,700 customers worldwide on our platform. We are proud to have a 4.5 Star Glassdoor rating and recently ranked; Inc.’s Best Workplaces of 2022 and Great Places to Work 2022. Passionate about helping dealmakers in the world’s biggest relationship-driven industries to find, manage, and close the most important deals; our Relationship Intelligence platform uses the data exhaust of trillions of interactions between Investment Bankers, Venture Capitalists, Consultants, and other strategic dealmakers with their networks to deliver automated relationship insights that drive over 450,000 deals every month.",4.1,"Affinity.co
4.1",Remote,201 to 500 Employees,2015,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
234,Data center infrastructure Engineer,Employer Provided Salary:$60.00 - $68.00 Per Hour,"Requirements and Responsibilities:
Must have Bachelor’s degree in IS or equivalent.
Must have experience with VMWARE, DATA CENTER OPERATIONS, IT INFRASTRUCTURE, and VIRTUALIZATION.
Experience working with Linux (CentOS/RHEL/Rocky) .
Experience working with VMWare 6.x / 7.x and storage area network (SAN).
Manage and support Microsoft domain / Active Directory Services, RBAC, Group Policy, Windows Servers, desktops and services automate tasks through scripting technologies Support execution of Information
Support and manage the VMWare virtualization and storage (SAN - Dell Extreme IO & Pure Storage) infrastructure to meet and exceed development operations growth expectations.
Job Type: Contract
Pay: $60.00 - $68.00 per hour
Benefits:
Vision insurance
Experience:
Data center operations: 4 years (Required)
IT infrastructure: 3 years (Required)
Work Location: In person",3.0,"BJ Services
3.0","Exton, PA",1001 to 5000 Employees,1872,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",$1 to $5 billion (USD)
235,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
236,Data Engineer,Employer Provided Salary:$122K - $144K,"Join a leading fintech company that's democratizing finance for all.
Robinhood was founded on a simple idea: that our financial markets should be accessible to all. With customers at the heart of our decisions, Robinhood is lowering barriers and providing greater access to financial information. Together, we are building products and services that help create a financial system everyone can participate in.
As we continue to build...
We're seeking curious, growth minded thinkers to help shape our vision, structures and systems; playing a key-role as we launch into our ambitious future. If you're invigorated by our mission, values, and drive to change the world — we'd love to have you apply.
Robinhood has a primary in-office working environment; please be sure you have reviewed the preferred working location(s) for this role before applying.
About the team:
The preferred location for this position is in or around Robinhood's offices in Menlo Park, CA or New York, NY with in-office work capabilities, as may be required by management.
Robinhood is a metrics driven company and data is foundational to all key decisions from growth strategy to product optimization to our day-to-day operations. We are looking for a Senior Data Engineer to build and maintain foundational datasets that will allow us to reliably and efficiently power decision making at Robinhood. These datasets include application events, database snapshots, and the derived datasets that describe and track Robinhood's key metrics across all products. You'll partner closely with engineers, data scientists and business teams to power analytics, experimentation, and machine learning use cases. We are a fast-paced team in a fast growing company and this is a unique opportunity to help lay the foundation for reliable, impactful, data-driven decisions across the company for years to come.
What you'll do day-to-day:
Help define and build key datasets across all Robinhood product areas. Lead the evolution of these datasets as use cases grow.
Build scalable data pipelines using Python, Spark and Airflow to move data from different applications into our data lake.
Partner with upstream engineering teams to enhance data generation patterns.
Partner with data consumers across Robinhood to understand consumption patterns and design intuitive data models.
Ideate and contribute to shared data engineering tooling and standards.
Define and promote data engineering best practices across the company.
About you:
CS background or any other relevant fields of study.
Strong product mindset
4+ years of experience and a Bachelors degree or 3+ years and a Masters degree
Experience building high-quality data solutions
Strong analytical and problem solving skills.
Expertise building data pipelines using open source frameworks (Hadoop, Spark, etc)
Expertise in one or more programming languages (ideally Python).
Strong SQL (Presto, Spark SQL, etc) skills.
Familiarity with data visualization tools (Looker, Tableau, etc).
Great communication skills and ability to democratize data through actionable insights and solutions.
Bonus points:
Passion for working and learning in a fast-growing company.
The expected salary range for this role is based on the location where the work will be performed and is aligned to one of 3 compensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood's equity plan.

US Zone 1: $157000 - $185000
US Zone 2: $139000 - $163000
US Zone 3: $122000 - $144000
Base pay for the successful applicant will depend on a variety of job-related factors, which may include education, training, experience, location, business needs, or market demands. You can view comp zones for our US office locations in the table below. For other locations not listed, compensation can be discussed with your recruiter during the interview process.
Office locations (by comp zone)
US Zone 1: Menlo Park, NYC, Seattle, Washington DC
US Zone 2: Denver, Westlake (Dallas), Chicago
US Zone 3: Lake Mary
Click here to learn more about Robinhood's Benefits.
Robinhood promotes diversity and provides equal opportunity for all applicants and employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and skills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone. Additionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy rights. To review Robinhood's Privacy Policy please visit Robinhood - US Applicant Privacy Policy. If you are an applicant located in the UK or EEA, please visit the Robinhood - UK/EEA Applicant Privacy Policy.",3.4,"Robinhood
3.4","Menlo Park, CA",1001 to 5000 Employees,2013,Company - Public,Investment & Asset Management,Financial Services,Unknown / Non-Applicable
237,"Data Center Network Engineer, Infrastructure Network Engineering",-1,"What to Expect
Tesla is currently seeking a Network Engineer to join our Data Center team. This role will provide network design, implementation, and operational support for Tesla's Data Centers.
What You’ll Do
Help design, build and maintain new and existing Data Centers
Work closely with other team members on design and initiatives; maintain and grow existing data center networks.
Work with Tesla’s key application teams to support their growth, including Tesla Autopilot team.
Provide high availability & reliability to network
Requirements gathering, analyze, and propose solution to networking needs.
Monitor, analyze, and report metrics of network services.
Develop automation methods to rapidly deploy, configure, and update network equipment.
Assist with network troubleshooting.
Conduct product POC evaluation.
Document network knowledge base and operational “Run-Book.”
Must be able to work occasional weekends, after hours, and holidays.
Participate in on call rotation.
May require unscheduled after-hours work. 10-20% travel required as necessary.
What You’ll Bring
4+ years’ experience mid-large global enterprise networking infrastructure
Experience with mid/large-scale networks in a global environment
Juniper, Arista and Palo Alto Networks hardware
Experience in IP networking, L2/L3 network protocols (spanning-tree, OSPF, BGP), TCP/IP, DHCP, DNS, end to end QOS, VLAN, VRRP, LACP, MC-LAG, EVPN with VXLAN, ACL and infrastructure cabling.
Basic knowledge of AWS, Azure or GCP.
Experience with various tools such as Protocol Analyzer, SNMP, flow, IPAM, RADIUS, Splunk, network taps, and load/stress testing",3.6,"Tesla
3.6","Fremont, CA",10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$1 to $5 billion (USD)
238,Data Engineer- Hadoop,Employer Provided Salary:$63.00 - $66.00 Per Hour,"Position Summary:
Synthetic data engineer is the experienced professional who can bring together skills and technologies to be able design and develop solutions to deliver data required for training models/test applications. Candidate will be working with a team of resources using Python, Sqoop, pyspark to create the data in a most efficient manner. Candidate should have prior experience & knowledge in highly scalable production grade applications with a strong engineering acumen. Should be able to work in a collaborative, fast paced environment. Candidate must have knowledge and/or experience working with Hadoop ecosystem.
Skills:
• Advanced knowledge in Python Programming
• Familiarity with Faker module
• pyspark, Sqoop, Hadoop, Hive, Shell script, Hadoop Shell commands
• Good experience dealing with bulk data extracts and bulk data loads
• Ability to translate requirements from stake holders into the technical requirements and the code
• Experience on statistical distribution, agent to model , deep learning is a plus
• Background of MF environment is a plus
Job Type: Contract
Pay: $63.00 - $66.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
Hadoop: 7 years (Required)
Python: 5 years (Required)
ETL: 3 years (Preferred)
Work Location: Hybrid remote in Charlotte, NC 28203",3.9,"Data Inc
3.9","Charlotte, NC",501 to 1000 Employees,1983,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
239,Sr. Data Engineer,Employer Provided Salary:$84K - $131K,"Title: Sr Data Engineer
Location: Remote
Length: Contract-To-Hire
WHAT YOU'LL NEED:
Bachelor’s degree in computer science, Management information systems (MIS) or related degree / experience commensurate to a degree preferred
5+ years of hands-on experience implementing, maintaining, and supporting data management solutions including program/project delivery
7+ years of experience with SQL & T-SQL code development - experience with Snowflake preferred
5+ years of experience with Python – A MUST HAVE!
4+ years of experience with Databricks
4+ years of experience designing, building and deploying solutions with Azure Data Factory – A MUST HAVE !
4+ years of experience with logical modeling for visualization tools (Tableau, Power BI, Sigma)
4+ years of Experience with Data lake technologies including ADLS Gen. 2, AWS S3, AWS Glue preferred
Job Type: Full-time
Pay: $84,181.34 - $130,840.44 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Data Engineer: 10 years (Required)
Azure Data Lake: 6 years (Required)
Data bricks: 5 years (Preferred)
Python: 4 years (Required)
Work Location: Remote",3.9,"G Associates LLC
3.9",Remote,501 to 1000 Employees,2003,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
240,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
241,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
242,"Manager, Data Engineer & ETL Processing",$83K - $118K (Glassdoor est.),"Company Description

About Spark Foundry:

Spark Foundry is a global media agency that exists to bring HEAT – Higher Engagement, Affinity, and Transactions – to brands. By combining flawless media fundamentals with aggressive innovation, Spark inspires consumers to pay more attention, to care more about our clients’ brands, and to buy more products and services from them.
Balancing the nimble spirit of a startup with the powerhouse soul of Publicis Media, Spark Foundry delivers the best of both worlds to a client roster that spans some of the world’s best and most beloved brands and companies. We combine boutique-caliber insights and service with the buying clout and first-look access of a global leader, bringing the heat to challenger brands that want to act like giants, and to giant brands that want to act like challengers.
With a bottom-up culture that celebrates diversity and aims for all voices to be heard, Spark has become a magnet for the industry’s best talent, with one of the best retention rates in the industry. And by applying a whole-person approach to professional and personal development, Spark develops a workforce that is well prepared for today’s challenges, and also poised to create meaningful careers in the years to come.
Because we know that heat arises the intersection of complementary forces, our professionals come from myriad disciplines and backgrounds: data, analytics, and insights, content and creative production, communications and strategy, finance and marketing, and sociology, psychology, and other liberal arts disciplines.

Job Description

Overview:
The Manager, Data Engineering & ETL Development is a key driver to build data platform leveraging best in class ETL practice and is also a strategic thinker and a talented data visualization expert with emphasis on dashboard reporting. The primary responsibility will be developing business intelligence platform to support media and marketing data for our clients. This position will allow you to be a significant contributor as part of the team to support easy access of data and visualizations to fuel stronger insights and media optimizations.
This position requires strong technical and tactical skill sets with an eye for numbers, intellectual curiosity, proficiency at problem solving, and a critical understanding of online media. The candidate must have a proven track record in managing ETL’s, APIs and business intelligence platforms. Candidate should be a team player. A “roll up the sleeves” approach is mandatory and a “get it done” attitude is a must. Specific responsibilities include coordination between the research, analytics & media teams ensuring high quality data projects are effectively delivered.
Successful candidates will be multi-dimensional ‘rising-stars’ who are able to employ complex problem solving skills, and are able to communicate these succinctly to a broad client and media stakeholder audience.
Role Objectives:
Responsible for loading and validating data into the data warehouse from various source systems
Analyze, develop, fix, test, review and deploy functionality, and bug fixes in ETL data pipelines
Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required
Building Data mappings between Source to Target systems
Provides support for technical issues and ensuring system availability
Work with business customers to identify and develop additional data and reporting needs
Understand how business intelligence platform/data technologies work and offer the ability to explain technical concepts in ordinary terms (be technically savvy - understand the opportunities and limitations)
Establish and manage data integrations utilizing a taxonomy nomenclature to make recommendations for data visualization to showcase media performance through the use of Datorama or Tableau
Perform regular quality assurance/quality control checks on assigned client campaigns to ensure the data is processing accurately
Design and build backend data streams and processes to automate reporting capability with data visualization tools
Contribute to client status and reporting calls, including presentation of reporting as required
Develop subject matter expertise in ETL, API development, and Business Intelligence platforms
Clearly define project deliverables, timelines, and dependencies for junior team members, internal stakeholders and clients
Collaborate on an inclusive team, where members openly communicate and collectively problem-solve
Strong ability to evaluate new technologies and present findings to team
Contribute to knowledge sharing efforts and mentorships
Complete other duties as assigned.

Qualifications
Bachelor’s degree or combination of education, and equivalent work experience is preferred in the field of computer science, management information systems or Information Technology
3+ years’ related experience ETL development and business intelligence platform management
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
3+ plus years’ development experience with database engines including Presto, Mongo db and Hadoop.
3+ years’ experience in ETL development, Strong Database (Modeling, SQL), SQL Server, Hadoop, AWS Redshift, Qubole.
Experience managing team of 2 or more associates/analysts preferred
Strong database modeling and SQL skills. Ability to write complex SQL statement, Procedures and data automation programs
Knowledge of ETL tools like Airflow, AWS Glue, Alteryx, SSIS, Qubole/Spark is a must
Knowledge of Python or similar programming languages required
Proficiency with Datorama, Tableau, or other data visualization tools is preferred
Experience working with AWS or other cloud technologies
Advanced user Microsoft Office Suite and reporting tools
Demonstrated expertise in core MS Excel functions (vlookup, pivot tables, data visualization)
Experience in designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.
Strong analytical and problem-solving skills
Strong verbal/written communication and interpersonal skills is required
Self-starter with ability to thrive in a fast-paced environment and able to function independently while providing status updates to a team of analysts
Cooperative, flexible, conscientious, dependable, resourceful, self-motivated, and team-oriented
Problem solving, time management, and critical thinking skills with a professional and positive attitude
Ability to work independently and as part of an agile team, participating in daily stand-ups, sprint planning and sprint review

Character:
The following qualities help drive success as member of the Spark Data and Analytics team:
Entrepreneurial, engaging, resourceful, curious, and self-directed spirit
Willing and easily roll sleeves up or down; love the nitty-gritty and the strategy
Collaborative approach to building cohesive, strong teams
Loving and living the intersections between brands, people, media, communications
Relentlessly passionate and resolute
Planning and time management excellence.
Embrace challenges
Proactive, especially in pushing for new opportunities, approaches, and ideas.
Keenly focused on action and solutions; thrives with deep critical thinking and analysis.
Pioneering insight attitude and research in-the-know.
Resourcefulness, flexibility and adaptability, strong ability to pivot when the need arises.
Inspired to be part of the insight journey/revolution with a growing, dedicated team

Additional Information

All your information will be kept confidential according to EEO guidelines.
23-2795",3.9,"Spark Foundry
3.9","Dallas, TX",51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
243,Data Communications Engineer - III,Employer Provided Salary:$91K - $181K,"Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","San Francisco, CA",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
244,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
245,Data Engineer- Hadoop,Employer Provided Salary:$63.00 - $66.00 Per Hour,"Position Summary:
Synthetic data engineer is the experienced professional who can bring together skills and technologies to be able design and develop solutions to deliver data required for training models/test applications. Candidate will be working with a team of resources using Python, Sqoop, pyspark to create the data in a most efficient manner. Candidate should have prior experience & knowledge in highly scalable production grade applications with a strong engineering acumen. Should be able to work in a collaborative, fast paced environment. Candidate must have knowledge and/or experience working with Hadoop ecosystem.
Skills:
• Advanced knowledge in Python Programming
• Familiarity with Faker module
• pyspark, Sqoop, Hadoop, Hive, Shell script, Hadoop Shell commands
• Good experience dealing with bulk data extracts and bulk data loads
• Ability to translate requirements from stake holders into the technical requirements and the code
• Experience on statistical distribution, agent to model , deep learning is a plus
• Background of MF environment is a plus
Job Type: Contract
Pay: $63.00 - $66.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
Hadoop: 7 years (Required)
Python: 5 years (Required)
ETL: 3 years (Preferred)
Work Location: Hybrid remote in Charlotte, NC 28203",3.9,"Data Inc
3.9","Charlotte, NC",501 to 1000 Employees,1983,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
246,Data Engineer,$77K - $107K (Glassdoor est.),"Position Summary:
Reporting to the Director of Data Science, the Data Engineer will play a crucial role in designing, developing, and maintaining data pipelines and workflows to facilitate data integration, storage, and analysis. With the support of the Data Science team, this position will utilize proven expertise in Microsoft Azure cloud computing to implement and automate processes, ensure the health and efficiency of the data ecosystem, and bring unmatched value to our +1.5 million members across the country and across the world.

Duties and Responsibilities:
Lead architectural efforts for implementing the Data Science team's models and solutions.
Design, implement, and manage data pipelines using Azure Data Factory to extract, transform, and load data from various sources efficiently and securely.
Develop and optimize storage solutions on Microsoft Azure, ensuring data availability, scalability, and reliability.
Automate data-related jobs, processes, and workflows to streamline data delivery
Collaborate closely with cross-functional teams, including data scientists, analysts, and developers, to understand data requirements and support their data-related needs.
Design and maintain data models and architecture, enabling efficient data consumption by business applications and analytical tools.
Ensure data quality, integrity, and security throughout the data lifecycle.
Troubleshoot and promptly resolve data-related issues, performance bottlenecks, and system failures.
Research new data sources, third-party platforms, and data engineering best practices.
Stay current with the latest data engineering and cloud computing trends, particularly in the Azure ecosystem.
Assist Data Scientists with general analysis and dashboards, as needed.
Qualifications:
3-5+ years of relevant Data Engineering, Software Engineering, and/or Analytics experience.
Proved experience with Azure cloud solutions, preferably through your work portfolio and Azure certifications.
Strong proficiency in Azure Data Factory, Azure Storage, and other relevant Azure services.
Extensive experience designing and implementing automated data pipelines, ETL processes, and data workflows.
Advanced skills in SQL and Python for data manipulation, querying, and scripting tasks.
Familiarity with data warehousing concepts, modeling, and integration techniques.
Expert in integrating cloud solutions with external API sources and knowledgeable in creating in-house API solutions that meet our needs.
Excellent problem-solving abilities and a track record of handling complex data engineering challenges.
Strong communication skills and the ability to collaborate effectively with multidisciplinary teams.
Demonstrated ability to prioritize tasks, meet deadlines, and adapt to evolving project requirements.
Azure certifications (e.g., Azure Data Engineer Associate) will be a plus.
Willingness to travel up to 10%.
Organizational Overview:
The National Association of REALTORS® (NAR) is a team of professionals dedicated to providing world-class service to over 1.5 million REALTORS® working in the United States and around the world. The real estate industry is fast-paced and fast-changing--each year, our members participate in the sale, lease, and management of billions of dollars in real estate. As in every industry, our members' value proposition is constantly being challenged by innovation.

It is our mission to empower REALTORS® as they preserve, protect and advance the right to real property for all. We cannot do that without the ideas, passion, and commitment from our talented employees. As our greatest assets, employees are offered their pick of competitive benefits/perks and flexible work options. NAR is certified as a Great Place to Work®. Our most successful employees are committed to our Core Values, which are:

Putting members first
Leading change
Advancing Diversity and Inclusion
Giving respect
Collaborating
Communicating",3.7,"National Association of REALTORS®
3.7","Washington, DC",201 to 500 Employees,1908,Nonprofit Organization,Real Estate,Real Estate,$100 to $500 million (USD)
247,Data Governance Engineer,Employer Provided Salary:$100K - $125K,"Performance Health is seeking a Data Governance Engineer to join our team. In this role, you will be responsible for leveraging advanced data analytics techniques to optimize our manufacturing operations, streamline supply chain logistics, enhance overall efficiency, and support master data governance initiatives.

Essential Job Duties & Responsibilities
Develop and implement predictive and prescriptive analytics models to forecast demand, optimize inventory levels, and improve production scheduling in alignment with healthcare industry standards.
Analyze and interpret large datasets from various sources, including production systems, distribution centers, and market trends, to identify opportunities for process improvement, cost reduction, and master data cleansing.
Collaborate with manufacturing, procurement, and distribution teams to define key performance indicators (KPIs), establish data-driven goals, and measure progress towards operational excellence.
Design and execute A/B tests to evaluate the impact of process enhancements and initiatives, continuously refining strategies for improved outcomes.
Utilize machine learning techniques for anomaly detection, fault prediction, and quality control to ensure compliance with regulatory standards and product quality assurance.
Develop data visualizations, dashboards, and reports to effectively communicate insights and recommendations to stakeholders at all levels.
Stay current with advancements in data science methodologies, tools, and technologies, and proactively identify opportunities to apply them to manufacturing and distribution challenges.
Collaborate with Operational Excellence and Data Governance and IT teams to ensure data accessibility, integrity, and security, and assist in data integration efforts.
Performs other duties as assigned

Job Qualifications
Bachelor’s degree in Data Analytics or related field
3-5 years of experience applying data science techniques to manufacturing, supply chain, or distribution challenges, preferably within the healthcare industry.
Proficiency in programming languages such as Python or R for data analysis and statistical modeling
Strong expertise in data manipulation, feature engineering, and data preprocessing techniques.
Proficiency in SQL for data querying and manipulation.
Experience with data visualization tools (e.g., Tableau, Power BI) to create clear and impactful visualizations.
Excellent problem-solving skills and the ability to work effectively in cross-functional teams.
Ability to travel 10% of the time, including overnight travel

Benefits
Our benefits include healthcare; insurance benefits; retirement programs; paid time off plans; family and parenting leaves; wellness programs; discount purchase programs.
This is a full-time position with a base salary range of $100,000-$125,000 plus benefits.

To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. The requirements listed above are representative of the knowledge, skills, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Performance Health is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to sex, gender, gender identity, sexual orientation, race, color, religion, national origin, disability status, protected Veteran status, age, genetic information, and any other characteristic protected by law.",2.7,"Performance Health Supply,LLC
2.7","Warrenville, IL",Unknown,-1,Company - Private,Health Care Products Manufacturing,Manufacturing,Unknown / Non-Applicable
248,AWS Data Engineer Snowflake,-1,"Company Description

Orchestrated by adept technical architects with over fifty years of applied expertise, KYNITE is an advanced technology company specializing in the disciplines of: Blockchain, Cloud Services, Big Data & Analytics, Artificial Intelligence, Enterprise, Staff Augmentation and Managed Services
We are BigData Experts
We are Cloud Experts
We are Enterprise Architects
We are Artificial Intelligence Innovators
We are Technological Evangelists
We are Doers
We are Kynite

Additional Information

All your
This job is only for individuals residing in US
US Citizens, Green Card holders, EAD's can apply
W2, C2c,
Information will be kept confidential according to EEO guidelines.",-1.0,Kynite,"Myrtle Point, OR",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
249,"Data Center Network Engineer, Infrastructure Network Engineering",-1,"What to Expect
Tesla is currently seeking a Network Engineer to join our Data Center team. This role will provide network design, implementation, and operational support for Tesla's Data Centers.
What You’ll Do
Help design, build and maintain new and existing Data Centers
Work closely with other team members on design and initiatives; maintain and grow existing data center networks.
Work with Tesla’s key application teams to support their growth, including Tesla Autopilot team.
Provide high availability & reliability to network
Requirements gathering, analyze, and propose solution to networking needs.
Monitor, analyze, and report metrics of network services.
Develop automation methods to rapidly deploy, configure, and update network equipment.
Assist with network troubleshooting.
Conduct product POC evaluation.
Document network knowledge base and operational “Run-Book.”
Must be able to work occasional weekends, after hours, and holidays.
Participate in on call rotation.
May require unscheduled after-hours work. 10-20% travel required as necessary.
What You’ll Bring
4+ years’ experience mid-large global enterprise networking infrastructure
Experience with mid/large-scale networks in a global environment
Juniper, Arista and Palo Alto Networks hardware
Experience in IP networking, L2/L3 network protocols (spanning-tree, OSPF, BGP), TCP/IP, DHCP, DNS, end to end QOS, VLAN, VRRP, LACP, MC-LAG, EVPN with VXLAN, ACL and infrastructure cabling.
Basic knowledge of AWS, Azure or GCP.
Experience with various tools such as Protocol Analyzer, SNMP, flow, IPAM, RADIUS, Splunk, network taps, and load/stress testing",3.6,"Tesla
3.6","Fremont, CA",10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$1 to $5 billion (USD)
250,Senior Data Engineer,Employer Provided Salary:$166K - $232K,"Patreon is the best place for creators to build memberships by providing exclusive access to their work and a deeper connection with their communities. We're building a content and community platform where creators can engage directly with their fans and monetize their creativity, while maintaining full ownership over the work they make and the communities they create.
We're leaders in the membership space with 250,000+ active creators and over $3.5 billion paid directly to creators on our platform. Our team is building tools to optimize the creator-to-fan relationship, including native video, enhanced podcasting features, improved creation tools, and new community experiences. We're continuing to invest heavily in building the most talented team in the Creator Economy and are looking for a Data Engineer to support our mission.
This role is available to those wishing to work in our SF and NY offices on a hybrid work model or those wishing to be fully remote in the United States.
About the Role
Work on a tight-knit team of highly motivated and experienced data engineers with frequent collaboration with data scientists, product managers and product engineers.
Work on both ""data analytics"" and ""data infrastructure"" type projects in a fast-paced, high-growth startup environment.
Build core data sets and metrics to power analytics, reports and experimentation.
Write real-time and batch data pipelines to support a wide range of projects and features including our creator-facing analytics product, executive reporting, FP&A, marketing initiatives, model training, data science analytics, A/B testing, etc.
Help manage and build out our data platform and suite of data tools.
Be a driver of a data-centric culture at Patreon. Work autonomously on large green field initiatives and help define data best practices at the company.
About You
Expert in SQL, Spark and Python or Scala
Significant experience modeling data and developing core data sets and metrics to support analytics, reports and experimentation. Solid understanding of how to use data to inform the product roadmap.
Enjoy collaborating with Data Scientists, Product Managers and Product Engineers. Comfortable playing the role of a Project Manager in order to drive results.
Have previously built real-time and batch data pipelines using tooling such as Airflow, Spark, Kafka, S3, Fivetran, Census, etc. Experience working with event tracking frameworks, data observability frameworks and experimentation frameworks.
Experience managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc.
Highly motivated self-starter that is keen to make an impact and is unafraid of tackling large, complicated problems and putting in the work to ensure high craft deliverables.
About Patreon
Patreon powers creators to do what they love and get paid by the people who love what they do. Our team is passionate about making this mission and our core values come to life every day in our work. Through this work, our Patronauts:
Put Creators First | They're the reason we're here. When creators win, we win.****
Build with Craft | We sign our name to every deliverable, just like the creators we serve.
Make it Happen | We don't quit. We learn and deliver.
Win Together | We grow as individuals. We win as a team.
We hire talented and passionate people from different backgrounds because we want our company to represent the vast diversity of our creators. If you're excited about a role but your prior experience doesn't match with every bullet point outlined above, we strongly encourage you to apply anyway. If you're a creator at heart, are energized by our mission, and share our company values, we'd love to hear from you.
Patreon is proud to be an equal-opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.
Patreon offers a competitive benefits package including and not limited to salary, equity plans, healthcare, unlimited paid time off, company holidays and recharge days, commuter benefits, lifestyle stipends, learning and development stipends, patronage, parental leave, and 401k plan with matching.
The posted range represents the expected salary range for this job requisition and does not include any other potential components of the compensation package, benefits and perks previously outlined. Ultimately, in determining pay, we'll consider your experience, leveling, location and other job-related factors.
San Francisco Pay Range
$166,000—$231,500 USD",3.2,"Patreon
3.2","San Francisco, CA",51 to 200 Employees,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
251,Junior Data Engineer,Employer Provided Salary:$55K - $75K,"About Open Road Integrated Media
Open Road Integrated Media is a prestige content brand delivering digital experiences that entertain and inform readers around the world. Open Road was founded in 2009 with the belief that great marketing and great content are the engines of growth for underserved authors and books. This philosophy is at the core of everything that we do. Open Road revolutionizes how publishers service authors, agents, and readers.
Summary
The Data Engineering and Analytics team is seeking a Junior Data Engineer. This role will work on various types of tasks and projects. The members of Data Engineering and Analytics team uses rigorous analytics to generate insights that inform product, marketing, and business decisions across the company. In the meantime, we build systems, infrastructure, data products for collecting, storing, and visualizing data to foster the data democracy in the company. We work in Python, SQL and we work with technologies like Airflow, Django, Tableau, Spark.
Essential Functions
ETL
Design and build ETL pipelines by using Airflow to collect data from different sources to data warehouses
Build pipeline integration with our various data products for long-running processes
Identify the room for optimizing relational data storage through design, query optimization, indices, replicas, partitioning, etc.
Automation
Build ad-hoc scripts or recurring processes to fully/partially automate labor-intensive workflows of other departments (e.g. Production, Marketing)
Requirements
1-2 years hands-on experience in ETL design, implementation and maintenance
Experience in schema design and data modeling
Experience in writing complex SQL queries to extract data from relational databases (e.g. MySQL, Redshift)
Experience in version control systems such as Git
Experience in the following tools/technology is a plus
Airflow, Django, Git, Docker
Comfortable with extensive Python coding
Willing to work with a codebase that is not originally written by you
Good communication skills; understand that being an effective engineer is about communicating with people as much as it is about writing code
Willing to learn any language/tools/frameworks that are necessary to get the job done
Compensation
Salary will be commensurate with qualifications and experience. The salary range for this position is $55,000.00 - $75,000.00.
wqwv4PNMQK",3.8,"Open Road Media
3.8","New York, NY",1 to 50 Employees,-1,Company - Private,Publishing,Media & Communication,Unknown / Non-Applicable
252,Data Insights Engineer,Employer Provided Salary:$85K - $95K,"Who We Are
We're purpose-driven. With every ride, we aim to redefine health and happiness. It's all about being more than a workout: SoulCycle is a mind-body-soul experience, built on community, love, respect, acceptance, and a lot of fun. It comes to life through the ride, the relationships, and the unparalleled hospitality. And all of that comes from our people. Join us—we'd love to have you.
Our Mission
To foster an open, diverse, & inclusive community—while embracing each unique individual exactly as they are. We empower each other by listening with an open mind, finding ways to learn and grow together, and always nurturing a sanctuary of trust. To make a real, lasting impact, we'll work nonstop to embrace and create change. Because nobody is equal until everyone is equal.
Job Description
The Data Insights Engineer will play a pivotal role in driving data-driven decisions at SoulCycle. You'll be responsible for building and maintaining the data infrastructure that supports all business functions, from marketing and operations to finance and customer experience, in addition to providing analysis to each of these teams. By leveraging your technical expertise and analytical skills, you will empower stakeholders to derive insights from data, enabling them to make strategic and informed decisions that positively impact the business.
Roles and Responsibilities
Insights and Recommendations: collaborate with cross-functional teams to understand business requirements, provide analytical support, and identify opportunities for data-driven improvements
Visualizations and Dashboarding: design and develop ad-hoc and recurring Looker reports; create and monitor business metrics; identify patterns, trends, and opportunities for performance improvement
Data Modeling: build, optimize, and document LookML data models that support quick and efficient analysis
Prediction: build predictive models that forecast business outcomes, customer behavior, and other relevant metrics
Qualifications
1-3 years of professional experience transforming and analyzing data across platforms such as Looker, Tableau, Mode, Jupyter Notebooks, Excel, and GCP/AWS. Looker/LookML experience is a plus.
Expert in SQL (able to write structured and efficient queries on large data sets) and familiarity with Python
Ability to identify patterns and trends in data and solve problems
Excellent communication skills to work with stakeholders to translate business needs and ideas into analyses and recommendations
Top-notch organizational skills and ability to manage projects in a fast-paced environment
Creative problem solving skills to find solutions to vague questions
Experience with Python data analysis and visualization packages is a plus (pandas, tensorflow, matplotlib, etc.)
Pay Range: $85,000 - $95,000 per year. This role is on-site 4 days a week.",3.9,"SoulCycle HQ
3.9","New York, NY",1001 to 5000 Employees,2006,Company - Private,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
253,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
254,Data Engineer,$78K - $112K (Glassdoor est.),"Translate business requirements and functional specifications into logical program designs and to deliver code modules, stable application systems, and software solutions.
Develop, configure, or modify complex integrated business and/or enterprise application solutions within various computing environments.
Implement and maintain complex business and enterprise software solutions to ensure successful deployment of released applications.
Translate complex cross-functional business requirements and functional specifications into logical program designs, code modules, stable application systems, and software solutions.
Partner with Product Team to understand business needs and functional specifications. .
Collaborate with cross-functional teams to ensure specifications are converted into flexible, scalable, and maintainable solution designs.
Evaluate project deliverables to ensure they meet specifications and architectural standards.
Coordinate, execute, and participate in component integration (CIT) scenarios, systems integration testing (SIT), and user acceptance testing (UAT) to identify application errors and to ensure quality software deployment.
Participate in all software development end-to-end product lifecycle phases by applying and sharing an in-depth understanding of complex company and industry methodologies, policies, standards, and controls.
Architect software design patterns and approaches, application level software architecture and make technical trade-off decisions at application level.
Automate and simplify team development, test, and operations processes.
Develop detailed architecture plans for large scale enterprise architecture projects and drives the plans to fruition.
Solve complex architecture/design and business problems; solutions are extensible; work to simplify, optimize, remove bottlenecks, etc.
Minimum Education Required:- All the responsibilities mentioned above are in line with the professional background and requires an absolute minimum of bachelor’s degree in computer science, computer information systems, information technology or a combination of education and experience equating to the U.S. equivalent of a bachelor's degree in one of the aforementioned subjects.",4.9,"Wise Equation Solutions
4.9","Mooresville, NC",51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
255,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
256,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
257,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
258,Senior Data Engineer,Employer Provided Salary:$71.00 Per Hour,"Role: Application Developer/Senior Data Engineer-RID00005396
Location: Melbourne FL (Hybrid Onsite and remote)
Duration: 12 months
Must- Haves (Hard Skills):
· 4+ years’ experience developing high performance queries using Microsoft SQL Server & SQL Server Management Studio
· Hands on performance tuning
· Looking for a software developer not a Database Administrator.
Must- Haves (Soft Skills):
· Strong verbal and written communication skills
· Good team player
· Collaboration
Job Description:
1. Our Client is seeking a Software Engineer to join the Mission Networks Engineering team that provides modern, secure, reliable and resilient telecommunications networks and information management systems. Join the team developing critical communication capabilities for air traffic control, air-to-ground data communication, secure access to situational awareness data and secure information sharing to state agencies.
2. The Software Engineer will be responsible for the design and development of an enterprise Java application leveraging SQL procedures, functions, and packages. Work with SQL Server Management Studio to evaluate and improve performance of queries and stored procedures. Collaborate in a cross-functional team that includes Software Developers, DBAs, System Engineers and System Administrators to maintain applications supporting managed services for the FAA.
Essential Functions:
· Develop & maintain high performance queries for relational database management systems
· Design, develop & maintain Java application(s) for Operational Support Systems (OSS) & Business Support Systems (BSS)
· Troubleshoot issues reported by internal & external customers in support of operational systems
Qualifications:
· Bachelor’s Degree and minimum 6 years of prior relevant experience or a Graduate Degree and a minimum of 4 years of prior related experience.
· 4+ years’ experience developing high performance queries using Microsoft SQL Server & SQL Server Management Studio
· 3+ years’ hands-on experience developing applications with Java (or similar Object Oriented language) and with front-end technologies such as JavaScript, HTML and CSS
· Obtain and maintain an FAA public trust clearance.
Preferred Additional Skills:
· Database Design, query optimization, index design, stored procedure.
· Agile development experience including Scrum and DevOps.
· Experience with Git, Jenkins, CI/CD.
· Experience with JavaScript libraries like jQuery, D3, or similar libraries.
· Experience working in Linux environment like RHEL.
Job Types: Full-time, Contract, Permanent
Salary: Up to $71.00 per hour
Schedule:
8 hour shift
Day shift
Holidays
Monday to Friday
On call
Weekend availability
Education:
Bachelor's (Preferred)
Experience:
SSAS, SSRS, SSIS: 5 years (Preferred)
SQL Server Management Studio: 4 years (Preferred)
HTML, CSS, Javascript: 3 years (Preferred)
Work Location: Remote",4.4,"COMTEC INFORMATION SYSTEMS
4.4","Melbourne, FL",501 to 1000 Employees,1996,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
259,Sr. Azure Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"LeadStack Inc. is an award-winning, one of the nation's fastest-growing, certified minority-owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Position Details:
Title: Azure Data Engineer
Location: Remote
Duration: 6+ Months Contract
Top 3 skills: SQL, Snowflake, and Data Warehousing/Design
Its a W2 Role. No C2C.
Client is seeking a talented and motivated Data Engineer to join our dynamic team. As a Data Engineer, you will play a critical role in developing and maintaining our data infrastructure, ensuring the smooth flow of data for our analytics and insights projects. The ideal candidate will have a strong background in SQL, data warehousing, ETL processes, and a preference for Snowflake experience. Additional experience with dbt or semantic layer tools will be a bonus, and proficiency in Python will also be advantageous.
Bachelor's degree in Computer Science, Engineering, or a related field.
Solid experience with SQL and data warehousing concepts.
Strong understanding of ETL processes and tools.
Preferred experience with Snowflake data warehouse platform.
Bonus for experience with dbt or other semantic layer tools.
Proficiency in Python is a plus.
Excellent problem-solving and analytical skills.
Strong attention to detail and ability to work in a fast-paced environment.
Effective communication skills and ability to collaborate with cross-functional teams.
Key Responsibilities:
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.
Optimize data ingestion and transformation processes to ensure efficient and scalable data flows.
Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.
Develop and maintain data models, schemas, and data dictionaries for efficient data storage and retrieval.
Perform data validation, quality checks, and troubleshooting to identify and resolve data-related issues.
Monitor and optimize database performance, including query tuning and index optimization.
Collaborate with data scientists and analysts to implement data models and support their data requirements.
Stay up-to-date with industry trends and best practices in data engineering, and proactively recommend and implement improvements to our data infrastructure
To know more about current opportunities at LeadStack, please visit us at https://leadstackinc.com/careers/
Should you have any questions, feel free to call me on 510-480-1006 or send an email on tarique.aziz@leadstackinc.com
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Data Warehousing/Design: 8 years (Required)
Snowflake: 5 years (Required)
Azure: 5 years (Required)
Work Location: Remote",4.3,"Leadstack Inc
4.3",Remote,51 to 200 Employees,2016,Company - Private,HR Consulting,Human Resources & Staffing,Unknown / Non-Applicable
260,Sr. Data Engineer I,$82K - $109K (Glassdoor est.),"Pax8 is the leading value-added cloud-based SaaS distributor, simplifying the cloud journey for our partners by integrating technology, business intelligence and proactive service to deliver an unparalleled experience. Serving thousands of partners through the indirect sales channel, our mission is to be the world's favorite place to buy cloud products. We are a fast-growing, dynamic and high-energy startup organization, allowing you to make a meaningful impact on the business. Culture is important to us, and at Pax8, it's business, and it IS personal. We are passionate, creative and humorously offbeat. We work hard, keep it fun, and expect the best.
We Elev8 each other. We Advoc8 for our partners. We Innov8 continuously. We Celebr8 life.
No matter who you are, Pax8 is a place you can call home. We know there's no such thing as a ""perfect"" candidate, so we don't look for the right ""fit"" – instead, we look for the add. We encourage you to apply for a role at Pax8 even if you don't meet 100% of the bullet points. We believe in cultivating an environment with a diversity of perspectives, in hopes that we can all thrive in an inclusive environment.
We are only as great as our people. And we have great people all over the world. No matter where you live and work, you're a part of the Pax8 team. This means embracing hybrid- and remote-work whenever possible.
Position Summary:
The Sr. Data Engineer I designs, develops, tests, deploys, maintains, and improves systems that collect, transform, store, and manage data for end users. They deliver individual projects based upon deadlines and required deliverables. The Sr. Engineer builds complex features independently and collaborates with other teams to conduct design and code reviews. They develop and/or provide technical leadership in the development of data systems involving the application of new technologies with significant technical risk. The Sr. Engineer prepares detailed plans, which may span over a year for complex projects. They determine test philosophy, goals, and objectives, and participate in the formation of project goals, scope, and schedule.
Essential Responsibilities:
Builds pipelines to ingest new data sources
Transforms data to support varied use cases
Includes testing in all aspects of the development process
Mentors junior and mid-level Engineers
Optimizes existing data pipelines and improves existing code quality
Makes updates and improvements to deployment processes
Participates in project planning and architecture discussions
Analyzes potential problems and finds solutions to pressing data issues
Participates in on-call rotation
Ideal Skills, Experience, and Competencies:
At least Four (4) years of relevant data engineering experience
Advanced experience with Python
Expert experience with SQL
Intermediate experience with a JVM language
Exposure to other software development languages
Advanced experience with Apache Spark or other distributed processing engines
Advanced experience with Apache Kafka or other stream processing frameworks
Intermediate experience with Terraform, Docker, Kubernetes, or other similar infrastructure tooling
Advanced experience with cloud data tools such as S3, Glue, and Athena
Intermediate experience with building CI/CD pipelines
Effective problem solving and troubleshooting abilities
Ability to consistently achieve results, even under tough circumstances
Effective technical leadership abilities
Excellent verbal and written communication skills
Experience with innovative application design and implementation
Ability to make sense of complex, high quantity, and sometimes contradictory information to effectively solve problems
Required Education & Certifications:
B.A./B.S. in related field or equivalent work experience
M.S./M.A. in related field or equivalent work experience
Compensation:
Qualified candidates can expect a salary beginning at $140,000 or more depending on experience
#LI-Remote #LI-JF1 #Dice-J #BI-Remote

Note: Compensation is benchmarked on local Denver Metro area market rates. Qualified candidates in other locations can expect a salary package that may be adjusted based off applicable cost of wages in their respective location.
At Pax8 we believe that your Total Rewards should include a benefits package that shows how much we value our greatest assets. All FTE Pax8 people enjoy the following benefits:
Non-Commissioned Bonus Plans or Variable Commission
401(k) plan with employer match
Medical, Dental & Vision Insurance
Employee Assistance Program
Employer Paid Short & Long Term Disability, Life and AD&D Insurance
Flexible, Open Vacation
Paid Sick Time Off
Extended Leave for Life events
RTD Eco Pass (For local Colorado Employees)
Career Development Programs
Stock Option Eligibility
Employee-led Resource Groups
Pax8 is an EEOC Employer.",4.1,"Pax8
4.1","Greenwood Village, CO",501 to 1000 Employees,2012,Company - Private,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
261,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1.0,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
262,Data Visualization Engineer,$77K - $101K (Glassdoor est.),"Passionate about precision medicine and advancing the healthcare industry?
Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.
With the advent of genomic sequencing, we can finally decode and process our genetic makeup. We now have more data than ever before but providers don't have the infrastructure or expertise to make sense of this data. We're on a mission to connect an entire ecosystem to redefine how genomic data is used in clinical settings.
What You'll Do:
Own the design, development and administration of advanced analytics dashboards and reporting and business intelligence tools. Beginning with requirements gathering across cross-functional teams, to the delivery of analytic solutions into production.
Collaborate with stakeholders, providing guidance, data and analytical support for developing new datasets, analyses and tools in support of strategic initiatives.
Perform deep-dive root cause and problem-solving analyses for and with laboratory, business operations and engineering teams.
Design and build data views to support advanced analytical reporting, self-service data analysis and exploration, utilizing multiple data and visualization technologies.
Work closely with technical and product teams to identify and align changes to internal systems in support of future data analysis needs and requirements.
Develop and facilitate data governance initiatives to assure data integrity across distributed systems.
Solve new and interesting problems.
Qualifications:
Bachelor's degree in Information Systems, Business Administration, Finance, Economics, Mathematics, Operations Research, Marketing or similar
3+ years of experience developing data sets for analysis use-cases
3+ years of experience with direct stakeholder management and ownership of full lifecycle of analytics/data projects and presenting analysis results to stakeholders
3+ years report-development and data visualization experience with BI tools (Looker, Tableau, etc.)
Ability to deliver both high-level explanations of complex problems while also able to deep dive into complex problems
Advanced SQL skills for querying and analysis of large, complex datasets
Must work well in a team environment and be able to partner with both business and technical team members and translate technical language to the non-technical need
Creative problem solver, ability to handle multiple projects, and strong work ethic needed
Excellent written and verbal communication skills
Highly organized and systematic, superb attention to detail
Able to complete tasks with a high degree of accuracy and an eye towards process improvement
Persistent and resilient person who thrives in a fast-paced, highly collaborative environment
Bonus points for:
Advanced analytics experience - classification/predictive modeling, optimization models, forecasting
Experience with Python for data engineering or advanced data analysis projects/use-cases
Healthcare domain knowledge and experience
Expert knowledge of database/data modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others
Experience building cloud-native applications and supporting technologies / patterns / practices including: GCP, AWS, Docker, CI/CD, DevOps, and microservices

#LI-EH1
#LI-Hybrid
We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.1,"Tempus
3.1","Chicago, IL",501 to 1000 Employees,2015,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
263,Sr. Data Engineer,$80K - $112K (Glassdoor est.),"Object Technology Solutions, Inc (OTSI) has an immediate opening for Sr. Data Engineer
This is an on-site position in Dallas, TX OR Kansas City, MO (Long term)
Skills & abilities required:
Minimum 5+ years of experience as Data Engineer.
Prefer to have Databricks experience
Prefer to have understanding of embedding Python into Azure environment (containerization)
Expectation is that this position works with Data Scientist to rapidly deploy models into Azure DW and/or pipeline DW required data to Data Scientist requirements to build and test new models.
About us:
OTSI is a leading global technology company offering solutions, consulting, and managed services for businesses worldwide since 1999. OTSI serves clients from its 15 offices across 6 countries around the globe with a “Follow-the-Sun” model. Headquartered in Overland Park, Kansas, we have a strong presence in North America, Central America, and Asia-Pacific with a Global Delivery Center based in India. These strategic locations offer our customers the competitive advantages of onshore, nearshore, and offshore engagement and delivery options, with 24/7 support. OTSI works with 100+ enterprise customers, of which many are Fortune ranked, OTSI focuses on industry segments such as Banking, Financial Services & Insurance, Healthcare & Life Sciences, Energy & Utilities, Communications & Media Entertainment, Engineering & Telecom, Retail & Consumer Services, Hi-tech, Manufacturing, Engineering, transport logistics, Government, Defense & PSUs.
Our Center of Excellence:
Data & Analytics
Digital Transformation
QA & Automation
Enterprise Applications
Disruptive Technologies
Job Type: Contract
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: In person",4.6,"OTSi
4.6","Dallas, TX",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
264,Data Engineer,$67K - $96K (Glassdoor est.),"Massanutten Resort
Job Summary
The Data Engineer will play a crucial role in designing, developing, and maintaining the data infrastructure and systems that enable efficient and reliable data processing, storage, and analysis. The primary responsibility is to transform raw data into usable formats for various business applications and support data-informed decision-making processes. This role includes working in various Microsoft SQL Environments as well as supporting a Snowflake Data Warehouse. The Data Engineer is expected to have a broad understanding of data concepts and be avid about continuing education to keep up with the ever-changing data landscape.
Required
Degree in computer science or related file and 3+ years of experience with Python, SQL, and data visualization/exploration tools (in lieu of a degree, at least 5 years of experience in the above is equivalent)
Preferred
A degree in computer science or a related field
Certifications in database management and/or related technical certifications
Familiarity with advanced data concepts (i.e., AI and Machine Learning) and business intelligence/visualization tools
Typical Schedule
Days:
Monday-Friday, weekends and holidays as needed
Position will be required to be part of rotating on-call schedule
Hours:
8am-5pm, additional hours as needed
Core Responsibilities
Develop Data Pipeline: Design and implement scalable and efficient data pipelines, integrating diverse data sources and ensuring smooth data flow from ingestion to storage and processing
Transform and Model Data: Develop and maintain data transformation processes, such as ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform), to convert raw data into structured formats suitable for analysis and reporting. Create and manage data models to facilitate data integration and provide a consistent view of information across systems.
Implement and oversee Data Warehousing and Storage: Build and maintain data warehouses or data lakes, optimizing storage and retrieval processes to ensure high availability, reliability, and performance. Implement appropriate data governance and security measures to protect sensitive data.
Ensure Data Quality and Governance: Collaborate with data analysts and business stakeholders to understand data requirements and define data quality standards. Implement data validation, data profiling, and data cleansing techniques to ensure data accuracy, consistency, and integrity.
Optimize Performance: Identify and address performance bottlenecks in data processing and storage systems, leveraging optimization techniques and technologies to improve overall data engineering efficiency and reduce latency
Assist with Data Problem Solving: Support team with data related escalation issues, including help desk support tickets that are generated from our business users
Manage Data Integration and API Development: Establish connections and integrations between internal and external data sources, including APIs, databases, and third-party platforms. Develop and maintain data APIs to enable seamless data access and consumption by other applications and services.
Collaborate and Document: Work closely with cross-functional teams, including analysts, software engineers, and business stakeholders, to understand data requirements and ensure data solutions align with business objectives. Document data engineering processes, including data pipelines, transformations, and system architectures, to facilitate knowledge sharing and maintain a robust knowledge base.
Stay up-to-date with the latest advancements in data engineering, data management, and big data technologies. Evaluate and recommend new tools, frameworks, and methodologies to improve data engineering practices and enhance overall data infrastructure
Other duties as assigned by management
For more information, contact Ana at 540-289-4939.",3.6,"Great Eastern Resort Management, Inc.
3.6","McGaheysville, VA",1001 to 5000 Employees,-1,Self-employed,-1,-1,Unknown / Non-Applicable
265,"Data Engineer - Python, Spark?",$91K - $131K (Glassdoor est.),"FlexIT client is looking for an immediate Data Engineer - Python, Sparkfor a 12-month remote contract.
The client is looking for great Engineers with talent and persistence who can leverage their existing skills and learn new ones. You should have some of the specific technical skills were looking for and be expert enough in one or two to help ramp others quickly.
Job Duties:
We are building petabyte-class solutions that consume fast-moving streams from eCommerce, retail, and partner channels and power the critical decisions that drive our business. We are building the Cloud Platform for Data and Analytics on AWS that fuels in digital transformation.
Focus areas include:
Data Streaming / Enrichment / Business Rules / MDM
Data Lake / Warehousing
Data Governance / GDPR / SOX
Data Strategy / Unified Access / IAM / RBAC
Be a great teammate on an agile/SCRUM team that sets and meets aggressive goals.
Mentor new and less experienced developers to advance their proficiency.
Leverage expert development skills and solid design skills to deliver reliable, scalable, performant solutions with modern tooling, data structures and algorithms.
Work with Product Owners, Engineering Managers and Principal Engineers to deliver solutions that enable digital transformation",4.0,"FlexIT Inc
4.0","Beaverton, OR",1 to 50 Employees,-1,Company - Private,-1,-1,$5 to $25 million (USD)
266,Power BI/Data Engineer,$73K - $112K (Glassdoor est.),"ClinDCast is seeking a skilled and motivated Power BI/Data Engineer to join our dynamic team. This role involves designing, developing, and maintaining data pipelines, transforming raw data into meaningful insights using Power BI, and contributing to data engineering efforts. The ideal candidate should possess a strong background in data processing, ETL (Extract, Transform, Load) processes, and visualization using Power BI. This role requires a blend of technical expertise, creativity, and a strong commitment to delivering high-quality data solutions.
Data Pipeline Development: Design, implement, and maintain robust data pipelines to extract, transform, and load data from various sources into target data repositories.
Data Transformation: Develop ETL processes to clean, transform, and enrich data, ensuring data quality, consistency, and accuracy.
Data Modeling: Design and optimize data models for efficient data storage, retrieval, and analysis in collaboration with data scientists and analysts.
Power BI Development: Create interactive and insightful Power BI dashboards and reports that provide actionable insights to various stakeholders.
Data Integration: Integrate data from different systems and sources, ensuring seamless data flow and integration points between various applications.
Performance Tuning: Identify and resolve performance bottlenecks in data pipelines, ETL processes, and Power BI reports to ensure optimal performance.
Data Governance: Implement data governance practices to ensure data privacy, security, compliance, and adherence to data quality standards.
Collaboration: Work closely with cross-functional teams including data scientists, analysts, business stakeholders, and IT teams to understand requirements and deliver solutions that meet business objectives.
Documentation: Maintain clear and concise documentation for data pipelines, transformations, and Power BI reports to facilitate knowledge sharing and future maintenance.",4.0,"ClinDCast LLC
4.0","Tampa, FL",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
267,Sr. Data Engineer,Employer Provided Salary:$120K - $130K,"Position Title: Data Engineer/ ETL Developer/ SQL Developer/ Business Intellegence
Location: San Antonio, TX /Herndon, VA
Yakshna Solutions, Inc., (YSI) is a CMMI Level 3 assessed, ISO 9001, 20000:1, 27001 certified, woman-owned small business enterprises, headquartered in Herndon, Virginia, USA. YSI provides professional IT solutions and services to business corporations and government organizations. YSI is committed to serve its business communities as a leading IT vendor providing innovative, quality and cost-effective IT business solutions and services.
Our benefits are very competitive that include 401(k), health, dental, and vision insurance, Life insurance, short-term and long-term disability insurance, paid time off, training and professional development assistance.
YSI is seeking a highly qualified Data Engineer. The selected candidate will be part of the Integrated Budget Documentation and Execution System multi-disciplinary team.
Job Responsibilities:
Builds extract, transform, load (ETL) pipelines to enable full spectrum data operations from ingest to query.
Required Qualifications and Skills:
5 years of minimum total relevant experience
BS in Engineering, Computer Science, or technical degree or industry experience equivalent
TS/SCI Clearance required.
Experience with developing scalable ETL and ELT workflows for reporting and analytics.
Experience with designing, developing, operationalizing, and maintaining complex data applications at enterprise scale.
Experience with programming (scripting) languages (i.e Bash, Python, .Net or Java)
Experience as a Data Scientist/Engineer supporting single or multiple domain areas
Knowledge of advanced statistical techniques and concepts
A drive to learn and master new technologies.
Demonstrated ability to use technical and analytic skills to solve complex problems.
Experience with developing scripts and programs for converting various types of data into usable formats and supporting project teams to scale, monitor, and operate data platforms.
Salary: $120K- $130K Annually with benefits
Job Type: Full-time
Pay: $120,000.00 - $130,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Health savings account
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
SQL: 8 years (Preferred)
Data Engineering: 8 years (Preferred)
Security clearance:
Top Secret (Preferred)
Work Location: In person",3.7,"YSI
3.7","Kirby, TX",10000+ Employees,2011,Company - Public,Machinery Manufacturing,Manufacturing,$1 to $5 billion (USD)
268,Data Engineer,Employer Provided Salary:$150K - $300K,"What we're looking for
Stytch is the platform for user authentication. We build infrastructure that sits in the critical path of our customer's applications. As a data engineer, you'll work on designing and building event-driven architecture systems to drive analytics insights and observability tooling for our customers.

What Excites You
Championing data-driven insights - you see data analytics and observability as a product critical to success
Solving problems with pragmatic solutions — you know when to make trade-offs between completeness and utility and you know when to cut scope to ship something good enough quickly
Building products that make developers lives easier — as a data engineer for a developer infrastructure company, what you build will have an immediate impact on our customers
Shaping the culture and growing the team through recruiting, mentorship, and establishing best practices
Learning new skills and technologies in a fast paced environment

What Excites Us
Comfort working in a modern data stack using tools like Snowflake, Redshift, DBT, Fivetran, ElasticSearch, and Kinesis
Appreciation for schema design and architecture that balance flexibility and simplicity
Experience designing and building highly reliable back-end and ETL systems
3+ years as a data or backend engineer

What Success Looks Like
Technical — build new, highly reliable services that our customers can depend on
Ownership — advocate for projects and solutions that you believe in and ship them to production
Leadership — level up your teammates by providing mentorship and guidance

Our Tech Stack
Data moves through Snowflake, ElasticSearch, MySQL, and Kinesis
Go and Node for application services
We run on AWS with Kubernetes for containerization
gRPC and protobufs for internal service communication

Expected base salary $150,000-$300,000. The anticipated base salary range is not inclusive of full benefits including equity, health care insurance, time off, paid parental leave, etc. This base salary is accurate based on information at the time of posting. Actual compensation for hired candidates will be determined using a number of factors including experience, skills, and qualifications.
We're looking to hire a GREAT team and that means hiring people who are highly empathetic, ambitious, and excited about building the future of user authentication. You should feel empowered to apply for this role even if your experience doesn't exactly match up to our job description (our job descriptions are directional and not perfect recipes for exactly what we need). We are committed to building a diverse, inclusive, and equitable workspace where everyone (regardless of age, education, ethnicity, gender, sexual orientation, or any personal characteristics) feels like they belong. We look forward to hearing from you!

Learn more about our team and culture here!",5.0,"Stytch
5.0",California,1 to 50 Employees,2020,Company - Public,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
269,Data Center (ACI/Nexus) Network Engineer,-1,"Data Center (ACI/Nexus) Network Engineer**
Job Details
City :
Louisville
State :
KY
Job Description:Encore is seeking a Sr. Data Center (ACI/Nexus) Network Engineer. Focus is on route/switch/network management in a large enterprise data center environment.
*
Responsibilities:*
Perform design and deployments for the Cisco data switching technology including Nexus/ACI with Nexus Dashboard
Troubleshoot and resolve tier2/3 data center network issues
Address and resolve escalated issues for routing/switching in a large data center environment.
Assist in the design/deployment of new locations and new solutions.
Design/deploy solutions that scale and adhere to data security standards
Manage project tasks
On-call one week every 2 months
*
Qualifications:*
Experience with Aruba 3810/5412/6200/6300 switches, Cisco 5K,7K,9Ks, ASR platforms and the associated management platforms (IOS, NXOS, ArubaOS, Aruba CX)
Strong understanding of routing/switching protocols
Experience with Aruba ClearPass as a NAC solution would be valuable in this role
Data center experience with load-balancing and firewalls would be a plus
Must be a self-starter, able to manage project tasks on your own
_
Encore Technologies is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills, and experiences within our workforce._
Job Type: Contract",4.4,"Encore Technologies
4.4","Louisville, KY",201 to 500 Employees,2014,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
270,Senior Data Engineer (Remote),Employer Provided Salary:$50.00 - $70.00 Per Hour,"Company Overview: IT Giants INC is a dynamic and innovative technology company at the forefront of delivering advanced data solutions across various industries. We are seeking an experienced Data Engineer with expertise in cloud platforms, programming languages, database technologies, and analytics tools. If you also have a strong background in the airline domain, this is an opportunity to join our dedicated team and shape the future of data-driven decision-making in the aviation sector.
Position Overview: As a Data Engineer at IT Giants INC, you will play a pivotal role in designing, building, and maintaining sophisticated data pipelines and analytical solutions for our airline industry clients. Leveraging your deep understanding of both data engineering and the airline domain, you will empower our clients to harness the power of their data to improve operations and customer experiences.
Key Responsibilities:
Data Pipeline Development: Architect, develop, and maintain end-to-end data pipelines, integrating data from diverse sources, including cloud platforms (GCP, Azure, AWS), APIs, and various databases (Teradata, Oracle, PostgreSQL, NoSQL).
Data Transformation and Modeling: Apply your expertise in SQL, Python, and Alteryx to transform raw data into meaningful insights, while ensuring data integrity and accuracy.
Cloud Expertise: Utilize your proficiency in GCP, Azure, and AWS to design and implement scalable and cost-efficient cloud-based solutions that meet the unique demands of the airline industry.
Database Management: Manage relational and NoSQL databases to store and retrieve data efficiently, ensuring optimal performance and security.
Analytics and Reporting: Collaborate with data scientists and analysts to develop interactive dashboards and reports using tools like Tableau, QuickSight, and CloudWatch, enabling stakeholders to make informed decisions.
Data Quality and Governance: Implement data quality checks, validation processes, and adherence to industry regulations to ensure data accuracy and compliance.
Version Control and Collaboration: Utilize GitLab and other version control tools to collaborate with cross-functional teams on code management and deployment.
Real-time Monitoring: Develop real-time data processing solutions using technologies like CloudWatch to enable timely insights into operational metrics.
Linux Environment: Leverage your proficiency in Linux to optimize system performance and troubleshoot issues as they arise.
Domain Knowledge: Apply your deep understanding of the airline industry to structure data effectively and drive actionable insights from the data.
Qualifications:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
years of proven experience in data engineering, with a focus on cloud platforms (GCP, Azure, AWS), Python, SQL, and relevant database technologies.
Experience with Alteryx, Tableau, and data visualization tools for creating impactful dashboards.
Proficiency in managing databases such as Teradata, Oracle, PostgreSQL, and NoSQL databases.
Strong command of Linux environments and version control using GitLab.
Familiarity with CloudWatch, QuickSight, and real-time data processing is advantageous.
Domain expertise in the airline industry, including a solid understanding of aviation data, standards, and operational challenges.
Excellent problem-solving skills and the ability to troubleshoot complex data issues.
Effective communication skills to collaborate with technical and non-technical teams.
Proven ability to work independently and in a team, delivering high-quality solutions.
Join Our Team: If you are a driven and knowledgeable Data Engineer with a passion for both technology and the airline industry, IT Giants INC offers an exciting opportunity to contribute to transformative data solutions. Join us in our mission to revolutionize data analytics within the aviation sector and drive meaningful impact.
Job Types: Full-time, Contract
Pay: $50.00 - $70.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Work Location: Remote",5.0,"IT Giants
5.0",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
271,BI Data Engineer (Remote),Employer Provided Salary:$75.00 - $85.00 Per Hour,"Stefanini Group is hiring!
Exciting opportunity awaits, let us help you get started!
Click Apply now or you may call: (248) 728-2611 / email: palak.agrawal@stefanini.com for faster processing!

Job Description:
We require a reliable, talented, highly motivated BI/Data Engineer with excellent SQL, PowerBI, Python skills, The firm is putting a huge focus on data and reporting, with an increased demand of data related tasks.
As a global leader in Asset management, you will be working as part of an innovative, fast paced technology driven culture who are always looking at ways to enhance how technology is used within their organization.
You will be a valued member of an effective data team working to deliver growth against ambitious targets.
You will play a key role in extracting useful information from data, providing important insights to global technology teams.
The role will focus on creating best practice, high quality data solutions in collaboration with numerous business stakeholders and technology partners.
We are looking for someone who enjoys problem solving, who has an inquisitive nature, a team player and can work in a collaborative environment with good communication skills.
Ability to help extract and analyze data is required for the job.



Requirements:
We are looking for a good mix of the following skills:
Excellent experience in the Microsoft BI stack, Excel.
Excellent experience in building data pipelines and integrations (ETL/ELT).
Excellent knowledge of data warehousing and analysis concepts, data normalization, dimensional modelling, and associated practices.
Knowledge and experience of data manipulation languages such as DAX or MDX, Visualization or dashboarding experience.
Python experience is a must. Azure cloud experience would be a bonus.
We believe in building a culture supported by strong values, trust and independence. Within the team, we are highly collaborative, supportive, open and have high standards in the work we produce.

Education:
Bachelor's degree or equivalent experience.

*Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives*

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",3.3,"Stefanini, Inc
3.3","New York, NY",10000+ Employees,1987,Company - Private,Enterprise Software & Network Solutions,Information Technology,$1 to $5 billion (USD)
272,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1.0,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
273,"Data Engineer (AWS, Python)",Employer Provided Salary:$50.00 - $52.00 Per Hour,"Title: Data Engineer (AWS, Python)
Location: Remote
Duration: 12+ Months
Job Description:
Typical task breakdown:-
Daily standup, collaborate with peers, work independently to complete sprint commitments, participate in agile ceremonies.
Interaction with team:-
Daily stand up meetings with team – working mainly with Helios Platform adoption team.
Work environment:
Education & Experience Required:
Bachelor's in computer science required plus 7+ Working experience .
Masters degree with 6+ years working expeirence acceptable as well.
Preferred: AWS cloud certifications
Technical Skills
(Required)
Exp with Object oriented programming - Python (4+ years)
AWS (3+ years overall exp) lambda and/or glue, EMR, S3
SQL (3+ years)
2+ years exp in test driven development, unit test, integration testing, regression testing
(Desired)
Event driven framework experience.
Kinesis or Kafka experience
Familiar with working in Agile methodology
Soft Skills
(Required)
Strong written and verbal communication, experience working as a member of agile engineering team, ability to commit to and deliver work within allocated sprint window.
Job Type: Contract
Pay: $50.00 - $52.00 per hour
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",5.0,"DSMH LLC
5.0",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
274,Data Engineer,Employer Provided Salary:$45.00 Per Hour,"Job Title: GCP Data Engineer
Duration: 12+ Months
Location: Hartford, CT (Onsite Role)
Tax term : W2 & 1099
Job Description:
As a GCP Data Engineer, you will play a pivotal role in designing, implementing, and optimizing data solutions on the Google Cloud Platform (GCP). Your expertise will be essential in leveraging various technologies to build robust data pipelines and contribute to the overall success of data-driven initiatives.
Roles and Responsibilities:
Design, develop, and maintain efficient data pipelines using Python or PySpark on the Google Cloud Platform.
Seamlessly integrate data solutions using GCP services like Google Cloud Storage, BigQuery, Dataflow, and Pub/Sub.
Utilize expertise in the Hadoop ecosystem, including HDFS and Hive, to craft seamless data workflows.
Transform raw data into insights by creating complex SQL queries for data transformation and analysis.
Implement data quality checks and validation procedures to ensure accuracy and completeness.
Optimize data pipelines and processes for maximum performance and efficiency.
Collaborate with cross-functional teams to understand data requirements and deliver tailored solutions.
Create comprehensive documentation for pipelines, workflows, and processes.
Stay updated with the latest data engineering and cloud technology trends.
Tackle complex challenges with a proactive problem-solving mindset.
Required Qualifications:
Expertise in Python/PySpark and hands-on experience with Google Cloud Platform (GCP) for data engineering tasks.
Strong understanding of the Hadoop ecosystem, including HDFS and Hive.
Proficiency in SQL for effective data manipulation and analysis.
Experience working with cloud platforms, preferably Google Cloud Platform (GCP).
Strong problem-solving skills and analytical thinking.
Effective communication and collaboration abilities.
Adaptability to evolving data engineering technologies and practices.
Job Type: Contract
Salary: From $45.00 per hour
Experience level:
9 years
Application Question(s):
What is your work authorization?
Experience:
Data engineer: 9 years (Required)
Google Cloud Platform: 5 years (Required)
Hadoop: 5 years (Required)
Python: 4 years (Preferred)
SQL: 4 years (Preferred)
Work Location: On the road",-1.0,Conglomerate IT,"Hartford, CT",-1,-1,-1,-1,-1,-1
275,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
276,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
277,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
278,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
279,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
280,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
281,Sr. Data Engineer,Employer Provided Salary:$105K - $110K,"Role : Data Engineer
Location : Remote
Note : We are looking Someone who can be technical and also be able to
structure and manage transformation projects as well.
JD :
* Data Engineer Skill: Spark, Python and Databricks
* 12+ years of experience in Data pipeline engineering for both batch
and streaming applications.
* Experience with data ingestion process, creating data pipelines
(should be able to move data between stages in data lake, apply
transformation rules) and performance tuning. Must be hands-on coding using
Python with Spark.
* Exposure working with complex data types like XML or JSON, CI/CD
pipeline and scheduling tools like Airflow.
* Proven experience in DWH (type-2 dimensions, aggregations, star
schema, etc.) design and implementation
* Proficient in writing complex SQL queries
* Effective communication and collaboration skills to work with
cross-functional teams and stakeholders.
* Oversee and monitor the work of offshore according to projects
timelines and, if necessary, resolve any issues as they arise
Job Type: Full-time
Salary: $105,000.00 - $110,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
technical and able to structure and manage projects: 5 years (Required)
Data pipeline engineering for both batch and streaming apps.: 10 years (Required)
data ingestion process, creating data pipelines: 5 years (Required)
Python and Spark: 4 years (Required)
Exposure of XML or JSON, CI/CD: 4 years (Required)
scheduling tools like Airflow: 5 years (Required)
writing complex SQL queries: 5 years (Required)
Oversee and monitor the work of offshore: 3 years (Required)
working on fulltime role: 2 years (Required)
Work Location: Remote",4.3,"keasis Inc
4.3",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
282,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
283,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
284,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
285,Azure Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Required Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 9 years (Required)
Python: 8 years (Required)
Data lake: 7 years (Required)
Data Bricks: 7 years (Required)
Work Location: Remote",-1.0,TekValue IT Solutions,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
286,Data Engineer,-1,"Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore.

Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office.
The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus.
Position Responsibilities
Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities
Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization
Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process
Qualifications & Desired Skills
Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance
Minimum 2 years' of development experience using either Python, C#/Java or C++
Minimum 2 years' years of database/SQL experience
Familiarity with object-oriented programming concepts
Must be comfortable with development across the application stack
Ability to complete complex projects independently
Detail-oriented with strong verbal and written communication skills
Ability to work effectively in a high-energy, time sensitive team environment
Interest or prior experience in the financial trading industry (particularly in fixed income) a plus
This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education.
Base Salary for this role is expected to be between:
$110,000—$150,000 USD",4.4,"Garda Capital Partners
4.4","New York, NY",51 to 200 Employees,-1,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
287,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
288,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department.
The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
289,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1.0,Percept Health,Remote,-1,-1,-1,-1,-1,-1
290,Data Engineer (MongoDB),$80K - $114K (Glassdoor est.),"CrossCountry Mortgage (CCM) is one of the nation’s top 3 retail mortgage lenders with more than 7,000 employees operating nearly 600 branches across all 50 states. We have been recognized nine times on the Inc. 5000 list of America’s fastest-growing private businesses and have received countless awards for our standout culture. Through our dedication to getting it done, we make every mortgage feel like a win.

A culture where you can grow! CCM has created an exceptional culture driving employee engagement, exceeding employee expectations, and directly impacting company success. At our core, our entrepreneurial spirit empowers every employee to be who they are to help us move forward together. You’ll get unwavering support from all departments and total transparency from the top down.

CCM offers eligible employees a competitive compensation plan and a robust benefits package, including medical, dental, vision, as well as a 401K with an employer match. We also offer company-provided short-term disability, an employee assistance program, and a wellness program.
Position Overview:
We are looking for an experienced MongoDB developer to work with our development teams to integrate innovative features on our new products and applications, as well as maintain existing applications. The MongoDB developer will engineer scripts that automate daily tasks and ensure the security of data sets.
Job Responsibilities:
Upgrade databases through patches.
Understand and convert company needs into technical specifications and creating scalable solutions using MongoDB.
Optimize database schema designs, ensuring data integrity, and promoting best practices to enhance the overall performance and scalability of MongoDB solutions.
Assist developers in identifying and resolving performance issues by utilizing Mongo Profiler to profile and fine-tune database queries and operations.
Leverage performance optimization techniques by implementing proper indexes, including B-Tree, Geospatial, and Text indexes, to enhance query performance and overall system efficiency.
Implement the most effective backup and recovery methods to safeguard critical data and ensure business continuity in case of data loss or system failures.
Collaborate with cross-functional teams to understand business requirements.
Stay updated with the latest MongoDB advancements, features, and best practices.
Qualifications and Skills:
Bachelor’s/Master’s degree in software engineering or Information Technology.
3+ years of experience in executing data solutions using MongoDB.
Recommend and implement best practices for Rest API integration framework/model.
Analytical and problem-solving skills.
Configuring schema and MongoDB data modelling.
Experience with database security management.
Administration of MongoDB in Atlas.
Thorough understanding of MongoDB architecture.
Excellent communication skills.
This job description is intended to convey information essential to understanding the scope of the job and the general nature and level of work performed by job holders within this job. However, this job description is not intended to be an exhaustive list of qualifications, skills, efforts, duties, responsibilities or working conditions associated with the position.
CrossCountry Mortgage, LLC offers MORE than a job, we offer a career. Apply now to begin your path to success! https://crosscountrymortgage.com/about-us/careers/apply/
California residents: Please see CrossCountry’s privacy statement for information about how CrossCountry collects and uses personal information about California consumers.
CrossCountry Mortgage, LLC. (NMLS3029) is an Equal Opportunity Employer committed to workforce diversity. Qualified applicants will receive consideration without regard to race, religion, creed, color, orientation, gender, age, national origin, veteran status, disability status, marital status, sexual orientation, gender identity, or gender expression, or any other protected status in accordance with all applicable federal, state and local laws. The collective sum of the individual differences, life experiences, knowledge, inventiveness, innovation, self-expression, unique capabilities and talent that our employees invest in their work represents a significant part of not only our culture, but our reputation. The Company is committed to fostering, cultivating and preserving a culture that welcomes diversity and inclusion. Employment is contingent upon successful completion of a background investigation. CrossCountry Mortgage, LLC. is an FHA Approved Lending Institution and is not acting on behalf of or at the direction of HUD/FHA or the Federal government. To verify licensing, please visit www.NMLSConsumerAccess.org.",3.3,"CrossCountry Mortgage, LLC.
3.3","Charlotte, NC",1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
291,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
292,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
293,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
294,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1.0,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
295,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
296,Data Engineer,$76K - $101K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
The Data Governance/Data Quality Subject Matter Expert (SME) is a professional responsible for designing, implementing, and managing the client’s data governance strategy and ensuring high data quality standards across data-centric operations and decision-making.
The SME is in charge of developing and maintaining robust data pipelines, ensuring data observability, managing data within the Microsoft Azure environment, and implementing data governance principles in alignment with industry best practices and regulatory standards.
Primary Responsibilities and Accountabilities:
Data Governance & Quality: This includes developing and implementing data governance policies and procedures, monitoring data quality, managing data within the Microsoft Azure environment, and ensuring compliance with data privacy regulations.
Data Pipeline & Workflow Development: Designing and implementing efficient and secure data pipelines in Azure Data Factory and workflows, ensuring optimal performance and incorporating data quality principals.
Data Integration & Observability: Designing data integration strategies, integrating data from multiple sources, ensuring data quality and observability through tools like Azure Synapse.
Data Security: Defining and implementing data security policies and procedures, including access control and data encryption within the Azure environment.
Performance Tuning: Optimizing data access, indexing, and query performance within Azure Data Factory to ensure scalable and reliable data infrastructure.
Monitoring & Troubleshooting: Monitoring data systems, troubleshooting issues, identifying performance bottlenecks, and collaborating with team members to resolve problems.
Collaboration & Communication: Working closely with stakeholders to understand their data needs and develop solutions adhering to industry best practices and the needs of the business.
Automation & Tooling: Automating repetitive tasks and maintaining tools necessary for data processing and storage within the Azure environment.
Qualifications
A Bachelor's degree in a related field such as computer science, mathematics, engineering, or information systems.
3+ years of hands-on experience programming in SQL.
2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes within the Microsoft Azure environment.
Experience in Data Governance OR Data Quality Management is a must.
Skills
Competencies:
Expertise in Microsoft Azure data tools, with a focus on Azure Data Factory and Azure Synapse for data processing, storage, and analysis.
Strong SQL skills and knowledge of data warehousing concepts and ETL (Extract, Transform, Load) processes within the Azure environment.
Proficiency in programming languages such as Python, and experience with big data technologies such as Hadoop, Spark, and Kafka.
Ability to design, build, and maintain data pipelines, with a particular focus on Azure Data Factory.
Strong analytical and problem-solving skills, with the ability to identify and resolve data-related issues effectively.
Experience with data security and privacy regulations, and understanding of HIPAA compliance.
Strong experience with data governance, data quality, and data observability principles.
Excellent written and verbal communication skills, ability to explain complex technical concepts to non-technical stakeholders.
Excellent collaboration and teamwork skills, with the ability to work in a team involving client data engineers, data scientists, and data analysts.
Strong attention to detail and commitment to ensuring high data quality.
Continuous learning mindset and ability to stay up-to-date with new developments and technologies in data governance and data quality
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
297,GCP Data Engineer,Employer Provided Salary:$50.00 - $56.00 Per Hour,"JOB Description: -
ABOUT CONGLOMERATE
Conglomerate IT is a certified and a pioneer in providing premium end-to-end Global Workforce Solutions and IT Services to diverse clients across various domains. Visit us at ConglomerateIThttp://www.conglomerateit.comConglomerateIT
Conglomerate IT mission is to establish global cross culture human connections that further the careers of our employees and strengthen the businesses of our clients. We are driven to use the power of global network to connect business with the right people without bias. We provide Global Workforce Solutions with affability.
We offer a gamut of services across diverse domains, categories, skill sets with varying lengths of assignments /core capabilities included:
· Application Development Services
· Business Intelligence Services
· Enterprise Resource Planning
· Infrastructure Support Services
· Testing Services
· Program / Project Management
· Temporary & Permanent Staffing
Conglomerate IT is headquartered in Addison, Texas with branch offices across Canada, India, Sri Lanka, Argentina to further serve our clients.
JOB ROLE: GCP Data Engineer
Location: Job Type:Hartford, CT (Onsite Role)
Contract : 12+ Months
Work Authorization: w2 ,1099
Must have Skills: - Skills: Python/PySpark, GCP Cloud , HDFS, Hive, Big data
About Role: -
As a GCP Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and data architecture on the Google Cloud Platform. You will work closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and storage. Your expertise in GCP services and data engineering best practices will play a crucial role in maintaining a scalable, reliable, and high-performance data infrastructure.
Roles and Responsibilities:
· Design and implement data pipelines: Develop ETL (Extract, Transform, Load) processes to move data from various sources into GCP storage solutions such as BigQuery, Cloud Storage, and Datastore.
· Data modeling: Design and optimize data models to support data analytics, reporting, and machine learning initiatives. Ensure data accuracy, integrity, and appropriate access controls.
· Performance optimization: Fine-tune data pipelines and query performance to maximize efficiency and reduce latency, utilizing techniques like partitioning, indexing, and caching.
· GCP services utilization: Leverage GCP services like Dataflow, Dataproc, Pub/Sub, and Composer to create, schedule, and manage data processing and orchestration tasks.
· Data integration: Collaborate with cross-functional teams to integrate data from various sources, both internal and external, to enable comprehensive analysis and reporting.
· Data quality and governance: Implement data quality checks, monitoring, and validation processes to ensure data accuracy and consistency across the platform.
· Security and compliance: Implement security measures and access controls to protect sensitive data. Ensure compliance with industry standards and regulations.
· Documentation: Create and maintain technical documentation for data pipelines, workflows, and processes.
· Troubleshooting and support: Diagnose and resolve data-related issues, performance bottlenecks, and operational challenges in a timely manner.
Ideal Profile
· You have experience with Python Hive, Big data
· You have experience withYou GCP Cloud
· have strong experience with HDFS and other relevant tools/technologies.
· You have advanced python skills.
· You have PySparkknowledge.
· You have excellent communication and collaboration skills.
· You have attention to details.
· You are self-directed.
What's on Offer?
· Join a team of world class IT professionals
· Comprehensive salary package
· Great opportunity for growth and development
Job Type: Contract
Salary: $50.00 - $56.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Experience:
google cloud platform: 4 years (Required)
Work Location: In person",-1.0,CONGLOMERATE IT LLC,"Novi, MI",-1,-1,-1,-1,-1,-1
298,Snowflake Data Engineer,Employer Provided Salary:$70.00 - $80.00 Per Hour,"Job: Snowflake Data Engineer – Contract (6+ Months) – Stillwater, MN
REMOTE - HAVE TO WORK ONISTE WHEN EVER CLIENT CALL. Preferably local to MN NEEDED
EAD – GC – H4- EAD G C - CITIZEN
W2 Contract
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Expected hours: 40 per week
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 7 years (Required)
Snowflake API: 4 years (Required)
Snowflake administration: 6 years (Required)
Snowflake RBAC, RLS, CLS: 5 years (Required)
Willingness to travel:
50% (Required)
Work Location: Remote",-1.0,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
299,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1.0,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
300,Data Engineer I,Employer Provided Salary:$90K - $120K,"Interested in joining a growing company where you will work with talented colleagues, enhance a supportive and energetic culture, and be part of the climate solution? At Energy Solutions, we focus on the big impacts. And we believe that market-based programs can be a powerful force to deliver large-scale energy, carbon, and water-use savings. For 28 years, we've harnessed that power to offer proven, performance-based solutions for our utility, government, and institutional customers.
Energy Solutions (ES) is currently seeking a full-time Data Engineer I to join our Business Operations Team. The Data Engineer role is part of the Business Operations Team and will work on critical projects such as the Corporate Operational Data Warehouse (CODW) and the new ERP system. The CODW project is one of ES's strategic goals, to build and maintain a system that enables ES to centralize, integrate, and analyze internal data from various sources. The goal is to provide a single source of truth for ES internal data and enable data-driven decision-making across the organization. This role will potentially lead the design and implementation of the infrastructure for internal reporting needs, driving operational efficiencies and helping our company's transition to operating internationally with 1000+ employees.
Energy Solutions has a remote-friendly work environment for staff located throughout the United States. We also have offices in Oakland and Orange, California, Boston, Massachusetts, New York, and Chicago, IL, for those that wish to work from one of our offices.

Daily responsibilities include but are not limited to:
Collaborate with stakeholders to understand data requirements and develop data-driven solutions that support business goals
Analyze data sources and pipelines needed for the CODW project
Design and implement scalable data pipelines, ETL processes, and data integration solutions for the CODW project using Python, SQL, and AWS technologies
Develop and maintain a data warehouse and optimize data storage, processing, and retrieval
Build schema, populate the database, and automate reporting
Document and maintain technical specifications, ETL processes, data mappings, and dictionaries
Support the Data Science Center of Excellence (DSCOE) with other data engineering needs

Minimum Qualifications:
BS/BA in Computer Science, Physics/Engineering, Business or Mathematics
Experience in building ETL data pipelines, real-time pipelines are a plus
Strong programming skills in Python and SQL
Minimum 2 years of data warehouse development and strong fundamentals in dimensional data modeling
Minimum 2 years of experience creating SQL queries and ETL design, implementation, and maintenance
Minimum 2 years of experience developing data pipelines using Python
Minimum 2 years of experience with AWS, AWS Redshift, AWS Data Engineering and ML Ops tools and services (ie: S3, Redshift, RDS)
Proven ability to aggregate, normalize, munge, analyze, and summarize value from disparate datasets
Strong oral, written, and interpersonal communication skills
High degree of accuracy and attention to detail
Ability to establish priorities, work independently with minimum supervision
Preferred Qualifications:
Ability to manage and communicate data warehouse plans to a non-technical audience
Expertise in cloud services architecting and designing secure AWS environments
Experience using BI tools like Tableau and Looker
Experienced in analyzing and troubleshooting data quality issues
Self-driven, highly motivated, and able to learn quickly
Familiarity with large data sets, cloud-based development and deployment, open-source practices and frameworks, and intelligent applications is desirable
Proven success in migrating data from legacy systems

Salary DOE: $90k - $120k / Annually
Compensation is commensurate with experience and includes a generous retirement package. Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP).
To see this and our other open positions, please visit https://energy-solution.com/company/careers/.
We encourage candidates from diverse backgrounds to apply. For more information about Energy Solutions and our Diversity, Equity and Inclusion efforts, please visit us on our website at www.energy-solution.com.
Background Check Information
Information will be requested to perform the compulsory background check. A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment. Energy Solutions is an equal opportunity employer.
Reasonable Accommodations
Energy Solutions is committed to providing access and reasonable accommodation for individuals with disabilities. If you require an accommodation in completing this application, interviewing, and/or completing any pre-employment testing, or otherwise participating in the employee selection process, please email accommodation@energy-solution.com.
Privacy Notice for Job Applicants",3.9,"Energy Solutions
3.9",Remote,10000+ Employees,2008,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$25 to $100 million (USD)
301,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
302,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
303,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
304,2024 Data Engineer I,-1,"2024 Data Engineer I
316114

Textron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.

Responsibilities:
Analyze, manipulate, or process large sets of data using statistical software.
Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces
Identify business problems or management objectives that can be addressed through data analysis.
Document data standards, semantic layer mapping, data flows and conceptual flows.
Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.
Perform statistical analysis to provide recommendations on best courses of action to business leaders.
Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.
Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.
Collaborate with business users to ensure data is available for self-service analytics.
Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.


Qualifications:
Education: Bachelor’s degree in computer science, engineering, mathematics, or similar field required
Years of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience required
Software Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferred
Experience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)
EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

Recruiting Company Textron Specialized Vehicles
Primary Location US-Georgia-Augusta
Job Field Information Technology
Schedule Full-time
Job Level Individual Contributor
Shift First Shift
Job Posting 08/24/2023, 8:20:05 AM",3.8,"Textron Specialized Vehicles
3.8","Augusta, GA",10000+ Employees,1923,Company - Public,Aerospace & Defense,Aerospace & Defense,$10+ billion (USD)
305,AI/ML Data Automation Engineer,$88K - $125K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
306,Apple Music - Data Engineer,-1,"Summary
Posted: Aug 18, 2023
Weekly Hours: 40
Role Number:200496819
This team is more than a group of engineers — it’s a group of music lovers. That passion has made Apple Music the world’s most complete music experience, with over 60 million songs, thousands of playlists, and daily selections from music experts for 115 countries. The team’s data-driven engineers focus relentlessly on the customer experience by running worldwide experiments and analyzing usage and latency, while collaborating with Apple’s product groups. As a result, someone can use the Shazam app to identify an intriguing song in a café in the morning, add it to their playlist from Apple Watch, listen to it through their AirPods on their commute, and share it with their family on HomePod at dinnertime. And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love. Apple Music is a huge part of Apple’s business because it’s a big part of people’s lives.
Key Qualifications
Expertise in designing, implementing and supporting highly scalable data systems and services in Scala
Experience with distributed computing technologies such as Hadoop MapReduce, Spark/Spark-SQL, YARN/MR2
Expertise building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Solid understanding of data-modeling and data-architecture optimized for big data patterns, such as efficient storage and query on HDFS Experience with distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues
Experience with low-latency NoSQL datastores, and traditional relational databases is a plus
Description
We are seeking a hardworking, performance-savvy, engineer to build out the big data platform and services, which power many of these customer features — existing and new. As a core member of the Data Engineering team you will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability. This will include crafting systems to model, ingest, process and compute large-scale, mission-critical data across Apple Music. High-throughput and reliability are essential. The computed datasets are produced for internal reporting used by executives and they are also shared across many teams within the ASE organization, such as the Search team, Recommendation team, AB team, Marketing team. You will also enjoy the benefits of working in a fast growing business where you are encouraged to “Think Different” to solve very interesting technical challenges and where your efforts play a key role in the success of Apple’s business. This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users. Come join us to help deliver the next amazing Apple product!
Education & Experience
BS CS or equivalent work experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $131,500 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",4.2,"Apple
4.2","Seattle, WA",10000+ Employees,1976,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
307,Data Engineer (Contract),$77K - $111K (Glassdoor est.),"About RevOpsforce:
At RevOpsforce, our mission is to drive sustainable revenue growth for our clients by leveraging data-driven insights, technology, and process optimization. We strive to deliver measurable results that exceed expectations, while building strong partnerships with our clients and continuously improving our own capabilities. Our Expert Network is composed of the highest skilled and certified professionals in revenue operations. We leverage this network to solve our clients' most complex operational challenges.
Type: Contract
Job Description:
We are seeking a talented and experienced Data Engineer to join our growing team. As a Data Engineer at our company, you will be responsible for designing, building, and maintaining the data pipelines that power our data-driven applications. In this role, you will work closely with our data science and engineering teams to develop high-quality, efficient data pipelines that enable us to derive insights from vast amounts of data. You should be comfortable working with large volumes of structured and unstructured data, and have a strong understanding of SQL, Python, and other programming languages.
Responsibilities:
Design, build, and maintain data pipelines to support data-driven applications and analytics
Analyze data to identify trends and patterns
Collaborate with data scientists and engineers to develop data-driven solutions
Write and maintain documentation for data pipelines
Monitor and optimize data pipelines for performance and efficiency
Qualifications:
Bachelor's or Master's degree in a related field (e.g. Computer Science, Data Science, Engineering)
3+ years of experience in a data engineering role
Strong skills in SQL and at least one programming language (e.g. Python, Java, C++, etc.)
Experience with big data technologies (e.g. Hadoop, Spark, etc.) and cloud platforms (e.g. AWS, Azure, GCP)
Strong problem-solving and analytical skills
Excellent communication skills and ability to work in a team environment
Join our dynamic and forward-thinking team to make a significant impact on our clients' revenue growth journey!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Website is coming soon and will be located at www.revopsforce.com",-1.0,RevOpsforce,"Austin, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
308,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
309,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
310,Data center infrastructure Engineer,Employer Provided Salary:$60.00 - $68.00 Per Hour,"Requirements and Responsibilities:
Must have Bachelor’s degree in IS or equivalent.
Must have experience with VMWARE, DATA CENTER OPERATIONS, IT INFRASTRUCTURE, and VIRTUALIZATION.
Experience working with Linux (CentOS/RHEL/Rocky) .
Experience working with VMWare 6.x / 7.x and storage area network (SAN).
Manage and support Microsoft domain / Active Directory Services, RBAC, Group Policy, Windows Servers, desktops and services automate tasks through scripting technologies Support execution of Information
Support and manage the VMWare virtualization and storage (SAN - Dell Extreme IO & Pure Storage) infrastructure to meet and exceed development operations growth expectations.
Job Type: Contract
Pay: $60.00 - $68.00 per hour
Benefits:
Vision insurance
Experience:
Data center operations: 4 years (Required)
IT infrastructure: 3 years (Required)
Work Location: In person",3.0,"BJ Services
3.0","Exton, PA",1001 to 5000 Employees,1872,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",$1 to $5 billion (USD)
311,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Job Role: Data Engineer with Snowflake
Location: REMOTE, Stillwater, MN.
HAVE TO WORK ONISTE WHEN EVER CLIENT CALL.
Duration: 6+ Months Contract
Visas: USC, GC, EAD-GC, H4-EAD
W2 Requirement
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Expected hours: 40 per week
Experience level:
7 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
Snowflake: 4 years (Required)
Work Location: Remote",-1.0,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
312,Senior Data Engineer,Employer Provided Salary:$115K - $140K,"ABOUT THE JOB
Data is one of the foundations of Everside Health and plays an integral role in delivering first-class healthcare services to our patient population. We utilize a wide variety of tools to produce valuable data and provide better patient care as a result.
As a Senior Data Engineer, you will be an established thought leader through close partnerships with expert resources to design, develop, and implement data assets for a wide range of new initiatives at Everside Health. The role involves heavy data exploration, proficiency with SQL, ETL, knowledge of service-based deployments and APIs, and the ability to discover and learn quickly through collaboration. There is a need to think analytically and outside of the box while questioning current processes and continuing to build your business acumen. There will be a combination of team collaboration and independent work efforts. This role involves interaction with the Analytics team as well as a wide range of business areas across Everside.
We seek candidates with a strong quantitative background and excellent analytical and problem-solving skills. This position combines business and technical skills involving interaction with business customers, Analytics partners, internal and external data suppliers, and information technology partners.
ESSENTIAL DUTIES & RESPONSIBILITIES
Deliver data warehouse and analytic solutions by ingesting, integrating, and curating data to deliver information to the business and stakeholders
Conduct ETL design, development, and maintenance including data extraction, manipulation, analysis, source-target mapping, change data capture, code performance
Ensure seamless integration of data across the enterprise and drive automation of common and repeated tasks
Conduct performance tuning and optimization of all processes executed across the data platform
Develop large scale data structures and pipelines to organize, collect, and standardize data that helps generate insights and addresses reporting needs
Collaborate closely with business users and stakeholders to define data for the design, development, and deployment of new solutions that support strategic business priorities
Provide coaching and training to junior and new team members on ETL architecture, standards, and documentation
QUALIFICATIONS
Bachelor’s degree in Computer Science or related field and 5+ years of Data Engineering work experience. Experience designing and implementing ETL pipelines, working with a variety of data warehousing models and design fundamentals, working with Matillion, Azure Data Factory, Data Bricks, or similar ETL tool. Experience working in Snowflake, Synapse, or similar MPP platform and experience in DataOps/DevOps and agile methodologies
DESIRED ATTRIBUTES
Experience with messaging streaming systems (e.g., Kafka, Azure Event Hubs)
Experience in hybrid data processing methods (batch and streaming)
Experience with AWS or Azure application deployment
Experience with API integration
Pay Range: $115,000 - $140,000/yr
The actual offer may vary dependent upon geographic location and the candidate’s years of experience and/or skill level.
Everside Benefits Summary
We believe in empowering teammates to do their best work and build better healthcare. Below are some of our benefit offerings. Eligibility is based on 24/hr week.
Health and Well-Being: Free Everside membership for in person and virtual care, employer paid life and disability insurance, and choice in medical/dental plans, vision, employer funded HSA, FSA, and voluntary illness, accident and hospitalization plans. Benefits are effective on the first of the month following date of hire.
Financial Support: Competitive compensation, 401k match, access to financial coaching through our Employee Assistance Program
Lifestyle: Paid time off for vacation, sick leave, and more, holiday schedule
Learn more at
https://www.eversidehealth.com/careers/",3.2,"Everside Health
3.2",Remote,1001 to 5000 Employees,2001,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
313,Data Engineer (Onsite),Employer Provided Salary:$85K - $100K,"If you are seeking a career that has tremendous impact on manufacturing operations, then come be a part of Tokai Carbon GE's team to design, develop and implement our new manufacturing execution system. You'll play a critical role in our organization's success, providing easier access to data that engineers, analysts and decision-makers need to do their jobs. You will set the foundation of data for growth, relying on your programming and problem-solving skills to create scalable solutions. You will be part of the team building BI and Analytics tools that will take our operations to the next level. If you are excited about applying your knowledge and skills to meaningful challenges, apply today!
JOB SUMMARY
The Data Engineer will be responsible for designing, implementing, and supporting solutions used to collect, transform, store, and analyze data from a variety of company applications. They analyze and organize raw data, build data systems and pipelines, review and analyze data infrastructure, plan, and implement solutions to store and manage data. Also, they create reports, dashboards, and other analytics solutions. They will work closely with the lines of business and external partners, to ensure successful development and delivery of BI and data analytics solutions. This is a hands-on development and support position with ownership responsibility for data architecture and engineering for BI and analytics purposes.
ESSENTIAL FUNCTIONS
Work with business users and other technology teams to understand their data requirements, identify appropriate data sources, design, and develop data pipelines and design appropriate data structures to store the required data.
Design data pipelines using SQL, other programming languages and data integration tools.
Gather and combine data from a variety of sources to support actionable decision making. Prepare data for reporting, and predictive and prescriptive tools.
Address BI/EDW technical development working with operations, sales, supply chain, technical services, ERP applications, analytics, and finance & accounting teams.
Contribute to our suite of reports and data products while collaborating with business analysts, business process owners, and other team members, including UAT, and support for technical testing.
Work closely with senior team members defining data architecture, optimizing EDW performance, and updating EDW documentation, including outlining options to improve resilience and data quality monitoring.
Maintain the Enterprise Data Warehouse (EDW) and influence data quality and reliability.

COMPETENCIES
Data modeling, relational databases, data warehousing, business intelligence, stream processing.
SQL programming skills, including writing efficient and optimized SQL queries for data retrieval and manipulation.
ETL/ELT processes and data integration.
Strong analytical skills, development experience, and excellent verbal and written communications.
Manage all stages of the development lifecycle, including planning, requirements gathering, designing, developing, documenting, testing, training, deployment, and support.
Own individual and cross-functional team deliverables on small to large projects.
Project and change management methodologies including Agile/Scrum.
Willingness and availability to travel, and perform other duties as needed.
Must be a self-starter, manage workload autonomously, and balance competing priorities.
Ability and desire to continuously learn about recent technologies and Analytics trends.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Computer Engineering, Analytics, Information Systems, or equivalent experience.
1-3 years of experience working with SQL and relational databases (MS SQL Server preferred).
1-3 years of experience working with Microsoft Azure, particularly Azure Data Factory, Azure Synapse and Power Platform.
1-3 years of experience preferred in reporting, dashboards, and BI tools (Power BI preferred) for ERP and MES platforms.
1-3 years of experience preferred with Visual Studio, SSRS.
Experience with MS D365 and AVEVA or similar ERP and MES solutions preferred.
Proficient in coding and scripting in Python preferred.
Experience in Manufacturing, Supply Chain, Logistics, and Finance and Accounting solutions preferred.
How to stand out: Microsoft Azure Data Engineer Associate Certification
COMPENSATION AND BENEFITS
Available on your first day: Full medical, vision, dental benefits, short-term disability, and long-term disability
Paid time off
Monthly wellness reimbursement
Health Savings, Flexible Spending and Dependent Care accounts
401(k) retirement savings plan with employer match
Annual bonus",2.6,"Tokai Carbon GE LLC
2.6","Charlotte, NC",1001 to 5000 Employees,-1,Company - Public,Chemical Manufacturing,Manufacturing,$1 to $5 billion (USD)
314,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
315,Data Engineer,$73K - $110K (Glassdoor est.),"Data Engineer

Posting Date: Aug 23, 2023
Location: Houston, TX, US
Lake Charles, LA, US
Company: Sasol
A job at Sasol is a career made by you, with purpose, development opportunities, benefits to support every stage of life and a working culture that embraces flexibility, diversity, and inclusion. Learn more.

Purpose of Job
The Data Engineer works as part of an agile team, interacts with data scientists/translators and industry experts to understand how data needs to be converted, loaded and presented. Collects, structures, analyses data and ensures quality. Responsible for the definition, conception, construction, and ongoing support of big data platforms and solutions.
Recruitment Description / Key Accountabilities
Integrate IT (Information Technology) and OT (Operational Technology) Systems.
Use programming languages and big data technologies following best practices and standards
Understands latest analytics tools and technologies (open source/ proprietary/ vendor supported) and determines fit with current architecture
Building and maintaining data pipelines that move data between various systems.
Ensuring data quality, integrity and security
Monitoring and optimizing data performance
Collaborate with technical teams to develop and maintain a flexible and sustainable workflow.
Collaborate with Product Owner, Data Scientists, Data Translators, and other Business Stakeholders to implement and deploy scalable solutions in an agile way of working; actively participate in scrum process and meetings with team where required
Assist with communication, improve transparency, radiate information, and address challenges effectively
Formal Education
University Bachelors Degree in Computer Science & Inf Systems or Engineering: Computer
Min Experience
Experience: 5+ relevant years
Long Description
Excellent communication and written skills.
Experience in building complex data pipelines using Cloud Services, REST APIs, SQL Databases. Python and SQL skills are are required. Experience in Azure ADF, Synapse or Databricks are beneficial.
Knowlege of Cloud Computing and Infrastructure, Data Models and Standards, Database Administration, Plant Information Management Systems, ERP Systems, Networking and Protocols, Security and Compliance, Change Managemnt, Problem Solving and Trouble Shooting are a plus.

Sasol (USA) Corporation is an Equal Opportunity Employer and gives consideration for employment to qualified applicants without regard to race, color, religion, creed, age, sex, sexual orientation, gender identity, pregnancy, national origin, disability or protected veteran status, as well as any other characteristic protected by applicable law, regulation or local ordinance. For more information about your rights under the law, see http://www.eeoc.gov/employers/upload/poster_screen_reader_optimized.pdf

Sasol treats work-authorized job applicants equally in recruiting and hiring without regard to their citizenship status or national origin unless required by law, regulation, executive order, government contract, or determination by the Attorney General. Please note Sasol will not sponsor, obtain, and/or petition for temporary visa status (for example, E, F-1, H-1, H-2, L, B, J, or TN) for candidates for this position and/or personnel hired for this position.


Nearest Major Market: Houston",3.9,"Sasol
3.9","Houston, TX",10000+ Employees,1950,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$10+ billion (USD)
316,Data Engineer,Employer Provided Salary:$30.05 - $44.13 Per Hour,"AMAZING BENEFITS PACKAGE!
Medical, Dental, & Vision Insurance
Paid Time Off (PTO) – up to 6 weeks of PTO per year
Childcare Stipend – contribution of $5,000 per year to the employee’s dependent care spending account
Loan Reimbursement - reimbursement for student loan payments up to $5,250 per year
Flexible Work Arrangements - opportunity to work a modified schedule, work part-time, or work from home
Retirement Plans - organization matches the employee’s contribution, up to 6% of gross wages
above benefits dependent upon eligibility criteria
Click here for more detail about the benefits package!
Pay Range: $30.05 - $44.13 USD hourly.

Purpose of Job
Improves the overall health of the communities we serve by providing superior customer service and advanced technical support to ensure quality patient care and user satisfaction as follows:
Essential Duties and Responsibilities
Designs and implements databases, ETL routines (using SSIS), stored procedures, and OLAP solutions in addition to providing support and enhancements to existing data solutions.
Designs and builds normalized and denormalized database solutions using healthcare industry best practices for data warehousing for specific client requirements, using Microsoft SQL Server Database programming (stored procedures and database design).
Works on multiple concurrent deadline-driven projects while ensuring data quality and meeting service level agreements.
Builds and maintains cubes (internal and client-facing).
Optimizes database schemas, queries, cubes, and reports, implementing complex logic requirements.
Conducts tuning reviews / assessments (DB and SQL tuning), reporting, and query monitoring.
Determines, enforces, and documents database policies, procedures, and standards.
Performs other duties as assigned, including supporting the CHAS Health Mission and Core Values.
Qualifications
Education/Experience: Associate’s degree or commensurate experience in a technical field required. Prior experience in database development using Microsoft SQL server, SSIS and SSAS preferred; understanding performance, deployment, configuration, security, migration, and troubleshooting; as well as proficiency in building and optimizing SQL queries preferred. Previous experience building and supporting business intelligence/data warehousing solutions; and knowledge of healthcare data models preferred.
Skills: Excellent logical and problem-solving abilities, verbal and written communication skills required. Ability to work independently in a self-directed environment, contribute to a team, maintain a positive attitude, demonstrate very high attention to detail, and a commitment to quality while working in a high availability environment is required. Commitment to supporting a safe, respectful, equitable, and inclusive environment required. Valid drivers’ license and insurance required.
Physical Demands
Required to stand, sit, and be mobile up to two-thirds of the time. Required to read from text and computer screen over two-thirds of the time. Climbing or balancing, stooping, kneeling, or crouching occurs less than one-third of the time. Communicating occurs constantly throughout the day. Lifting occurs about half the time up to 10 lbs. and less than one-third of the day from 25-40 lbs. Rarely is there a need to lift more than 41 lbs.",3.8,"CHAS Health
3.8","Spokane, WA",1001 to 5000 Employees,1994,Nonprofit Organization,Health Care Services & Hospitals,Healthcare,$25 to $100 million (USD)
317,Energy Data Engineer - Mid Level,$81K - $121K (Glassdoor est.),"About the Organization
Now is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.

The Data Engineer will support Redhorse in its execution of specific task areas related to the Federal Government Carbon Pollution Free Electricity (CFE) Technical and Strategy Support contract. The program will establish technical frameworks and analytical tools and methods to inform CFE procurement decisions for the entirety of the federal government. Work will be conducted primarily at Redhorse office locations (hybrid remote work) and on government client site as required. The ideal candidate will live in the DC/Virginia/Maryland area and willing to undergo a background check to obtain a Public Trust.
Primary Duties and Responsibilities for this position include
Review and analyze the US Federal Government Executive Agencies electric load data
Develop and test modeling techniques to understand an agency’s hourly load profile to inform the agency’s hourly CFE procurement requirement as an off taker from a new CFE technology source.
Support the configuration and ingestion of designated structured, unstructured, and semi-structured data repositories into capabilities that satisfy client requirements and support a data analytics pipeline to drive rapid delivery of functionality to the client.
Maintain all operational aspects of data transfers while accounting for the security posture of the underlying infrastructure and the systems and applications that are supported and monitoring the health of the environment through a variety of health tracking capabilities.
Automate configuration management, leverage tools, and stay current on data extract, transfer, and load (ETL) technologies and services.
Work under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.
Apply comprehension of data engineering-specific technologies and services, leverage expertise in databases and a variety of approaches to structuring and retrieving of data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically using vendor consoles.
Engage with multiple functional groups to comprehend client challenges, prototype new ideas and new technologies, help to create solutions to drive the next wave of innovation, and design, implement, schedule, test, and deploy full features and components of solutions.
Identify and implement scalable and efficient coding solutions.
Minimum Requirements for skills, Experience, and Credentials
Master’s Degree in Engineering, Computer Science, Mathematics or Statistics
Four (4) years of experience in enterprise data analysis with a preferred focus on utility data, to include advanced metering infrastructure (AMI) electricity data
Local to Washington D.C. metro area preferred, remote work is an option
Willingness to travel in support of on-site customer meetings
Willingness to obtain a Public Trust
Desired Basic Requirements for Skills, Experience, and Credentials include
Knowledge of U.S. Federal Government Energy Legislation and Executive Orders (especially EO 14057)
Experience in or knowledge of utility interval data and electricity pricing
Executive Order Requirement:
While currently on HOLD, Redhorse recognizes that we may need to comply with Executive Order 14042 which requires Federal Contractors to ensure all U.S. new hires be fully vaccinated for COVID-19. As required by the Executive Order, Redhorse will work in coordination with applicable contract agencies to consider requests for Reasonable Accommodations.
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.
EOE/M/F/Vet/Disabled",3.5,"Redhorse
3.5","Arlington, VA",51 to 200 Employees,2008,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
318,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
319,Data Engineer II,$90K - $119K (Glassdoor est.),"Data Engineer II
Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What’s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.
Join us to do the best work of your career and make a profound social impact as a Data Engineer on our Growth Marketing team in Round Rock, Texas, Hopkinton, Massachusetts, or Remote United States
What you’ll achieve
As a Data Engineer II on a growing team, you will bring in your industry experience to develop, deploy, and maintain productionized data pipelines that support the training and productionization of machine-learning models.
You will:
Build, test, and implement data pipelines using tools like Apache Airflow, Spark, and Python for the training and inferencing of machine-learning models
Ensure data quality, monitor data pipelines, and troubleshoot production issues
Document metadata, data models, data pipelines and APIs to help data science teams understand and leverage data assets.
Work with data scientists and business analysts to understand data requirements
Take the first step toward your dream career
Every Dell Technologies team member brings something unique to the table. Here’s what we are looking for in this role:
Essential Requirements
U.S. citizen residing in the U.S.
Bachelor’s Degree in Computer Science, Engineering, IT, or equivalent professional experience
1 to 2 years experience developing production-quality Python and performant SQL, working with data warehouses, JSON, Apache Hive, Hadoop or Spark, or related technologies
1 to 2 years experience with orchestration tools, such as Apache AirFlow
Experience with monitoring tools and observability
Who we are

We believe that each of us has the power to make an impact. That’s why we put our team members at the center of everything we do. If you’re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we’re looking for you.

Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us.

Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. Read the full Equal Employment Opportunity Policy here.
Job ID:R227305

Dell’s Flexible & Hybrid Work Culture

At Dell Technologies, we believe our best work is done when flexibility is offered.

We know that freedom and flexibility are crucial to all our employees no matter where you are located and our flexible and hybrid work style allows team members to have the freedom to ideate, be innovative, and drive results their way. To learn more about our work culture, please visit our locations page.",4.2,"Dell Technologies
4.2","Round Rock, TX",10000+ Employees,1984,Company - Public,Information Technology Support Services,Information Technology,$10+ billion (USD)
320,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
321,Data Scientist / ML Engineer (REMOTE /Contract),-1,"Role: Data Scientist / Machine Learning Engineer
Location: REMOTE
Duration: 6+ months
Scheduling Interviews
Skills/Experience:
6+ years of experience working in Data Science and Machine Learning field.
Extensive experience in applying data preprocessing, statistical analysis, data analytics tools, predictive modelling, model deployment and evidence-based approaches to find lean, actionable solutions to various real-world enterprise business problems.
Experience in the application of Supervised and Unsupervised Learning algorithms, Naïve Bayes, Regression Analysis, Neural Networks/Deep Learning, Support Vector Machines (SVM), Random Forest, K-Means, Hierarchical, Spectral clustering, DBSCAN, Collaborative Filtering and other advanced machine learning techniques.
Used AWS Cloud Services Sagemaker, ECS, EKS, S3, Redshift, QuickSight
Experience with Google Cloud Platform (GCP) AutoML, Vertex AI, BigQuery, Colab, DataProc, Data Studio,
Exposure to Azure ML, PowerBI, Azure SQL and DataBricks
Design custom BI reporting dashboards or interactive data visualizations and widgets in R and Python using Shiny, Tableau, Ggplot2, Plotly, Matplotlib, and Seaborn.
Produce custom BI reporting dashboards in R and Python using Shiny, and Plotly for rapid dissemination of actionable, data driven insights.
Strong experience in Software Development Life Cycle (SDLC), MLOPS
Experience in working with relational databases (Teradata, Oracle) with advanced SQL skills.",3.5,"Glow Networks
3.5",Remote,51 to 200 Employees,2003,Subsidiary or Business Segment,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
322,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
323,Data Warehouse & Artificial Intelligence Engineer,$83K - $120K (Glassdoor est.),"Job Description:

Design, Implement and Maintain data warehouse.

Duties and Responsibilities:

Develop, test, and deploy extraction, transformation and loading (ETL) routines to load the data warehouse using SSIS, stored procedures, etc.
Develop analytics including reports, dashboards, and data visualizations
Troubleshoot BI-related problems and tune for performance
Implement and oversee a company-wide data governance program to support master data management and resolve data definition discrepancies among stakeholders
Plan the technical approach for implementing analytics strategy across multiple channels
Plan and implement activities to maintain data confidence
Provide technical direction and technical support for all member of the team
Architect and implement analytics solutions to meet data collection and reporting needs.
Work with team to recommend analytic software
Provide general troubleshooting assistance for analytic reports and break fixes
Wrangle data (a.k.a. data engineering) from a wide variety of complex data sets large volumes of data into normalized and enriched data sets.
Assemble and evaluate data such that new insights, solutions, and visualizations can be derived.
Perform data discoveries to understand data formats, source systems, file sizes, etc. and engage with internal and external business partners in this discovery process.
Deliver data acquisition, transformations, normalization, mapping, and loading of data into a variety of data models.
Collaborate extensively with Technology to design data ingestion, data models, and automated operational metrics for consistently high quality data
Supports the research, analysis and presentation of information by: producing reports, compiling and summarizing information, producing supporting documentation and exhibits, and verifying information received from external sources.
Assists other associates with preparation of reports and use of information systems, software and related sources of information. Trains other users on report preparation and database access.
Perform other duties as assigned.

Required Knowledge, skills and abilities:

Ability to manage change successfully in a fast-paced environment
A passion for learning and the ability to do so on the fly
Must be strategic, dynamic, and detail-oriented. The ideal candidate is an analytical thinker, a self-starter and a quick learner.
Ability to design and architect method of extracting data from its original source to another location (i.e. a data lake) to make that data consumable for a variety of downstream purposes such as applications, visualizations, analytics and other related data science activities.
Basic understanding of data science
Ability to transforming and map one ""raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.
Exceptional ability to clearly and effectively communicate with IT peers and users at all levels of technical and business acumen.
Strong time management skills and multi-tasking capabilities
Ability to understand business rules governing financial and budgeting process
Exceptional oral and written communication skills
Self-motivated, ability to set priorities and meet deadlines

Education and Experience:

Bachelor's degree in Computer Science, Business, Economics, Information Science, or Analytics
Minimum of 3 years of experience with data analytics across multiple channels
Experienced with the technologies required to implement Analytics across multiple channels, (for example, Data Layer, Custom JavaScript, JAVA, AJAX, HTML, Objective-C, Swift)
Knowledge of Tableau is a plus
Familiarity with Microsoft Power BI
Extensive experience with ETL and/or other Big Data processes.
Contract manufacturing experience is a plus.

Foxconn Assembly, LLC is an Equal Opportunity Employer (EOE). All qualified candidates will receive consideration without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or marital status in accordance with applicable federal, state and local laws.
Foxconn Assembly, LLC participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

NbXn0DyWY6",3.4,"Foxconn Corporation
3.4","Houston, TX",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$5 to $25 million (USD)
324,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
325,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
326,Senior Data Center Engineer,-1,"Marco is a one-stop shop for all things business tech. Our employees are “movers and shakers” and our company is always striving to do what’s right. Does this sound like a culture you want to be a part of? We’re hiring a new team member to help take Marco’s technology further – working full-time, Monday - Friday, 8am-5pm.
More about us. We do it all – from copy and print solutions to IT and managed services. We are an organization led by salespeople with 650+ engineers ready to fix any and all issues. We have offices in 12 states and service nationally.
Join our growing team. You won’t regret it.

The Senior Data Center Engineer is responsible for providing quality services and solutions to our internal clients while maintaining a high level of client satisfaction. This position works cooperatively with other teams in a coordinated effort to develop, design, implement and support Marco’s Data Centers, along with the IT environment that depends upon them. While not a leadership position, the Senior datacenter engineer displays leadership qualities and may be called upon to lead others in certain situations.

ESSENTIAL FUNCTIONS:
Lead the overall design and implementation of the technology stack within the Marco Data Centers
Maintain physical access to the Marco Data Centers and be a primary contact for essential services
Perform proactive maintenance and reactive support to the equipment within the Marco Data Centers
Create and maintain documentation for all equipment within the Marco Data Center. Review and assist others with their documentation, offering constructive feedback and pointing out deviations from our policies and standards where necessary.
Report on the health of the Marco Data Centers, perform capacity planning and proactively provide recommendations for future growth
Monitor and provide proactive recommendations for backup systems and disaster recovery processes
Review all systems and make technical/process recommendations for improving efficiency
Assist in defining standards for all systems and processes provided by the Data Center Services group
Administer and maintain a reliable, stable, secure and cost-effective hosting environment to deliver 24x7x365 operational service
Provide escalated remote technical support to Marco Data Center clients. Handle requests with due urgency, empathy, and a spirit of cooperation.
Document your work, including troubleshooting steps, time, and resolution in the service ticketing system.
Ensure your work and the work of others adheres to Marco internal policies and applicable regulatory requirements.
Create and maintain standard templates, documentation, images, policy, and automation that will aid in a consistent and rapid deployment of Marco Data Centers
Research, test and recommend new tools and products to enhance the offering and efficiency of Marco’s Data Centers
Document the procedures, infrastructure and environments and keep updated with changes to the environment
Implement and maintain Data Center system builds that are consistent, optimized for client experience, overall performance and supportability
Set deadlines for projects, then organize your time to meet these deadlines; be willing to put in extra time and effort to meet a time-bound goal when necessary
Mentor other staff while you assist with implementation or utilization within your area of technical specialty
Work multiple projects simultaneously and provide consistent, high-quality results
Assist with implementation and or migration of clients’ existing data and applications
Perform system and application testing to provide functional environment
Provide onsite and or remote technical support to Marco clients during onboarding process
Implement changes and work orders for existing Marco clients
Review customer hosted systems and make technical/process recommendations for improving efficiency
Stay up to date with relevant state-of-the-art technology, equipment, and/or systems
QUALIFICATIONS:
Bachelor’s Degree and 4+ years of experience or equivalent experience
Experience and current certifications or specializations aligning with the technologies used in Marco’s Data Center to include Cisco NX-OS, SAN-OS, IOS, UCS-B, ASAv, EMC VNX, Meraki MX, VMWare vSphere, Citrix XenApp, Zscaler and XenDesktop preferred.
Current high level industry recognized certifications including one or more of the following: CCNP, CCE, VCDX, EMCTA
Solid understanding of the administration of network operating systems and services found in a Data Center environment
Benefits:
We’re not just competitive when it comes to business tech – we’re also pretty proud of what we offer our employees. Our benefits include medical, dental, and vision insurance. We also have paid holidays and vacation, 401k with generous company match, flexible spending accounts, employee purchase program, employer-paid life insurance, voluntary-term life insurance, short and long-term disability, critical illness and accident benefits, and pet insurance. Yes, we care about your furry family too.

all benefits are dependent on employment status
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities
Applicant Labor Law Posters",4.0,"Marco Technologies LLC
4.0",Remote,1001 to 5000 Employees,1973,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
327,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
328,Data Engineer,Employer Provided Salary:$100K - $105K,"Job Description
▪ 4+ years of experience in Python with Data background
▪ BA/BS Computer Science or a technical/quantitative degree
▪ Excellent communication and collaboration skills
▪ Interest in learning and adopting new tools and techniques
▪ Humble enough to learn from others, confident enough to teach others new things
Full time position, Salary (+ Benefits ( Healthcare Insurance + PTO + Bonus + 401k) )
Job Type: Full-time
Salary: $100,000.00 - $105,000.00 per year
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",-1.0,Konnectingtree.Inc,Remote,-1,-1,-1,-1,-1,-1
329,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
330,AI/ML Data Automation Engineer,$88K - $125K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
331,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
332,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
333,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
334,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
335,Data Engineer (multiple openings) - IHM,Employer Provided Salary:$88K - $157K,"Discover. A brighter future.
With us, you’ll do meaningful work from Day 1. Our collaborative culture is built on three core behaviors: We Play to Win, We Get Better Every Day & We Succeed Together. And we mean it — we want you to grow and make a difference at one of the world's leading digital banking and payments companies. We value what makes you unique so that you have an opportunity to shine.

Come build your future, while being the reason millions of people find a brighter financial future with Discover.

Job Description:
Employer: DFS Corporate Services LLC

Job Title: Data Engineer (multiple openings)

Job Location: Riverwoods, Illinois

Job Type: Full Time

Duties: Responsible for designing, developing, testing, and maintaining complex data solutions for the product. Mentor and influence peers to achieve commitments on data solutions on time and with quality. Telecommuting and/or working from home may be permissible pursuant to company policies.

Requirements: Employer will accept a Bachelor's degree in Computer Science, Computer Engineering, or a related field and 3 years of experience in Senior Data Engineer; ODI/DW/Batch Developer or related occupation.

Position required skills: Three (3) years of experience in the job offered or related occupation: working with Agile software development methodology and Agile management tools, including Jira; utilizing modern engineering practices, including the design and development of ETL data integration solutions in data warehouse environments; working with relational databases and Cloud-based technologies; utilizing design and architecture experience on large-scale ETL solutions; and working with Enterprise Data warehouse data models and dimensional modeling concepts, source to target mapping, and data integration architecture.

Position eligible for incentives under Employee Referral Program.

Rate of Pay: The base pay for this position generally ranges between $88,150.00 to $157,000.00. Additional incentives may be provided as part of a market competitive total compensation package. Factors, such as but not limited to, geographical location, relevant experience, education, and skill level may impact the pay for this position. We also offer a range of benefits and programs based on eligibility. Learn more at MyDiscoverBenefits.com .

QUALIFIED APPLICANTS: Please apply directly through our website by clicking on “Apply Now.” No calls. Equal Opportunity Employer/disability/vet.

What are you waiting for? Apply today!

All Discover employees place our customers at the very center of our work. To deliver on our promises to our customers, each of us contribute every day to a culture that values compliance and risk management.

Discover is committed to a diverse and inclusive workplace. Discover is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or other legally protected status. (Know Your Rights)",3.9,"Discover Financial Services
3.9","Riverwoods, IL",10000+ Employees,1985,Company - Public,Banking & Lending,Financial Services,$1 to $5 billion (USD)
336,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
337,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
338,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
339,Data Engineer I - People Analytics,$73K - $111K (Glassdoor est.),"POSITION SUMMARY Our People Analytics team is expanding! The Data Engineer I is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.

This position can work in a flexible, hybrid model with 3 days on-site in Appleton, WI and 2 days remote.

JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.

QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming. Preferred experience in Python applied to data engineering and transformation.
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated

#LI-Hybid

Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101 .",4.1,"US Venture
4.1","Appleton, WI",1001 to 5000 Employees,1951,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
340,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
341,Data Engineers,$71K - $105K (Glassdoor est.),"Job: Data Engineers / Interns – Entry level
Positions: Multiple
Preferable Locations: Scottsdale AZ, Naperville, IL
We are looking for talented individuals to become part of our growing business and grow with us. The right person should have the interest to learn new tools and adapt to fast paced environment. Experience/knowledge on databases, data management, data integration, Big Data, Data warehouse, reporting, and data analytics is preferable.
The role and compensation will depend upon any prior experience and exposure to technology areas and demonstrated education.
Education, Skills, and Abilities:
BS/BA in Computer Science or related discipline required.
Experience in writing SQL queries and basic functions & Stored procedures.
Fundamental Knowledge of databases, data warehousing, data integration, and BI tools.
Exposure to data management concepts, reporting, and analytics tools is an advantage.
Experience in RDBMS, No SQL and Java is an advantage concepts/ exposure to Cloud Computing
Attention to detail, Analytical, and problem-solving skills.
Ability and willingness to learn new tools and applications.
Effective written and verbal communication skills with the ability to convey complex technical concepts to business users and management.",4.1,"Artha Solution
4.1","Scottsdale, AZ",51 to 200 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
342,Associate Data & AI Engineer - Data Analyst - Telecom,Employer Provided Salary:$44K - $94K,"Life at Capgemini
Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer:
Flexible work
Healthcare including dental, vision, mental health, and well-being programs
Financial well-being programs such as 401(k) and Employee Share Ownership Plan
Paid time off and paid holidays
Paid parental leave
Family building benefits like adoption assistance, surrogacy, and cryopreservation
Social well-being benefits like subsidized back-up child/elder care and tutoring
Mentoring, coaching and learning programs
Employee Resource Groups
Disaster Relief
About Capgemini Engineering
World leader in engineering and R&D services, Capgemini Engineering combines its broad industry knowledge and cutting-edge technologies in digital and software to support the convergence of the physical and digital worlds. Coupled with the capabilities of the rest of the Group, it helps clients to accelerate their journey towards Intelligent Industry. Capgemini Engineering has more than 55,000 engineer and scientist team members in over 30 countries across sectors including Aeronautics, Space, Defense, Naval, Automotive, Rail, Infrastructure & Transportation, Energy, Utilities & Chemicals, Life Sciences, Communications, Semiconductor & Electronics, Industrial & Consumer, Software & Internet.
Capgemini Engineering is an integral part of the Capgemini Group, a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided every day by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get the Future You Want | www.capgemini.com
Capgemini discloses salary range information in compliance with state and local pay transparency obligations. The disclosed range represents the lowest to highest salary we, in good faith, believe we would pay for this role at the time of this posting, although we may ultimately pay more or less than the disclosed range, and the range may be modified in the future. The disclosed range takes into account the wide range of factors that are considered in making compensation decisions including, but not limited to, geographic location, relevant education, qualifications, certifications, experience, skills, seniority, performance, sales or revenue-based metrics, and business or organizational needs. At Capgemini, it is not typical for an individual to be hired at or near the top of the range for their role. The base salary range for the tagged location is $44,300 - $93,660 / year.
This role may be eligible for other compensation including variable compensation, bonus, or commission. Full time regular employees are eligible for paid time off, medical/dental/vision insurance, 401(k), and any other benefits to eligible employees.
Note: No amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. The amount and availability of any bonus, commission, or any other form of compensation that are allocable to a particular employee remains in the Company's sole discretion unless and until paid and may be modified at the Company’s sole discretion, consistent with the law.
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.

Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.

Key responsibilities:
The candidate is responsible to understand and analyze the Call Detail Records (CDRs) coming from Mediation Platform, Datadog or other sources in a 5G Wireless BSS/OSS!
The Data Analyst will use the appropriate tools to analyze the data and generate reports as requested by Product team or executives.
They will also prepare procedures to take appropriate actions based on the data patterns observed in the network.
Required Skill:
Proven track record to analyze and process data with extensive experience as an engineer in data analytics.
Experience in working with any industry where data analytics tools are used and experience in crafting reports.
Excellent problem-solving skills along with ability to work independently with little or no mentoring.
Should be strong in written and verbal communication.
Good to have:
High-level knowledge of Telecom BSS-OSS.
Experience in SAFe Agile methodologies.
Experience in log analytics, Applications Performance Monitoring, infrastructure, and fault monitoring and analysis.

Job Programmer/Analyst
Schedule Full-time
Primary Location US-CO-Denver
Organization ERD PPL US",3.8,"Capgemini
3.8","Denver, CO",10000+ Employees,1967,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
343,Data Engineer,Employer Provided Salary:$30.05 - $44.13 Per Hour,"AMAZING BENEFITS PACKAGE!
Medical, Dental, & Vision Insurance
Paid Time Off (PTO) – up to 6 weeks of PTO per year
Childcare Stipend – contribution of $5,000 per year to the employee’s dependent care spending account
Loan Reimbursement - reimbursement for student loan payments up to $5,250 per year
Flexible Work Arrangements - opportunity to work a modified schedule, work part-time, or work from home
Retirement Plans - organization matches the employee’s contribution, up to 6% of gross wages
above benefits dependent upon eligibility criteria
Click here for more detail about the benefits package!
Pay Range: $30.05 - $44.13 USD hourly.

Purpose of Job
Improves the overall health of the communities we serve by providing superior customer service and advanced technical support to ensure quality patient care and user satisfaction as follows:
Essential Duties and Responsibilities
Designs and implements databases, ETL routines (using SSIS), stored procedures, and OLAP solutions in addition to providing support and enhancements to existing data solutions.
Designs and builds normalized and denormalized database solutions using healthcare industry best practices for data warehousing for specific client requirements, using Microsoft SQL Server Database programming (stored procedures and database design).
Works on multiple concurrent deadline-driven projects while ensuring data quality and meeting service level agreements.
Builds and maintains cubes (internal and client-facing).
Optimizes database schemas, queries, cubes, and reports, implementing complex logic requirements.
Conducts tuning reviews / assessments (DB and SQL tuning), reporting, and query monitoring.
Determines, enforces, and documents database policies, procedures, and standards.
Performs other duties as assigned, including supporting the CHAS Health Mission and Core Values.
Qualifications
Education/Experience: Associate’s degree or commensurate experience in a technical field required. Prior experience in database development using Microsoft SQL server, SSIS and SSAS preferred; understanding performance, deployment, configuration, security, migration, and troubleshooting; as well as proficiency in building and optimizing SQL queries preferred. Previous experience building and supporting business intelligence/data warehousing solutions; and knowledge of healthcare data models preferred.
Skills: Excellent logical and problem-solving abilities, verbal and written communication skills required. Ability to work independently in a self-directed environment, contribute to a team, maintain a positive attitude, demonstrate very high attention to detail, and a commitment to quality while working in a high availability environment is required. Commitment to supporting a safe, respectful, equitable, and inclusive environment required. Valid drivers’ license and insurance required.
Physical Demands
Required to stand, sit, and be mobile up to two-thirds of the time. Required to read from text and computer screen over two-thirds of the time. Climbing or balancing, stooping, kneeling, or crouching occurs less than one-third of the time. Communicating occurs constantly throughout the day. Lifting occurs about half the time up to 10 lbs. and less than one-third of the day from 25-40 lbs. Rarely is there a need to lift more than 41 lbs.",3.8,"CHAS Health
3.8","Spokane, WA",1001 to 5000 Employees,1994,Nonprofit Organization,Health Care Services & Hospitals,Healthcare,$25 to $100 million (USD)
344,Data Engineer,-1,"Duration: 11+ months

Job Description:

Aviation connects the world and Connected Aviation Solutions (CAS) connects Aviation. Sustainably. Seamlessly. Securely. The Data Management & Data Science (DM&DS) team is tasked with the end to end responsibility to ensure that CAS data assets are managed with integrity and quality prior to consumption by our critical customer facing applications - whether via APIs, analytics and/or data visualizations. As a senior data engineer on the DM&DS team, you will be responsible for the design, development and maintenance of data processes and pipelines supporting critical CAS Strategic Business Unit (SBU) Data initiatives in support of the Digital Transformation of CAS as well as for the cross-SBU Connected Ecosystem. In this endeavor, you will be working closely with data architecture, data analytics & visualization teams leaders across CAS, SBU and Digital Technology (DT) teams to ensure the technical solutions are efficient, scalable and meet long term Connected Ecosystem needs.

Primary Responsibilities:
Design, develop and support the processes and pipelines for moving data throughout the CAS and cross SBU environments.
Develop automation and monitoring processes that support the data pipelines
Work closely with the architecture team to implement modern data repositories that support the CAS use cases (Pipelines, APIs, Data Science, Applications and Visualizations)
Work with internal business customers and software development teams to gather and document requirements for data publishing and data consumption via data warehouse, data lake, and analytics solutions
Support the operation of the CAS and Digital Technology owned Data Platforms, Data Warehouse and Data Lakes with a view to leveraging capabilities and resources over-time
Work with the CAS and DT Enterprise Data Architects to automate cloud deployments, as well as build CI/CD pipeline to support Cloud-Based workloads. Develop views, materialized views, and SQL scripts
Work with the CAS and DT Enterprise Data Architects to recommend investments or changes in technology, resources, procedures, equipment, systems, or other assets to improve the quality of the organizations projects.
May travel domestically and internationally up to 15%.

Qualifications / Required Skills:
Bachelors degree and 5 years of prior relevant experience OR Advanced Degree in a related technical field and minimum years 3 experience OR In absence of a degree, 10 years of relevant experience is required
3+ years of demonstrated engineering leadership in a relevant engineering function, such as software/service development and deployment, system design and integration, or data analytics.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.",3.8,"Capgemini
3.8",Remote,10000+ Employees,1967,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
345,Data Engineer,Employer Provided Salary:$80K - $85K,"Simatree is searching for a Data Engineer Consultant to support our client’s operational requirements on a mission focused program within a challenging, dynamic setting. Successful candidates in this position will collaborate with a diverse group of individuals across the homeland security community and directly contribute to management and analysis of operational data using data science tools. This role supports a US Government contract. An US Citizenship is required.
The Role
The Data Engineer will work alongside a team to generate management tools that effectively control programs and generate reports while supporting the programmatic development of operational applications of the tools or analysis that are focused on operations research and document findings. They will conduct research into management, organizational, and business practices to implement improvements to existing processes. They will utilize commercial or open-source software to manage tasks and present findings to a broad audience, including agency executives. They will maintain a database to produce monthly financial reports, tracks processes, distribute deliverables, and monitor comment status. The Data Engineer will be required to use Microsoft Office to perform queries, analyses, and produce reports. Other responsibilities include:
Use R and Python to perform analyses and develop new tools using popular data visualization libraries such as ggplot2, plotly, matplotlib, or bokeh.
Use Databricks to Extract, Transform, and Load (ETL) data from different sources and transform it into a usable, reliable resource.
Manage data collection and transformation from federal law enforcement and other databases to be used in a wide spectrum of analyses including statistical modeling, performance metrics analysis, crosschecking data with other federal agencies, and risk analysis.
Use ArcGIS to perform analyses and develop new web map applications that summarize results from deep learning/AI algorithms.
Maintain and enhance business intelligence (BI) dashboards using best practices in UI/UX design
Develop and test new methodologies and algorithms which would increase the efficiency and accuracy of existing models, including pilot deployments of automated scripts
Requirements
This role supports a US Government contract. A US citizenship is required.
Strong Experience with SQL programming
Experience with R statistical programming and Python programming
Experience with BI software such as Qlik, Tableau or PowerBI
Demonstrated experience in performing data analysis and modeling on an independent basis
Demonstrated experience in developing and applying quantitative methods to find patterns and relationships in large data sets using statistical and graphical packages
Ability to translate complex, technical, or analytic findings into an easily understood narrative—tell a story with graphical, verbal or written form
3 years experience in supporting quantitative and analytical efforts
Bachelors degree in relevant fields such as statistics, mathematics, computer science, physical science, economics or engineering
Job Type: Full-time
Pay: $80,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Experience level:
1 year
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This question MUST be answered for consideration: This position supports a US Government contract which requires a US Citizenship. Are you a US Citizen?
Work Location: Hybrid remote in Washington, DC 20001",5.0,"Simatree
5.0","Washington, DC",1 to 50 Employees,2018,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
346,Data Engineer,-1,"Remote
Contract
Opened 4 months ago
Job Description
Data Engineer (with Healthcare experience) Required Skills: SQL Databricks data engineering Snowflake data engineering QA experience for ETL experienced with data acquisition and ingestion using API and/or batch channels Experience with Healthcare",3.3,"Crackajack Solutions
3.3",Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
347,Data Engineer,-1,"Hummingbird is a remote-first, fully distributed team united by the shared mission of helping fight financial crime. Since our launch in 2017, we’ve helped major financial institutions and tech-savvy trailblazers alike (e.g. Stripe, Affirm, etc.) orchestrate their compliance programs through our thoughtfully designed, intuitive SaaS product. We believe finding and stopping financial crime is a problem rooted in code, language and design, so we built the product that the heroes doing this work deserve.
We are customer-obsessed, and we love building and shipping great products. We set a high bar, challenge our assumptions, seek diverse opinions, and support each other to do our best work.
We do our best to write inclusive, descriptive and accurate job descriptions, but we’re not always perfect. If you’re interested in the role, we’d love to hear from you even if you don’t feel like you meet everything we’re looking for. We’re always iterating and improving, and it’s possible that your experience is even more impactful than we could have imagined.

About the Role
We are looking for a driven data engineer to join our team and champion the use of data at Hummingbird. Data plays a crucial role in our mission to fight financial crime and you will help us find new and innovative ways to leverage it to provide powerful tools for our users, and allow us to better understand the usage of our product. Hummingbird is uniquely positioned at the intersection of financial technology, security, policy, and law enforcement and as such we have built up a one of a kind data set that we can now use to give our customers the edge in their efforts to stay ahead of criminals. As an employee at a small startup, you will have the opportunity to wear many hats, working from product discovery through implementation.
What you’re looking to do:
Level up our use of data to make better decisions, build more powerful features, and fight financial crime
Leverage Infrastructure as Code (IaC) to manage and deploy infrastructure that supports a variety of different projects, such as data replication and orchestration for machine learning workflows
Build new data pipelines for ingesting data into our data warehouse via both batch and streaming architectures
Work closely with data science to enable us to build products that benefit our customers while keeping compliance and security at the forefront
Achieve goals through a combination of independent building, educating your peers, and influencing others to contribute towards your vision
What we’re looking for:
A data engineer with a history of taking projects from the earliest stages through successful rollout to production
Someone who is excited by the prospect of pioneering data as a practice at a fast growing startup and who is unafraid to dig in to discover what is possible
A flexible self starter that will cut across organizational lines to understand the business and identify the most valuable work
An engineer who brings a pragmatic approach to problem solving, favoring simplicity and shortening delivery cycles
Experience building data pipelines for sensitive data, including best practices for de-identification and data security
Experience deploying infrastructure via terraform or a similar infrastructure as code tool
Expertise in SQL and one or more programming languages, especially python
What’s in it for you:
The chance to help build from the ground up. The hires we’re making now are foundational to our growth as a company, so you will have an opportunity to help shape the future of Hummingbird.
Competitive compensation including cash and equity.
Remote-first, fully distributed company with flexible working hours.
Awesome health, vision & dental benefits, and 401k.
Safe, respectful & comfortable work environment with colleagues and leadership who prioritize diversity, equity, inclusion, and belonging.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please don't hesitate to contact us to request accommodation.",-1.0,Hummingbird,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
348,Data Engineer (Fully Remote),$56K - $91K (Glassdoor est.),"Position Description: Who We Are
NIP Group www.nipgroup.com is a rapidly growing insurance service provider of specialty programs for commercial insurance brokers and carriers providing underwriting, distribution, product management, administration, and risk management services primarily by acting as a managing underwriter (MGA) and a Reciprocal Services Manager (RSM).
Our culture is one that empowers and encourages employees to be innovative, collaborative, and forward-thinking. If you are interested in being a part of a growing, entrepreneurial spirited organization, wed love to hear from you!

About the Position
Reporting into the Senior Actuary, you will own the continuous improvement culture for designing, building, and maintaining the infrastructure and systems required for collecting, storing, processing, and analyzing large volumes of data. As a Data Engineer, you will play a crucial role in ensuring that data is readily available, accessible, and usable by other data professionals, analysts, and stakeholders within an organization. You will be responsible for managing a data Lake / warehouse and implementing efficient data integration processes with internal and external systems.

What Youll Do
Design and maintain a scalable and secure Azure data warehouse.
Build data pipelines: Create and manage data integration processes, including data extraction, transformation, and loading (ETL) from various sources into the data warehouse.
Develop and implement data management strategies to ensure data quality, consistency, and accuracy.
Collaborate with cross-functional teams, including to identify data requirements and develop data models.
Develop, adhere, and enforce data governance policies and procedures to ensure compliance.
Monitor data quality and develop response mechanisms to address problems.
Collaborate with IT teams to ensure the availability, reliability, and performance of data systems and infrastructure.
Develop documentation related to data management processes, procedures, and data dictionaries.
Stay up to date with industry trends and advancements in data management technologies and techniques.
What Were Looking For
Education/Experience
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Proven experience working as Data Engineer or similar role, preferably in the insurance or financial industry.
Strong knowledge of data management principles, data quality assurance, and data governance practices.
Proficiency in data warehousing concepts, and ETL processes.
Experience with data integration tools and technologies (e.g., SQL, Python, ETL frameworks).
Solid understanding of regulatory requirements related to data privacy and security (e.g., GDPR, HIPAA).
Excellent problem-solving and analytical skills, with the ability to analyze complex data sets.
Strong communication and interpersonal skills to collaborate with cross-functional teams and communicate complex ideas effectively.
Technical Competencies
Proficiency in Microsoft services in Azure, Data Factory, Data Lake/warehouse storage.
Ability to design and implement data integration solutions using Azure Data Factory.
Expertise in designing and optimizing data warehousing solutions Azure SQL Data Warehouse.
Familiarity with Azure Data Lake Storage and data processing using Azure Data Lake Analytics.
Data Governance and Security: Understanding of data governance principles and compliance requirements within Azure, including access control and data privacy.
Monitoring and Troubleshooting: Skills in monitoring, optimizing, and troubleshooting data pipelines within Azure Data Factory.
Proficiency in scripting languages like PowerShell or Python for automation tasks in Azure.
Strong communication and collaboration skills to work effectively with cross-functional teams.
Proactive in staying updated with the latest Azure data services and advancements.
What You'll Receive
At NIP Group, we recognize there are many factors that contribute to your overall satisfaction both at work, and in your personal life. For that reason, we provide a perfect mix of compensation, benefits, company culture, and resources to ensure your everyday happiness. Below are some benefits that youll receive.
Competitive compensation to reward you for your hard work every day.
Progressive Paid-Time Off program for you to enjoy time out of the office, including time off for volunteering and life events.
Group Medical, Dental, Vision and Life insurance to encourage a healthy lifestyle.
Pretax Health and Dependent Care Spending Accounts to ease taxes on spending.
Discounts in retail and entertainment.",3.0,"NIP Group, Inc.
3.0","Woodbridge, NJ",51 to 200 Employees,-1,Company - Private,Insurance Agencies & Brokerages,Insurance,Unknown / Non-Applicable
349,Data Engineer,-1,"Blackwell Security Inc. is a start-up backed by venture capital, focused on bridging the technology gap in healthcare. Our purpose-built ecosystem provides comprehensive cybersecurity managed services for life sciences and healthcare. We are building a customizable product that ensures health systems have access to a suite of security solutions, with built-in visualization and optimization to ensure the safety of patient information.
As we continue to build out our core team, we are adding a Data Engineer to our Engineering Team. You will be working cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data models. You will design, implement, and scale data pipelines that transform billions of records into action and insight.
This is a unique opportunity to jump into an early-stage start-up at a pivotal time and make a meaningful impact. If you thrive in a small, growing environment and love the energy of start-ups, this is the role for you!
While our headquarters are in Detroit, Michigan, this is a remote role but ideally a candidate would live in Detroit or Minneapolis, location of our core Engineering Team. This role is not eligible for visa sponsorship.
What you will do in the Data Engineer role:
Collaborate with engineering to build and maintain an enterprise data ecosystem including ingestion, storage, organization and interface.
Analyze the business and technical requirements for data systems and applications; Coordinate the integration of IT policies, procedures and development practices.
Translate business requirements into data models that are easy to understand and used by different disciplines across the company.
Design, implement, build/enhance pipelines that deliver data with measurable quality under the SLA.
Partner with business domain experts, data analysts and engineering teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service.
Champion the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements.
Own and document foundational company metrics and benchmarks with clear definition and data lineage.
Identify, document and promote best practices.
Design and architect data systems, focusing not only on performance and scalability, but also on crafting a beautiful user experience.
Define/Implement data visualizations & UX for external/internal customers.
Taking a thoughtful approach to decision making; balance speed and quality, with a focus on tangible results.
Explore Blackwell’s data to discover trends and opportunities, identify what questions we should be asking of our data.
Analyze & evaluate transactional system data for transformation and use in reporting, analytics, and AI/ML.
Evaluate and establish early strategies and usage of AI (machine learning, generative AI, etc.).
Qualities and skills for success in the Data Engineer role:
Bachelor's degree in Computer Science, Engineering, or related technical or business field.
Attention to detail, and Agile development experience.
Experience with Python and AWS services.
Experience with various data storage systems, RDBMS, Document/NoSQL DBs, etc.
Experience implementing data pipelines via methods such as ETL, ELT, EL/TL, DaaS, Data Lake or ODS.
Experience experimenting with and applying AI (machine learning, generative AI, etc.) in an enterprise environment.
Experience working with multi-customer multi-tenant environments preferred. Experience with cybersecurity data is not required but is a plus.
Adaptable and focused on solutions.
Equal Employment Opportunity
We’re proud to be an equal opportunity employer and welcome our employee’s differences, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or Veteran status. Difference makes us better. Join us.",2.8,"Blackwell Security, Inc.
2.8",Remote,51 to 200 Employees,1999,Company - Private,Security & Protective,Management & Consulting,$1 to $5 million (USD)
350,Data Engineer,-1,"ArchWell Health is a new, innovative healthcare provider devoted to improving the lives of our senior members. We deliver best-in-class care at comfortable, accessible neighborhood clinics where seniors can feel at home and become part of a vibrant, wellness-focused community. Our members experience greater continuity of care, as well as the comfort of knowing they will be treated with respect by people who genuinely care about them, their families, and their communities.

Duties/Responsibilities:
Build data integrations from internal and external sources to centralize data into a Data Warehouse environment.
Monitor data integration operations, data quality, troubleshoot, and resolve problems.
Profile data sources and map to target table formats.
Develop and monitor data quality processes and address problems.
Develop, unit test and system test integration components.
Create support documentation describing the functionality of the integrations.
Participating in technical design & requirements gathering meetings.
Participate in planning and implementing data integration and data migration activities.
Perform QA tests to ensure data integrity and quality.
Research data issues between source systems and the data warehouse.
Required Skills/Experience:
Bachelor’s degree required; Master's degree (in data science, computer science or MIS, mathematics, engineering, or related field) preferred.
5+ years of prior experience in Data Management / ETL / ELT / Data Warehousing
Experience in writing Data Quality routines for cleansing of data and capturing confidence score
Experience with master data management
Strong knowledge of Structured Query Language (SQL) and Transact-SQL (T-SQL)
Experience using scripting languages such as JavaScript or Python
Experience Healthcare data models, datasets, and source systems (e.g. EHR, claims, labs, etc.)
Experience with healthcare reference data (ICD, CPT etc.)
Experience with agile delivery methodologies
Data Modeling experience preferred.
Strong organizational, administrative, and analytical skills required.
Experience managing and working in cloud environments such as Amazon Web Services or Azure
Knowledge of HIPAA; ability to implement systems and processes in accordance with regulations
Excellent interpersonal communication skills, both written and verbal
ArchWell Health is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to their race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected classification.",4.0,"ArchWell Health
4.0",Remote,Unknown,-1,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
351,Data Engineer,Employer Provided Salary:$60K - $112K,"Job Description:
Role Summary/Purpose:
The candidate Big Data Engineer will join an Agile scrum team and perform functional & system development for Synchrony’s Enterprise Data Lake.

As a Big Data Engineer the ability to integrate data across internal and external sources, provide analytical insights, and integrate with our critical systems are key skills. The engineer will participate in data analysis efforts to ensure the delivery of high-quality data ingestion, standardization and curation and maintain compliance with the applicable Data Sourcing, Data Quality, and Data Governance standards. The engineer will drive quality through the entire software development lifecycle with focus on functional requirements, efficiency, and methodology. The engineer will work cross-functionally with operations, other data engineers and product owner to assure capabilities are delivered that meet business needs.

We’re proud to offer you choice and flexibility. You have the option to be remote, and work from home, or come into one of our offices. You may be occasionally requested to commute to our nearest office for in person engagement activities such as team meetings, training and culture events.
Essential Responsibilities:
Develop big data applications for Synchrony in Hadoop ecosystem
Participate in the agile development process including backlog grooming, coding, code reviews, testing and deployment
Work with team members to achieve business results in a fast paced and quickly changing environment
Work independently to develop analytic applications leveraging technologies such as: Hadoop, NoSQL, In-memory Data Grids, Kafka, Spark, Ab Initio
Provide data analysis for Synchrony’s data ingestion, standardization and curation efforts ensuring all data is understood from a business context
Identify enablers and level of effort required to properly ingest and transform data for the data lake.
Profile data to assist with defining the data elements, propose business term mappings, and define data quality rules
Work with the Data Office to ensure that data dictionaries for all ingested and created data sets are properly documented in data dictionary repository
Ensure the lineage of all data assets are properly documented in the appropriate enterprise metadata repositories
Assist with the creation and implementation of data quality rules
Ensure the proper identification of sensitive data elements and critical data elements
Create source-to-target data mapping documents
Test current processes and identify deficiencies
Investigate program quality to make improvements to achieve better data accuracy
Understand functional and non-functional requirement and prepare test data accordingly
Plan, create and manage the test case and test script
Identify process bottlenecks and suggest actions for improvement
Execute test script and collect test results
Present test cases, test results, reports and metrics as required by the Office of Agile
Perform other duties as needed to ensure the success of the team and application and ensure the team’s compliance with the applicable Data Sourcing, Data Quality, and Data Governance standards


Qualifications/Requirements:
Bachelor's degree in a quantitative field (such as Engineering, Computer Science, Statistics, Econometrics); in lieu of degree, High School Diploma/GED and minimum 2 years of Information Technology experience
Hands-on experience writing shell scripts, complex SQL queries, Hive scripts, Hadoop commands and Git
Ability to write abstracted, reusable code components
Programming experience in at least one of the following languages: Scala, Java or Python
Analytical mindset
Willingness and aptitude to learn new technologies quickly
Superior oral and written communication skills;
Ability to collaborate across teams of internal and external technical staff, business analysts, software support and operations staff.

Desired Characteristics:
Performance tuning experience
Exposure to the following Ab Initio tools: GDE – Graphical Development Environment; Co>Operating System ; Control Center; Metadata Hub; Enterprise Meta>Environment; Enterprise Meta>Environment Portal; Acquire>It; Express>It; Conduct>It; Data Quality Environment; Query>It.
Familiar with Ab Initio, Hortonworks/Cloudera, Zookeeper, Oozie and Kafka
Familiar with Public Cloud (i.e. AWS, GCP, Azure) data engineering services
Familiar with data management tools (i.e. Collibra)
Background in ETL, data warehousing or data lake
Strong business acumen including a broad understanding of Synchrony business processes and practices
Demonstrated ability to work effectively in an agile team environment
Financial Industry or Credit processing experience
Experience with working on a geographically distributed team managing onshore/offshore resources with shifting priorities
Previous experience working in client facing environment
Proficient in the maintenance of data dictionaries and other information in Collibra
Excellent analytical, organizational and influencing skills with a proven track record of successfully executing on assignments and initiatives

Grade/Level: 09

The salary range for this position is 60,000.00 - 112,000.00 USD Annual

Salaries are adjusted according to market in CA, NY Metro and Seattle.
Some positions are bonus eligible.
Eligibility Requirements:
You must be 18 years or older
You must have a high school diploma or equivalent
You must be willing to take a drug test, submit to a background investigation and submit fingerprints as part of the onboarding process
You must be able to satisfy the requirements of Section 19 of the Federal Deposit Insurance Act.
New hires (Level 4-7) must have 9 months of continuous service with the company before they are eligible to post on other roles. Once this new hire time in position requirement is met, the associate will have a minimum 6 months’ time in position before they can post for future non-exempt roles. Employees, level 8 or greater, must have at least 18 months’ time in position before they can post. All internal employees must consistently meet performance expectations and have approval from your manager to post (or the approval of your manager and HR if you don’t meet the time in position or performance expectations).
Legal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job opening. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
Our Commitment:
When you join us, you’ll be part of a diverse, inclusive culture where your skills, experience, and voice are not only heard—but valued. We celebrate the differences in all of us and believe that our individual, unique perspectives is what makes Synchrony truly a great place to work. Together, we’re building a future where we can all belong, connect and turn ideals into action. Through the power of our 8 Diversity Networks+, with more than 60% of our workforce engaged, you’ll find community to connect with an opportunity to go beyond your passions.
This starts when you choose to apply for a role at Synchrony. We ensure all qualified applicants will receive consideration for employment without regard to age, race, color, religion, gender, sexual orientation, gender identity, national origin, disability, or veteran status.
Reasonable Accommodation Notice:
Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.
If you need special accommodations, please call our Career Support Line so that we can discuss your specific situation. We can be reached at 1-866-301-5627. Representatives are available from 8am – 5pm Monday to Friday, Central Standard Time
Job Family Group:
Information Technology",4.2,"Synchrony
4.2","Chicago, IL",10000+ Employees,2014,Company - Public,Banking & Lending,Financial Services,$10+ billion (USD)
352,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1.0,Radiant System,Remote,-1,-1,-1,-1,-1,-1
353,"Data Science Engineer(SQL, Snowflake, AWS Connect)",-1,"Required: SQL – Strong skill set Snowflake Data review/Dashboard review AWS Connect – platform, wfm, call recording, crm Preferred: Contact Center understanding Telephony/CRM knowledge of data Legacy Telephony knowledge of Avaya, Genesys Power BI would be a plus for Dashboard mapping Responsibilities: Reviewing logs/reports related to the AWS Connect Platform. Report out areas of improvement related to the call center Focus on containment improvements Volume drivers for the call center Measure improvements for developments",4.3,"Intone Networks
4.3",Remote,201 to 500 Employees,-1,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
354,Senior Data Engineer,Employer Provided Salary:$87K - $180K,"Oakland, California; Atlanta, Georgia; Irving, Texas
Regular Employee Full-Time
R-2023-08-81

Who We Are:
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners - in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
Pandora
Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. Pandora provides consumers with a uniquely-personalized music and podcast listening experience with its proprietary Music Genome Project® and Podcast Genome Project® technology. Pandora is available through its mobile app, the web, and integrations with more than 2,000 connected products.
How you’ll make an impact:
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What you’ll do:
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What you’ll need:
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to pay attention to details and be organized.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $87,200 to $180,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.

Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-08-81",3.6,"Adswizz
3.6","Oakland, CA",51 to 200 Employees,2007,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
355,Engineer II - Imaging Data and Solutions,$73K - $101K (Glassdoor est.),"About the Department
Novo Nordisk Data Management and Informatics within the Digital Science and Innovation Organization provides informatics solutions, data products and analysis support to the research organization in Novo Nordisk. Data Management and Informatics is establishing a data products organization across our research sites. Staff will be co-located to one of our global sites in Seattle, WA, Fremont, CA, Lexington, MA, Oxford, UK and Denmark.

The Position
The Imaging Data and Solutions Engineer has a subject matter knowledge on imaging systems and image processing. They will set directions and deliver well curated imaging data and software products via appropriately architected data pipelines to solve complex scientific problems. This role focuses on providing findable, accessible, interoperable, and reusable imaging data within our diabetes, obesity, chronic and rare disease therapeutic areas. They will provide support to data consumers including computational scientists, citizen data scientists, bots and laboratory scientists and contribute through application of expertise. They will use their knowledge of digital to speed the ability of scientists and data scientists to access and work with imaging data, and to use scientific applications
They will use their hands on expertise to provide solutions using preferred tooling and technologies, and will work with other data engineers, product owners and specialist across Data Management & Informatics and Global IT to address larger needs. This position will also enable the visualization needs for imaging data and be well versed in best practices required to streamline data handling, and represent the local needs at the sites in the context of the global data management and informatics organization.
Do you believe that the digitalization journey in Research and Early Development (R&ED) is crucial for the success of pharmaceutical companies in the future? Then apply to become part of the next wave of scientific discovery by joining Digital Science & Innovation (DSI).

Relationships
The Imaging Data and Solutions Engineer reports to the Director, Imaging Data and Solutions Engineering. Internal partners include therapeutic area scientists, computational biologists, data and software engineers, software developers, platform and compute engineers in Research and Development and Information Technology.
External relationships include relationships with commercial and academic collaboration partners.

Essential Functions
Strong grasp of the image processing specifically DICOM headers and image transforms
Gather/organize large, complex data sets and develop ETL/ELT systems to manage and utilize this data. This will involve profiling, cleansing, transforming and developing data structures, schema and dictionaries to create more efficient workflows
Build automated monitoring mechanisms to ensure compliance and integrity of the pipelines and database
Being a core contributor to agile product delivery teams with a focus on publishing data products into the research and enterprise data catalog, as well as platform engineering and delivering clinical data for research capability
Ensure scientists and data scientists are aware of available imaging data and can access, integrate and query it in a performant manner
Enable streamlined sharing of rich data with collaborators through Cloud, accelerated compute and AI/ML approaches
Assist with provisioning of compute and data pipelines to deliver performant imaging data products via the research and enterprise data ecosystem
Optimize workflows and exchange of research imaging data within and to the global organization
Ensure researchers are familiar with and can use preferred applications
Develop or acquire new systems and software in collaboration with internal and external partners
Conduct end-user training and technical support of informatics systems
Advocate for the use of data and data science including computational and machine learning approaches in research projects
Participate in sustaining a suite of tools and application such as Python, R, Jupyter Hub, Domino, DataLab, Omero.
This role is an individual contributor role that must leverage agile software development practices. They will have a proven track record of creating business results with impact at the VP level.

Physical Requirements
0-10% overnight travel required.

Qualifications
Bachelor’s degree preferred. Degree within insert subject mater expertise preferred
Bachelor’s degree with 3+ years’ and Master’s degree with 1+ years’ relevant experience can be considered
Relevant experience includes:
Experience with imaging data sets including the formulation of scientific hypotheses and analysis plans that require the integration of multi-modal data into integrated data products
Basic understanding of the ontologies, vocabularies and standards for data representation across the research and development value chain.
Proficiency in one or more programming languages such as Python, Matlab, C/C++ and Java.
Experience adapting, developing and adding functionality to programming tools such as ImageIO, python imaging library, scikit-image, Flask, FastAPI, Jinja preferred.
Experience on machine learning tools (Pytorch and Tensorflow) is desirable.
Experience open-source contributions and publications demonstrating value to life sciences and drug discovery projects preferred.
Proficiency in Amazon, Azure or Google Cloud is desirable.
2+ years imaging data and image processing expertise and experience designing, developing, implementing and maintaining data pipelines and using cloud research data platforms.
Experience with entire data and solution engineering lifecycle from design, deployment, implementing and maintenance of proprietary and commercial software
Preferred:
2+ years experience in life sciences, medical device or pharmaceutical industry
Broad expertise spanning data and digital, including the ability to perform hands on technical work
Strong analytical skills, the ability to plan and realize robust and scalable solutions with a structured approach and detail oriented
Ability to work independently or need occasional guidance from manager/senior colleagues
Automated testing skills preferred
Excellent communication skills at bench scientist/end-user level and with middle management
Excellent written and oral communication skills is required
We commit to an inclusive recruitment process and equality of opportunity for all our job applicants.

At Novo Nordisk we recognize that it is no longer good enough to aspire to be the best company in the world. We need to aspire to be the best company for the world and we know that this is only possible with talented employees with diverse perspectives, backgrounds and cultures. We are therefore committed to creating an inclusive culture that celebrates the diversity of our employees, the patients we serve and communities we operate in. Together, we’re life changing.

Novo Nordisk is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, ethnicity, color, religion, sex, gender identity, sexual orientation, national origin, disability, protected veteran status or any other characteristic protected by local, state or federal laws, rules or regulations.

If you are interested in applying to Novo Nordisk and need special assistance or an accommodation to apply, please call us at 1-855-411-5290. This contact is for accommodation requests only and cannot be used to inquire about the status of applications.",4.3,"Novo Nordisk
4.3","Lexington, MA",10000+ Employees,1923,Company - Public,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,$10+ billion (USD)
356,Data Warehouse Engineer,$88K - $119K (Glassdoor est.),"At Apiture, our mission is to empower financial institutions to know and serve their clients with the care of a traditional community institution at the scale, speed, and efficiency required in today's digital world. With more than 300 clients throughout the U.S., we deliver comprehensive online and mobile solutions that support banks and credit unions, ranging from small community financial institutions to new, innovative direct banks.

Summary:
Reporting to the Data Engineering Manager, the Senior Data Warehouse Engineer will work closely with the Data Architect to implement the Data Model. You will be working with a team of highly talented engineers to develop practical, scalable data reporting and analytics solutions.

Location (Wilmington, NC, Austin, TX, Remote):
We have offices in Wilmington, NC and Austin, TX and while some positions are office based, we will also consider remote candidates depending on their time zone.

Responsibilities:
Core tasks will involve writing SQL and some Python to transform raw data from the staging layer into the human readable Data Model.
Write new fact and dimension tables and add to existing ones.
Review data in source systems, in databases, or APIs to understand how the data comes into the warehouse and which transformations are needed.
Work with the Data Architect to define and enforce data warehouse standards that align with the larger data management guidelines in place.
Work with data analysts and data scientists to define and refine data analysis goals and implement needed changes in the data warehouse.
Identify and pursue opportunities to automate processes and execute validation strategies to maintain high standards of efficiency and data quality.
Build and maintain documentation around data sets, data classes, data flows, transformations, etc.
Work with the information security and compliance teams at Apiture to build, monitor, and enforce data cataloging, asset tracking, and privacy rules/metrics.
Provide input and feedback to support continuous improvement in data governance processes.

Requirements:
Bachelor's in computer science or equivalent work experience.
4+ years of hands-on experience with coding data transformations in SQL that involved large data sets.
Well-versed with Advanced SQL scripting.
Experience with programming languages: Python, Java, Scala, etc.
Experience building data pipelines that integrate data from structured and unstructured data sources.
Experience troubleshooting data integrations to visualization platforms like Domo, Tableau or PowerBI.
Hands-on experience with data warehousing platforms like Snowflake, RedShift, or Synapse Analytics.
Excellent understanding of ETL/ELT fundamentals and building efficient data pipelines.
Excellent verbal and written communication skills.

Nice To Have:
Experience with cloud technologies (Strong preference for AWS technologies like Lambda, DMS, etc.)
Experience with Data Management platforms like OneTrust
Experience working with REST APIs, Streaming APIs, or other Data Ingress techniques.
Experience with data engineering and monitoring for ML applications.
Exposure to test-driven development and automated testing frameworks.",4.0,"Apiture
4.0","Atlanta, GA",201 to 500 Employees,2017,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
357,Senior Data Integration Engineer,-1,"If you are passionate about energy efficiency & carbon footprint reduction, growing your technical skills, working with clients, or love working in a cross functional team setup, this is the role for you!
The implementation engineering team helps our utility partners integrate with the Opower platform and plays an essential technical role in implementing Opower’s SaaS-based energy efficiency and customer engagement programs. You will work directly with our utility partners’ technical teams to integrate their data with our platform and to configure our applications to meet their program requirements. Additionally, you will work with a broad group of other teams at Opower, including project managers and product engineering teams.
Implementation engineers have a diverse set of technical skills, with a focus on delivering high-quality products customized for our utility partners' needs. A typical week might involve ingesting and analyzing utility data; running Unix command-line text manipulation tools; writing python scripts to automate work; adding documentation; working with a utility partner's technical team; and collaborating with R&D teams on future roadmap projects.
You will be an ideal candidate for this job if you have solid understanding of relational databases, are comfortable with advanced UNIX commands, have done some basic programming, and do not hesitate to ask questions. You will work with a variety of teams in different contexts, making the role a good opportunity if you are looking to develop new skills while helping us accomplish our mission of energy efficiency & carbon footprint reduction for our utility partners.
There is flexibility to work remotely full-time, otherwise we have teams located in Oracle’s San Francisco, CA and Arlington, VA offices. You will join a team of extremely helpful engineers with different cultural and professional backgrounds. Besides standard company benefits like 401k matches and unlimited PTO, you will have an excellent work-life balance, the ability to direct your career, and access to an abundance of educational material and professional trainings to help you grow

Responsibilities:
Work with utility project teams on data integrations and energy efficiency product implementations
Explain technical specifications of Opower data integration and products - including highlighting risks with customer experience when requirements are not met
Analyze, transform, and load utility provided data to meet Opower's data requirements for a successful delivery of downstream end-user communications and web experiences
Develop new or maintain tooling (in ruby or python) to make our data integration and product implementations more cost & time effective
Configure and customize Opower's energy efficiency SaaS platform to meet the specific needs of each client
Develop a deep understanding of our products with the ability to explain them to others with non-technical backgrounds
Improve our ability to customize and deliver energy efficiency products to our customers by optimizing delivery processes and writing useful documentation
About You:
Experience writing effective SQL or Hive queries to analyze large relational datasets
Have experience writing data transformations scripts using ETL tools
Experience performing advanced file searches and text manipulation using the Unix/Linux command-line
Comfortable working directly with client teams
Experience writing software tools using object-oriented programming
You understand code versioning concepts and have experience with tools like git
Can connect dots among difference pieces of information gained from multiple sources
Experience troubleshooting software applications that involve APIs, databases, and frontend
Can effectively prioritize multiple tasks at one time
Enjoy being part of a team, helping and learning from others.
Have 8+ years of professional experience. We are open to hiring at different levels too if there’s a better fit.",3.9,"Oracle
3.9",United States,10000+ Employees,1977,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
358,Junior Data Engineer,$83K - $119K (Glassdoor est.),"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a junior data engineer, you will be part of a team that is responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.
The position:
Develop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Support various components of the data pipelines, including ingestion, validation, cleansing and curation
Manage and ensure the success of ongoing data pipeline routines
Collaborate with our implementation team to assist with the identification and reconciliation of data anomalies
Create and maintain documentation on data pipelines
Engage with our software engineering team to ensure precise data points per application specifications
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering, or applicable experience
Expertise with SQL, database design and data manipulation methodologies
Expertise with ETL/ELT and the development of automated validation and data pipelines
Strong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW, master data management and other database design principles
Comfortable working with high volume data in a variety of formats
Excellent verbal and written communication
Self-motivated
Passionate at learning
Familiarity with healthcare data is a plus
Experience with CI/CD and version control tools is a plus
Experience working within hybrid cloud environment such as AWS is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation
Social outings",5.0,"Decision Point Healthcare
5.0","Boston, MA",1 to 50 Employees,2013,Company - Public,-1,-1,Less than $1 million (USD)
359,Sr Data Engineer,$103K - $140K (Glassdoor est.),"Job Description
This Senior Data Engineer role designs and develops data pipelines that power Motion’s data integration solutions. The role will work with data partners all over the world to build enterprise-class solutions that drive analytics, foster partner integration, and optimize process workflows. Motion offers an excellent benefits package that includes options for healthcare coverage, 401(k), tuition reimbursement, vacation, sick, and holiday pay. Must be eligible to work in the US without Visa sponsorship
Responsibilities
Develop and maintain the framework and pipelines for consuming and storing data at enterprise scale from various sources in various formats
Design and develop database objects/models to support data consumption and storage
Maintain knowledge of the tools and technology available to forward data movement and storage
Collaborate with internal and external stakeholders to understand business needs related to data movement
Champion ETL design standards and practices related to data engineering
Work in an Agile environment
Qualifications
4+ years of data development, ETL experience
Proficiency in SQL
Proficiency with database design, concepts, and practices
Proficiency with data integration/ETL tools, Informatica specifically
Attention to detail
A Bachelor's, or associates, Degree in a related field or equivalent work experience
Preferred
Experience with DB2
ETL experience correlated to merges and/or acquisitions
Experience with data visualization tools such as Qlik, Tableau, Power BI
Experience with data warehousing and related concepts
Experience with Java or Python
Understanding of service-oriented development
Not the right fit? Let us know you're interested in a future opportunity by joining our Talent Community on jobs.genpt.com or create an account to set up email alerts as new job postings become available that meet your interest!
GPC conducts its business without regard to sex, race, creed, color, religion, marital status, national origin, citizenship status, age, pregnancy, sexual orientation, gender identity or expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. GPC's policy is to recruit, hire, train, promote, assign, transfer and terminate employees based on their own ability, achievement, experience and conduct and other legitimate business reasons.",4.5,"Motion
4.5","Birmingham, AL",51 to 200 Employees,-1,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
360,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
361,Data Engineer,$76K - $101K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
The Data Governance/Data Quality Subject Matter Expert (SME) is a professional responsible for designing, implementing, and managing the client’s data governance strategy and ensuring high data quality standards across data-centric operations and decision-making.
The SME is in charge of developing and maintaining robust data pipelines, ensuring data observability, managing data within the Microsoft Azure environment, and implementing data governance principles in alignment with industry best practices and regulatory standards.
Primary Responsibilities and Accountabilities:
Data Governance & Quality: This includes developing and implementing data governance policies and procedures, monitoring data quality, managing data within the Microsoft Azure environment, and ensuring compliance with data privacy regulations.
Data Pipeline & Workflow Development: Designing and implementing efficient and secure data pipelines in Azure Data Factory and workflows, ensuring optimal performance and incorporating data quality principals.
Data Integration & Observability: Designing data integration strategies, integrating data from multiple sources, ensuring data quality and observability through tools like Azure Synapse.
Data Security: Defining and implementing data security policies and procedures, including access control and data encryption within the Azure environment.
Performance Tuning: Optimizing data access, indexing, and query performance within Azure Data Factory to ensure scalable and reliable data infrastructure.
Monitoring & Troubleshooting: Monitoring data systems, troubleshooting issues, identifying performance bottlenecks, and collaborating with team members to resolve problems.
Collaboration & Communication: Working closely with stakeholders to understand their data needs and develop solutions adhering to industry best practices and the needs of the business.
Automation & Tooling: Automating repetitive tasks and maintaining tools necessary for data processing and storage within the Azure environment.
Qualifications
A Bachelor's degree in a related field such as computer science, mathematics, engineering, or information systems.
3+ years of hands-on experience programming in SQL.
2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes within the Microsoft Azure environment.
Experience in Data Governance OR Data Quality Management is a must.
Skills
Competencies:
Expertise in Microsoft Azure data tools, with a focus on Azure Data Factory and Azure Synapse for data processing, storage, and analysis.
Strong SQL skills and knowledge of data warehousing concepts and ETL (Extract, Transform, Load) processes within the Azure environment.
Proficiency in programming languages such as Python, and experience with big data technologies such as Hadoop, Spark, and Kafka.
Ability to design, build, and maintain data pipelines, with a particular focus on Azure Data Factory.
Strong analytical and problem-solving skills, with the ability to identify and resolve data-related issues effectively.
Experience with data security and privacy regulations, and understanding of HIPAA compliance.
Strong experience with data governance, data quality, and data observability principles.
Excellent written and verbal communication skills, ability to explain complex technical concepts to non-technical stakeholders.
Excellent collaboration and teamwork skills, with the ability to work in a team involving client data engineers, data scientists, and data analysts.
Strong attention to detail and commitment to ensuring high data quality.
Continuous learning mindset and ability to stay up-to-date with new developments and technologies in data governance and data quality
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
362,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
363,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
364,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1.0,Percept Health,Remote,-1,-1,-1,-1,-1,-1
365,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
366,Cloud Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"We are seeking a highly experienced Principal Data Engineer with a deep understanding of PySpark using Databricks or AWS Glue or AWS EMR and cloud-based databases such as Snowflake. Proficiency in workflow management tools like Airflow is essential. Healthcare industry experience is a significant advantage. The ideal candidate will be responsible for designing, implementing, and maintaining data pipelines while ensuring the highest performance, security, and data quality.
Responsibilities:
Design, develop, and maintain scalable, reliable, and secure data pipelines to process large volumes of structured and unstructured healthcare data using PySpark and cloud-based databases.
Collaborate with data architects, data scientists, and analysts to understand data requirements and implement solutions that meet business and technical objectives.
Leverage AWS or Azure cloud services for data storage, processing, and analytics, optimizing cost and performance.
Utilize tools like Airflow for workflow management and Kubernetes for container orchestration to ensure seamless deployment, scaling, and management of data processing applications.
Develop and implement data ingestion, transformation, and validation processes to ensure data quality, consistency, and reliability across various healthcare datasets.
Monitor and troubleshoot data pipelines, proactively identifying and resolving issues to minimize downtime and ensure optimal performance.
Establish and enforce data engineering best practices, ensuring compliance with data privacy and security regulations specific to the healthcare industry.
Continuously evaluate and adopt new tools, technologies, and frameworks to improve the data infrastructure and drive innovation.
Mentor and guide junior data engineers, fostering a culture of collaboration, learning, and growth within the team.
Collaborate with cross-functional teams to align data engineering efforts with broader organizational goals and strategies.
Is familiar with SOC 2 compliance and its impact on company policies and processes.
Understands the importance of adhering to SOC 2 requirements and maintains an effort to do so.
Reviews and understands the Employee Handbook, and internal policies that define individual security responsibilities, and maintains segregation of duties in accordance with their role requirements.
Requirements:
Bachelor's or Master’s degree in Computer Science, Engineering, or a related field.
8+ years of experience in data engineering, with a strong background in Apache Spark and cloud-based databases such as Snowflake.
Strong Knowledge in Big Data Technologies, Databricks, AWS Services, and PySpark, thorough in one or more programming languages like Python.
Proven experience with AWS or Azure cloud services for data storage, processing, and analytics.
Expertise in workflow management tools like Airflow and container orchestration systems such as Kubernetes.
Strong knowledge of SQL and NoSQL databases, as well as data modeling and schema design principles.
Familiarity with healthcare data standards, terminologies, and regulations, such as HIPAA and GDPR, is highly desirable.
Excellent problem-solving, communication, and collaboration skills, with the ability to work effectively in cross-functional teams.
Demonstrated ability to manage multiple projects, prioritize tasks, and meet deadlines in a fast-paced environment.
A strong desire to learn, adapt, and contribute to a rapidly evolving data landscape.
Benefits:
401(k).
Dental Insurance.
Health insurance.
Vision insurance.
We are an equal-opportunity employer and value diversity, equality, inclusion, and respect for people.
The salary will be determined based on several factors including, but not limited to, location, relevant education, qualifications, experience, technical skills, and business needs.
Additional Responsibilities:
Participate in OrangePeople monthly team meetings, and participate in team-building efforts.
Contribute to OrangePeople technical discussions, peer reviews, etc.
Contribute content and collaborate via the OP-Wiki/Knowledge Base.
Provide status reports to OP Account Management as requested.
About us:
OrangePeople is an Enterprise Architecture and Project Management solutions company. Our most valuable asset is our people: dynamic, creative thinkers, who are passionate about doing quality work. As a member of the OrangePeople team, you will have access to industry-leading consulting practices, strategies & technologies, innovative training & education. An ideal Orange Person is a technology leader with a proven track record of technical achievements and a strong process/methodology orientation.
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Application Question(s):
Do you require sponsorship for this job?
Work Location: Remote",4.1,"OrangePeople
4.1",Remote,51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
367,Data Engineer,$100K - $144K (Glassdoor est.),"Position: Data Engineer
Duration: Contract
Location: Fremont, California
Skills:
Strong in SQL and Python
Data warehousing concepts , Data Modeling and RDBMS concepts
Good to have exposure to big data technologies – Apache Spark, Kafka, Airflow, NoSQL
Good to have exposure on Insurance & Finance business processes
Regards
Tejash Gupta
Recruiter
tejash@swifttechinc.com
Job Type: Contract
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Python: 7 years (Required)
SQL: 7 years (Required)
Data modeling: 6 years (Required)
RDBMS: 6 years (Required)
Work Location: In person",4.1,"Swift Technologies Inc
4.1","Fremont, CA",1 to 50 Employees,-1,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
368,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
369,AWS Data Engineer II,$77K - $105K (Glassdoor est.),"Overview:
Shift4 (NYSE: FOUR) is boldly redefining commerce by simplifying complex payments ecosystems across the world. As the leader in commerce-enabling technology, Shift4 powers billions of transactions annually for hundreds of thousands of businesses in virtually every industry. For more information, visit www.shift4.com.

Shift4 Payments is seeking a Business Intelligence Engineer with an AWS Focus to join our growing internal Information Technology team. As a BI Engineer, you will help design and maintain the flow of information and analytics to the entire organization utilizing AWS capabilities to store, move and report on data. The position requires at least 3 years of prior experience as a Business Intelligence AWS engineer. We are looking for individuals that are extremely self-sufficient, available to work flexible hours and hold themselves to the highest standards of professionalism.Maintain and expand BI AWS architecture and content.
Responsibilities:
Support of BI AWS architecture and content up to and including off hours support when necessary.
Work on necessary bugs, enhancements and new system developments utilizing our Software Development Life Cycle (SDLC) standards.
Complete root cause analysis and provide recommendations to leadership to address opportunities.
Contribute to and enforce the use of the organization's Business Intelligence solution.
Develop software solutions that will provide solid foundations for the growth of the company.
Support key metrics and shared goals that determine success for a particular area.
Qualifications:
3-5+ years of professional experience in business intelligence AWS development or a related field.
Advanced experience working with AWS: S3, Glue, Lambda, Step Functions, Redshift, Athena.
Advanced experience working with various data sources such as AWS S3 and Redshift, Postgres, SQL Server, Amazon Athena, Excel, Flat Files, etc.
Advanced experience working with large data sets from sources such as AWS S3 and Redshift, Postgres, SQL Server, etc.
Strong PySpark, Python, CLI proficiency
Strong SQL proficiency (T-SQL, MySQL, and PostgreSQL).
Experience working with an Enterprise Data Warehouse and Dimensional Databases is a plus.
Advanced experience of the SDLC and how Development processes fit into KANBAN and Scrum Frameworks.
Experience with Jira and Confluence is a plus.
Ability to prioritize multiple tasks and easily adjust to changing priorities.
Ability to identify problems, initiate solutions, and effectively collaborate and communicate with team members.
Have excellent verbal and written communication skills.

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.",3.5,"Shift4 Payments
3.5","Allentown, PA",1001 to 5000 Employees,1999,Company - Public,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
370,Senior Data Engineer,-1,"JOB DESCRIPTION:
Need to be expertise Coding on PySpark.
Supply Chain and Azure.
Experience in working with end-to-end technology implementation project, preferable those related to Data warehousing/Analytics/Reporting
Strong communication skills with the ability to collaborate effectively with business owners/users and technical teams.
Proven ability to gather product requirements from stakeholders and prioritize feature implementations based on business needs and product strategy.
Strong problem-solving skills and attention to detail.
Good to have experience with CPG/ Retail industry (preferable CPG companies such as Nestle, Unilever, P&G, Coca coal, PepsiCo etc)
Bachelor's/master's degree in business administration, computer science, engineering, or a related field.
Job Type: Contract
Experience:
PySpark: 5 years (Preferred)
SQL: 10 years (Preferred)
Data analytics: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Azure: 5 years (Preferred)
Data Bricks: 5 years (Preferred)
Data Lake: 5 years (Preferred)
Synapse: 5 years (Preferred)
Retail Domain: 8 years (Preferred)
CPG: 7 years (Preferred)
Work Location: Remote",4.0,"Quadrant Resource
4.0",Remote,1001 to 5000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
371,Data Engineer,Employer Provided Salary:$120K - $130K,"Role – Data Engineer- DataBricks -
Location – O Fallon, Missouri (Initially Remote)
Yrs. of experience – 8+
Full-Time
Job Description :
o 5+ years in a customer facing Technical architecture, consulting role with the expertise in The following technology.
o Developing modern Data warehouse solutions using Databricks and AWS/Azure stack.
o Drive technical discussion with client architect and team members.
o Knowledge in Databricks DELTA lake for the Analytical data lake use case
o Hand on experience in create MLOPS data pipeline creation.
o AIML Models develop, train and implement for the AIML use cases.
o Knowledge in Banking domain on card and payment areas.
o Any relation database – Data classification/ Data profiling for MLOPS use cases
o Good experience in offshore onsite coordination.
o Experience in translating a customer’s business needs.
Job Type: Full-time
Salary: $120,000.00 - $130,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
O'Fallon, MO 63366: Reliably commute or planning to relocate before starting work (Required)
Experience:
DataBricks: 1 year (Required)
MLOPS: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: In person",-1.0,Sbase Technologies,"O Fallon, MO",-1,-1,-1,-1,-1,-1
372,Data Engineer,-1,"Clear Demand Company Overview
Clear Demand is the leader in Intelligent Price Management and Optimization (IPMO) for retail. We were the first company to deliver an omni-channel lifecycle pricing solution that synchronizes prices, promotions, and markdowns online and in-store to produce a consistent brand and shopping experience. Clear Demand is the leading innovator in retail pricing solutions with patented science that analyzes historical sales to understand shoppers’ sensitivity to price and generate price and promotion strategies that account for pricing rules, cost changes, and competitor prices to achieve profit and revenue goals. Architected on big data and delivered through Software-as-a-Service (SaaS), Clear Demand’s Intelligent IPMO solution can be administered from a public or private cloud. Clear Demand’s innovations in retail science simplify adoption and use, while allowing retailers to see value in just weeks with more transparency and minimal disruption to existing business.
Clear Demand offers a competitive salary with stock options and a benefits package, including medical, dental, 401k.
Job Description – Data Engineer
This is a permanent position with tremendous potential for growth. The successful candidate will be exceptionally talented and hardworking---a self-starter able to multi-task and deliver results in a fast-paced environment. We are looking for a team player with experience developing high-performance applications for large enterprises. The software developer will be experienced in agile product-development methodologies. The ideal candidate will have a proven track record showing commitment to and sense of urgency for project timelines. This position will report to the Director of Engineering.
Primary Responsibilities
Designing, developing, testing, deploying, and maintaining applications to support business requirements.
Develop and improve data solutions for
- Designing relational schemas for persisting complex business objects
- ETL of customer data into our solution.
- Performing validation and mitigation strategies to handle invalid incoming data.
- Exchanging data between CDI’s user facing application and backend pricing optimization science solutions.
- Versioning database schemas, stored procedures
- Visualizing analytics and reporting
Working closely with both the Engineering and Science departments to build cohesive data-centric solutions.
Providing development expertise on migrating from a document store to a relational database.
Resolving technical issues through debugging and troubleshooting is also required.
Estimating level of effort for user stories and tasks.
Participate in Agile/SCRUM processes and ceremonies.
Required Skills
5+ years of experience in software development
Team player and effective communicator
Knowledge, experience, and proficiency with:
- Agile Development Methodology
- Relational Databases (SQL Server, Postgres)
- Document stores (i.e., MongoDB)
- Postgres
- Python
- Database query performance optimization
- Database versioning and migration
- ETL
- Git
- Object-Relational Mapping (ORM)
Performance optimization and debugging
Takes initiative to identify and address technology issues and opportunities, and proactively contributes to the business.
Good interpersonal, written, and oral communication skills.
Experience working in a team-oriented, collaborative environment.
Technical documentation skills.
Desired Skills
Google Cloud
CI/CD build and release pipeline
Jira
C#
HTML, JavaScript
Security/SSO/SSL
SaaS (Software-as-a-Service)
Experience with the Retail industry
Education
College degree in Computer Science or equivalent.
Clear Demand offers a competitive salary with stock options and a benefits package, including medical, dental, 401(k).
To apply, please your send resume to HumanResources@ClearDemand.com.
To learn more about Clear Demand, visit http://ClearDemand.com.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Employee discount
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Retirement plan
Vision insurance
Experience level:
5 years
Application Question(s):
Would you be willing to do a coding exercise as part of this application process?
Experience:
Agile: 3 years (Preferred)
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Scottsdale, AZ 85258",-1.0,Clear Demand,"Scottsdale, AZ",-1,-1,-1,-1,-1,-1
373,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
374,ETL Data Pipeline Engineer,Employer Provided Salary:$60.00 Per Hour,"ETL Data Pipeline Engineer
We do not work with 3rd party employers. Visa Sponsorship NOT available.
We are seeking a ETL DATA Pipeline Engineer for a consulting engagement with a major entertainment and media company. This person will be hands-on-date engineering development across multiple projects.
Required Skills:
10+ years of experience as Data Engineer with Large Data Pipelines
Strong SQL skills
Distributed Systems (Spark, Hadoop)
Cloud experience
STRONG ETL Experience
Python/Bash
Agile/Scrum
----------------------------------------
ABOUT MOORECROFT
A quality oriented national consulting firm. For more than two decades, providing expert IT professionals to our clients around the nation. Our core values are professionalism, honesty and integrity. We are only interested in working with professionals that subscribe to the same values and are driven to truly serve clients.
Job Type: Contract
Pay: From $60.00 per hour
Experience level:
10 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Santa Monica, CA 90404: Reliably commute or planning to relocate before starting work (Required)
Work Location: Hybrid remote in Santa Monica, CA 90404",-1.0,Moorecroft Systems,"Santa Monica, CA",Unknown,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
375,Data Engineer,Employer Provided Salary:$115K - $135K,"**This position is a majority Remote role with occasional in-office meetings on an as-needed basis. Because of this, interested applicants must live within a reasonable driving distance of Symmetry Lending's office in Anaheim, CA.**
Job Description
This role will play a pivotal role in assisting the IT team with the evolution of the company data architecture. In addition to supporting extensions to the data warehouse design, this role will design and develop ETL required to onboard new types of business data. Finally, this role will also provide support for Analytics assignments as needed.
Responsibilities
Display sense of ownership over assigned work, requiring minimal direction and driving to completion in a sometimes fuzzy and uncharted environment.
Designing and building new data pipelines that support business requirements.
Work with IT and other business partners to support the evolution of the enterprise data architecture.
Work with IT to build new ETLs to take data from various operational systems and extend existing data warehouse data model for analytics and reporting.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business constituents.
Support the development of the company BI infrastructure, including the construction of scalable analytic solutions, and on-premises reporting tools.
Utilize Power BI development to build scalable reporting models to serve BI reporting needs.
Demonstrate the ability to gather requirements, extract and manipulate data residing in multiple disparate databases, and articulate solutions to support the business.
Design, develop, and maintain performant data models in Power BI utilizing best practices.
Serve as a subject matter expert for all data warehouse and enterprise systems
Use new or existing technologies to produce analytics solutions (in the form of excel spreadsheets, dashboards, etc.).
Work with IT and Analytics team to architect and build data pipelines to optimize for performance, data quality, scalability, ease of future development, and cost.
Gather requirements, assess gaps and build roadmaps to help the analytics driven organization achieve its goals.
Develop data-related Proof of Concepts in order to demonstrate feasibility and value to Symmetry business constituents.
Qualifications / Requirements
Bachelor’s degree in Information Systems, Computer Science, Finance, or similar education from an accredited college
2-3 years Accounting and Finance background preferred
Strong skillset in Microsoft Excel (2-3 years, financial formulas & VBA is a plus) and familiarity with Microsoft Office Productivity Suite (Excel, Word, etc.)
5+years’ experience with advanced SQL concepts and writing SQL statements (SQL Server or similar).
Prefer 2 years’ experience with Microsoft BI Suite, PowerBI, Power Query, PowerPivot, Reporting Service (SSRS), SharePoint
Experience in mentoring other team members in development best practice, and methodologies.
You are passionate about data quality control and know how and where to anticipate potential errors.
Knowledge of the software development lifecycle, agile methodologies, and structured software development methodologies.
Experience performing analysis with large datasets in a cloud-based environment.
Ability to work effectively with stakeholders at all levels within the organization
Strong communication and time management skills and a self-motivated approach
Ability to work independently, detail-oriented, and execution focused
Highly collaborative and team oriented
Tenacious (doesn’t give up easily)
Genuine passion for clean and reliable data
At least 5 years of work experience
About Symmetry
Symmetry Lending, specializes in providing mortgage fulfillment services to include origination, servicing, and capital markets needs to various Lenders across the country with whom we partner. We have offices in Atlanta, GA, Eden Prairie, MN, Denver, CO, Orlando, FL, and Anaheim, CA, and we do business from coast to coast. We take great pride in building a diverse team of motivated professionals that contribute to an exciting work atmosphere. We provide a competitive benefits package including medical, dental, and vision plan options, paid time off, and more.
California Disclosure - Employee Notice at Collection
This disclosure is intended to comply with the California Consumer Privacy Act (CCPA), which gives California residents who are applicants, employees, or contractors of Symmetry Lending (“Symmetry”) the right to know what categories of personal information Symmetry collects about them and the purposes for which Symmetry uses that information. As used in this Privacy Notice, “Personal Information” means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a particular individual or household. Personal Information includes, but is not limited to, the categories of personal information identified below if such information identifies, relates to, describes, is reasonably capable of being associated with, or could be reasonably linked, directly or indirectly, with a particular individual or household.
The following is a list of the categories of Personal Information that we may collect about consumers:
Identifiers. This may include a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, Social Security number, driver's license number, passport number, or other similar identifiers.
Personal information described in the California Customer Records Statute (Cal. Civ. Code § 1798.80(e)). This may include a name, signature, Social Security number, physical characteristics or description, address, telephone number, passport number, driver's license or state identification card number, insurance policy number, education, employment, employment history, bank account number, or any other financial information, medical information, or health insurance information.
Characteristics of Protected Classification under California or Federal Law. This may include age, race, color, ancestry, national or ethnic origin, citizenship status, religion or belief, marital status, a childbirth or related medical condition, physical or mental disability, sex (including gender, gender identity, gender expression, pregnancy or childbirth, and related medical conditions), sexual orientation, veteran or military status.
Biometric information. This may include voice and video recordings.
Sensory data. This may include audio, electronic, visual, or similar information, including photos.
Professional or employment-related information. This may include current or past job history, compensation data, performance evaluations, or employee benefits.
Beneficiaries, dependents, and emergency contact information. This may include the name, gender, phone number, and relationship of beneficiaries, dependents, and emergency contacts.
We may use the categories of Personal Information for the following business or commercial purposes:
To perform background checks necessary to comply with licensing requirements, to perform reference checks, to verify eligibility to work in the United States, for contact purposes, to assess your qualification for employment, to conduct performance evaluations, for payrolls and budgeting purposes, for implementation of employee benefits, for internal organizational purposes to establish proper accommodations for sick time, PTO, leaves of absences, or emergency situations, and to conduct health screenings as allowed by OSHA and the CDC to protect the safety of our employee during pandemic situations.
For internal use, such as tracking access into buildings, timekeeping, activity logs, etc.
To comply with laws and regulations, including but not limited to applicable tax, health and safety, anti-discrimination, immigration, labor and employment, and social welfare laws.
For security or the prevention, detection, or investigation of fraud, suspected or actual illegal activity, violations of company policy or rules, or other misconduct.
To comply with civil, criminal, judicial, or regulatory inquiries, investigations, subpoenas, or summons.
To comply with all licensing requirements necessary for our business operations, including state exam audits.
To exercise or defend the legal rights of Symmetry and its employees, affiliates, customers, contractors, and agents.
To seek advice from lawyers, auditors, or other professional advisors.
If Symmetry uses Personal Information of an applicant, employee, or contractor for a purpose materially different than those disclosed in this notice, Symmetry will notify the employee and obtain explicit consent from the employee to use the Personal Information for this new purpose.
Job Type: Full-time
Pay: $115,000.00 - $135,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Paid time off
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
Accounting and Finance: 2 years (Preferred)
advanced SQL concepts and writing SQL statements: 5 years (Required)
Work Location: Hybrid remote in Anaheim, CA 92807",3.9,"Symmetry Lending
3.9","Anaheim, CA",51 to 200 Employees,-1,Company - Public,Banking & Lending,Financial Services,$5 to $25 million (USD)
376,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
377,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1.0,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
378,Data Engineer,Employer Provided Salary:$55.00 - $75.00 Per Hour,"Senior Data Engineer - 10+ Years of Total Experience Required
Location: Dallas, TX and Remote
Job Description:
Slesha inc is looking for a Data Engineer to join our team in our new location in Dallas, TX. This role will be responsible for the following:
Data Engineer
Responsibilities
· Writes ETL (Extract / Transform / Load) processes, designs database systems and, develops tools for real-time and offline analytic processing.
· Troubleshoots software and processes for data consistency and integrity. Integrates complex and large scale data from a variety of sources for business partners to generate insight and make decisions.
· Translates business specifications into design specifications and code. Responsible for writing complex programs, ad hoc queries, and reports. Ensures that all code is well structured, includes sufficient documentation, and is easy to maintain and reuse.
· Partners with internal clients to gain an expert understanding of business functions and informational needs. Works closely with other technical and data analytics experts across the business to implement data solutions.
· Leads all phases of solution development. Explains technical considerations at related meetings, including those with internal clients and less experienced team members.
· Assesses data quality and tests code thoroughly for accuracy of intended purpose. Provides data analysis guidance and serves as a technical consultant for the client.
· Educates and develops junior data engineers on the team while applying quality control to their work. Develops data engineering standards and contributes expertise to other data expert teams across Vanguard.
· Tests and implements new software releases through regression testing. Identifies issues and engages with vendors to resolve and elevate software into production.
· Participates in special projects and performs other duties as assigned.
Qualifications
· Deep technical knowledge – including proficiency in at least two of Python, SQL, Hive, Spark, Amazon Web Services / cloud computing (e.g., Elastic MapReduce, EC2, S3), Bash shell scripting
· Experience writing production quality code to create data products
· Ability to effectively communicate technical concepts to non-technical audiences
Job Type: Contract
Pay: $55.00 - $75.00 per hour
Compensation package:
Hourly pay
Experience level:
10 years
Schedule:
8 hour shift
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
AWS: 3 years (Required)
ETL: 3 years (Required)
Data warehouse: 3 years (Required)
10 key typing: 9 years (Required)
Work Location: Remote",-1.0,Slesha inc,"Dallas, TX",-1,-1,-1,-1,-1,-1
379,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
380,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
381,Data Engineer,-1,"At MNTN, we've built a culture based on quality, trust, ambition, and accountability – but most importantly, we really enjoy working here. We pride ourselves on our self-service platform, originally coded by our President and CEO, and are constantly seeking to improve the user experience for our customers and scale for efficiency. Our startup spirit powers our growth mindset and supports our teammates as they build the future of ConnectedTV. We're looking for people who naturally want to do more, own more, and make an impact in their careers – and we're seeking someone to be part of our next stage of growth.
As a Senior Data Engineer on the Data team, you will help build the platform to generate, track, manage and triage key business and client success metrics. The goal is to have rapid insights across all available information to mitigate issues and identify opportunities for a smooth marketing experience.
You will:
Become the expert on the MNTN platforms, UI, data infrastructure, and data processes
Extract meaningful business metrics from raw data using SQL and other tools
Create and manage ETL/ELT workflows that transform our billions of raw data points daily into quickly accessible information across our databases and data warehouses
Organize data and metrics for measurable and trackable confidence in reporting and client performance to fulfill agreed-upon quality standards
Organize visualizations, reporting, and alerting necessary to rapidly illustrate performance, data quality, trends and opportunities
Investigate critical incidents and otherwise ensure that any issues reach resolution by the relevant parties
You have:
5+ years of experience related to data engineering, analysis and modeling complex data
Strong experience in SQL, data modeling, and manipulating and extracting large data sets.
Hands-on experience working with data warehouse technologies. Familiarity with building data pipelines and architectures and designing ETL flows.
Experience with programming languages such as Python, Java, or shell scripting. Familiarity with algorithms.
Familiarity with software processes and tools such as Git, CI/CD pipelines, Linux, and Airflow
Experience with working in a cloud computing environment such as AWS, Azure, or GCP
Familiarity in a business intelligence tool such as Domo, Looker or Tableau
Written and verbal communication skills to convey complex technical topics to non-technical audiences across the organization
MNTN Perks:
100% remote
Open-ended vacation policy with an annual vacation allowance
Three-day weekend every month of the year
Competitive compensation
100% healthcare coverage
401k plan
Flexible Spending Account (FSA) for dependent, medical, and dental care
Access to coaching, therapy, and professional development
About MNTN:
MNTN provides advertising software for brands to reach their audience across Connected TV, web, and mobile. MNTN Performance TV has redefined what it means to advertise on television, transforming Connected TV into a direct-response, performance marketing channel. Our web retargeting has been leveraged by thousands of top brands for over a decade, driving billions of dollars in revenue.
Our solutions give advertisers total transparency and complete control over their campaigns – all with the fastest go-live in the industry. As a result, thousands of top brands have partnered with MNTN, including, Petsmart, Build with Ferguson Master, Simplisafe, Yieldstreet and National University.
#Li-Remote",3.9,"MNTN
3.9",Remote,201 to 500 Employees,2009,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
382,Data Engineer,Employer Provided Salary:$91K - $116K,"A reasonable salary expectation is between $91,000.00 and $116,022.40, based upon experience and internal equity.
This position is on a hybrid work schedule. (Mon & Fri - remote, Tues - Thurs onsite in Rancho Cucamonga, CA)
Position Summary/Position
The Data Engineer II assists in the implementation of methods to improve data reliability and quality. This role is responsible for combining raw information from different sources to create consistent and machine-readable formats. The Data Engineer II must also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling. The Data Engineer II will focus on data accessibility, which will enable the organization to utilize data for performance evaluation and optimization. The data Engineer II is also responsible for managing the entire back-end development life cycle for the company's enterprise data warehouse. In this role the incumbent will handle tasks associated with the implementation of ETL procedures, building warehouse databases, database performance management, and dimensional modeling and design of the table structures.
Major Functions (Duties and Responsibilities)
1. Design and develop data warehouse Extraction, Transformation and Loading (ETL) solutions using Microsoft SQL Server Integration Services (SSIS), Azure Data Factory, Synapse Analytics, Az Data Bricks, PySpark ETL.
2. Develop and implement data collection processes in conjunction with the data warehouse. Source data from legacy systems supporting a centralized data warehouse and reporting platform.
3. Develop technical solutions to meet the requirements for Data Warehouse, BI & Analytics
4. Work closely with the data engineering and BI & Analytics teams to design data and analytics solutions to increase the usability, completeness, and accuracy of enterprise data
5. Analyze user requirements and translate into database requirements and implement in database code
6. Create and maintain the optimal data pipeline architectures based on micro services based on platform and application requirements
7. Assemble large, complex data sets that meet functional / non-functional business requirements
8. Identify, design, and implement process improvements: automating manual pipeline processes, optimizing data ingestion and consumption, re-designing infrastructure for greater scalability, micro services, etc
9. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
10. Work closely with Data Warehouse Architect and Data Systems Architect to design data and analytics solutions to increase the usability, completeness, and accuracy of enterprise data
11. Create, maintain, and optimize SQL queries and routines
12. Analyze potential data quality issues to determine the root cause and create effective solutions.
13. Develop, adopt, and enforce Data Warehouse and ETL standards and architecture
14. Monitor and support ETL processes ensuring integrity and proper integration of all data sources
15. Create high throughput historical and incremental ETL jobs
16. Facilitate problem management, and communication among data architects, managers, informaticists and analysts
17. Provide detailed analysis of data issues; data mapping; and the process for automation and enhancement of data quality
18. Perform development activities such as source to target mapping validations, identify, document and execute unit test cases/scripts, peer and lead code reviews per code review checklist and document test and review results.
19. Collaborate and contribute to data integration strategies and visions
20. Provide ongoing proactive technical support for ETL and data warehouse system to ensure business continuity.
21. Work with Informaticists and Analysts to translate analytic requirements into technical solutions.
Experience Qualifications
Four (4) years of relevant work experience. Experience and knowledge in logical, rational, dimensional, and physical data modeling. Background in database systems along with a strong knowledge of SQL. Experience with Orchestration tools, Azure DevOps, and CI/CD. Intermediate experience with the following tools and technologies:
a. Azure Data Catalogue / Purview
b. Azure Cloud
c. Databricks
d. Power BI Dataflows
e. Power Query
f. Azure Cosmos
g. Azure Monitor
h. PowerShell
i. Python
Preferred Experience
Development experience using PySpark, Spark, Hadoop, Kubernetes, and RDMIS is highly desired.
Education Qualifications
Bachelor's degree from an accredited institution required.
Preferred Education
Master’s degree from an accredited institution preferred.
Professional Certification
Azure Data Engineering Certification is preferred.
Knowledge Requirement
Multi-server environment knowledge such as linked servers, data replication, backup/restore with MS SQL Server 2008+. Knowledge of applicable data privacy practices and laws.
Skills Requirement
Highly skilled in developing and optimizing T-SQL (DDL, DML, DCL) queries, stored procedures, functions, and views for various applications that involve numerous database tables and complex business logic. Good written and oral communication skills. Strong technical documentation skills. Good interpersonal skills.
Abilities Requirement
Highly self-motivated and directed. Keen attention to detail. Proven analytical and problem-solving abilities. Ability to effectively prioritize and execute tasks in a high-pressure environment.
Commitment to Team Culture
The IEHP Team environment requires a Team Member to participate in the IEHP Team Culture. A Team Member demonstrates support of the Culture by developing professional and effective working relationships that include elements of respect and cooperation with Team Members, Members and associates outside of our organization.
Working Conditions
Word processing and programming involving computer keyboard and screens.
Position is eligible for Hybrid work location upon completing the necessary steps and receiving HR approval. All IEHP positions approved for telecommute or hybrid work locations may periodically be required to report to IEHP’s main campus for mandatory in-person meetings or for other business needs as determined by IEHP leadership.
Inland Empire Health Plan (IEHP) is the largest not-for-profit Medi-Cal and Medicare health plan in the Inland Empire. We are also one of the largest employers in the region, designated as “Great Place to Work.” With a provider network of more than 5,000 and a team of more than 3,000 employees, IEHP provides quality, accessible healthcare services to more than 1.5 million members. And our Mission, Vision, and Values help guide us in the development of innovative programs and the creation of an award-winning workplace. As the healthcare landscape is transformed, we’re ready to make a difference today and in the years to come. Join our Team and make a difference with us! IEHP offers a competitive salary and stellar benefit package with a value estimated at 35% of the annual salary, including medical, dental, vision, team bonus, and state pension plan.
Job Types: Full-time, Permanent
Pay: $91,000.00 - $116,022.40 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Parental leave
Retirement plan
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Rancho Cucamonga, CA 91730: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
data engineering: 4 years (Required)
Work Location: Hybrid remote in Rancho Cucamonga, CA 91730",3.7,"Inland Empire Health Plan
3.7","Rancho Cucamonga, CA",1001 to 5000 Employees,1996,Company - Public,Health Care Services & Hospitals,Healthcare,$1 to $5 billion (USD)
383,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
384,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
385,Data Engineer I - People Analytics,$73K - $111K (Glassdoor est.),"POSITION SUMMARY Our People Analytics team is expanding! The Data Engineer I is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.

This position can work in a flexible, hybrid model with 3 days on-site in Appleton, WI and 2 days remote.

JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.

QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming. Preferred experience in Python applied to data engineering and transformation.
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated

#LI-Hybid

Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101 .",4.1,"US Venture
4.1","Appleton, WI",1001 to 5000 Employees,1951,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
386,Senior Data Engineer,Employer Provided Salary:$190K,"Welcome to the MOMENTUM Family!
MOMENTUM is not just our company name; it is the highest value we deliver to our customers. We are a rapidly growing technology solutions company delivering innovative technology, engineering, and intelligence solutions across the DoD sector. The efforts of our high-capacity team ultimately strengthen our Nation and the warfighter.

Our team is dispersed throughout the US, which means we value the diversity and unique collaboration fostered throughout our team. We work incredibly hard for our customers and believe deeply in our core values. We're a high-energy, high-growth team and we love to win.

Data Engineer
The Data Engineer provides engineering support to the data science and software engineering team members. Includes augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments. Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, to include hardware and software support to existing servers.

In this role, you will:
Support ad-hoc data analysis requirements defined by the client's Leadership.
Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives.
Knowledge of business intelligence reporting tools and data visualization software including Tableau.
Will work on Data cleaning and transformation efforts in delivery of CDRLs.
Experienced in extracting and aggregating structured and unstructured data.
Experienced in data programming languages and tools such as Python and R.
Experience with SQL or similar database language.
Experience designing and implementing data models to enable, sustain and enhance the value of information they contain.
Strong analytical and critical thinking skills.
Ability to work collaboratively and effectively in a team environment.
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers
Make sure pedigree and provenance of the data is maintained such that the access to data is protected
Clean and preprocess data to enable analytic access
Collaborate with enterprise working groups to advance the state of data standards
Collaborate with the engineering team, data stewards, and mission partners to aid in processes getting actionable value out of the data holdings architects complex, repeatable ETL
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic and others
Supports Experience with Targeting using Sponsor Tools, Reverse Engineering
Develop API connectors to enable ingest of new data catalog entries from databases and files to ensure that data mappings will provide the best performance for expected user experience
Supports Deliverables and Reports

If you're suitable for this role, you have:
Top Secret SCI with FULL SCOPE POLY REQUIRED
9+ Years of verifiable experience


To learn more about us, check out our website at www.gomomentum.tech!

MOMENTUM is an EEO/M/F/Veteran/Disabled Employer:
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The qualifications listed above are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions.

Accommodations:
Consistent with the Americans with Disabilities Act (ADA) and Alabama civil rights law, it is the policy of Momentum to provide reasonable accommodation when requested by a qualified applicant or employee with a disability, unless such accommodation would cause an undue hardship. The policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If reasonable accommodation is needed, please include a request when applying.",3.6,"Momentum
3.6","Chantilly, VA",501 to 1000 Employees,1987,Company - Public,Advertising & Public Relations,Media & Communication,$100 to $500 million (USD)
387,Data Engineer,Employer Provided Salary:$150K - $300K,"What we're looking for
Stytch is the platform for user authentication. We build infrastructure that sits in the critical path of our customer's applications. As a data engineer, you'll work on designing and building event-driven architecture systems to drive analytics insights and observability tooling for our customers.

What Excites You
Championing data-driven insights - you see data analytics and observability as a product critical to success
Solving problems with pragmatic solutions — you know when to make trade-offs between completeness and utility and you know when to cut scope to ship something good enough quickly
Building products that make developers lives easier — as a data engineer for a developer infrastructure company, what you build will have an immediate impact on our customers
Shaping the culture and growing the team through recruiting, mentorship, and establishing best practices
Learning new skills and technologies in a fast paced environment

What Excites Us
Comfort working in a modern data stack using tools like Snowflake, Redshift, DBT, Fivetran, ElasticSearch, and Kinesis
Appreciation for schema design and architecture that balance flexibility and simplicity
Experience designing and building highly reliable back-end and ETL systems
3+ years as a data or backend engineer

What Success Looks Like
Technical — build new, highly reliable services that our customers can depend on
Ownership — advocate for projects and solutions that you believe in and ship them to production
Leadership — level up your teammates by providing mentorship and guidance

Our Tech Stack
Data moves through Snowflake, ElasticSearch, MySQL, and Kinesis
Go and Node for application services
We run on AWS with Kubernetes for containerization
gRPC and protobufs for internal service communication

Expected base salary $150,000-$300,000. The anticipated base salary range is not inclusive of full benefits including equity, health care insurance, time off, paid parental leave, etc. This base salary is accurate based on information at the time of posting. Actual compensation for hired candidates will be determined using a number of factors including experience, skills, and qualifications.
We're looking to hire a GREAT team and that means hiring people who are highly empathetic, ambitious, and excited about building the future of user authentication. You should feel empowered to apply for this role even if your experience doesn't exactly match up to our job description (our job descriptions are directional and not perfect recipes for exactly what we need). We are committed to building a diverse, inclusive, and equitable workspace where everyone (regardless of age, education, ethnicity, gender, sexual orientation, or any personal characteristics) feels like they belong. We look forward to hearing from you!

Learn more about our team and culture here!",5.0,"Stytch
5.0","San Francisco, CA",1 to 50 Employees,2020,Company - Public,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
388,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
389,Azure Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Required Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 9 years (Required)
Python: 8 years (Required)
Data lake: 7 years (Required)
Data Bricks: 7 years (Required)
Work Location: Remote",-1.0,TekValue IT Solutions,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
390,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
391,"Manager, Data Engineer & ETL Processing",$83K - $118K (Glassdoor est.),"Company Description

About Spark Foundry:

Spark Foundry is a global media agency that exists to bring HEAT – Higher Engagement, Affinity, and Transactions – to brands. By combining flawless media fundamentals with aggressive innovation, Spark inspires consumers to pay more attention, to care more about our clients’ brands, and to buy more products and services from them.
Balancing the nimble spirit of a startup with the powerhouse soul of Publicis Media, Spark Foundry delivers the best of both worlds to a client roster that spans some of the world’s best and most beloved brands and companies. We combine boutique-caliber insights and service with the buying clout and first-look access of a global leader, bringing the heat to challenger brands that want to act like giants, and to giant brands that want to act like challengers.
With a bottom-up culture that celebrates diversity and aims for all voices to be heard, Spark has become a magnet for the industry’s best talent, with one of the best retention rates in the industry. And by applying a whole-person approach to professional and personal development, Spark develops a workforce that is well prepared for today’s challenges, and also poised to create meaningful careers in the years to come.
Because we know that heat arises the intersection of complementary forces, our professionals come from myriad disciplines and backgrounds: data, analytics, and insights, content and creative production, communications and strategy, finance and marketing, and sociology, psychology, and other liberal arts disciplines.

Job Description

Overview:
The Manager, Data Engineering & ETL Development is a key driver to build data platform leveraging best in class ETL practice and is also a strategic thinker and a talented data visualization expert with emphasis on dashboard reporting. The primary responsibility will be developing business intelligence platform to support media and marketing data for our clients. This position will allow you to be a significant contributor as part of the team to support easy access of data and visualizations to fuel stronger insights and media optimizations.
This position requires strong technical and tactical skill sets with an eye for numbers, intellectual curiosity, proficiency at problem solving, and a critical understanding of online media. The candidate must have a proven track record in managing ETL’s, APIs and business intelligence platforms. Candidate should be a team player. A “roll up the sleeves” approach is mandatory and a “get it done” attitude is a must. Specific responsibilities include coordination between the research, analytics & media teams ensuring high quality data projects are effectively delivered.
Successful candidates will be multi-dimensional ‘rising-stars’ who are able to employ complex problem solving skills, and are able to communicate these succinctly to a broad client and media stakeholder audience.
Role Objectives:
Responsible for loading and validating data into the data warehouse from various source systems
Analyze, develop, fix, test, review and deploy functionality, and bug fixes in ETL data pipelines
Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required
Building Data mappings between Source to Target systems
Provides support for technical issues and ensuring system availability
Work with business customers to identify and develop additional data and reporting needs
Understand how business intelligence platform/data technologies work and offer the ability to explain technical concepts in ordinary terms (be technically savvy - understand the opportunities and limitations)
Establish and manage data integrations utilizing a taxonomy nomenclature to make recommendations for data visualization to showcase media performance through the use of Datorama or Tableau
Perform regular quality assurance/quality control checks on assigned client campaigns to ensure the data is processing accurately
Design and build backend data streams and processes to automate reporting capability with data visualization tools
Contribute to client status and reporting calls, including presentation of reporting as required
Develop subject matter expertise in ETL, API development, and Business Intelligence platforms
Clearly define project deliverables, timelines, and dependencies for junior team members, internal stakeholders and clients
Collaborate on an inclusive team, where members openly communicate and collectively problem-solve
Strong ability to evaluate new technologies and present findings to team
Contribute to knowledge sharing efforts and mentorships
Complete other duties as assigned.

Qualifications
Bachelor’s degree or combination of education, and equivalent work experience is preferred in the field of computer science, management information systems or Information Technology
3+ years’ related experience ETL development and business intelligence platform management
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
3+ plus years’ development experience with database engines including Presto, Mongo db and Hadoop.
3+ years’ experience in ETL development, Strong Database (Modeling, SQL), SQL Server, Hadoop, AWS Redshift, Qubole.
Experience managing team of 2 or more associates/analysts preferred
Strong database modeling and SQL skills. Ability to write complex SQL statement, Procedures and data automation programs
Knowledge of ETL tools like Airflow, AWS Glue, Alteryx, SSIS, Qubole/Spark is a must
Knowledge of Python or similar programming languages required
Proficiency with Datorama, Tableau, or other data visualization tools is preferred
Experience working with AWS or other cloud technologies
Advanced user Microsoft Office Suite and reporting tools
Demonstrated expertise in core MS Excel functions (vlookup, pivot tables, data visualization)
Experience in designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.
Strong analytical and problem-solving skills
Strong verbal/written communication and interpersonal skills is required
Self-starter with ability to thrive in a fast-paced environment and able to function independently while providing status updates to a team of analysts
Cooperative, flexible, conscientious, dependable, resourceful, self-motivated, and team-oriented
Problem solving, time management, and critical thinking skills with a professional and positive attitude
Ability to work independently and as part of an agile team, participating in daily stand-ups, sprint planning and sprint review

Character:
The following qualities help drive success as member of the Spark Data and Analytics team:
Entrepreneurial, engaging, resourceful, curious, and self-directed spirit
Willing and easily roll sleeves up or down; love the nitty-gritty and the strategy
Collaborative approach to building cohesive, strong teams
Loving and living the intersections between brands, people, media, communications
Relentlessly passionate and resolute
Planning and time management excellence.
Embrace challenges
Proactive, especially in pushing for new opportunities, approaches, and ideas.
Keenly focused on action and solutions; thrives with deep critical thinking and analysis.
Pioneering insight attitude and research in-the-know.
Resourcefulness, flexibility and adaptability, strong ability to pivot when the need arises.
Inspired to be part of the insight journey/revolution with a growing, dedicated team

Additional Information

All your information will be kept confidential according to EEO guidelines.
23-2795",3.9,"Spark Foundry
3.9","Dallas, TX",51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
392,Data Engineer,$80K - $115K (Glassdoor est.),"Data Engineer
Syracuse, NY
Overview
With more than 70 years as an established, family-owned company, Raymour and Flanigan has grown into the largest furniture retailer in the Northeast with more than 100 showrooms in seven states, and we are continually expanding our territory. We have a strong foundation of experienced IT professionals with expertise in harnessing and maximizing organizational processes and their associated information to solve a wide variety of complex business needs.
PositionOverview
We are looking for an accomplished pipeline-centric Data Engineer to join our growing team of analytics experts. The position will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.

Essential Functions and Responsibilities
Lead and develop sustainable data driven solutions with current and next gen data technologies to meet the needs of our organization and business customers
Design robust systems with an eye on the long-term maintenance and support of the application
Assemble large, complex data sets and data marts that meet functional/non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Leverage and establish standards across the team and organization
Develop within a hybrid-cloud-based data-processing architecture to integrate Raymour & Flanigan data with third parties
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Develop with a strong emphasis on security, reliability, fault tolerance and performance
Develop stream processing applications to process data in near real time
Build batch data pipelines to integrate disparate data sources in a central data warehouse to support business applications and analytics
Design new analytical and physical data models for analysis and optimization
Develop solutions within a data visualization technology or platform (e.g. Tableau, D3)
Provide technical guidance to team members
Help to enable team success through fostering a positive work environment
Required Skills and Experience
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a bachelor's degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases like Snowflake, Oracle, Dynamo DB etc..
Experience with data pipeline and workflow management tools: SSIS, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with programming languages: Python, C# etc.
Experience with a tool like DBT a plus
Raymour & Flanigan offers competitive compensation and a comprehensive benefits package:
Excellent Health, Dental & Vision Coverage
401(k) with a Company Match
Paid Vacation and Holidays
Health Savings Account
Flexible Spending Account
Training and Development
Tuition Reimbursement Program
Generous Merchandise Discount
Short & Long Term Disability
Group Life Insurance
Specified Disease Insurance
Raymour & Flanigan proudly supports a drug and smoke free work environment.
Raymour & Flanigan is an Equal Employment Opportunity employer that does not discriminate against any associate or applicant on the basis of race, creed, color, religion, sex (including pregnancy), age, national origin, physical or mental disability, sexual orientation, sexual and other reproductive health decisions, marital or familial status, genetic information or other basis protected by law.",3.6,"Raymour and Flanigan
3.6","Liverpool, NY",5001 to 10000 Employees,1947,Company - Private,Home Furniture & Housewares Stores,Retail & Wholesale,$1 to $5 billion (USD)
393,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
394,Data Engineer,$88K - $124K (Glassdoor est.),"Recently awarded one of Crain's Best Places to Work in Chicago®, Premier International is a privately held and private equity backed software and technology consulting firm headquartered in downtown Chicago, serving large enterprise consulting and Fortune 500 firms deploying large-scale systems implementations.
You will work in a fast-paced environment that exposes you to diverse project experiences as we collaborate to solve our clients' biggest data challenges.
The Opportunity:
Premier International is hiring an experienced Data Engineer to join our growing Data Governance Practice. You will work in a fast-paced environment that exposes you to diverse project experiences, leading-edge technologies, and continuous learning experiences that will grow your career while solving clients' biggest challenges.
Our Data Governance Practice delivers end-to-end business advisory services, implementation, and technical solutions for the Data Governance Lifecycle including Consulting, Metadata Integration, Reference Data Management, Sensitive Data Management, Tool Evaluation, and Product Implementation.
What You'll Be Doing:
Designing, implementing, and maintaining Data Warehouse environments
Creating and maintaining comprehensive documentation of data engineering processes, pipelines, and workflows
Collaborating effectively with cross-functional teams, including data scientists and analysts, to understand their data needs and support data-related initiatives
What You'll Bring to the Team:
Bachelor's degree in Data Science, Computer Science, Statistics, or a related field
8+ years of relevant experience in data migration
Proficient in Python and Spark development/programming with a focus on performance and scalability
Experience with version control systems, GitHub, for code integrity
Strong analytical mindset and the ability to derive actionable insights from data
Excellent communication and presentation skills, capable of conveying complex information in a clear and concise manner
Ability to work independently and collaboratively in a fast-paced and dynamic environment
Premier Perks & Benefits:
Highly competitive compensation with annual bonus incentive
401K plan with company match
Company paid individual health, dental, vision, disability, and life insurance coverage
Four weeks of paid time off
Nine company paid holidays
Employee referral bonuses
Much more at one of Chicago's Best and Brightest Companies to Work For®!
Premier has been named one of The Best and Brightest Companies to Work For® in Chicago (2019, 2020 & 2021), one of Crain's top 100 Best Places to Work in Chicago (2020 & 2021) and recently made the 2021 Inc. 5000 list of America's Fastest-Growing Private Companies. While we are relentlessly client-focused, we are proud to have our culture and company recognized by others.
Premier is an EEO Employer and provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",4.2,"Premier International
4.2","Chicago, IL",1 to 50 Employees,1985,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
395,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
396,Sr. Data Engineer( ETL testing experience),Employer Provided Salary:$50.00 - $60.00 Per Hour,"Key Skills to evaluate – Python (advanced level), Pyspark, data flow pipeline in AWS, distributed system, Snowflake, Redshift, ETL testing, QE knowledge
JD:
· Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Job Type: Full-time
Pay: $50.00 - $60.00 per hour
Experience level:
8 years
Experience:
python advanced: 10 years (Preferred)
pyspark: 10 years (Preferred)
aws: 10 years (Preferred)
snowflake: 10 years (Preferred)
Work Location: Remote",-1.0,Sana Pivot Inc,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
397,Data Engineer,$77K - $107K (Glassdoor est.),"Position Summary:
Reporting to the Director of Data Science, the Data Engineer will play a crucial role in designing, developing, and maintaining data pipelines and workflows to facilitate data integration, storage, and analysis. With the support of the Data Science team, this position will utilize proven expertise in Microsoft Azure cloud computing to implement and automate processes, ensure the health and efficiency of the data ecosystem, and bring unmatched value to our +1.5 million members across the country and across the world.

Duties and Responsibilities:
Lead architectural efforts for implementing the Data Science team's models and solutions.
Design, implement, and manage data pipelines using Azure Data Factory to extract, transform, and load data from various sources efficiently and securely.
Develop and optimize storage solutions on Microsoft Azure, ensuring data availability, scalability, and reliability.
Automate data-related jobs, processes, and workflows to streamline data delivery
Collaborate closely with cross-functional teams, including data scientists, analysts, and developers, to understand data requirements and support their data-related needs.
Design and maintain data models and architecture, enabling efficient data consumption by business applications and analytical tools.
Ensure data quality, integrity, and security throughout the data lifecycle.
Troubleshoot and promptly resolve data-related issues, performance bottlenecks, and system failures.
Research new data sources, third-party platforms, and data engineering best practices.
Stay current with the latest data engineering and cloud computing trends, particularly in the Azure ecosystem.
Assist Data Scientists with general analysis and dashboards, as needed.
Qualifications:
3-5+ years of relevant Data Engineering, Software Engineering, and/or Analytics experience.
Proved experience with Azure cloud solutions, preferably through your work portfolio and Azure certifications.
Strong proficiency in Azure Data Factory, Azure Storage, and other relevant Azure services.
Extensive experience designing and implementing automated data pipelines, ETL processes, and data workflows.
Advanced skills in SQL and Python for data manipulation, querying, and scripting tasks.
Familiarity with data warehousing concepts, modeling, and integration techniques.
Expert in integrating cloud solutions with external API sources and knowledgeable in creating in-house API solutions that meet our needs.
Excellent problem-solving abilities and a track record of handling complex data engineering challenges.
Strong communication skills and the ability to collaborate effectively with multidisciplinary teams.
Demonstrated ability to prioritize tasks, meet deadlines, and adapt to evolving project requirements.
Azure certifications (e.g., Azure Data Engineer Associate) will be a plus.
Willingness to travel up to 10%.
Organizational Overview:
The National Association of REALTORS® (NAR) is a team of professionals dedicated to providing world-class service to over 1.5 million REALTORS® working in the United States and around the world. The real estate industry is fast-paced and fast-changing--each year, our members participate in the sale, lease, and management of billions of dollars in real estate. As in every industry, our members' value proposition is constantly being challenged by innovation.

It is our mission to empower REALTORS® as they preserve, protect and advance the right to real property for all. We cannot do that without the ideas, passion, and commitment from our talented employees. As our greatest assets, employees are offered their pick of competitive benefits/perks and flexible work options. NAR is certified as a Great Place to Work®. Our most successful employees are committed to our Core Values, which are:

Putting members first
Leading change
Advancing Diversity and Inclusion
Giving respect
Collaborating
Communicating",3.7,"National Association of REALTORS®
3.7","Washington, DC",201 to 500 Employees,1908,Nonprofit Organization,Real Estate,Real Estate,$100 to $500 million (USD)
398,Senior Data Engineer,$87K - $114K (Glassdoor est.),"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.

This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration.
How will you make an impact?
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
What are we looking for?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework
You might be a good fit if you like to:
Leverage technology to deliver data solutions to drive business outcomes
Drive for continuous improvement of processes and solutions
Bring energy and commitment to excellence to drive delivery of high-quality solutions
Share knowledge and mentor team members on technologies and solutions
What you can learn on the job:
Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers
Grow practical understanding of agile work practices through application of the scrum framework to deliver work products
Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science
What does success look like?
First 30 days:
Understand the existing analytics data platform and technologies
Understand team’s work management processes
First 6 months:
Develop data solutions for the business including shared components and specific data sets
Learn business processes and develop necessary contacts throughout the organization
First year:
Continue to develop data solutions for the business
Become expert on the team for other data engineers to go to for mentoring in completing their work
Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,"AE Business Solutions
4.2","Milwaukee, WI",51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
399,Data Engineer- Hadoop,Employer Provided Salary:$63.00 - $66.00 Per Hour,"Position Summary:
Synthetic data engineer is the experienced professional who can bring together skills and technologies to be able design and develop solutions to deliver data required for training models/test applications. Candidate will be working with a team of resources using Python, Sqoop, pyspark to create the data in a most efficient manner. Candidate should have prior experience & knowledge in highly scalable production grade applications with a strong engineering acumen. Should be able to work in a collaborative, fast paced environment. Candidate must have knowledge and/or experience working with Hadoop ecosystem.
Skills:
• Advanced knowledge in Python Programming
• Familiarity with Faker module
• pyspark, Sqoop, Hadoop, Hive, Shell script, Hadoop Shell commands
• Good experience dealing with bulk data extracts and bulk data loads
• Ability to translate requirements from stake holders into the technical requirements and the code
• Experience on statistical distribution, agent to model , deep learning is a plus
• Background of MF environment is a plus
Job Type: Contract
Pay: $63.00 - $66.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
Hadoop: 7 years (Required)
Python: 5 years (Required)
ETL: 3 years (Preferred)
Work Location: Hybrid remote in Charlotte, NC 28203",3.9,"Data Inc
3.9","Charlotte, NC",501 to 1000 Employees,1983,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
400,Data Science Engineer,-1,"Build the solution that transforms the real estate industry!
We are looking for a Data Science Engineer superhero with a sixth sense for data who can ignite our day-to-day activities with their creativity.
Want to infuse a $30B+ sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? Looking for a fully remote startup culture supported by a profitable business model? Join Mashvisor and help us build an entirely new type of real estate model.

Our Values

Customer Obsessed – We always put our customers first.
Solution Driven – We solve problems that other people are afraid to.
Product led: We are always one step ahead of our customer's needs and create / add features they love every time
One Team – We believe inclusion and teamwork produce the best results.
Open and Direct – We communicate with honesty and respect to our colleagues, customers, and partners.

What You’ll Do
Designing, developing, and researching Machine Learning systems, models, and schemes
Studying, transforming, and converting data science prototypes
Searching and selecting appropriate data sets
Performing statistical analysis and using results to improve models
Training and retraining ML systems and models as needed
Identifying differences in data distribution that could affect model performance in real-world situations
Visualizing data for deeper insights
Analyzing the use cases of ML algorithms and ranking them by their success probability
Understanding when your findings can be applied to business decisions
Enriching existing ML frameworks and libraries
Verifying data quality and/or ensuring it via data cleaning

What You’ll Need
BS or Masters degree in Mathematics, Statistics, Economics, Data Science or another quantitative field
3+ years of hands-on experience utilizing data science to manage, enhance and develop models and deploy solutions to solve complex business problems
Expertise in SQL and programming in SQL, R, Python, C++, Java, and beneficial to know Lisp and Prolog
Strong organizational, interpersonal, and communication skills (both written and verbal)
A bias towards solving problems from a customer-centric lens and an intuitive sense for how the work aligns closely with business objectives
Solid experience with managing databases and datasets and structuring and optimizing the framework
A thorough understanding of SQL databases
Bonus: background in US real estate data, insurance or financial markets analysis
The ideal candidate will be a creative problem solver with an excellent work history on data analytics projects.

We want the work you do here to be the best work of your life.
Compensation: We offer a great salary with a yearly bonus based on performance.
Attitude: Work with a Can-Do team across the world.
Freedom: Work anywhere, anytime.
Time Off: Yearly vacations and sick leaves.
Responsibility: Ability to excel in a fully remote work environment.
Are you the one?",3.7,"Mashvisor Inc.
3.7",Remote,1 to 50 Employees,2015,Company - Private,Real Estate,Real Estate,$1 to $5 million (USD)
401,Sr. ETL DEV/Data Engineer,Employer Provided Salary:$96K - $159K,"Data Engineers will be responsible for transformation and modernization of enterprise data solutions on Cloud Platforms integrating Azure services and 3rd party data technologies. Data Engineer will work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions.
ESSENTIAL FUNCTIONS
Reasonable Accommodations Statement
To accomplish this job successfully, an individual must be able to perform, with or without reasonable accommodation, each essential function satisfactorily. Reasonable accommodations may be made to help enable qualified individuals with disabilities to perform the essential functions.
Essential Functions Statement
As a Data Engineer, you will be responsible for assisting our clients envision, design, and deploy data engineering workloads as part of our solutions. As part of a small, dynamic team, you will have the opportunity to contribute to multiple phases of the solution life cycle including designing and implementing models and processes for large-scale datasets used for descriptive, diagnostic, predictive, and prescriptive purposes
Work with the team to evaluate business needs and priorities, liaise with key business partners and address team needs related to data systems and management.
Translate business requirements into technical specifications; establish and define details, definitions, and requirements of applications, components and enhancements.
Generate design, development, test plans, detailed functional specifications documents, user interface design, and process flow charts for execution of programming.
Develop data pipelines / APIs using Python, SQL, potentially Spark and AWS, Azure or GCP Methods.
Build large-scale batch and real-time data pipelines with data processing frameworks in Azure cloud platform.
Assist in the migration from on-prem SQL Server data analytics platform to MS Azure cloud platform.
Work as part of a team to build upon ingestion framework to intake new data sources.
Analyze, design, code and test multiple components of application code across one or more clients.
Perform maintenance, enhancements and/or development work

Qualifications
BA/BS in computer science, mathematics, information management, business, or equivalent experience
6+ years of experience in SQL
4+ years of experience in Cloud Platforms: Azure or AWS or GCP
4+ years of experience in Python and Pyspark
4+ years of experience in Synapse highly preferred
Experience using SQL, dB Visualizer, AWS, Azure, Cloud technologies
Experience with Power BI or similar data visualization tools
knowledge of HL7 v2, HL7 CDA and FHIR interface mapping highly preferred
Exposure to non-relational databases and tools, such as Cassandra, JSON, JAVA, Python, and Spark
In-depth knowledge of healthcare interoperability and patient data aggregation
Ability to effectively communicate, at times in a non-technical language, with customers at all levels of the organization.",3.7,"Gold Coast Health Plan
3.7","Camarillo, CA",Unknown,-1,Self-employed,Insurance Agencies & Brokerages,Insurance,Unknown / Non-Applicable
402,Senior Data Engineer,Employer Provided Salary:$140K - $160K,"Senior Data Engineer, San Diego, CA
Company
PointPredictive is a fast-growing technology start-up that leverages a patented combination of artificial and natural intelligence [Ai+Ni] to provide risk assessments in the auto lending, mortgage, and retail space. The platform has been proven to reduce lender loan losses by 40-60% with review rates of 5-10% of their applications, resulting in higher productivity of lender risk management departments, significantly lower losses to their bottom lines, and improved customer experience. The company was founded in 2013 by a seasoned team of technology entrepreneurs with over 20 years of experience in the startup space (including several acquisitions) and has financial backing from top tier investors.
Role:
The company is in search of an exceptional Senior Data Engineer with a strong focus on scaling our Database Architecture and Data Asset for high performance and reliability. To succeed in this role, you should have expertise in handling large volumes of data in various formats and destinations. Proficiency in Python, Snowflake, database systems, SQL, and Amazon Web Services (AWS) is crucial for this position. We are particularly interested in individuals with a deep understanding of database architecture and the ability to design and implement robust and scalable ETL/ELT processes and pipelines. Your contributions will be key in enhancing the company's data infrastructure and ensuring data delivery and reliability.
Responsibilities:
· Develop and maintain robust and scalable ETL/ELT processes and pipelines, handling large volumes of data in various formats and destinations.
· Identify, design, and implement process and architecture improvements to enhance infrastructure scalability, data delivery, and automated manual processes.
· Proficiently write complex SQL queries and demonstrate a strong command of relational database systems, showcasing exceptional analytical and problem-solving abilities.
· Represent complex algorithms in software, showcasing a deep understanding of database technologies, management systems, data structures, and algorithms, as well as expertise in database architecture testing methodologies.
· Design and execute comprehensive test plans, develop debugging and testing scripts, and build testing tools to ensure the reliability and quality of data pipelines and processes, while documenting test results and procedures for future reference.
· Oversee Data Governance and Data Cleansing initiatives, ensuring data integrity and compliance with relevant regulations, while also providing support for production issues and customer requests.
· Offer engineering support for customer issues and bugs, conducting research and implementing effective fixes to maintain high-quality data solutions.
About you:
· You have 5+ years of relevant software development industry experience building and operating scalable, fault-tolerant, distributed systems.
· Database and software development experience with Python, SQL, Redshift, and experience with Amazon Web Services, Snowflake along with pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming, etc.
· Bonus: familiarity with Change Data Capture (CDC) tools such as Debezium, Oracle GoldenGate, or similar technologies.
· Experience with container services.
· Fluid with Amazon Web Services.
· Experience with concurrency, multithreading, and the deployment of distributed system architectures.
· Experience leading and shipping large scope technical projects in collaboration with multiple experienced engineers.
· You have excellent communication skills and the ability to work well within a team and across engineering teams.
· You are a strong problem solver and have solid production debugging skills.
· You Thrive in a fast-paced environment and see yourself as a partner with the business with the shared goal of moving the business forward.
· You have a high level of responsibility, ownership, and accountability.
Job Type: Full-time
Competitive pay, bonus, equity, and benefits:
Benefits:
· Competitive health, dental, and vision insurance
· Flexible spending account
· Life insurance
· 401(k) retirement plan
· Paid time off, company holidays, and wellness days
Location and Schedule:
· Work Location: San Diego (Del Mar)
Supplemental pay types:
· Bonus pay
· Equity
Education:
· Bachelor's or Master’s (Preferred)
Job Type: Full-time
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Stock options
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Are you willing to be in the office 5 days a week to influence culture and capabilities ?
Are you willing to move to San Diego within 60 days if not within the commuting region ?
Work Location: Hybrid remote in San Diego, CA 92130",-1.0,"Point Predictive, Inc.","San Diego, CA",-1,-1,-1,-1,-1,-1
403,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
404,Data Engineer,$95K - $129K (Glassdoor est.),"About the Job
Loopback Analytics is hiring a Data Engineer to join our team in managing and optimizing the healthcare clinical data pipeline. The ideal candidate will have a background in data engineering, an understanding of healthcare clinical data, and hands-on experience with Databricks. You will collaborate with cross-functional teams to design, build, and maintain robust data pipelines, ensuring the quality, availability, reliability, and security of critical clinical data. The Data Engineer will report to the Senior Director of Engineering.

Job Duties to Include
Collaborate with data scientists, data integration specialists, analysts, and other stakeholders to understand data requirements and design efficient data pipelines.
Develop, maintain, and optimize data pipelines for collecting, processing, and storing healthcare clinical data.
Implement data validation, cleansing, and transformation processes to ensure data accuracy and consistency.
Monitor and troubleshoot data pipeline performance, addressing any issues or bottlenecks proactively.
Ensure data security and compliance with industry regulations, including HIPAA.
Work closely with the DevOps team to deploy and manage data pipeline infrastructure in our Azure cloud environment.
Stay updated on industry trends, emerging technologies, and advancements in healthcare data management.

Requirements
Technical Experience:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field preferred.
A minimum of 4 years’ hands-on experience in data engineering, with an emphasis on healthcare clinical data.
Proficient use of Databricks for building and optimizing data pipelines.
Skilled in programming languages such as Python, Scala, or Java for data processing.
Sound familiarity with healthcare data standards (HL7, FHIR, DICOM) and exposure to clinical data systems (EHR, EMR).
Familiarity with cloud-based data platforms (e.g., AWS, Azure, Google Cloud) and related services.
Strong SQL skills for data manipulation, querying, and performance optimization.
Basic understanding of data modeling and ETL design principles.
Aptitude for troubleshooting intricate data-related challenges and devising solutions.
Strong communication skills to collaborate with cross-functional teams and present technical concepts effectively.

Preferred Experience:
Relevant certifications in data engineering, cloud platforms such as Databricks, or healthcare data management.
Exposure to big data technologies such as Apache Spark, Hadoop, or Kafka.
Understanding of machine learning concepts and their application to healthcare data.

Personal Characteristics:
Effective communication skills, with the ability to convey technical concepts to non-technical stakeholders.
Meticulous attention to detail, crucial for maintaining data quality and accuracy in healthcare contexts.
Strong problem-solving capabilities, essential for navigating complex healthcare data scenarios.
Collaborative mindset, adept at working with diverse stakeholders and contributing to cross-functional projects.
Eagerness for continuous learning to stay current in the evolving realms of data engineering and healthcare.
Leadership and mentorship acumen, enabling guidance of junior team members and effective project leadership.
Innovative outlook, open to exploring novel strategies for leveraging data to foster innovation.
Adaptability, embracing change and remaining receptive to new tools and techniques.

Travel expectation less than 5%

This employer will not sponsor applicants for employment visa status (e.g., H1-B) for this position. All applicants must be currently authorized to work in the United States on a full-time basis.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin. For immediate full-time consideration, please forward your resume via email at careers@loopbackanalytics.com

About Loopback

Founded in 2009, Loopback was rated as one of the best places to work in Dallas by the DBJ. Loopback Analytics is a leading provider of data-driven solutions for hospitals and health systems. The company’s comprehensive analytics platform drives growth for specialty and ambulatory pharmacy programs while connecting pharmacy activities with clinical and economic outcomes. Loopback’s clients include leading academic medical centers, health systems, and life sciences companies. For more information about our company and services please visit our website at www.loopbackanalytics.com.",4.4,"Loopback Analytics
4.4","Dallas, TX",51 to 200 Employees,2009,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
405,Senior Data Engineer (remote),Employer Provided Salary:$111K - $121K,"As the Senior AWS Data Engineer, you will have the opportunity to make a significant impact on the success and growth of KeHE by engaging with internal business stakeholders of all levels. You will consult with them, provide support & guidance in defining and delivering their BI needs by turning data into actionable information.
Perform full lifecycle Business Intelligence (BI) development
Identify data sources, provide data flow diagrams and document source to target mapping and process.
Collaborate with others on requirements specifications and documentation
Design and develop SQL for large complex ETL jobs and conduct related database troubleshooting and maintenance
Publish and consume data to and from the enterprise data lake on AWS Perform code reviews, unit and integration testing
Maintain the data warehouse performance by optimizing batch processing through parallelization, performance tuning etc.
Design, develop, and maintain scalable reports, dashboards, and metrics
Maintain the design and development standards for Business Intelligence (BI) Keep current with Business Intelligence data trends and technological innovations
Execute on POC’s with new technologies, drive innovation, and new ideas
REQUIREMENTS--
4 Year College Degree from an accredited university in one of the following: Computer Science Information Systems, Operations Research, Mathematics, Statistics, or related technical field 3+ years of direct experience using databases, including PostgreSQL, MySQL, SQL Server or Redshift
3+ years hands-on experience developing with TSQL, pgSQL, SSIS, SSAS (Tabular model), PowerBI and C# or Python
MUST KNOW AWS, S3, Lambda, and Redshift
Strong understanding of data modeling (i.e. conceptual, logical and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Strong experience with business intelligence and data warehousing design principles and industry best practices including multi-dimensional modelling (star schemas, snowflakes, de-normalized models, handling slowly changing dimensions)
Experience developing and deploying SSRS reports, dashboards, and metrics Experience in creating Entity Relationship Diagram’s using tools like Toad Ability to work independently as well as with a team
Self-directed and able to learn and apply new technologies quickly Ability to prioritize and multi-task across multiple workstreams
Job Types: Full-time, Part-time
Pay: $110,701.80 - $121,000.00 per year
Benefits:
401(k)
Dental insurance
Employee discount
Flexible schedule
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
What are your compensation expectations???? (required)
What is the best email address to reach you?
Are you willing to take a skills/coding test before your interview? (required)
Will you now or at any time require sponsorship? (required)
Experience:
SQL: 5 years (Preferred)
Python: 5 years (Preferred)
AWS S3: 4 years (Preferred)
AWS Redshift: 4 years (Preferred)
AWS Lambda: 4 years (Preferred)
Work Location: Hybrid remote in Naperville, IL 60563",3.4,"KeHE Distributors, LLC
3.4","Naperville, IL",5001 to 10000 Employees,1954,Company - Private,Wholesale,Retail & Wholesale,Unknown / Non-Applicable
406,Data Communications Engineer - III,Employer Provided Salary:$91K - $181K,"Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","San Francisco, CA",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
407,Data Engineer ll,Employer Provided Salary:$130K - $160K,"The Company
Have you ever found yourself or a loved one waiting hours and hours in a hospital Emergency Room to get care? Or have you ever had a surgery scheduled for months in the future that needed to happen sooner? Unfortunately, our healthcare system is full of these types of operational problems. Our work saves lives and helps hospitals cut tens of millions of dollars in operational costs, while improving the quality of care they’re able to deliver.
Qventus is a real-time decision making platform for hospital operations. Our mission is to simplify how healthcare operates, so that hospitals and caregivers can focus on delivering the best possible care to patients. We use artificial intelligence and machine learning to create products that help nurses, doctors, and hospital staff anticipate issues and make operational decisions proactively.
Qventus works with leading public, academic and community hospitals across the United States. The company was recognized by the 2019 Black Book Awards in healthcare for patient flow and by CB Insights as a 2019 top 100 Most Promising Company in Artificial Intelligence. Recently, Qventus won the Robert Wood Johnson Foundation Emergency Response for the Healthcare System Innovation Challenge through its work helping health systems across the country plan for and operate in the COVID pandemic.
The role
Qventus is looking for a Data Platform Engineer II to help scale our solutions, focusing on our analytical and data science needs. The Data Platform team acts as stewards for Qventus’ data. We stream hospital EMR to our core warehouses in real time, build out curated data layers to power our Healthcare AI & Analytical insights and overall ensure Qventus data users have the tools they need to explore and power the Qventus product at scale and cost to improve the lives of patients and doctors across the country.
As a Data Platform Engineer II, you will build and own significant components of the Qventus solution pipelines. You will be comfortable designing, building, and leading cross-functional initiatives with analytical and data science partners - from schema design, to pipeline design, to scaling services to support company expansion within the healthcare space (and HIPAA restrictions). You will have a strong passion for well designed data models and be motivated and excited to have an impact on the team and in the company and to improve the quality of healthcare operations.
Key Responsibilities
Work closely with core data users to understand product needs and design, build, tune and improve our core data assets and the overall end-to-end workflow of data users at Qventus (incl. designing data structures, building and scheduling data transformation pipelines, improving transparency etc.).
Automate & manage the lifecycle of data sets (schema development, deprecation, and iteration).
Improve the data quality and transparency of the pipelines (defining data requirements, identifying and implementing data observability tooling - lineage, sources, transformations).
Work closely with core team members to develop, test, deploy, and operate high quality, scalable software and raise engineering standards.
Key Qualifications
Demonstrated experience in data modeling / schema design and transformation pipeline implementation in collaboration with data science and analytics partners
Experience developing & coordinating execution in a fast paced dynamic environment across multiple technologies
Strong cross-functional communication - ability to break down complex technical components for technical and non-technical partners alike
Interest in mentoring and supporting new developers particularly in data modeling and analytical collaboration
3+ years of professional experience working with modern programming languages such as Java, C/C++, Python with a dedication to high code quality.
Nice to Have Skills
Degree in Computer Science, Engineering, or related field, or equivalent training / experience
Competence participating in technical architecture discussions to help drive high quality technical development within your team
Practical hands on experience with:
building large-scale, high complexity metrics and monitoring
ELK, DBT, Snowflake, AWS, Terraform, Looker, Ansible experience
Experience building and maintaining robust and efficient backend data systems with functional proficiency with AWS cloud services & modern data warehouse services (Snowflake)
We consider several factors when determining compensation, including location, experience, and other job-related factors.
Salary Range: $130,000 to $160,000 annually + equity + benefits- Qventus expects to hire for this position near the middle of the range. Only in truly rare or exceptional circumstances where a candidate's experience, credentials, or expertise far exceed those required or expected will we consider and offer at the top of the salary range.
Qventus offers a competitive benefits package including medical, dental, vision, paid time off, company holidays, and a stock option plan.
Qventus is an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
Candidate information will be treated in accordance with our candidate privacy notice which can be found here: https://qventus.com/ccpa-privacy-notice/
This position does not provide visa sponsorship.
Employment is contingent upon the satisfactory completion of our pre-employment background investigation and drug test.
#LI-REMOTE",4.3,"Qventus
4.3",Remote,51 to 200 Employees,2012,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
408,Data Engineer,Employer Provided Salary:$120K - $200K,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!",-1.0,shaped.ai Inc.,"New York, NY",1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
409,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1.0,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
410,Big Data Engineer,Employer Provided Salary:$60.00 Per Hour,"""Architect, build, and launch new data models that provide intuitive analytics to the team.
Lead large-scale data engineering, integration and warehousing projects, build
custom integrations between cloud-based systems using APIs and write complex and
efficient queries to transform raw data sources into easily accessible models by using
the Data integration tool with coding across several languages such as Java, Python,
and SQL.""
Banking/Finance domain experience, relational dimensional and/or unstructured data modeling experience
Big Data applications development experience, NoSql experience, Azure (preferred), Java, Kafka/Spark
Job Type: Contract
Salary: Up to $60.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Berkeley Heights, NJ 07922: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 6 years (Preferred)
NoSQL: 5 years (Preferred)
Java: 5 years (Preferred)
Work Location: In person",-1.0,Comprise IT Solutions,"Berkeley Heights, NJ",1 to 50 Employees,-1,Self-employed,-1,-1,Unknown / Non-Applicable
411,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
412,Data Visualization Engineer,$77K - $101K (Glassdoor est.),"Passionate about precision medicine and advancing the healthcare industry?
Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.
With the advent of genomic sequencing, we can finally decode and process our genetic makeup. We now have more data than ever before but providers don't have the infrastructure or expertise to make sense of this data. We're on a mission to connect an entire ecosystem to redefine how genomic data is used in clinical settings.
What You'll Do:
Own the design, development and administration of advanced analytics dashboards and reporting and business intelligence tools. Beginning with requirements gathering across cross-functional teams, to the delivery of analytic solutions into production.
Collaborate with stakeholders, providing guidance, data and analytical support for developing new datasets, analyses and tools in support of strategic initiatives.
Perform deep-dive root cause and problem-solving analyses for and with laboratory, business operations and engineering teams.
Design and build data views to support advanced analytical reporting, self-service data analysis and exploration, utilizing multiple data and visualization technologies.
Work closely with technical and product teams to identify and align changes to internal systems in support of future data analysis needs and requirements.
Develop and facilitate data governance initiatives to assure data integrity across distributed systems.
Solve new and interesting problems.
Qualifications:
Bachelor's degree in Information Systems, Business Administration, Finance, Economics, Mathematics, Operations Research, Marketing or similar
3+ years of experience developing data sets for analysis use-cases
3+ years of experience with direct stakeholder management and ownership of full lifecycle of analytics/data projects and presenting analysis results to stakeholders
3+ years report-development and data visualization experience with BI tools (Looker, Tableau, etc.)
Ability to deliver both high-level explanations of complex problems while also able to deep dive into complex problems
Advanced SQL skills for querying and analysis of large, complex datasets
Must work well in a team environment and be able to partner with both business and technical team members and translate technical language to the non-technical need
Creative problem solver, ability to handle multiple projects, and strong work ethic needed
Excellent written and verbal communication skills
Highly organized and systematic, superb attention to detail
Able to complete tasks with a high degree of accuracy and an eye towards process improvement
Persistent and resilient person who thrives in a fast-paced, highly collaborative environment
Bonus points for:
Advanced analytics experience - classification/predictive modeling, optimization models, forecasting
Experience with Python for data engineering or advanced data analysis projects/use-cases
Healthcare domain knowledge and experience
Expert knowledge of database/data modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others
Experience building cloud-native applications and supporting technologies / patterns / practices including: GCP, AWS, Docker, CI/CD, DevOps, and microservices

#LI-EH1
#LI-Hybrid
We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.1,"Tempus
3.1","Chicago, IL",501 to 1000 Employees,2015,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
413,Data Insights Engineer,Employer Provided Salary:$85K - $95K,"Who We Are
We're purpose-driven. With every ride, we aim to redefine health and happiness. It's all about being more than a workout: SoulCycle is a mind-body-soul experience, built on community, love, respect, acceptance, and a lot of fun. It comes to life through the ride, the relationships, and the unparalleled hospitality. And all of that comes from our people. Join us—we'd love to have you.
Our Mission
To foster an open, diverse, & inclusive community—while embracing each unique individual exactly as they are. We empower each other by listening with an open mind, finding ways to learn and grow together, and always nurturing a sanctuary of trust. To make a real, lasting impact, we'll work nonstop to embrace and create change. Because nobody is equal until everyone is equal.
Job Description
The Data Insights Engineer will play a pivotal role in driving data-driven decisions at SoulCycle. You'll be responsible for building and maintaining the data infrastructure that supports all business functions, from marketing and operations to finance and customer experience, in addition to providing analysis to each of these teams. By leveraging your technical expertise and analytical skills, you will empower stakeholders to derive insights from data, enabling them to make strategic and informed decisions that positively impact the business.
Roles and Responsibilities
Insights and Recommendations: collaborate with cross-functional teams to understand business requirements, provide analytical support, and identify opportunities for data-driven improvements
Visualizations and Dashboarding: design and develop ad-hoc and recurring Looker reports; create and monitor business metrics; identify patterns, trends, and opportunities for performance improvement
Data Modeling: build, optimize, and document LookML data models that support quick and efficient analysis
Prediction: build predictive models that forecast business outcomes, customer behavior, and other relevant metrics
Qualifications
1-3 years of professional experience transforming and analyzing data across platforms such as Looker, Tableau, Mode, Jupyter Notebooks, Excel, and GCP/AWS. Looker/LookML experience is a plus.
Expert in SQL (able to write structured and efficient queries on large data sets) and familiarity with Python
Ability to identify patterns and trends in data and solve problems
Excellent communication skills to work with stakeholders to translate business needs and ideas into analyses and recommendations
Top-notch organizational skills and ability to manage projects in a fast-paced environment
Creative problem solving skills to find solutions to vague questions
Experience with Python data analysis and visualization packages is a plus (pandas, tensorflow, matplotlib, etc.)
Pay Range: $85,000 - $95,000 per year. This role is on-site 4 days a week.",3.9,"SoulCycle HQ
3.9","New York, NY",1001 to 5000 Employees,2006,Company - Private,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
414,Data Pipeline Engineer,$76K - $106K (Glassdoor est.),"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
The position:
Design and develop scalable data pipeline processes (including ingestion, cleansing, curation, unification, etc.)
Automate the processing of inbound client data feeds
Design and develop tools and processes to support automated data profiling and data quality methodologies
Work with our data science team to assist with the development of feature store data including data prep, enrichment, and feature engineering for AI/ML
Write and maintain documentation on data pipelines
Provide periodic support to our customer success team Skills & Experience
BS / MS in Computer Science, Engineering, or applicable experience
3+ Year using Python (Pandas/NumPy) in a production environment
3+ Year using PowerShell in a production environment
Expertise with ETL/ELT and the development of automated validation and data pipelines
Understand database design and data manipulation and transformation methodologies
Keen understanding of EDW, master data management and other database design principles
Experience designing solutions using a range of AWS Services
Experience with data engineering and workflow management frameworks such as Airflow and dbt
Comfortable working with high volume data in a variety of formats
Experience with CI/CD such as Jenkins
Experience with version control tools: Git preferred
Excellent verbal and written communication
Familiarity with healthcare data is a plus
Familiarity with ML pipelines, principles and libraries is a plus
Experience with REST API is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation",5.0,"Decision Point Healthcare
5.0","Boston, MA",1 to 50 Employees,2013,Company - Public,-1,-1,Less than $1 million (USD)
415,Data Engineer,Employer Provided Salary:$150K,"Data Engineer
We are looking for a Data Engineer that will architect and implement systems handling the ingestion, cleaning, and structuring of data. They will be a member of the Data Science team and will have direct interaction with the people using the data ingested. The hire will also be responsible for managing our group’s core infrastructure including our region redundant pipeline orchestration servers (Airflow) and webserver stack (NGINX + Gunicorn + Django). The ideal candidate is an experienced data pipeline builder and core infrastructure guru.
Responsibilities:
Create and maintain an optimal data pipelines in Python + SQL
Build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python and SQL (Snowflake & SQL Server) in both on premise and cloud environments
Manage and improve the user facing server infrastructure (web + api) including authorization and load balancing
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that that provide actionable insights on both the user interactions and infrastructure loads
Technical Qualifications:
Python expert, notably savvy with data science stack (Pandas, NumPy, SciPy)
Relational SQL databases, Microsoft SQL Server and Postgres preferred
Snowflake cloud database experience
Pipeline and workflow management tools: Airflow, Luigi, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Athena, SQS, etc.
2+ professional years’ experience with Masters or 3+ professional years with Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Highly Valued:
Data Science/Analysis background
Computer science understanding, data structures, processes, threading, memory usage
Unix/Linux command-line experience
Broad understanding of equities, derivatives, futures, FX, or other financial-services instruments
Other Qualifications:
Excellent listening, and communication (both oral and written) skills
Self-starter and critical thinker, takes ownership of own projects and makes improvement suggestions for the entire infrastructure.
Proactive, assertive and attentive to details.
Can work independently and in a collaborative environment.
Can handle several projects with different priorities at the same time in a fast-paced environment.
Excellent self-management and problem-solving skills.
Results-oriented, can deliver quality code quickly
Millennium pays a total compensation package which includes a base salary, discretionary performance bonus, and a comprehensive benefits package. The estimated base salary range for this position is $150,000, which is specific to New York and may change in the future. When finalizing an offer, we take into consideration an individual’s experience level and the qualifications they bring to the role to formulate a competitive total compensation package.",3.8,"Millennium Management LLC
3.8","New York, NY",1001 to 5000 Employees,1989,Company - Private,Investment & Asset Management,Financial Services,Unknown / Non-Applicable
416,AWS Data Engineer,-1,"5+ years of data engineer experience in developing, implementing, delivering, and managing end-to-end data solutions using AWS Glue
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications
Proficiency in cloud data technologies, such as AWS S3, AWS Glue, EC2
Knowledge of Snowflake/AWS Redshift as a data warehousing solution
Advanced knowledge in designing, developing, implementing and managing data pipelines to deliver data or data insights for application, reporting, or analytics
Strong experience creating and maintaining functional and technical specifications documents
Strong experience creating test plans, test data sets, and automated testing ot ensure all components of the system meet specifications
Strong SQL technical experience such as linking IT applications to databases and creating and handling metadata
Strong programming skill in Python or Scala
Bonus skills:
Strong experience in NoSQL database (i.e., MongoDB)
Strong experience in streaming technology (i.e., Kafka, data bricks streaming)
Strong experience in working in the healthcare industry including PHI, HIPAA regulations, and BAA processes
?
AWS DATA ENGINEER RESPONSIBILITIES:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics
Support the creation of the new cloud infrastructure and data ecosystem in the cloud",5.0,"CEDENT
5.0",United States,1 to 50 Employees,-1,Contract,Computer Hardware Development,Information Technology,Less than $1 million (USD)
417,Data Engineer,$67K - $96K (Glassdoor est.),"Massanutten Resort
Job Summary
The Data Engineer will play a crucial role in designing, developing, and maintaining the data infrastructure and systems that enable efficient and reliable data processing, storage, and analysis. The primary responsibility is to transform raw data into usable formats for various business applications and support data-informed decision-making processes. This role includes working in various Microsoft SQL Environments as well as supporting a Snowflake Data Warehouse. The Data Engineer is expected to have a broad understanding of data concepts and be avid about continuing education to keep up with the ever-changing data landscape.
Required
Degree in computer science or related file and 3+ years of experience with Python, SQL, and data visualization/exploration tools (in lieu of a degree, at least 5 years of experience in the above is equivalent)
Preferred
A degree in computer science or a related field
Certifications in database management and/or related technical certifications
Familiarity with advanced data concepts (i.e., AI and Machine Learning) and business intelligence/visualization tools
Typical Schedule
Days:
Monday-Friday, weekends and holidays as needed
Position will be required to be part of rotating on-call schedule
Hours:
8am-5pm, additional hours as needed
Core Responsibilities
Develop Data Pipeline: Design and implement scalable and efficient data pipelines, integrating diverse data sources and ensuring smooth data flow from ingestion to storage and processing
Transform and Model Data: Develop and maintain data transformation processes, such as ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform), to convert raw data into structured formats suitable for analysis and reporting. Create and manage data models to facilitate data integration and provide a consistent view of information across systems.
Implement and oversee Data Warehousing and Storage: Build and maintain data warehouses or data lakes, optimizing storage and retrieval processes to ensure high availability, reliability, and performance. Implement appropriate data governance and security measures to protect sensitive data.
Ensure Data Quality and Governance: Collaborate with data analysts and business stakeholders to understand data requirements and define data quality standards. Implement data validation, data profiling, and data cleansing techniques to ensure data accuracy, consistency, and integrity.
Optimize Performance: Identify and address performance bottlenecks in data processing and storage systems, leveraging optimization techniques and technologies to improve overall data engineering efficiency and reduce latency
Assist with Data Problem Solving: Support team with data related escalation issues, including help desk support tickets that are generated from our business users
Manage Data Integration and API Development: Establish connections and integrations between internal and external data sources, including APIs, databases, and third-party platforms. Develop and maintain data APIs to enable seamless data access and consumption by other applications and services.
Collaborate and Document: Work closely with cross-functional teams, including analysts, software engineers, and business stakeholders, to understand data requirements and ensure data solutions align with business objectives. Document data engineering processes, including data pipelines, transformations, and system architectures, to facilitate knowledge sharing and maintain a robust knowledge base.
Stay up-to-date with the latest advancements in data engineering, data management, and big data technologies. Evaluate and recommend new tools, frameworks, and methodologies to improve data engineering practices and enhance overall data infrastructure
Other duties as assigned by management
For more information, contact Ana at 540-289-4939.",3.6,"Great Eastern Resort Management, Inc.
3.6","McGaheysville, VA",1001 to 5000 Employees,-1,Self-employed,-1,-1,Unknown / Non-Applicable
418,Data Engineer,$74K - $107K (Glassdoor est.),"Analyze Business Requirement Documents and Implement Technical Solutions for privacy related applications.
Develop ETL process for supporting Data Extraction, transformations and loading.
Perform data conversions and aggregations using different transformations such as Merge, Merge join, Union condition split, sort, order by. Derived columns convert and cast transformations and row count and lookup and fuzzy lookup transformations.
Develop UNIX scripts to load the data from Source server to Teradata and validate the files between different servers.
Develop new process to implement state level privacy regulations based on each state law in Big Data Platform.
Create Temperory/Fact tables, loading with data and writing Teradata and Spark SQL queries.
Optimize/tune ETL objects, indexing and partitioning for better performance and efficiency.
Validate the performance metrics and work on performance tuning for SQL, HQL and Spark SQL queries.
Perform testing and Provide test support for various level of testing phases like Unit, User Acceptance, Regression, Parallel and System testing.
Promote the components to production environment through CI/CD process by using Git hub .
Script task and execute SQL tasks to execute SQL code. Work on containers for loop and for each loop container to run a group of tasks into a single container and repeating tasks.
Create the data flow to extract data from sources to OLEDB Source, Excel, XML, flat files sources and destination is SQL data warehouse.
Minimum Education Required:- All the responsibilities mentioned above are in line with the professional background and requires an absolute minimum of a Bachelor’s degree in computer science, computer information systems, technology management, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.",4.9,"Wise Equation Solutions
4.9","La Vista, NE",51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
419,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
420,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
421,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
422,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
423,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
424,Sr. Data Engineer- Contractor- Remote Work Eligible,-1,"Notes to applicants:
This position is eligible for full-time remote work in Texas, or North Carolina, or, in the alternative, to work in accordance with Dimensional’s best-of-both hybrid working model, which involves working in the office on Tuesdays, Wednesdays and Thursdays, and choosing to work in the office or remotely on Mondays and Fridays.
Resumes and portfolios (when applicable) are required as part of your application. When applying from a mobile device or tablet, you may not be able to attach a resume. If you cannot include an attachment at the time of your application, you will receive a follow up email asking you to attach your resume from a computer.
Here at Dimensional, we strive to be an inclusive workplace for all. Even if you do not match every qualification listed, if you are interested in who we are, what we do, and why we do it, we suggest and encourage you to apply.

Job Description:
About Dimensional:
Dimensional was built around a set of ideas bigger than the firm itself. With a confidence in markets, deep connections to the academic community, and a focus on implementation, we go where the science leads, and continue to pursue new insights, both large and small, that can benefit our clients.
The Technology Department at Dimensional leverages the rapidly evolving state of the art to engineer scalable, innovative, and research driven solutions to improve our client’s financial lives.
Software Engineers at Dimensional participate in the design and development of software solutions across an array of domains from Research and Investments to Sales and Marketing; collaboratively developing MVPs to test their ideas and rapidly iterate with constant feedback from users. Dimensional invests heavily in developer tools, platforms, paradigms and experience enabling teams to provide modern solutions that contribute profoundly to our client’s success.
We are looking for a Python Data Engineer to join our team and translate our customers’ goals into working software throughout the stack from automated configurations to model definitions, calculation APIs, and building robust data pipelines. The most important qualifications are a passion for quality software and enthusiasm for learning new technologies and approaches. The level of seniority for this position is negotiable based on experience.
You may be a fit for this role if you:
Are open-minded, curious, and resourceful
Are passionate about/stay current with modern technologies
Solve problems systematically and transparently
Share ideas, solicit/integrate feedback, design and solve collaboratively
Take a software engineering approach and demonstrate automation and security mindsets
What you might work on:
As a Data engineer at Dimensional, you will have the opportunity to understand the users’ needs and solve problems at all levels of the stack from automating infrastructure and deployments to building complex data pipelines to designing user friendly data applications.
Collaborate with subject matter experts in a variety of areas to drive the success of our clients
Perform software and data architecture and design
Develop complex software solutions using ETL and/or back-end technologies
Demonstrate and mentor software engineering best practices and participate in code reviews
Develop configurations and automations to enable testing, infrastructure and deployments
The successful candidate will be self-motivated and have a strong drive for learning and self-improvement.
Qualifications:
Bachelor’s degree in a technical field or equivalent practical experience.
5-10+ years of software development experience in a professional and/or academic setting (seniority of the role is negotiable).
5+ years of hands-on experience in developing ETL solutions using python.
Working knowledge of DevOps concepts, tools, and continuous delivery pipelines such as Octopus, TeamCity, Stash, Bitbucket, Jira, GIT, etc.
Advanced SQL knowledge and experience working with relational databases and working familiarity with various cloud data warehouses.
Experience in building processes supporting data transformation, data structures, metadata, dependency, and workload management.
Experience with data pipeline and workflow management tools: Airflow, etc.
Preferred Competencies:
Interest and ability to learn other coding languages as needed
Ability to write in English fluently and idiomatically
Advanced degree or equivalent experience in engineering, computer science or other technical related field
Experience with agile/scrum methodologies
Financial services industry experience
Experience with any of the following:
Redis, Postgresql, MongoDB, SQLServer
Airflow, Kafka, AWS, serverless/microservice architecture
TDD, BDD, Numpy/Scipy/Pandas, Ansible

#LI-Remote

Dimensional offers a variety of programs to help take care of you, your family, and your career, including comprehensive benefits, educational initiatives, and special celebrations of our history, culture, and growth.
It is the policy of the Company to provide equal employment opportunity for all applicants and employees. The Company does not unlawfully discriminate on the basis of race, color, religion, creed, sex, gender, gender identity, gender expression, national origin, age, disability, genetic information, ancestry, medical condition, marital status, covered veteran status, citizenship status, sexual orientation, or any other protected status. This policy applies to all areas of employment including recruitment, hiring, training, job assignment, promotion, compensation, benefits, transfer, discipline, termination, and social and recreational programs.",3.7,"Dimensional Fund Advisors
3.7",Texas,1001 to 5000 Employees,1981,Company - Private,Investment & Asset Management,Financial Services,$1 to $5 billion (USD)
425,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1.0,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
426,Data Engineer / Architect (Remote - US based),Employer Provided Salary:$101K - $216K,"Company Description

Due to the nature of this role, candidates must be geographically located, and authorized to work, in the United States.

Tidepool is a leading healthcare technology nonprofit company that is revolutionizing the way people with diabetes manage their condition. Our mission is to make diabetes data more accessible, actionable, and meaningful, empowering individuals and healthcare providers to make informed decisions and improve health outcomes. By leveraging cutting-edge technology and data-driven solutions, Tidepool is dedicated to making a positive impact on the lives of millions affected by diabetes.

Job Description

We are seeking a skilled and motivated Data Engineer / Architect to join our growing team. As a Data Engineer at Tidepool, you will play a pivotal role in designing, implementing, and optimizing our diabetes, product and business data infrastructure pipeline. You will collaborate closely with our Data Science and Data Analytics teams to ensure efficient data flow and enable advanced analytics and insights. Your expertise in data architecture, ETL processes, and database management will contribute to the development of innovative solutions that transform raw data into actionable information.

Essential Duties and Responsibilities
Design, develop, and maintain scalable and robust data pipelines, ensuring the efficient extraction, transformation, and loading (ETL) of data from various sources.
Collaborate with cross-functional teams, including Product, Engineering, to understand data requirements and develop data models that facilitate advanced analytics, machine learning, and predictive modeling. Work closely with Data Scientists and Data Analysts to ensure that our data pipeline architecture meets their needs.
Optimize data storage and retrieval systems to ensure high performance, scalability, and data integrity.
Implement and maintain data governance practices, including data quality monitoring, data lineage tracking, and metadata management.
Troubleshoot data-related issues and perform root cause analysis, ensuring timely resolution to minimize impact on data availability and accuracy.
Continuously evaluate and recommend improvements to existing data infrastructure, tools, and processes to enhance efficiency and reliability.
Stay up to date with emerging technologies, industry trends, and best practices in data engineering, and apply this knowledge to drive innovation within the team.

Qualifications
Bachelor's degree in Computer Science, Engineering, or a related field. Advanced degree is a plus.
Proven experience as a Data Engineer or similar role, with a strong understanding of data warehousing concepts, data modeling, and ETL processes.
Proficient in SQL and programming languages such as Python, Java, or Scala.
Experience with data processing tools and frameworks like Data Bricks or Apache Spark is highly desirable.
Solid knowledge of relational and NoSQL databases, data lakes, and data integration techniques.
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data services, such as S3, Redshift, BigQuery, or similar.
Experience in implementing data governance practices and ensuring data quality and integrity.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication skills with the ability to effectively articulate complex technical concepts to non-technical stakeholders.
Familiarity with diabetes or other types of health data is a plus
Understanding and knowledge of diabetes treatments, therapy and diabetes data is a big plus.

Additional Information

Salary range

$101,028 - $216,027 annually. To learn more about Tidepool's compensation philosophy please see Tidepool's Employee Handbook.
Benefits
Flexible PTO
Paid parental leave
Medical, Dental, and Vision coverage
Health and Childcare FSA
Flexible work schedule
Wellness and Productivity stipend
Continuing Education Reimbursement
Other Information
While many of Tidepool’s team members have a personal connection to diabetes, this is not a requirement. We ask that you have empathy for chronic conditions and you are prepared to learn about the diabetes experience.
This is a remote position. You’ll be working from home and interacting with a team of colleagues that works around the world. Learn more about working at Tidepool, including our approach to inclusion and diversity in this blog post.
Tidepool is an Equal Opportunity Employer. The company supports diversity and inclusion in its core values and does not discriminate against qualified employees or applicants because of race, color, religion, gender identity, sex, sexual preference, sexual identity, pregnancy, national origin, ancestry, citizenship, age, marital status, physical disability, mental disability, medical condition, military status, or any other characteristic protected by U.S. federal or state law or local ordinance. When necessary, the company will reasonably accommodate employees and applicants with disabilities if the person is otherwise qualified to safely perform all of the essential functions of the position.",-1.0,Tidepool,"Palo Alto, CA",1 to 50 Employees,2013,Nonprofit Organization,Health Care Services & Hospitals,Healthcare,$5 to $25 million (USD)
427,Senior Data Analytics Engineer,Employer Provided Salary:$75.00 Per Hour,"JOB PROFILE: NCDHHS-Senior Data Analytics Engineer (714485)
JOB LOCATION: 820 s Boylan Ave ,Raleigh, NC, 27603
JOB TYPE: Hybrid
CONTRACT TENNURE: (1 YEAR WITH POSSIBLE EXTENTION)
PAY RATE: $75.00/hr on W2
Candidate must have US Citizenship or Green Card/Permanent Residency in the US to be considered for this position.
Responsibilities:
Provide strategic insights using SAS/SQL, Tableau, and Congas to analyze complex data and assist DHB management in informed decision-making.
Engage directly with clients to understand data needs, translating requirements into effective solutions.
Expertly manage technical aspects, ensuring data quality control and accurate implementation of specifications.
Propose and implement operational healthcare reporting solutions for improved decision-making.
Aggregate, analyze, and report on complex healthcare data from sources like MMIS and claims data.
Develop operational reports and dashboards aligned with healthcare management goals.
Collaborate with staff to assess needs, design solutions, and conduct statistical analysis for insights.
Create advanced analytics based on technical specifications, aiding healthcare program oversight.
Efficiently manage client data requests, ensuring timely delivery and effective communication.
Skills Required:
SQL Proficiency.
Cognos Proficiency.
Tableau Expertise.
Data Quality Control.
Data Aggregation and Analysis.
Technical Specifications.
Local Candidate Consideration.
Job Type: Full-time
Pay: $75.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 5 years (Required)
Tableau: 5 years (Required)
Cognos: 5 years (Required)
data quality control: 5 years (Required)
Data aggregation with MMIS: 5 years (Required)
SAS and SQL coding: 3 years (Required)
technical specifications to develop reports: 5 years (Required)
Work Location: In person",-1.0,"Changing Technologies, Inc.","Raleigh, NC",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
428,Senior Data Engineer,Employer Provided Salary:$166K - $232K,"Patreon is the best place for creators to build memberships by providing exclusive access to their work and a deeper connection with their communities. We're building a content and community platform where creators can engage directly with their fans and monetize their creativity, while maintaining full ownership over the work they make and the communities they create.
We're leaders in the membership space with 250,000+ active creators and over $3.5 billion paid directly to creators on our platform. Our team is building tools to optimize the creator-to-fan relationship, including native video, enhanced podcasting features, improved creation tools, and new community experiences. We're continuing to invest heavily in building the most talented team in the Creator Economy and are looking for a Data Engineer to support our mission.
This role is available to those wishing to work in our SF and NY offices on a hybrid work model or those wishing to be fully remote in the United States.
About the Role
Work on a tight-knit team of highly motivated and experienced data engineers with frequent collaboration with data scientists, product managers and product engineers.
Work on both ""data analytics"" and ""data infrastructure"" type projects in a fast-paced, high-growth startup environment.
Build core data sets and metrics to power analytics, reports and experimentation.
Write real-time and batch data pipelines to support a wide range of projects and features including our creator-facing analytics product, executive reporting, FP&A, marketing initiatives, model training, data science analytics, A/B testing, etc.
Help manage and build out our data platform and suite of data tools.
Be a driver of a data-centric culture at Patreon. Work autonomously on large green field initiatives and help define data best practices at the company.
About You
Expert in SQL, Spark and Python or Scala
Significant experience modeling data and developing core data sets and metrics to support analytics, reports and experimentation. Solid understanding of how to use data to inform the product roadmap.
Enjoy collaborating with Data Scientists, Product Managers and Product Engineers. Comfortable playing the role of a Project Manager in order to drive results.
Have previously built real-time and batch data pipelines using tooling such as Airflow, Spark, Kafka, S3, Fivetran, Census, etc. Experience working with event tracking frameworks, data observability frameworks and experimentation frameworks.
Experience managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc.
Highly motivated self-starter that is keen to make an impact and is unafraid of tackling large, complicated problems and putting in the work to ensure high craft deliverables.
About Patreon
Patreon powers creators to do what they love and get paid by the people who love what they do. Our team is passionate about making this mission and our core values come to life every day in our work. Through this work, our Patronauts:
Put Creators First | They're the reason we're here. When creators win, we win.****
Build with Craft | We sign our name to every deliverable, just like the creators we serve.
Make it Happen | We don't quit. We learn and deliver.
Win Together | We grow as individuals. We win as a team.
We hire talented and passionate people from different backgrounds because we want our company to represent the vast diversity of our creators. If you're excited about a role but your prior experience doesn't match with every bullet point outlined above, we strongly encourage you to apply anyway. If you're a creator at heart, are energized by our mission, and share our company values, we'd love to hear from you.
Patreon is proud to be an equal-opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.
Patreon offers a competitive benefits package including and not limited to salary, equity plans, healthcare, unlimited paid time off, company holidays and recharge days, commuter benefits, lifestyle stipends, learning and development stipends, patronage, parental leave, and 401k plan with matching.
The posted range represents the expected salary range for this job requisition and does not include any other potential components of the compensation package, benefits and perks previously outlined. Ultimately, in determining pay, we'll consider your experience, leveling, location and other job-related factors.
San Francisco Pay Range
$166,000—$231,500 USD",3.2,"Patreon
3.2","San Francisco, CA",51 to 200 Employees,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
429,Data Engineer - Python,-1,"Position: Data Engineer

Location: Fully Remote

Duration: 6 months assignment with the possibility of extension

AWS, Python , All Access, ServiceNow, Partner Gateway

Job Description:
Qualifications :
Bachelor’s degree or equivalent experience.
Experience in Data Engineering and Processing tools and services in AWS.
Experience with Python scripting.
Practical experience on a Data Migration effort from on-prem to AWS using DMS and/or similar data migration services.
Strong working knowledge in AWS covering networking, security and data services
Experience with Data lakes and S3 centric data processing designs.
Ability to research, design and implement DBOps paradigm and have it incorporated in CICD automation using tools like GitLab.
Experience working on agile projects and participate in daily scrums and updates
Strong analytical ability and technical skill, as well as the ability to provide innovative solutions to technical needs and business requirements.
Strong attention to detail with a high level of data integrity and accuracy.
Proficient oral and written communication, ability to interact on required information and concepts with people at all levels of the organization.
Proficient ability to translate highly technical information into non-technical terms.
Broad knowledge of the concepts, practices, and principles of programming including design, implementation, and testing.
Ability to interact with customers, understand business requirements and collaborate with team members to explore existing system, determine areas of complexity, identify potential risk to successful implementations.
Responsibilities :
Contribute in an agile and collaborative environment to the development, testing, implementation, and review and evaluation of complex solutions.
Work as part of a team to move on-prem applications to a Cloud environment.
Contribute to the design of technology infrastructure and configurations, recommend process improvements.
Compile and maintain technical documentation, including use cases and scripts; conduct technical research and maintain viable knowledge of technology trends.
Contribute in an agile and collaborative environment to the development, testing, implementation, and review and evaluation of complex solutions.
Work as part of a team to move on-prem applications to a Cloud environment.
Contribute to the design of technology infrastructure and configurations, recommend process improvements.
Compile and maintain technical documentation, including use cases and scripts; conduct technical research and maintain viable knowledge of technology trends.",3.8,"Ampcus Incorporated
3.8",Remote,501 to 1000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
430,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
431,Data Engineer,Employer Provided Salary:$130K - $175K,"Rightworks is hiring a Data Engineer in Fort Worth for a Multi-Billion dollar private equity fund in Fort Worth. This position is fully in-office in Fort Worth (client will pay relocation if needed). Compensation is aggressive & flexible, typical 10% above your previous compensation level with tremendous growth opportunities. (Expected compensation range for this data engineer will fall somewhere between 130 & 180,000 per year depending on experience). 50 hours per week, paid lunch, casual office atmosphere.
-Looking for data engineer with multiple years of SQL Server maintenance experience.
-Optimally would like a data engineer with formal education in either MIS, Mathematics, Software Engineering, or Computer Science
-SQL maintenance, Data Extraction, Transforms, Macros
-Database maintenance
Job Type: Full-time
Pay: $130,000.00 - $175,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Tuition reimbursement
Vision insurance
Schedule:
10 hour shift
Ability to commute/relocate:
Fort Worth, TX 76102: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What compensation range are you looking for?
Are you willing to work in-person in Fort Worth, TX 50 hrs a week?
Experience:
Microsoft SQL Server: 1 year (Required)
Work Location: In person",4.8,"RightWorks Inc.
4.8","Fort Worth, TX",51 to 200 Employees,2006,Company - Private,HR Consulting,Human Resources & Staffing,$25 to $100 million (USD)
432,"Senior Data Engineer, AdTech",$121K - $165K (Glassdoor est.),"Engineering
Data
Our mission on the Advertising Product & Technology team is to build a next generation advertising platform that aligns with our unique value proposition for audio and video. We work to scale the user experience for hundreds of millions of fans and hundreds of thousands of advertisers. This scale brings unique challenges as well as tremendous opportunities for our artists and creators.
Location
New York or Remote Americas
Job type
Permanent
We are looking for Data Engineers to build and drive data engineering initiatives within our advertising and podcast monetization teams. In this role, you will be instrumental in streamlining data ingested from multiple sources to not only recognize value and insights, but to also set standards along the way. You will help us to create a user-first ad experience that's personalized and relevant, and develop and own software solutions on our fast-paced podcast and ad services technology. You will help us grow to billions of fans, increase engagement with our listeners, and provide better value to our advertisers. Above all, your work will impact the way the world experiences music and podcasts.
What You'll Do
Work closely with key partners across the ads organization, contributing to the improvement of many different pipelines and services
Build new distributed data pipelines across ad experience
Use best practices in continuous integration and delivery
Help drive optimization, testing and tooling to improve reliability and data quality
Apply a Data centric approach to all the platforms, pipelines and engineering activities
Jump into data pipelines, identify issues, and propose solutions across teams
Design, develop, and maintain Java services
Work in cross-functional agile teams to continuously experiment, iterate and deliver on new product objectives
Who You Are
You have a strong understanding of data systems
You are knowledgeable and passionate about improving and building new distributed data pipelines
You are knowledgeable about data modeling, data access, and data storage techniques
You are familiar with current engineering practices such as distributed architecture, and are curious about new technologies that help derive insights and value from data
You have experience working on and building distributed data pipelines that ingest huge amounts of data across multiple sources and brands
You are very comfortable working with datasets in SQL (we also use Scio!)
You are comfortable in at least one core language like Python, Java or Scala
You have experience working with Apache Beam, Hadoop or a similar streaming data technology
You have experience with GCP (preferred) or AWS
You enjoy close collaboration with backend and ML engineers, and are passionate about software architecture across the stack
Ad Tech and/or Podcast Monetization experience is a plus
Where You'll Be
We are a distributed workforce enabling our band members to find a work mode that is best for them!
Where in the world? For this role, it can be within the Americas region in which we have a work location and is within working hours: https://lifeatspotify.com/locations
Working hours? We operate within the Eastern Standard time zone for collaboration and ask that all be located in that time zone.
Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here.
Our global benefits
Extensive learning opportunities, through our dedicated team, GreenHouse.
Flexible share incentives letting you choose how you share in our success.
Global parental leave, six months off - fully paid - for all new parents.
All The Feels, our employee assistance program and self-care hub.
Flexible public holidays, swap days off according to your values and beliefs.
Learn about life at Spotify
The United States base range for this position is $156,275 - $223,25, plus equity. The benefits available for this position include health insurance, six month paid parental leave, 401(k) retirement plan, monthly meal allowance, 23 paid days off, 13 paid flexible holidays. This range encompasses multiple levels. Leveling is determined during the interview process. Placement in a level depends on relevant work history and interview performance. These ranges may be modified in the future.

Spotify is an equal opportunity employer. You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens.

Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service.",4.0,"Spotify
4.0","New York, NY",5001 to 10000 Employees,2006,Company - Public,Internet & Web Services,Information Technology,Unknown / Non-Applicable
433,Data Engineer,$79K - $115K (Glassdoor est.),"Job Title :- Data Engineer
Location:- San Antonio, TX
Required:- Active Top Secret Clearance
We are looking for a talented Data Engineer to support the acquisition of mission-critical and mission-support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is performed mostly on customer site in Columbia, MD.
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.
Minimum Qualifications
Security Clearance - Must have a current Secret level security clearance and therefore all candidates must be a U.S. Citizen with a willingness to go to TS/SCI and take the CI poly after starting the position.
5 years experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.
Experience with programming languages such as Python and Java.
Proficiency with acquisition and understanding of network data and the associated metadata.
Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
Experience with Kibana and Elasticsearch.
Familiarity with various log formats such as JSON, XML, and others.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must have a Security+ or similar certification or the ability to obtain it immediately.
Must be able to work on customer site in Columbia, MD at least 4 days a week. (Subject to change)
Desired Skills (Optional)
Experience with NOSQL databases such as Accumulo desired
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
Job Type: Full-time
Schedule:
8 hour shift
Security clearance:
Top Secret (Preferred)
Work Location: In person",3.9,"Helm360
3.9","San Antonio, TX",201 to 500 Employees,1997,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
434,Data Engineer,$77K - $104K (Glassdoor est.),"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Paid time off
Referral program
Vision insurance
Schedule:
Monday to Friday
No nights
No weekends
Ability to commute/relocate:
Atlanta, GA 30309: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 2 years (Required)
Language:
English (Required)
Work Location: Hybrid remote in Atlanta, GA 30309",3.7,"United Digestive
3.7","Atlanta, GA",501 to 1000 Employees,2018,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
435,"Data Engineer, Election Platforms (all-levels)",-1,"Job Description
The Washington Post is hiring a data engineer for our Election Platforms team, to work on our data pipelines, tools, and infrastructure ahead of the 2024 presidential election.
The Election Platforms team focuses on leveraging data, partnering with stakeholders, and pioneering serviceable tools to facilitate The Post’s election coverage. This role is part of Elections Engineering, a cross-functional group of full-stack engineers and data scientists, with varied technical and academic backgrounds, responsible for delivering live election results across The Post’s platforms.
The role will be focused on contributing to our technical stack and developing new technology to facilitate robust and lively election coverage. In particular, we’re looking for candidates who would be comfortable working on deploying high-impact code in production and have an interest in data and newsroom tooling. We also work on other data engineering projects related to elections, such as voter registration data and campaign finance, and occasionally write
open
source
software
. Most of our work is in Python and TypeScript/React/Node.
We typically work at a higher level of abstraction than a single story, and aim to create general purpose tools that are reusable and easy to deploy. Our ideal candidates are creative problem-solvers, who are comfortable writing production level code to solve real-world newsroom problems. This position is remote-friendly for work in the US.
Motivation
You have a desire to contribute to The Post’s journalism through technology.
You are passionate about refining and improving team process, culture, documentation and communication while tackling complex data science and engineering challenges.
You want to work at a deeper level of abstraction than a single story or beat.
You are excited about learning new technologies and applying them to the practice of journalism.
Responsibilities
Work on the software components of our live election architecture including deployment, observability, participating in code reviews, and more.
Work closely with our engineers and data scientists to develop and maintain pipelines and administrative apps/dashboards for election data.
Work on tooling to make data preparation easier and more efficient for our live election model.
Work with newsroom stakeholders to assess needs, build useful and reusable tooling, and contribute to storytelling.
Foster effective team collaboration and communication by improving and documenting internal tools, processes and guidelines.
Assist in attracting and hiring team members who want to work at the intersection of journalism and technology by actively participating in recruiting – and contributing feedback on our overall hiring process.
Qualifications
A minimum of one year of experience building and maintaining backend and data engineering applications.
Proficiency across some of the languages and frameworks used in our engineering stack such as Python, TypeScript/React/Node, and AWS services.
A desire to work on cutting edge, cross-functional tools to help with features, data science, and machine learning/big data work in a team setting.
Experience with election data is not necessary, but is a plus.
Experience developing and deploying newsroom tools and applications is not necessary, but is a plus.
Levels
If you have less than three years, but at least one year, we’d love to evaluate you for a data engineer role.
If you have more than three years but less than about seven years, we’d love to evaluate you for a senior data engineer role.
If you have more than about seven years, we’d love to talk to you about a principal data engineer role.
Wherever you are in your life or career, The Washington Post offers comprehensive and inclusive benefits for every step of your journey:
Competitive medical, dental and vision coverage
Company-paid pension and 401(k) match
Three weeks of vacation and up to three weeks of paid sick leave
Nine paid holidays and two personal days
20 weeks paid parental leave for any new parent
Robust mental health resources
Backup care and caregiver concierge services
Gender affirming services
Pet insurance
Free Post digital subscription
Leadership and career development programs
Benefits may vary based on the job, full-time or part-time schedule, location, and collectively bargained status.
The Post strives to provide its readers with high-quality, trustworthy news and information while constantly innovating. That mission is best served by a diverse, multi-generational workforce with varied life experiences and perspectives. All cultures and backgrounds are welcomed.
The innovation doesn’t end in the Newsroom – dozens of teams power The Washington Post. We are now hiring the next innovator – how will you Impact Tomorrow?
#washpostlife",4.2,"The Washington Post
4.2","Washington, DC",1001 to 5000 Employees,1877,Company - Private,Publishing,Media & Communication,Unknown / Non-Applicable
436,Data Engineer,-1,"Company Information
Clinical ink is the global life science company that brings data, technology, and patient science together to unlock clinical discovery. Our deep therapeutic-area expertise, coupled with Direct Data Capture, eCOA, eConsent, telehealth, neurocognitive testing, and digital biomarkers advancement, drive the industry standard for data precision and usher in a new generation of clinical trials. With offices in Philadelphia, PA, Winston Salem, NC, and Iowa City, IA, Clinical ink is rewriting the clinical development experience.

Job Description
Clinical ink is seeking a Data Engineer to join our Data Team based remotely across the United States! The Data Engineer will work to develop solutions used in applications for clinical trials. The ideal candidate will be a minimum of two years of experience as a software engineer and prior experience working with a variety of tools and frameworks. The Data Engineer's responsibilities include:
Develop data engineering solutions used in applications for clinical trial data collection that both make data available for further use and generate value out of data
Contribute to the methodology by which advanced analytics projects are delivered to clients and codify the tooling needed to support them
Build and support tools that allow data analysts and data scientists to work in complex projects
Implement quality, availability, and integrity of code, solutions, and respective systems and follow best practices related to data integrity, security, scalability, etc.
Participate in code inspections, reviews, and other activities to ensure quality
Qualifications
Bachelors in Mathematics, Statistics, Computer Engineering, Computer Science, or related field of study
2-3 years of experience in software engineering, working on multi-discipline teams
Experience with a variety of tools and frameworks such as Snowflake, Airflow, Spark, Kafka, RedShift, Sage Maker, Kubernetes, etc., AWS ecosystem (Lambda, Glue, S3, E2C, etc.), programming tools and querying languages (i.e., Python, C++, SQL, Scala, Java, etc.)
At least 2+ years of experience with Python
Data modelling and database development experience required
Data visualization experience preferred in Tableau and/or AWS QuickSight
Nice to have experience with issue tracking tools such as JIRA and Confluence
Ability to think creatively and take initiative; ability to learn new technical topics and develop new technical skills quickly
Willingness to learn and explore bleeding-edge/cutting-edge technologies
Additional Information
Clinical ink is an equal opportunity employer and does not discriminate against otherwise qualified applicants on the basis of race, color, creed, religion, ancestry, age, sex, marital status, national origin, disability or handicap, or veteran status.
www.clinicalink.com",4.2,"Clinical Ink
4.2",Remote,201 to 500 Employees,2007,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
437,Data Engineer,-1,"Center 2 (19050), United States of America, McLean, Virginia
Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 2 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
3+ years of experience in application development including Python, SQL, Scala, or Java
1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
2+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
1+ years experience working on real-time data and streaming applications
1+ years of experience with NoSQL implementation (Mongo, Cassandra)
1+ years of data warehousing experience (Redshift or Snowflake)
2+ years of experience with UNIX/Linux including basic commands and shell scripting
1+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
No agencies please. Capital One is an Equal Opportunity Employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",4.1,"Capital One
4.1","McLean, VA",10000+ Employees,1994,Company - Public,Banking & Lending,Financial Services,$10+ billion (USD)
438,Lead Data Engineer,$116K - $156K (Glassdoor est.),"Sony Music Publishing is the leading global music publisher, which is home to world-class songwriters, legendary catalogues, and industry-leading synchronization licensing and production music businesses. With an international network of 38 offices, Sony Music Publishing represents many of the most iconic songs ever written by celebrated songwriters such as The Beatles, Bob Dylan, Aretha Franklin, Marvin Gaye, Michael Jackson, Carole King, Queen and The Rolling Stones, as well as contemporary superstars including Beyoncé, Ed Sheeran, Pharrell Williams, Lady Gaga, P!nk, and Sam Smith.
Job Purpose: To lead new solutions for backend infrastructure & databases, and AWS cloud services.

What You’ll Do:
A Lead Data Engineer within our Enterprise Systems will lead new solutions that seamlessly integrate the back end of a website and/or application. The applicant must have an in-depth knowledge of design of modern programming languages & web technologies, backend infrastructure & databases, and AWS cloud services to be effective. This role will have some managerial oversight of junior engineers including delegation of tasks and mentorship.

Who You Are:
Basic Qualifications
5+ years of AWS experience with some years including team oversight.
AWS Serverless, Cloud Security, DevOps, Cloud Migration, Containers
Build pixel-perfect Web UI in: HTML, CSS, SAAS/Less and JavaScript
Translate high-level requirements into wireframes, user flows, and interactive prototypes.
Angular/React Material for front end development and Integration.
Knowledgeable in SQL/NOSQL; Aurora, Redshift, Dynamo DB
Knowledge in modern data engineering languages and AWS features including Angular, React, Node JS, Chart JS, Lambda, CloudWatch
Collaborating with team members, Product Managers, subject matter experts, and other teams to refine requirements and translate into functional software using standardized coding techniques and conventions.
Provide leadership and on development projects working closely with the product manager, business analyst, and solutions architect to develop the technical design and solutions.
Knowledgeable about all development life cycle phases and solution delivery for cloud systems with experience in unit, integration testing and strong documentation.
Ability to work hybrid work schedule (remote & onsite from our Nashville office)
Must be authorized to work in the United States
7.5-hour business workday but variations in work volume frequently require extended working hours for evening and late-night events
Nice to Haves
Experience with CRM/Finance/Accounting systems a plus.
Knowledge of the software development lifecycle and concepts such as Agile, SAFE, scrum, CI/CD, and DevOps.
Knowledge of coding best practices such as CI/CD, Cloud Security, DevOps.
Knowledge on ETL solutions such as Glue & Lambda functions.
Knowledge on dashboards and Dashboard platforms such as AWS QuickSight, Google Looker, Tableau, PowerBI.
Knowledge of big data technologies like AWS Redshift, Hive and Spark.
Experience with AWS Database Migration Service (DMS)
AWS Certification.

What We Give You:
Relocation Assistance Available
You join an inclusive, collaborative and global community where you have the opportunity to fuel the creative journey
A modern office environment designed to foster productivity, creativity, and teamwork
An attractive and comprehensive benefits package including medical, dental, vision, life & disability coverage, and 401K + employer matching
Voluntary benefits like company-paid identity theft protection and resources for pets, mental health and meditation resources, industry-leading fertility coverage, fully paid leave for childbirth or bonding, fully paid leave for caregivers, programs for loved ones with developmental disabilities and neurodiversity, subsidized back-up child and elder care, and reimbursement for adoption, surrogacy, tuition and student loans
We invest in your professional growth & development
Flexible Time Off
Time off for a winter recess
Sony is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, sex (including pregnancy), gender, national origin, citizenship, ancestry, age, physical or mental disability, military status, status as a veteran or disabled veteran, sexual orientation, gender identity or expression, marital or family status, genetic information, medical condition, or any other basis protected by applicable federal, state, or local law, ordinance, or regulation.
EEO is the Law
EEO is the Law Supplement
Right to Work (English/Spanish)
E-Verify Participation (English/Spanish)
As part of our commitment to health and safety, this position requires that the job candidate be fully vaccinated against COVID-19. Please note that an applicant will be considered fully vaccinated two weeks after their second dose in a 2-dose vaccine series (Pfizer or Moderna), or two weeks after a single-dose vaccine (Johnson & Johnson’s Janssen vaccine). The Company will consider requests for reasonable accommodations for documented medical reasons and for sincerely held religious beliefs in accordance with applicable law. Please do not include proof of vaccine status or any indication of a possible request for an accommodation when submitting your application materials. If applicable, the Company will follow up with you directly to request proof of vaccination and to discuss any potential accommodations.
Sony is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, sex (including pregnancy), gender, national origin, citizenship, ancestry, age, physical or mental disability, military status, status as a veteran or disabled veteran, sexual orientation, gender identity or expression, marital or family status, genetic information, medical condition, or any other basis protected by applicable federal, state, or local law, ordinance, or regulation.
EEO is the Law
EEO is the Law Supplement
Right to Work (English/Spanish)
E-Verify Participation (English/Spanish)
While SMP does not require employees to be vaccinated against COVID-19, there are certain Sony offices that require employees to be vaccinated in order to enter. If you will be located at or travel to those offices, you will be required to be fully vaccinated to enter. The Company will consider requests for reasonable accommodations for documented medical reasons and for sincerely held religious beliefs in accordance with applicable law. Please do not include proof of vaccination status or any indication of a possible request for a vaccination accommodation when submitting your application materials. If applicable, the Company will follow up with you directly to request proof of vaccination and to discuss any potential accommodations.",4.2,"Sony Electronics
4.2","Nashville, TN",10000+ Employees,1946,Subsidiary or Business Segment,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
439,Data Engineer II,$90K - $119K (Glassdoor est.),"Data Engineer II
Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What’s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.
Join us to do the best work of your career and make a profound social impact as a Data Engineer on our Growth Marketing team in Round Rock, Texas, Hopkinton, Massachusetts, or Remote United States
What you’ll achieve
As a Data Engineer II on a growing team, you will bring in your industry experience to develop, deploy, and maintain productionized data pipelines that support the training and productionization of machine-learning models.





R227305",4.2,"Dell Technologies
4.2","Round Rock, TX",10000+ Employees,1984,Company - Public,Information Technology Support Services,Information Technology,$10+ billion (USD)
440,"Data Center Network Engineer, Infrastructure Network Engineering",-1,"What to Expect
Tesla is currently seeking a Network Engineer to join our Data Center team. This role will provide network design, implementation, and operational support for Tesla's Data Centers.
What You’ll Do
Help design, build and maintain new and existing Data Centers
Work closely with other team members on design and initiatives; maintain and grow existing data center networks.
Work with Tesla’s key application teams to support their growth, including Tesla Autopilot team.
Provide high availability & reliability to network
Requirements gathering, analyze, and propose solution to networking needs.
Monitor, analyze, and report metrics of network services.
Develop automation methods to rapidly deploy, configure, and update network equipment.
Assist with network troubleshooting.
Conduct product POC evaluation.
Document network knowledge base and operational “Run-Book.”
Must be able to work occasional weekends, after hours, and holidays.
Participate in on call rotation.
May require unscheduled after-hours work. 10-20% travel required as necessary.
What You’ll Bring
4+ years’ experience mid-large global enterprise networking infrastructure
Experience with mid/large-scale networks in a global environment
Juniper, Arista and Palo Alto Networks hardware
Experience in IP networking, L2/L3 network protocols (spanning-tree, OSPF, BGP), TCP/IP, DHCP, DNS, end to end QOS, VLAN, VRRP, LACP, MC-LAG, EVPN with VXLAN, ACL and infrastructure cabling.
Basic knowledge of AWS, Azure or GCP.
Experience with various tools such as Protocol Analyzer, SNMP, flow, IPAM, RADIUS, Splunk, network taps, and load/stress testing",3.6,"Tesla
3.6","Fremont, CA",10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$1 to $5 billion (USD)
441,Senior Data Engineer (Remote),Employer Provided Salary:$91K - $155K,"** This role is remote; if in area near an ICF office, hybrid may be recommended.**
Our Public Sector Group (PSG) is growing and we are looking for a talented, highly motivated Senior Data Engineer to join our Engineering and Emerging Technologies (EET) line of business for supporting a large government contract in the DC metro area.
Key Responsibilities:
Responsible for the data engineering on a large-scale technical modernization project in accordance with contract requirements and company policies, procedures and guidelines. Strong experience in I.T. modernization projects. Has a good mix of data requirements and data engineering, and knowledge of I.T. modernization efforts.
What you’ll be doing:
Support technical Data Warehouse/BI tasks for a major federal initiative, working as part of an extended system development team on project execution.
Develop extract, transform, and load (ETL) processing routines and data feeds, creating necessary data structures and data models to support data at all stages.
Perform extensive data profiling and analysis based on client data.
Work with UI and business analysis team members and the client to define BI and reporting requirements.
Design and implement custom data analytic and BI/reporting products, custom reports, and data visualization products.
What you must have:
Bachelor’s degree (e.g., Computer Science, Engineering or related discipline)
8+ years of experience in SQL and/or Python coding language.
8+ year of experience with procedural, functional, and object-oriented programming
3+ years of experience developing database ETL environments with business intelligence applications
3+ years of experience with AWS database, analytics, and compute services such as RDS/Aurora, AWS Glue, and Lambda
3+ years of experience with data warehouse design and development with Amazon Redshift
3+ years of experience working with BI tools such as Tableau, PowerBI or AWS Quicksight
3+ years of development experience in a DevSecOps environment, with programming languages such as Java, Node, or Python
Employment must be compliant with eligibility for Public Trust Clearance due to Government Contract.
Candidate must reside in the US and be a US Citizen
What we’d like you to know:
Understand ETL concepts of data flow, data enrichment, data consolidation, change data capture and transformation
Understand database concepts of referential integrity, indexes and keys and table metadata
Demonstrated experience showing strong critical thinking and problem-solving skills paired with a desire to take initiative
Knowledge of Big Data integration tools such as Hive, Airflow, Storm, Spark, AWS Kinesis, and Kafka
Experience building CI/CD pipelines with tools such as Jenkins and CodeBuild
Experience with containerization platforms including Docker
Experience with Agile development process
Technologies you’ll use in this role:
SQL, Node, Python, Tableau, Quicksight, Aurora, Lambda, Sequelize
Aurora, AWS Glue, S3, RedShift, AWS Kinesis
Git, Terraform, CodeBuild
Professional Skills:
Must have excellent written and oral communications skills.
Must be comfortable working in a fast-paced, matrixed team environment, with a client-centric culture, and an environment of high performers.
Must be able to multi-task and shift priorities as needed.
Why you’ll love working here:
Comprehensive health benefits
Generous vacation and retirement plans
Employee support program
Participation in charity initiatives
About ICF International:
ICF International (NASDAQ:ICFI) partners with government and commercial clients to deliver professional services and technology solutions in the energy and climate change; environment and infrastructure; health, human services, and social programs; and homeland security and defense markets. The firm combines passion for its work with industry expertise and innovative analytics to produce compelling results throughout the entire program life cycle, from research and analysis through implementation and improvement. Since 1969, ICF has been serving government at all levels, major corporations, and multilateral institutions. More than 4,000 employees serve these clients worldwide. ICF's Web site is
www.icf.com
ICF offers an excellent benefits package, an award-winning talent development project, and fosters a highly skilled, energized and empowered workforce.
ICF is an equal opportunity employer that values diversity at all levels. (EEO/AA – Minorities/Females/Veterans/Individuals with Disabilities)
For a listing of other career opportunities at ICF, please visit our Career Center at
www.icf.com/careers
Working at ICF
ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our
EEO & AA policy
.
Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email
icfcareercenter@icf.com
and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination:
Know Your Rights
and
Pay Transparency Statement.

Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
$90,940.00 - $154,598.00
Nationwide Remote Office (US99)",3.8,"ICF
3.8","Reston, VA",5001 to 10000 Employees,1969,Company - Public,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
442,Data Scientist / ML Engineer (REMOTE /Contract),-1,"Role: Data Scientist / Machine Learning Engineer
Location: REMOTE
Duration: 6+ months
Scheduling Interviews
Skills/Experience:
6+ years of experience working in Data Science and Machine Learning field.
Extensive experience in applying data preprocessing, statistical analysis, data analytics tools, predictive modelling, model deployment and evidence-based approaches to find lean, actionable solutions to various real-world enterprise business problems.
Experience in the application of Supervised and Unsupervised Learning algorithms, Naïve Bayes, Regression Analysis, Neural Networks/Deep Learning, Support Vector Machines (SVM), Random Forest, K-Means, Hierarchical, Spectral clustering, DBSCAN, Collaborative Filtering and other advanced machine learning techniques.
Used AWS Cloud Services Sagemaker, ECS, EKS, S3, Redshift, QuickSight
Experience with Google Cloud Platform (GCP) AutoML, Vertex AI, BigQuery, Colab, DataProc, Data Studio,
Exposure to Azure ML, PowerBI, Azure SQL and DataBricks
Design custom BI reporting dashboards or interactive data visualizations and widgets in R and Python using Shiny, Tableau, Ggplot2, Plotly, Matplotlib, and Seaborn.
Produce custom BI reporting dashboards in R and Python using Shiny, and Plotly for rapid dissemination of actionable, data driven insights.
Strong experience in Software Development Life Cycle (SDLC), MLOPS
Experience in working with relational databases (Teradata, Oracle) with advanced SQL skills.",3.5,"Glow Networks
3.5",Remote,51 to 200 Employees,2003,Subsidiary or Business Segment,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
443,Senior Data Engineer,Employer Provided Salary:$143K - $300K,"The Data team leverages data from our autonomous vehicles and operations to determine autonomy and service readiness. We provide the foundation for strategic decision-making at Zoox. You will develop and implement the next generation of our data pipeline to ensure visibility into our business as we scale toward the launch of an autonomous mobility service. You will define the system and build the pipeline to enable Zoox to develop and scale with a data-first culture.

You will join a diverse, experienced team with rapidly growing scope and responsibility while also having access to one of the most unique data sets in the autonomous vehicle industry. Hence, we are seeking all skill levels to grow with the team.
Responsibilities
Designing, building, and maintaining the infrastructure that transforms autonomous vehicle data at scale to support analytics throughout the company
Defining and executing on how data from perception, prediction, planning and other parts of the autonomous stack is consumed to generate valuable insights by data scientists, engineers, and business users
Establishing robust data integrity monitoring so that company-wide metrics are based on accurate data
Partnering with engineering and product teams to define data consumption patterns and establish best practices
Qualifications
Experience designing and building complex data infrastructure at scale
Exceptional Python or Scala skills
Advanced SQL and data warehousing experience
Experience operating a workflow manager such as Airflow
Experience with large scale streaming platforms (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Hadoop) and storage engines (e.g. HDFS, HBase)
A strong DataOps mindset and opinions on next-generation warehousing tools
Bonus Qualifications
Basic fluency in C++
Familiarity with or exposure to experimentation platforms
Compensation
There are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights. The salary will range from $143,000 to $300,000. A sign-on bonus may be part of a compensation package. Compensation will vary based on geographic location, job-related knowledge, skills, and experience.

Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance.

ABOUT ZOOX

Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of artificial intelligence, robotics, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.

Follow us on LinkedIn

Vaccine Mandate
Employees working in this position will be required to have received a vaccine approved by the U.S. Food and Drug Administration and/or the World Health Organization. In addition, employees who are eligible for a COVID-19 booster vaccine (“Booster”) will be required to receive a Booster. Employees will be required to show proof of vaccination status upon receipt of a conditional offer of employment. That offer of employment will be conditioned upon, among other things, an Applicant’s ability to show proof of vaccination status. Please note the Company provides reasonable accommodations in accordance with applicable state, federal, and local laws.

About Zoox
Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of robotics, machine learning, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.

Follow us on LinkedIn

A Final Note:
You do not need to match every listed expectation to apply for this position. Here at Zoox, we know that diverse perspectives foster the innovation we need to be successful, and we are committed to building a team that encompasses a variety of backgrounds, experiences, and skills.",4.0,"Zoox
4.0","Foster City, CA",1001 to 5000 Employees,2014,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
444,Data Communications Engineer - III,Employer Provided Salary:$91K - $181K,"Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","San Francisco, CA",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
445,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
446,Sr. Data Engineer,$83K - $115K (Glassdoor est.),"Provides application design, analysis, programming, testing and documentation for the development, integration, enhancement, and maintenance of information systems solutions
Job Description:
BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Job Responsibilities
Formulates and defines system scope and objectives through research and fact-finding to design, develop, modify, or integrate complex information systems.
Devises or modifies application systems and procedures to optimize functional requirements including capacity, operating time, response time, and form of desired results.
Designs, codes, tests, debugs, and documents programs, subroutines, and scripts.
Develops and implements program/system test plans. Devises data verification methods and standard system procedures.
Responds to system failures and performance events by taking appropriate measures to reduce system downtime and eliminate recurrence of problems.
May conduct research on software and hardware products to justify recommendations and support management in budgeting and purchasing efforts.
Acts as expert technical resource to development staff in all phases of the development and implementation process.
Job Qualifications
Education
Bachelors Degree in Computer Science or equivalent work experience required. Equivalent years of experience are determined as one year of technical experience for every year of college requested.
Experience
1 year - Experience with information technology concepts, application development methodology, terminology, and standards required
5 years - Systems and programming experience required
1 year - Expert working knowledge of at least one programming language in use at BCBST.
Skills\Certifications
Proven skill with information technology in use at BCBST is required
Proven analysis, design, and coding skills, and demonstrated success in leading large and complex projects
Top level technical experience in one or more specialized areas of applications systems analysis and programming
Demonstrated ability to interpret and translate technical and/or or complex concepts into information meaningful to project team members and/or business personnel.
Be organized, reliable, and able to manage multiple tasks with exceptional work ethic.
Leadership skills
Must be able to communicate effectively with both technical and non-technical co-workers
Job Specific Requirements:
Preference for candidates with Cloud experience (GCP specifically)
Python knowledge/experience a plus
Preferred Skills:
Number of Openings Available:
1
Worker Type:
Employee
Worker Sub-Type:
Employee
Company:
BCBST BlueCross BlueShield of Tennessee, Inc.
Applying for this job indicates your acknowledgement and understanding of the following statements:
BCBST is an Equal Opportunity employer (EEO), and all employees and applicants will be entitled to equal employment opportunities when employment decisions are made. BCBST will take affirmative action to recruit, hire, train and promote individuals in all job classifications without regard to race, religion, color, age, sex, national origin, citizenship, pregnancy, veteran status, sexual orientation, physical or mental disability, gender identity, or any other characteristic protected by applicable law.
Further information regarding BCBST's EEO Policies/Notices may be found by reviewing the following page:
BCBST's EEO Policies/Notices
BlueCross BlueShield of Tennessee is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at BlueCross BlueShield of Tennessee via-email, the Internet or any other method without a valid, written Direct Placement Agreement in place for this position from BlueCross BlueShield of Tennessee HR/Talent Acquisition will not be considered. No fee will be paid in the event the applicant is hired by BlueCross BlueShield of Tennessee as a result of the referral or through other means.
All applicants will be advised that BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Tobacco-Free Hiring Statement
To further our mission of peace of mind through better health, effective 2017, BlueCross BlueShield of Tennessee and its subsidiaries no longer hire individuals who use tobacco or nicotine products (including but not limited to cigarettes, cigars, pipe tobacco, snuff, chewing tobacco, gum, patch, lozenges and electronic or smokeless cigarettes) in any form in Tennessee and where state law permits. A tobacco or nicotine free hiring practice is part of an effort to combat serious diseases, as well as to promote health and wellness for our employees and our community. All offers of employment will be contingent upon passing a background check which includes an illegal drug and tobacco/nicotine test. An individual whose post offer screening result is positive for illegal drugs or tobacco/nicotine and/or whose background check is verified to be unsatisfactory, will be disqualified from employment, the job offer will be withdrawn, and they may be disqualified from applying for employment for six (6) months from the date of the post offer screening results.
Resources to help individuals discontinue the use of tobacco/nicotine products include smokefree.gov or 1-800-QUIT-NOW.",3.8,"BlueCross BlueShield of Tennessee
3.8","Chattanooga, TN",5001 to 10000 Employees,1945,Nonprofit Organization,Insurance Carriers,Insurance,$5 to $10 billion (USD)
447,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
448,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
449,Senior Data Engineer,$130K - $170K (Glassdoor est.),"Responsibilities:
Who We Are:
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners - in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
Pandora
Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. Pandora provides consumers with a uniquely-personalized music and podcast listening experience with its proprietary Music Genome Project® and Podcast Genome Project® technology. Pandora is available through its mobile app, the web, and integrations with more than 2,000 connected products.
How you’ll make an impact:
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What you’ll do:
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What you’ll need:
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to pay attention to details and be organized.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $87,200 to $180,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.

Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.",3.6,"Sirius XM
3.6","Oakland, CA",1001 to 5000 Employees,1990,Company - Public,Broadcast Media,Media & Communication,$1 to $5 billion (USD)
450,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
451,Data Engineer,$76K - $101K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
The Data Governance/Data Quality Subject Matter Expert (SME) is a professional responsible for designing, implementing, and managing the client’s data governance strategy and ensuring high data quality standards across data-centric operations and decision-making.
The SME is in charge of developing and maintaining robust data pipelines, ensuring data observability, managing data within the Microsoft Azure environment, and implementing data governance principles in alignment with industry best practices and regulatory standards.
Primary Responsibilities and Accountabilities:
Data Governance & Quality: This includes developing and implementing data governance policies and procedures, monitoring data quality, managing data within the Microsoft Azure environment, and ensuring compliance with data privacy regulations.
Data Pipeline & Workflow Development: Designing and implementing efficient and secure data pipelines in Azure Data Factory and workflows, ensuring optimal performance and incorporating data quality principals.
Data Integration & Observability: Designing data integration strategies, integrating data from multiple sources, ensuring data quality and observability through tools like Azure Synapse.
Data Security: Defining and implementing data security policies and procedures, including access control and data encryption within the Azure environment.
Performance Tuning: Optimizing data access, indexing, and query performance within Azure Data Factory to ensure scalable and reliable data infrastructure.
Monitoring & Troubleshooting: Monitoring data systems, troubleshooting issues, identifying performance bottlenecks, and collaborating with team members to resolve problems.
Collaboration & Communication: Working closely with stakeholders to understand their data needs and develop solutions adhering to industry best practices and the needs of the business.
Automation & Tooling: Automating repetitive tasks and maintaining tools necessary for data processing and storage within the Azure environment.
Qualifications
A Bachelor's degree in a related field such as computer science, mathematics, engineering, or information systems.
3+ years of hands-on experience programming in SQL.
2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes within the Microsoft Azure environment.
Experience in Data Governance OR Data Quality Management is a must.
Skills
Competencies:
Expertise in Microsoft Azure data tools, with a focus on Azure Data Factory and Azure Synapse for data processing, storage, and analysis.
Strong SQL skills and knowledge of data warehousing concepts and ETL (Extract, Transform, Load) processes within the Azure environment.
Proficiency in programming languages such as Python, and experience with big data technologies such as Hadoop, Spark, and Kafka.
Ability to design, build, and maintain data pipelines, with a particular focus on Azure Data Factory.
Strong analytical and problem-solving skills, with the ability to identify and resolve data-related issues effectively.
Experience with data security and privacy regulations, and understanding of HIPAA compliance.
Strong experience with data governance, data quality, and data observability principles.
Excellent written and verbal communication skills, ability to explain complex technical concepts to non-technical stakeholders.
Excellent collaboration and teamwork skills, with the ability to work in a team involving client data engineers, data scientists, and data analysts.
Strong attention to detail and commitment to ensuring high data quality.
Continuous learning mindset and ability to stay up-to-date with new developments and technologies in data governance and data quality
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
452,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
453,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
454,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1.0,Percept Health,Remote,-1,-1,-1,-1,-1,-1
455,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
456,Cloud Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"We are seeking a highly experienced Principal Data Engineer with a deep understanding of PySpark using Databricks or AWS Glue or AWS EMR and cloud-based databases such as Snowflake. Proficiency in workflow management tools like Airflow is essential. Healthcare industry experience is a significant advantage. The ideal candidate will be responsible for designing, implementing, and maintaining data pipelines while ensuring the highest performance, security, and data quality.
Responsibilities:
Design, develop, and maintain scalable, reliable, and secure data pipelines to process large volumes of structured and unstructured healthcare data using PySpark and cloud-based databases.
Collaborate with data architects, data scientists, and analysts to understand data requirements and implement solutions that meet business and technical objectives.
Leverage AWS or Azure cloud services for data storage, processing, and analytics, optimizing cost and performance.
Utilize tools like Airflow for workflow management and Kubernetes for container orchestration to ensure seamless deployment, scaling, and management of data processing applications.
Develop and implement data ingestion, transformation, and validation processes to ensure data quality, consistency, and reliability across various healthcare datasets.
Monitor and troubleshoot data pipelines, proactively identifying and resolving issues to minimize downtime and ensure optimal performance.
Establish and enforce data engineering best practices, ensuring compliance with data privacy and security regulations specific to the healthcare industry.
Continuously evaluate and adopt new tools, technologies, and frameworks to improve the data infrastructure and drive innovation.
Mentor and guide junior data engineers, fostering a culture of collaboration, learning, and growth within the team.
Collaborate with cross-functional teams to align data engineering efforts with broader organizational goals and strategies.
Is familiar with SOC 2 compliance and its impact on company policies and processes.
Understands the importance of adhering to SOC 2 requirements and maintains an effort to do so.
Reviews and understands the Employee Handbook, and internal policies that define individual security responsibilities, and maintains segregation of duties in accordance with their role requirements.
Requirements:
Bachelor's or Master’s degree in Computer Science, Engineering, or a related field.
8+ years of experience in data engineering, with a strong background in Apache Spark and cloud-based databases such as Snowflake.
Strong Knowledge in Big Data Technologies, Databricks, AWS Services, and PySpark, thorough in one or more programming languages like Python.
Proven experience with AWS or Azure cloud services for data storage, processing, and analytics.
Expertise in workflow management tools like Airflow and container orchestration systems such as Kubernetes.
Strong knowledge of SQL and NoSQL databases, as well as data modeling and schema design principles.
Familiarity with healthcare data standards, terminologies, and regulations, such as HIPAA and GDPR, is highly desirable.
Excellent problem-solving, communication, and collaboration skills, with the ability to work effectively in cross-functional teams.
Demonstrated ability to manage multiple projects, prioritize tasks, and meet deadlines in a fast-paced environment.
A strong desire to learn, adapt, and contribute to a rapidly evolving data landscape.
Benefits:
401(k).
Dental Insurance.
Health insurance.
Vision insurance.
We are an equal-opportunity employer and value diversity, equality, inclusion, and respect for people.
The salary will be determined based on several factors including, but not limited to, location, relevant education, qualifications, experience, technical skills, and business needs.
Additional Responsibilities:
Participate in OrangePeople monthly team meetings, and participate in team-building efforts.
Contribute to OrangePeople technical discussions, peer reviews, etc.
Contribute content and collaborate via the OP-Wiki/Knowledge Base.
Provide status reports to OP Account Management as requested.
About us:
OrangePeople is an Enterprise Architecture and Project Management solutions company. Our most valuable asset is our people: dynamic, creative thinkers, who are passionate about doing quality work. As a member of the OrangePeople team, you will have access to industry-leading consulting practices, strategies & technologies, innovative training & education. An ideal Orange Person is a technology leader with a proven track record of technical achievements and a strong process/methodology orientation.
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Application Question(s):
Do you require sponsorship for this job?
Work Location: Remote",4.1,"OrangePeople
4.1",Remote,51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
457,Data Engineer,$100K - $144K (Glassdoor est.),"Position: Data Engineer
Duration: Contract
Location: Fremont, California
Skills:
Strong in SQL and Python
Data warehousing concepts , Data Modeling and RDBMS concepts
Good to have exposure to big data technologies – Apache Spark, Kafka, Airflow, NoSQL
Good to have exposure on Insurance & Finance business processes
Regards
Tejash Gupta
Recruiter
tejash@swifttechinc.com
Job Type: Contract
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Python: 7 years (Required)
SQL: 7 years (Required)
Data modeling: 6 years (Required)
RDBMS: 6 years (Required)
Work Location: In person",4.1,"Swift Technologies Inc
4.1","Fremont, CA",1 to 50 Employees,-1,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
458,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
459,AWS Data Engineer II,$77K - $105K (Glassdoor est.),"Overview:
Shift4 (NYSE: FOUR) is boldly redefining commerce by simplifying complex payments ecosystems across the world. As the leader in commerce-enabling technology, Shift4 powers billions of transactions annually for hundreds of thousands of businesses in virtually every industry. For more information, visit www.shift4.com.

Shift4 Payments is seeking a Business Intelligence Engineer with an AWS Focus to join our growing internal Information Technology team. As a BI Engineer, you will help design and maintain the flow of information and analytics to the entire organization utilizing AWS capabilities to store, move and report on data. The position requires at least 3 years of prior experience as a Business Intelligence AWS engineer. We are looking for individuals that are extremely self-sufficient, available to work flexible hours and hold themselves to the highest standards of professionalism.Maintain and expand BI AWS architecture and content.
Responsibilities:
Support of BI AWS architecture and content up to and including off hours support when necessary.
Work on necessary bugs, enhancements and new system developments utilizing our Software Development Life Cycle (SDLC) standards.
Complete root cause analysis and provide recommendations to leadership to address opportunities.
Contribute to and enforce the use of the organization's Business Intelligence solution.
Develop software solutions that will provide solid foundations for the growth of the company.
Support key metrics and shared goals that determine success for a particular area.
Qualifications:
3-5+ years of professional experience in business intelligence AWS development or a related field.
Advanced experience working with AWS: S3, Glue, Lambda, Step Functions, Redshift, Athena.
Advanced experience working with various data sources such as AWS S3 and Redshift, Postgres, SQL Server, Amazon Athena, Excel, Flat Files, etc.
Advanced experience working with large data sets from sources such as AWS S3 and Redshift, Postgres, SQL Server, etc.
Strong PySpark, Python, CLI proficiency
Strong SQL proficiency (T-SQL, MySQL, and PostgreSQL).
Experience working with an Enterprise Data Warehouse and Dimensional Databases is a plus.
Advanced experience of the SDLC and how Development processes fit into KANBAN and Scrum Frameworks.
Experience with Jira and Confluence is a plus.
Ability to prioritize multiple tasks and easily adjust to changing priorities.
Ability to identify problems, initiate solutions, and effectively collaborate and communicate with team members.
Have excellent verbal and written communication skills.

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.",3.5,"Shift4 Payments
3.5","Allentown, PA",1001 to 5000 Employees,1999,Company - Public,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
460,Senior Data Engineer,-1,"JOB DESCRIPTION:
Need to be expertise Coding on PySpark.
Supply Chain and Azure.
Experience in working with end-to-end technology implementation project, preferable those related to Data warehousing/Analytics/Reporting
Strong communication skills with the ability to collaborate effectively with business owners/users and technical teams.
Proven ability to gather product requirements from stakeholders and prioritize feature implementations based on business needs and product strategy.
Strong problem-solving skills and attention to detail.
Good to have experience with CPG/ Retail industry (preferable CPG companies such as Nestle, Unilever, P&G, Coca coal, PepsiCo etc)
Bachelor's/master's degree in business administration, computer science, engineering, or a related field.
Job Type: Contract
Experience:
PySpark: 5 years (Preferred)
SQL: 10 years (Preferred)
Data analytics: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Azure: 5 years (Preferred)
Data Bricks: 5 years (Preferred)
Data Lake: 5 years (Preferred)
Synapse: 5 years (Preferred)
Retail Domain: 8 years (Preferred)
CPG: 7 years (Preferred)
Work Location: Remote",4.0,"Quadrant Resource
4.0",Remote,1001 to 5000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
461,Data Engineer,Employer Provided Salary:$120K - $130K,"Role – Data Engineer- DataBricks -
Location – O Fallon, Missouri (Initially Remote)
Yrs. of experience – 8+
Full-Time
Job Description :
o 5+ years in a customer facing Technical architecture, consulting role with the expertise in The following technology.
o Developing modern Data warehouse solutions using Databricks and AWS/Azure stack.
o Drive technical discussion with client architect and team members.
o Knowledge in Databricks DELTA lake for the Analytical data lake use case
o Hand on experience in create MLOPS data pipeline creation.
o AIML Models develop, train and implement for the AIML use cases.
o Knowledge in Banking domain on card and payment areas.
o Any relation database – Data classification/ Data profiling for MLOPS use cases
o Good experience in offshore onsite coordination.
o Experience in translating a customer’s business needs.
Job Type: Full-time
Salary: $120,000.00 - $130,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
O'Fallon, MO 63366: Reliably commute or planning to relocate before starting work (Required)
Experience:
DataBricks: 1 year (Required)
MLOPS: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: In person",-1.0,Sbase Technologies,"O Fallon, MO",-1,-1,-1,-1,-1,-1
462,Data Engineer,-1,"Clear Demand Company Overview
Clear Demand is the leader in Intelligent Price Management and Optimization (IPMO) for retail. We were the first company to deliver an omni-channel lifecycle pricing solution that synchronizes prices, promotions, and markdowns online and in-store to produce a consistent brand and shopping experience. Clear Demand is the leading innovator in retail pricing solutions with patented science that analyzes historical sales to understand shoppers’ sensitivity to price and generate price and promotion strategies that account for pricing rules, cost changes, and competitor prices to achieve profit and revenue goals. Architected on big data and delivered through Software-as-a-Service (SaaS), Clear Demand’s Intelligent IPMO solution can be administered from a public or private cloud. Clear Demand’s innovations in retail science simplify adoption and use, while allowing retailers to see value in just weeks with more transparency and minimal disruption to existing business.
Clear Demand offers a competitive salary with stock options and a benefits package, including medical, dental, 401k.
Job Description – Data Engineer
This is a permanent position with tremendous potential for growth. The successful candidate will be exceptionally talented and hardworking---a self-starter able to multi-task and deliver results in a fast-paced environment. We are looking for a team player with experience developing high-performance applications for large enterprises. The software developer will be experienced in agile product-development methodologies. The ideal candidate will have a proven track record showing commitment to and sense of urgency for project timelines. This position will report to the Director of Engineering.
Primary Responsibilities
Designing, developing, testing, deploying, and maintaining applications to support business requirements.
Develop and improve data solutions for
- Designing relational schemas for persisting complex business objects
- ETL of customer data into our solution.
- Performing validation and mitigation strategies to handle invalid incoming data.
- Exchanging data between CDI’s user facing application and backend pricing optimization science solutions.
- Versioning database schemas, stored procedures
- Visualizing analytics and reporting
Working closely with both the Engineering and Science departments to build cohesive data-centric solutions.
Providing development expertise on migrating from a document store to a relational database.
Resolving technical issues through debugging and troubleshooting is also required.
Estimating level of effort for user stories and tasks.
Participate in Agile/SCRUM processes and ceremonies.
Required Skills
5+ years of experience in software development
Team player and effective communicator
Knowledge, experience, and proficiency with:
- Agile Development Methodology
- Relational Databases (SQL Server, Postgres)
- Document stores (i.e., MongoDB)
- Postgres
- Python
- Database query performance optimization
- Database versioning and migration
- ETL
- Git
- Object-Relational Mapping (ORM)
Performance optimization and debugging
Takes initiative to identify and address technology issues and opportunities, and proactively contributes to the business.
Good interpersonal, written, and oral communication skills.
Experience working in a team-oriented, collaborative environment.
Technical documentation skills.
Desired Skills
Google Cloud
CI/CD build and release pipeline
Jira
C#
HTML, JavaScript
Security/SSO/SSL
SaaS (Software-as-a-Service)
Experience with the Retail industry
Education
College degree in Computer Science or equivalent.
Clear Demand offers a competitive salary with stock options and a benefits package, including medical, dental, 401(k).
To apply, please your send resume to HumanResources@ClearDemand.com.
To learn more about Clear Demand, visit http://ClearDemand.com.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Employee discount
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Retirement plan
Vision insurance
Experience level:
5 years
Application Question(s):
Would you be willing to do a coding exercise as part of this application process?
Experience:
Agile: 3 years (Preferred)
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Scottsdale, AZ 85258",-1.0,Clear Demand,"Scottsdale, AZ",-1,-1,-1,-1,-1,-1
463,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
464,ETL Data Pipeline Engineer,Employer Provided Salary:$60.00 Per Hour,"ETL Data Pipeline Engineer
We do not work with 3rd party employers. Visa Sponsorship NOT available.
We are seeking a ETL DATA Pipeline Engineer for a consulting engagement with a major entertainment and media company. This person will be hands-on-date engineering development across multiple projects.
Required Skills:
10+ years of experience as Data Engineer with Large Data Pipelines
Strong SQL skills
Distributed Systems (Spark, Hadoop)
Cloud experience
STRONG ETL Experience
Python/Bash
Agile/Scrum
----------------------------------------
ABOUT MOORECROFT
A quality oriented national consulting firm. For more than two decades, providing expert IT professionals to our clients around the nation. Our core values are professionalism, honesty and integrity. We are only interested in working with professionals that subscribe to the same values and are driven to truly serve clients.
Job Type: Contract
Pay: From $60.00 per hour
Experience level:
10 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Santa Monica, CA 90404: Reliably commute or planning to relocate before starting work (Required)
Work Location: Hybrid remote in Santa Monica, CA 90404",-1.0,Moorecroft Systems,"Santa Monica, CA",Unknown,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
465,Data Engineer,Employer Provided Salary:$115K - $135K,"**This position is a majority Remote role with occasional in-office meetings on an as-needed basis. Because of this, interested applicants must live within a reasonable driving distance of Symmetry Lending's office in Anaheim, CA.**
Job Description
This role will play a pivotal role in assisting the IT team with the evolution of the company data architecture. In addition to supporting extensions to the data warehouse design, this role will design and develop ETL required to onboard new types of business data. Finally, this role will also provide support for Analytics assignments as needed.
Responsibilities
Display sense of ownership over assigned work, requiring minimal direction and driving to completion in a sometimes fuzzy and uncharted environment.
Designing and building new data pipelines that support business requirements.
Work with IT and other business partners to support the evolution of the enterprise data architecture.
Work with IT to build new ETLs to take data from various operational systems and extend existing data warehouse data model for analytics and reporting.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business constituents.
Support the development of the company BI infrastructure, including the construction of scalable analytic solutions, and on-premises reporting tools.
Utilize Power BI development to build scalable reporting models to serve BI reporting needs.
Demonstrate the ability to gather requirements, extract and manipulate data residing in multiple disparate databases, and articulate solutions to support the business.
Design, develop, and maintain performant data models in Power BI utilizing best practices.
Serve as a subject matter expert for all data warehouse and enterprise systems
Use new or existing technologies to produce analytics solutions (in the form of excel spreadsheets, dashboards, etc.).
Work with IT and Analytics team to architect and build data pipelines to optimize for performance, data quality, scalability, ease of future development, and cost.
Gather requirements, assess gaps and build roadmaps to help the analytics driven organization achieve its goals.
Develop data-related Proof of Concepts in order to demonstrate feasibility and value to Symmetry business constituents.
Qualifications / Requirements
Bachelor’s degree in Information Systems, Computer Science, Finance, or similar education from an accredited college
2-3 years Accounting and Finance background preferred
Strong skillset in Microsoft Excel (2-3 years, financial formulas & VBA is a plus) and familiarity with Microsoft Office Productivity Suite (Excel, Word, etc.)
5+years’ experience with advanced SQL concepts and writing SQL statements (SQL Server or similar).
Prefer 2 years’ experience with Microsoft BI Suite, PowerBI, Power Query, PowerPivot, Reporting Service (SSRS), SharePoint
Experience in mentoring other team members in development best practice, and methodologies.
You are passionate about data quality control and know how and where to anticipate potential errors.
Knowledge of the software development lifecycle, agile methodologies, and structured software development methodologies.
Experience performing analysis with large datasets in a cloud-based environment.
Ability to work effectively with stakeholders at all levels within the organization
Strong communication and time management skills and a self-motivated approach
Ability to work independently, detail-oriented, and execution focused
Highly collaborative and team oriented
Tenacious (doesn’t give up easily)
Genuine passion for clean and reliable data
At least 5 years of work experience
About Symmetry
Symmetry Lending, specializes in providing mortgage fulfillment services to include origination, servicing, and capital markets needs to various Lenders across the country with whom we partner. We have offices in Atlanta, GA, Eden Prairie, MN, Denver, CO, Orlando, FL, and Anaheim, CA, and we do business from coast to coast. We take great pride in building a diverse team of motivated professionals that contribute to an exciting work atmosphere. We provide a competitive benefits package including medical, dental, and vision plan options, paid time off, and more.
California Disclosure - Employee Notice at Collection
This disclosure is intended to comply with the California Consumer Privacy Act (CCPA), which gives California residents who are applicants, employees, or contractors of Symmetry Lending (“Symmetry”) the right to know what categories of personal information Symmetry collects about them and the purposes for which Symmetry uses that information. As used in this Privacy Notice, “Personal Information” means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a particular individual or household. Personal Information includes, but is not limited to, the categories of personal information identified below if such information identifies, relates to, describes, is reasonably capable of being associated with, or could be reasonably linked, directly or indirectly, with a particular individual or household.
The following is a list of the categories of Personal Information that we may collect about consumers:
Identifiers. This may include a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, Social Security number, driver's license number, passport number, or other similar identifiers.
Personal information described in the California Customer Records Statute (Cal. Civ. Code § 1798.80(e)). This may include a name, signature, Social Security number, physical characteristics or description, address, telephone number, passport number, driver's license or state identification card number, insurance policy number, education, employment, employment history, bank account number, or any other financial information, medical information, or health insurance information.
Characteristics of Protected Classification under California or Federal Law. This may include age, race, color, ancestry, national or ethnic origin, citizenship status, religion or belief, marital status, a childbirth or related medical condition, physical or mental disability, sex (including gender, gender identity, gender expression, pregnancy or childbirth, and related medical conditions), sexual orientation, veteran or military status.
Biometric information. This may include voice and video recordings.
Sensory data. This may include audio, electronic, visual, or similar information, including photos.
Professional or employment-related information. This may include current or past job history, compensation data, performance evaluations, or employee benefits.
Beneficiaries, dependents, and emergency contact information. This may include the name, gender, phone number, and relationship of beneficiaries, dependents, and emergency contacts.
We may use the categories of Personal Information for the following business or commercial purposes:
To perform background checks necessary to comply with licensing requirements, to perform reference checks, to verify eligibility to work in the United States, for contact purposes, to assess your qualification for employment, to conduct performance evaluations, for payrolls and budgeting purposes, for implementation of employee benefits, for internal organizational purposes to establish proper accommodations for sick time, PTO, leaves of absences, or emergency situations, and to conduct health screenings as allowed by OSHA and the CDC to protect the safety of our employee during pandemic situations.
For internal use, such as tracking access into buildings, timekeeping, activity logs, etc.
To comply with laws and regulations, including but not limited to applicable tax, health and safety, anti-discrimination, immigration, labor and employment, and social welfare laws.
For security or the prevention, detection, or investigation of fraud, suspected or actual illegal activity, violations of company policy or rules, or other misconduct.
To comply with civil, criminal, judicial, or regulatory inquiries, investigations, subpoenas, or summons.
To comply with all licensing requirements necessary for our business operations, including state exam audits.
To exercise or defend the legal rights of Symmetry and its employees, affiliates, customers, contractors, and agents.
To seek advice from lawyers, auditors, or other professional advisors.
If Symmetry uses Personal Information of an applicant, employee, or contractor for a purpose materially different than those disclosed in this notice, Symmetry will notify the employee and obtain explicit consent from the employee to use the Personal Information for this new purpose.
Job Type: Full-time
Pay: $115,000.00 - $135,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Paid time off
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
Accounting and Finance: 2 years (Preferred)
advanced SQL concepts and writing SQL statements: 5 years (Required)
Work Location: Hybrid remote in Anaheim, CA 92807",3.9,"Symmetry Lending
3.9","Anaheim, CA",51 to 200 Employees,-1,Company - Public,Banking & Lending,Financial Services,$5 to $25 million (USD)
466,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
467,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1.0,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
468,Data Engineer,Employer Provided Salary:$55.00 - $75.00 Per Hour,"Senior Data Engineer - 10+ Years of Total Experience Required
Location: Dallas, TX and Remote
Job Description:
Slesha inc is looking for a Data Engineer to join our team in our new location in Dallas, TX. This role will be responsible for the following:
Data Engineer
Responsibilities
· Writes ETL (Extract / Transform / Load) processes, designs database systems and, develops tools for real-time and offline analytic processing.
· Troubleshoots software and processes for data consistency and integrity. Integrates complex and large scale data from a variety of sources for business partners to generate insight and make decisions.
· Translates business specifications into design specifications and code. Responsible for writing complex programs, ad hoc queries, and reports. Ensures that all code is well structured, includes sufficient documentation, and is easy to maintain and reuse.
· Partners with internal clients to gain an expert understanding of business functions and informational needs. Works closely with other technical and data analytics experts across the business to implement data solutions.
· Leads all phases of solution development. Explains technical considerations at related meetings, including those with internal clients and less experienced team members.
· Assesses data quality and tests code thoroughly for accuracy of intended purpose. Provides data analysis guidance and serves as a technical consultant for the client.
· Educates and develops junior data engineers on the team while applying quality control to their work. Develops data engineering standards and contributes expertise to other data expert teams across Vanguard.
· Tests and implements new software releases through regression testing. Identifies issues and engages with vendors to resolve and elevate software into production.
· Participates in special projects and performs other duties as assigned.
Qualifications
· Deep technical knowledge – including proficiency in at least two of Python, SQL, Hive, Spark, Amazon Web Services / cloud computing (e.g., Elastic MapReduce, EC2, S3), Bash shell scripting
· Experience writing production quality code to create data products
· Ability to effectively communicate technical concepts to non-technical audiences
Job Type: Contract
Pay: $55.00 - $75.00 per hour
Compensation package:
Hourly pay
Experience level:
10 years
Schedule:
8 hour shift
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
AWS: 3 years (Required)
ETL: 3 years (Required)
Data warehouse: 3 years (Required)
10 key typing: 9 years (Required)
Work Location: Remote",-1.0,Slesha inc,"Dallas, TX",-1,-1,-1,-1,-1,-1
469,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
470,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
471,Data Engineer,-1,"At MNTN, we've built a culture based on quality, trust, ambition, and accountability – but most importantly, we really enjoy working here. We pride ourselves on our self-service platform, originally coded by our President and CEO, and are constantly seeking to improve the user experience for our customers and scale for efficiency. Our startup spirit powers our growth mindset and supports our teammates as they build the future of ConnectedTV. We're looking for people who naturally want to do more, own more, and make an impact in their careers – and we're seeking someone to be part of our next stage of growth.
As a Senior Data Engineer on the Data team, you will help build the platform to generate, track, manage and triage key business and client success metrics. The goal is to have rapid insights across all available information to mitigate issues and identify opportunities for a smooth marketing experience.
You will:
Become the expert on the MNTN platforms, UI, data infrastructure, and data processes
Extract meaningful business metrics from raw data using SQL and other tools
Create and manage ETL/ELT workflows that transform our billions of raw data points daily into quickly accessible information across our databases and data warehouses
Organize data and metrics for measurable and trackable confidence in reporting and client performance to fulfill agreed-upon quality standards
Organize visualizations, reporting, and alerting necessary to rapidly illustrate performance, data quality, trends and opportunities
Investigate critical incidents and otherwise ensure that any issues reach resolution by the relevant parties
You have:
5+ years of experience related to data engineering, analysis and modeling complex data
Strong experience in SQL, data modeling, and manipulating and extracting large data sets.
Hands-on experience working with data warehouse technologies. Familiarity with building data pipelines and architectures and designing ETL flows.
Experience with programming languages such as Python, Java, or shell scripting. Familiarity with algorithms.
Familiarity with software processes and tools such as Git, CI/CD pipelines, Linux, and Airflow
Experience with working in a cloud computing environment such as AWS, Azure, or GCP
Familiarity in a business intelligence tool such as Domo, Looker or Tableau
Written and verbal communication skills to convey complex technical topics to non-technical audiences across the organization
MNTN Perks:
100% remote
Open-ended vacation policy with an annual vacation allowance
Three-day weekend every month of the year
Competitive compensation
100% healthcare coverage
401k plan
Flexible Spending Account (FSA) for dependent, medical, and dental care
Access to coaching, therapy, and professional development
About MNTN:
MNTN provides advertising software for brands to reach their audience across Connected TV, web, and mobile. MNTN Performance TV has redefined what it means to advertise on television, transforming Connected TV into a direct-response, performance marketing channel. Our web retargeting has been leveraged by thousands of top brands for over a decade, driving billions of dollars in revenue.
Our solutions give advertisers total transparency and complete control over their campaigns – all with the fastest go-live in the industry. As a result, thousands of top brands have partnered with MNTN, including, Petsmart, Build with Ferguson Master, Simplisafe, Yieldstreet and National University.
#Li-Remote",3.9,"MNTN
3.9",Remote,201 to 500 Employees,2009,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
472,Data Engineer,Employer Provided Salary:$91K - $116K,"A reasonable salary expectation is between $91,000.00 and $116,022.40, based upon experience and internal equity.
This position is on a hybrid work schedule. (Mon & Fri - remote, Tues - Thurs onsite in Rancho Cucamonga, CA)
Position Summary/Position
The Data Engineer II assists in the implementation of methods to improve data reliability and quality. This role is responsible for combining raw information from different sources to create consistent and machine-readable formats. The Data Engineer II must also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling. The Data Engineer II will focus on data accessibility, which will enable the organization to utilize data for performance evaluation and optimization. The data Engineer II is also responsible for managing the entire back-end development life cycle for the company's enterprise data warehouse. In this role the incumbent will handle tasks associated with the implementation of ETL procedures, building warehouse databases, database performance management, and dimensional modeling and design of the table structures.
Major Functions (Duties and Responsibilities)
1. Design and develop data warehouse Extraction, Transformation and Loading (ETL) solutions using Microsoft SQL Server Integration Services (SSIS), Azure Data Factory, Synapse Analytics, Az Data Bricks, PySpark ETL.
2. Develop and implement data collection processes in conjunction with the data warehouse. Source data from legacy systems supporting a centralized data warehouse and reporting platform.
3. Develop technical solutions to meet the requirements for Data Warehouse, BI & Analytics
4. Work closely with the data engineering and BI & Analytics teams to design data and analytics solutions to increase the usability, completeness, and accuracy of enterprise data
5. Analyze user requirements and translate into database requirements and implement in database code
6. Create and maintain the optimal data pipeline architectures based on micro services based on platform and application requirements
7. Assemble large, complex data sets that meet functional / non-functional business requirements
8. Identify, design, and implement process improvements: automating manual pipeline processes, optimizing data ingestion and consumption, re-designing infrastructure for greater scalability, micro services, etc
9. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
10. Work closely with Data Warehouse Architect and Data Systems Architect to design data and analytics solutions to increase the usability, completeness, and accuracy of enterprise data
11. Create, maintain, and optimize SQL queries and routines
12. Analyze potential data quality issues to determine the root cause and create effective solutions.
13. Develop, adopt, and enforce Data Warehouse and ETL standards and architecture
14. Monitor and support ETL processes ensuring integrity and proper integration of all data sources
15. Create high throughput historical and incremental ETL jobs
16. Facilitate problem management, and communication among data architects, managers, informaticists and analysts
17. Provide detailed analysis of data issues; data mapping; and the process for automation and enhancement of data quality
18. Perform development activities such as source to target mapping validations, identify, document and execute unit test cases/scripts, peer and lead code reviews per code review checklist and document test and review results.
19. Collaborate and contribute to data integration strategies and visions
20. Provide ongoing proactive technical support for ETL and data warehouse system to ensure business continuity.
21. Work with Informaticists and Analysts to translate analytic requirements into technical solutions.
Experience Qualifications
Four (4) years of relevant work experience. Experience and knowledge in logical, rational, dimensional, and physical data modeling. Background in database systems along with a strong knowledge of SQL. Experience with Orchestration tools, Azure DevOps, and CI/CD. Intermediate experience with the following tools and technologies:
a. Azure Data Catalogue / Purview
b. Azure Cloud
c. Databricks
d. Power BI Dataflows
e. Power Query
f. Azure Cosmos
g. Azure Monitor
h. PowerShell
i. Python
Preferred Experience
Development experience using PySpark, Spark, Hadoop, Kubernetes, and RDMIS is highly desired.
Education Qualifications
Bachelor's degree from an accredited institution required.
Preferred Education
Master’s degree from an accredited institution preferred.
Professional Certification
Azure Data Engineering Certification is preferred.
Knowledge Requirement
Multi-server environment knowledge such as linked servers, data replication, backup/restore with MS SQL Server 2008+. Knowledge of applicable data privacy practices and laws.
Skills Requirement
Highly skilled in developing and optimizing T-SQL (DDL, DML, DCL) queries, stored procedures, functions, and views for various applications that involve numerous database tables and complex business logic. Good written and oral communication skills. Strong technical documentation skills. Good interpersonal skills.
Abilities Requirement
Highly self-motivated and directed. Keen attention to detail. Proven analytical and problem-solving abilities. Ability to effectively prioritize and execute tasks in a high-pressure environment.
Commitment to Team Culture
The IEHP Team environment requires a Team Member to participate in the IEHP Team Culture. A Team Member demonstrates support of the Culture by developing professional and effective working relationships that include elements of respect and cooperation with Team Members, Members and associates outside of our organization.
Working Conditions
Word processing and programming involving computer keyboard and screens.
Position is eligible for Hybrid work location upon completing the necessary steps and receiving HR approval. All IEHP positions approved for telecommute or hybrid work locations may periodically be required to report to IEHP’s main campus for mandatory in-person meetings or for other business needs as determined by IEHP leadership.
Inland Empire Health Plan (IEHP) is the largest not-for-profit Medi-Cal and Medicare health plan in the Inland Empire. We are also one of the largest employers in the region, designated as “Great Place to Work.” With a provider network of more than 5,000 and a team of more than 3,000 employees, IEHP provides quality, accessible healthcare services to more than 1.5 million members. And our Mission, Vision, and Values help guide us in the development of innovative programs and the creation of an award-winning workplace. As the healthcare landscape is transformed, we’re ready to make a difference today and in the years to come. Join our Team and make a difference with us! IEHP offers a competitive salary and stellar benefit package with a value estimated at 35% of the annual salary, including medical, dental, vision, team bonus, and state pension plan.
Job Types: Full-time, Permanent
Pay: $91,000.00 - $116,022.40 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Parental leave
Retirement plan
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Rancho Cucamonga, CA 91730: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
data engineering: 4 years (Required)
Work Location: Hybrid remote in Rancho Cucamonga, CA 91730",3.7,"Inland Empire Health Plan
3.7","Rancho Cucamonga, CA",1001 to 5000 Employees,1996,Company - Public,Health Care Services & Hospitals,Healthcare,$1 to $5 billion (USD)
473,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
474,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
475,Data Engineer I - People Analytics,$73K - $111K (Glassdoor est.),"POSITION SUMMARY Our People Analytics team is expanding! The Data Engineer I is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.

This position can work in a flexible, hybrid model with 3 days on-site in Appleton, WI and 2 days remote.

JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.

QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming. Preferred experience in Python applied to data engineering and transformation.
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated

#LI-Hybid

Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101 .",4.1,"US Venture
4.1","Appleton, WI",1001 to 5000 Employees,1951,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
476,Senior Data Engineer,Employer Provided Salary:$190K,"Welcome to the MOMENTUM Family!
MOMENTUM is not just our company name; it is the highest value we deliver to our customers. We are a rapidly growing technology solutions company delivering innovative technology, engineering, and intelligence solutions across the DoD sector. The efforts of our high-capacity team ultimately strengthen our Nation and the warfighter.

Our team is dispersed throughout the US, which means we value the diversity and unique collaboration fostered throughout our team. We work incredibly hard for our customers and believe deeply in our core values. We're a high-energy, high-growth team and we love to win.

Data Engineer
The Data Engineer provides engineering support to the data science and software engineering team members. Includes augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments. Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, to include hardware and software support to existing servers.

In this role, you will:
Support ad-hoc data analysis requirements defined by the client's Leadership.
Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives.
Knowledge of business intelligence reporting tools and data visualization software including Tableau.
Will work on Data cleaning and transformation efforts in delivery of CDRLs.
Experienced in extracting and aggregating structured and unstructured data.
Experienced in data programming languages and tools such as Python and R.
Experience with SQL or similar database language.
Experience designing and implementing data models to enable, sustain and enhance the value of information they contain.
Strong analytical and critical thinking skills.
Ability to work collaboratively and effectively in a team environment.
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers
Make sure pedigree and provenance of the data is maintained such that the access to data is protected
Clean and preprocess data to enable analytic access
Collaborate with enterprise working groups to advance the state of data standards
Collaborate with the engineering team, data stewards, and mission partners to aid in processes getting actionable value out of the data holdings architects complex, repeatable ETL
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic and others
Supports Experience with Targeting using Sponsor Tools, Reverse Engineering
Develop API connectors to enable ingest of new data catalog entries from databases and files to ensure that data mappings will provide the best performance for expected user experience
Supports Deliverables and Reports

If you're suitable for this role, you have:
Top Secret SCI with FULL SCOPE POLY REQUIRED
9+ Years of verifiable experience


To learn more about us, check out our website at www.gomomentum.tech!

MOMENTUM is an EEO/M/F/Veteran/Disabled Employer:
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The qualifications listed above are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions.

Accommodations:
Consistent with the Americans with Disabilities Act (ADA) and Alabama civil rights law, it is the policy of Momentum to provide reasonable accommodation when requested by a qualified applicant or employee with a disability, unless such accommodation would cause an undue hardship. The policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If reasonable accommodation is needed, please include a request when applying.",3.6,"Momentum
3.6","Chantilly, VA",501 to 1000 Employees,1987,Company - Public,Advertising & Public Relations,Media & Communication,$100 to $500 million (USD)
477,Data Engineer,Employer Provided Salary:$150K - $300K,"What we're looking for
Stytch is the platform for user authentication. We build infrastructure that sits in the critical path of our customer's applications. As a data engineer, you'll work on designing and building event-driven architecture systems to drive analytics insights and observability tooling for our customers.

What Excites You
Championing data-driven insights - you see data analytics and observability as a product critical to success
Solving problems with pragmatic solutions — you know when to make trade-offs between completeness and utility and you know when to cut scope to ship something good enough quickly
Building products that make developers lives easier — as a data engineer for a developer infrastructure company, what you build will have an immediate impact on our customers
Shaping the culture and growing the team through recruiting, mentorship, and establishing best practices
Learning new skills and technologies in a fast paced environment

What Excites Us
Comfort working in a modern data stack using tools like Snowflake, Redshift, DBT, Fivetran, ElasticSearch, and Kinesis
Appreciation for schema design and architecture that balance flexibility and simplicity
Experience designing and building highly reliable back-end and ETL systems
3+ years as a data or backend engineer

What Success Looks Like
Technical — build new, highly reliable services that our customers can depend on
Ownership — advocate for projects and solutions that you believe in and ship them to production
Leadership — level up your teammates by providing mentorship and guidance

Our Tech Stack
Data moves through Snowflake, ElasticSearch, MySQL, and Kinesis
Go and Node for application services
We run on AWS with Kubernetes for containerization
gRPC and protobufs for internal service communication

Expected base salary $150,000-$300,000. The anticipated base salary range is not inclusive of full benefits including equity, health care insurance, time off, paid parental leave, etc. This base salary is accurate based on information at the time of posting. Actual compensation for hired candidates will be determined using a number of factors including experience, skills, and qualifications.
We're looking to hire a GREAT team and that means hiring people who are highly empathetic, ambitious, and excited about building the future of user authentication. You should feel empowered to apply for this role even if your experience doesn't exactly match up to our job description (our job descriptions are directional and not perfect recipes for exactly what we need). We are committed to building a diverse, inclusive, and equitable workspace where everyone (regardless of age, education, ethnicity, gender, sexual orientation, or any personal characteristics) feels like they belong. We look forward to hearing from you!

Learn more about our team and culture here!",5.0,"Stytch
5.0","San Francisco, CA",1 to 50 Employees,2020,Company - Public,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
478,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
479,Azure Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Required Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 9 years (Required)
Python: 8 years (Required)
Data lake: 7 years (Required)
Data Bricks: 7 years (Required)
Work Location: Remote",-1.0,TekValue IT Solutions,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
480,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
481,Data Engineer,$80K - $115K (Glassdoor est.),"Data Engineer
Syracuse, NY
Overview
With more than 70 years as an established, family-owned company, Raymour and Flanigan has grown into the largest furniture retailer in the Northeast with more than 100 showrooms in seven states, and we are continually expanding our territory. We have a strong foundation of experienced IT professionals with expertise in harnessing and maximizing organizational processes and their associated information to solve a wide variety of complex business needs.
PositionOverview
We are looking for an accomplished pipeline-centric Data Engineer to join our growing team of analytics experts. The position will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.

Essential Functions and Responsibilities
Lead and develop sustainable data driven solutions with current and next gen data technologies to meet the needs of our organization and business customers
Design robust systems with an eye on the long-term maintenance and support of the application
Assemble large, complex data sets and data marts that meet functional/non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Leverage and establish standards across the team and organization
Develop within a hybrid-cloud-based data-processing architecture to integrate Raymour & Flanigan data with third parties
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Develop with a strong emphasis on security, reliability, fault tolerance and performance
Develop stream processing applications to process data in near real time
Build batch data pipelines to integrate disparate data sources in a central data warehouse to support business applications and analytics
Design new analytical and physical data models for analysis and optimization
Develop solutions within a data visualization technology or platform (e.g. Tableau, D3)
Provide technical guidance to team members
Help to enable team success through fostering a positive work environment
Required Skills and Experience
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a bachelor's degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases like Snowflake, Oracle, Dynamo DB etc..
Experience with data pipeline and workflow management tools: SSIS, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with programming languages: Python, C# etc.
Experience with a tool like DBT a plus
Raymour & Flanigan offers competitive compensation and a comprehensive benefits package:
Excellent Health, Dental & Vision Coverage
401(k) with a Company Match
Paid Vacation and Holidays
Health Savings Account
Flexible Spending Account
Training and Development
Tuition Reimbursement Program
Generous Merchandise Discount
Short & Long Term Disability
Group Life Insurance
Specified Disease Insurance
Raymour & Flanigan proudly supports a drug and smoke free work environment.
Raymour & Flanigan is an Equal Employment Opportunity employer that does not discriminate against any associate or applicant on the basis of race, creed, color, religion, sex (including pregnancy), age, national origin, physical or mental disability, sexual orientation, sexual and other reproductive health decisions, marital or familial status, genetic information or other basis protected by law.",3.6,"Raymour and Flanigan
3.6","Liverpool, NY",5001 to 10000 Employees,1947,Company - Private,Home Furniture & Housewares Stores,Retail & Wholesale,$1 to $5 billion (USD)
482,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
483,"Manager, Data Engineer & ETL Processing",$83K - $118K (Glassdoor est.),"Company Description

About Spark Foundry:

Spark Foundry is a global media agency that exists to bring HEAT – Higher Engagement, Affinity, and Transactions – to brands. By combining flawless media fundamentals with aggressive innovation, Spark inspires consumers to pay more attention, to care more about our clients’ brands, and to buy more products and services from them.
Balancing the nimble spirit of a startup with the powerhouse soul of Publicis Media, Spark Foundry delivers the best of both worlds to a client roster that spans some of the world’s best and most beloved brands and companies. We combine boutique-caliber insights and service with the buying clout and first-look access of a global leader, bringing the heat to challenger brands that want to act like giants, and to giant brands that want to act like challengers.
With a bottom-up culture that celebrates diversity and aims for all voices to be heard, Spark has become a magnet for the industry’s best talent, with one of the best retention rates in the industry. And by applying a whole-person approach to professional and personal development, Spark develops a workforce that is well prepared for today’s challenges, and also poised to create meaningful careers in the years to come.
Because we know that heat arises the intersection of complementary forces, our professionals come from myriad disciplines and backgrounds: data, analytics, and insights, content and creative production, communications and strategy, finance and marketing, and sociology, psychology, and other liberal arts disciplines.

Job Description

Overview:
The Manager, Data Engineering & ETL Development is a key driver to build data platform leveraging best in class ETL practice and is also a strategic thinker and a talented data visualization expert with emphasis on dashboard reporting. The primary responsibility will be developing business intelligence platform to support media and marketing data for our clients. This position will allow you to be a significant contributor as part of the team to support easy access of data and visualizations to fuel stronger insights and media optimizations.
This position requires strong technical and tactical skill sets with an eye for numbers, intellectual curiosity, proficiency at problem solving, and a critical understanding of online media. The candidate must have a proven track record in managing ETL’s, APIs and business intelligence platforms. Candidate should be a team player. A “roll up the sleeves” approach is mandatory and a “get it done” attitude is a must. Specific responsibilities include coordination between the research, analytics & media teams ensuring high quality data projects are effectively delivered.
Successful candidates will be multi-dimensional ‘rising-stars’ who are able to employ complex problem solving skills, and are able to communicate these succinctly to a broad client and media stakeholder audience.
Role Objectives:
Responsible for loading and validating data into the data warehouse from various source systems
Analyze, develop, fix, test, review and deploy functionality, and bug fixes in ETL data pipelines
Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required
Building Data mappings between Source to Target systems
Provides support for technical issues and ensuring system availability
Work with business customers to identify and develop additional data and reporting needs
Understand how business intelligence platform/data technologies work and offer the ability to explain technical concepts in ordinary terms (be technically savvy - understand the opportunities and limitations)
Establish and manage data integrations utilizing a taxonomy nomenclature to make recommendations for data visualization to showcase media performance through the use of Datorama or Tableau
Perform regular quality assurance/quality control checks on assigned client campaigns to ensure the data is processing accurately
Design and build backend data streams and processes to automate reporting capability with data visualization tools
Contribute to client status and reporting calls, including presentation of reporting as required
Develop subject matter expertise in ETL, API development, and Business Intelligence platforms
Clearly define project deliverables, timelines, and dependencies for junior team members, internal stakeholders and clients
Collaborate on an inclusive team, where members openly communicate and collectively problem-solve
Strong ability to evaluate new technologies and present findings to team
Contribute to knowledge sharing efforts and mentorships
Complete other duties as assigned.

Qualifications
Bachelor’s degree or combination of education, and equivalent work experience is preferred in the field of computer science, management information systems or Information Technology
3+ years’ related experience ETL development and business intelligence platform management
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
3+ plus years’ development experience with database engines including Presto, Mongo db and Hadoop.
3+ years’ experience in ETL development, Strong Database (Modeling, SQL), SQL Server, Hadoop, AWS Redshift, Qubole.
Experience managing team of 2 or more associates/analysts preferred
Strong database modeling and SQL skills. Ability to write complex SQL statement, Procedures and data automation programs
Knowledge of ETL tools like Airflow, AWS Glue, Alteryx, SSIS, Qubole/Spark is a must
Knowledge of Python or similar programming languages required
Proficiency with Datorama, Tableau, or other data visualization tools is preferred
Experience working with AWS or other cloud technologies
Advanced user Microsoft Office Suite and reporting tools
Demonstrated expertise in core MS Excel functions (vlookup, pivot tables, data visualization)
Experience in designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.
Strong analytical and problem-solving skills
Strong verbal/written communication and interpersonal skills is required
Self-starter with ability to thrive in a fast-paced environment and able to function independently while providing status updates to a team of analysts
Cooperative, flexible, conscientious, dependable, resourceful, self-motivated, and team-oriented
Problem solving, time management, and critical thinking skills with a professional and positive attitude
Ability to work independently and as part of an agile team, participating in daily stand-ups, sprint planning and sprint review

Character:
The following qualities help drive success as member of the Spark Data and Analytics team:
Entrepreneurial, engaging, resourceful, curious, and self-directed spirit
Willing and easily roll sleeves up or down; love the nitty-gritty and the strategy
Collaborative approach to building cohesive, strong teams
Loving and living the intersections between brands, people, media, communications
Relentlessly passionate and resolute
Planning and time management excellence.
Embrace challenges
Proactive, especially in pushing for new opportunities, approaches, and ideas.
Keenly focused on action and solutions; thrives with deep critical thinking and analysis.
Pioneering insight attitude and research in-the-know.
Resourcefulness, flexibility and adaptability, strong ability to pivot when the need arises.
Inspired to be part of the insight journey/revolution with a growing, dedicated team

Additional Information

All your information will be kept confidential according to EEO guidelines.
23-2795",3.9,"Spark Foundry
3.9","Dallas, TX",51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
484,Sr. Data Engineer( ETL testing experience),Employer Provided Salary:$50.00 - $60.00 Per Hour,"Key Skills to evaluate – Python (advanced level), Pyspark, data flow pipeline in AWS, distributed system, Snowflake, Redshift, ETL testing, QE knowledge
JD:
· Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Job Type: Full-time
Pay: $50.00 - $60.00 per hour
Experience level:
8 years
Experience:
python advanced: 10 years (Preferred)
pyspark: 10 years (Preferred)
aws: 10 years (Preferred)
snowflake: 10 years (Preferred)
Work Location: Remote",-1.0,Sana Pivot Inc,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
485,Data Engineer,$88K - $124K (Glassdoor est.),"Recently awarded one of Crain's Best Places to Work in Chicago®, Premier International is a privately held and private equity backed software and technology consulting firm headquartered in downtown Chicago, serving large enterprise consulting and Fortune 500 firms deploying large-scale systems implementations.
You will work in a fast-paced environment that exposes you to diverse project experiences as we collaborate to solve our clients' biggest data challenges.
The Opportunity:
Premier International is hiring an experienced Data Engineer to join our growing Data Governance Practice. You will work in a fast-paced environment that exposes you to diverse project experiences, leading-edge technologies, and continuous learning experiences that will grow your career while solving clients' biggest challenges.
Our Data Governance Practice delivers end-to-end business advisory services, implementation, and technical solutions for the Data Governance Lifecycle including Consulting, Metadata Integration, Reference Data Management, Sensitive Data Management, Tool Evaluation, and Product Implementation.
What You'll Be Doing:
Designing, implementing, and maintaining Data Warehouse environments
Creating and maintaining comprehensive documentation of data engineering processes, pipelines, and workflows
Collaborating effectively with cross-functional teams, including data scientists and analysts, to understand their data needs and support data-related initiatives
What You'll Bring to the Team:
Bachelor's degree in Data Science, Computer Science, Statistics, or a related field
8+ years of relevant experience in data migration
Proficient in Python and Spark development/programming with a focus on performance and scalability
Experience with version control systems, GitHub, for code integrity
Strong analytical mindset and the ability to derive actionable insights from data
Excellent communication and presentation skills, capable of conveying complex information in a clear and concise manner
Ability to work independently and collaboratively in a fast-paced and dynamic environment
Premier Perks & Benefits:
Highly competitive compensation with annual bonus incentive
401K plan with company match
Company paid individual health, dental, vision, disability, and life insurance coverage
Four weeks of paid time off
Nine company paid holidays
Employee referral bonuses
Much more at one of Chicago's Best and Brightest Companies to Work For®!
Premier has been named one of The Best and Brightest Companies to Work For® in Chicago (2019, 2020 & 2021), one of Crain's top 100 Best Places to Work in Chicago (2020 & 2021) and recently made the 2021 Inc. 5000 list of America's Fastest-Growing Private Companies. While we are relentlessly client-focused, we are proud to have our culture and company recognized by others.
Premier is an EEO Employer and provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",4.2,"Premier International
4.2","Chicago, IL",1 to 50 Employees,1985,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
486,Data Engineer,Employer Provided Salary:$120K - $200K,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!",-1.0,shaped.ai Inc.,"New York, NY",1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
487,Data Engineer,Employer Provided Salary:$100K - $145K,"Garner's mission is to transform the healthcare economy, delivering high quality and affordable care for all. By helping employers restructure their healthcare benefit to provide clear incentives and data-driven insights, we direct employees to higher quality and lower cost healthcare providers. The result is that patients get better health outcomes while doctors are rewarded for practicing well, not performing more procedures. We are backed by top-tier venture capital firms, are growing rapidly and looking to expand our team.
We are looking for a Data Engineer to support our technical teams by ensuring ease of access to data within our organization. The ideal candidate for this role will have strong technical skills, including Python, SQL, and AWS as well as a desire to be a hands-on contributor to building out a data platform from the ground up.
Main Responsibilities:
Build the data pipelines that power our business
Collaborate across disciplines to high-quality datasets
Protect our users' privacy and security through best practices
Support data pipelines in production
Our Tools:
Python, AWS, Snowflake, dbt, Terraform, Postgres
The ideal candidate has:
2+ years of experience building data pipelines in a fast-paced environment
Strong Python knowledge
Experience with Big Data technologies such as Snowflake, RedShift, BigQuery, or DataBricks
Ability to think in principles and frameworks to understand and decompose abstract problems
An aptitude to learn new technologies and tools quickly
Why You Should Join Our Team:
You are mission-driven and want to work at a company that can change the healthcare system
You want to be on a small, fast-paced team that nimbly moves to meet new challenges
You love ideating on new features and working with data to find new insights
You're excited about researching and working with the latest tools and technologies
The salary range for this position is $100,000 - $145,000. Compensation for this role will depend on a variety of factors including qualifications, skills and applicable laws. In addition to base compensation this role is eligible to participate in our equity incentive and competitive benefits plans.

Garner Health is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics.
Garner Health is committed to providing accommodations for qualified individuals with disabilities in our recruiting process. If you need assistance or an accommodation due to a disability, you may contact us at talent@getgarner.com.",3.5,"Garner Health
3.5","Dallas, TX",51 to 200 Employees,2019,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
488,Data Science Engineer,-1,"Build the solution that transforms the real estate industry!
We are looking for a Data Science Engineer superhero with a sixth sense for data who can ignite our day-to-day activities with their creativity.
Want to infuse a $30B+ sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? Looking for a fully remote startup culture supported by a profitable business model? Join Mashvisor and help us build an entirely new type of real estate model.

Our Values

Customer Obsessed – We always put our customers first.
Solution Driven – We solve problems that other people are afraid to.
Product led: We are always one step ahead of our customer's needs and create / add features they love every time
One Team – We believe inclusion and teamwork produce the best results.
Open and Direct – We communicate with honesty and respect to our colleagues, customers, and partners.

What You’ll Do
Designing, developing, and researching Machine Learning systems, models, and schemes
Studying, transforming, and converting data science prototypes
Searching and selecting appropriate data sets
Performing statistical analysis and using results to improve models
Training and retraining ML systems and models as needed
Identifying differences in data distribution that could affect model performance in real-world situations
Visualizing data for deeper insights
Analyzing the use cases of ML algorithms and ranking them by their success probability
Understanding when your findings can be applied to business decisions
Enriching existing ML frameworks and libraries
Verifying data quality and/or ensuring it via data cleaning

What You’ll Need
BS or Masters degree in Mathematics, Statistics, Economics, Data Science or another quantitative field
3+ years of hands-on experience utilizing data science to manage, enhance and develop models and deploy solutions to solve complex business problems
Expertise in SQL and programming in SQL, R, Python, C++, Java, and beneficial to know Lisp and Prolog
Strong organizational, interpersonal, and communication skills (both written and verbal)
A bias towards solving problems from a customer-centric lens and an intuitive sense for how the work aligns closely with business objectives
Solid experience with managing databases and datasets and structuring and optimizing the framework
A thorough understanding of SQL databases
Bonus: background in US real estate data, insurance or financial markets analysis
The ideal candidate will be a creative problem solver with an excellent work history on data analytics projects.

We want the work you do here to be the best work of your life.
Compensation: We offer a great salary with a yearly bonus based on performance.
Attitude: Work with a Can-Do team across the world.
Freedom: Work anywhere, anytime.
Time Off: Yearly vacations and sick leaves.
Responsibility: Ability to excel in a fully remote work environment.
Are you the one?",3.7,"Mashvisor Inc.
3.7",Remote,1 to 50 Employees,2015,Company - Private,Real Estate,Real Estate,$1 to $5 million (USD)
489,Data Communications Engineer - III,Employer Provided Salary:$91K - $181K,"Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","San Francisco, CA",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
490,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1.0,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
491,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
492,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
493,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
494,Data Engineer,$88K - $119K (Glassdoor est.),"About the Job

Loopback is hiring an innovative and team-oriented Data Engineer/Operation who will be responsible for building and maintaining optimized and highly available data pipelines that facilitate deeper analysis and reporting across clinical sites in areas of Life Sciences industry analytics.

This engineer’s role will be to manage the data flow processes, analyze data, and lead partnerships with other Data and Analytics teams to identify and implement systems and process improvements. This engineer also designs, architects, implements, and supports key datasets.

Duties to Include

Manage ongoing shifts in data ingestion formats across Health Systems, Life Science, and Enterprise Partner types
Own code base, documentation, and roadmap for all data transformations via Data Lake and underlying Tables
Develop and sequence jobs and processes to transform data into data lake and data warehouse
Own integral Loopback data model, code base, and software to deliver against client/application SLA's
Design and develop highly scalable and reliable data engineering pipelines to process large volumes of data across diverse data sources and analytics use cases
Identify, design, and implement internal process improvements by automating manual processes and optimizing data delivery
Plan, coordinate and implement security measures to safeguard information in computer files against accidental or unauthorized damage, modification or disclosure.
Identify and implement ways to improve data reliability, efficiency, and quality
Develop and promote and implement best practices in data engineering
Business meetings to understand use cases and questions, capture agreement on business rules, understand analyst and stakeholder objectives, and support usage of data to solve business problems
Data profiling, data documentation, and measuring data quality with manual verification and
development of automated data quality tests

Requirements

You will thrive if you:
Exhibit a “self-starter” mindset in taking ownership over delegated responsibilities
Have excellent program/task organizational skills
Are detail and results oriented

You bring a toolkit of your past experiences of:
3-5 Years of experience as a Data Engineer/Operations
Implementing and designing data curation, and data analysis
Orchestrating data transformation through cloud native analytics platforms (Snowflake, Databricks) across cloud environments (Azure, AWS, GCP)
Building and modeling data in relational and non-relational data storage technologies including schema design, stored procedure development and performance and optimization techniques (e.g. SQL & NoSQL, C#, Python, etc.)
Effective collaboration, experienced in creating technical partnerships across teams
Learning and understanding the various technical domains across the enterprise and able to communicate complex technical and business concepts across the enterprise and various business stakeholders
Effectively delivering results in a fast-paced environment while managing multiple priorities
Documenting and testing your designed solutions, engaging with QA and DevOps teams
Contact

Founded in 2009, Loopback was rated as one of the best places to work in Dallas by the DBJ, Loopback Analytics is a leading provider of data-driven solutions for hospitals and health systems. The company’s comprehensive analytics platform drives growth for specialty and
ambulatory pharmacy programs while connecting pharmacy activities with clinical and economic outcomes. Loopback’s clients include leading Academic Medical Centers, health systems, and Life Sciences companies. For more information about our company and services please visit our website at www.loopbackanalytics.com.

This employer will not sponsor applicants for employment visa status (e.g., H1-B) for this position. All applicants must be currently authorized to work in the United States on a full-time basis.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.",4.4,"Loopback Analytics
4.4","Dallas, TX",51 to 200 Employees,2009,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
495,Data Engineer,$77K - $107K (Glassdoor est.),"Position Summary:
Reporting to the Director of Data Science, the Data Engineer will play a crucial role in designing, developing, and maintaining data pipelines and workflows to facilitate data integration, storage, and analysis. With the support of the Data Science team, this position will utilize proven expertise in Microsoft Azure cloud computing to implement and automate processes, ensure the health and efficiency of the data ecosystem, and bring unmatched value to our +1.5 million members across the country and across the world.

Duties and Responsibilities:
Lead architectural efforts for implementing the Data Science team's models and solutions.
Design, implement, and manage data pipelines using Azure Data Factory to extract, transform, and load data from various sources efficiently and securely.
Develop and optimize storage solutions on Microsoft Azure, ensuring data availability, scalability, and reliability.
Automate data-related jobs, processes, and workflows to streamline data delivery
Collaborate closely with cross-functional teams, including data scientists, analysts, and developers, to understand data requirements and support their data-related needs.
Design and maintain data models and architecture, enabling efficient data consumption by business applications and analytical tools.
Ensure data quality, integrity, and security throughout the data lifecycle.
Troubleshoot and promptly resolve data-related issues, performance bottlenecks, and system failures.
Research new data sources, third-party platforms, and data engineering best practices.
Stay current with the latest data engineering and cloud computing trends, particularly in the Azure ecosystem.
Assist Data Scientists with general analysis and dashboards, as needed.
Qualifications:
3-5+ years of relevant Data Engineering, Software Engineering, and/or Analytics experience.
Proved experience with Azure cloud solutions, preferably through your work portfolio and Azure certifications.
Strong proficiency in Azure Data Factory, Azure Storage, and other relevant Azure services.
Extensive experience designing and implementing automated data pipelines, ETL processes, and data workflows.
Advanced skills in SQL and Python for data manipulation, querying, and scripting tasks.
Familiarity with data warehousing concepts, modeling, and integration techniques.
Expert in integrating cloud solutions with external API sources and knowledgeable in creating in-house API solutions that meet our needs.
Excellent problem-solving abilities and a track record of handling complex data engineering challenges.
Strong communication skills and the ability to collaborate effectively with multidisciplinary teams.
Demonstrated ability to prioritize tasks, meet deadlines, and adapt to evolving project requirements.
Azure certifications (e.g., Azure Data Engineer Associate) will be a plus.
Willingness to travel up to 10%.
Organizational Overview:
The National Association of REALTORS® (NAR) is a team of professionals dedicated to providing world-class service to over 1.5 million REALTORS® working in the United States and around the world. The real estate industry is fast-paced and fast-changing--each year, our members participate in the sale, lease, and management of billions of dollars in real estate. As in every industry, our members' value proposition is constantly being challenged by innovation.

It is our mission to empower REALTORS® as they preserve, protect and advance the right to real property for all. We cannot do that without the ideas, passion, and commitment from our talented employees. As our greatest assets, employees are offered their pick of competitive benefits/perks and flexible work options. NAR is certified as a Great Place to Work®. Our most successful employees are committed to our Core Values, which are:

Putting members first
Leading change
Advancing Diversity and Inclusion
Giving respect
Collaborating
Communicating",3.7,"National Association of REALTORS®
3.7","Washington, DC",201 to 500 Employees,1908,Nonprofit Organization,Real Estate,Real Estate,$100 to $500 million (USD)
496,Sr. Data Engineer,$80K - $112K (Glassdoor est.),"Object Technology Solutions, Inc (OTSI) has an immediate opening for Sr. Data Engineer
This is an on-site position in Dallas, TX OR Kansas City, MO (Long term)
Skills & abilities required:
Minimum 5+ years of experience as Data Engineer.
Prefer to have Databricks experience
Prefer to have understanding of embedding Python into Azure environment (containerization)
Expectation is that this position works with Data Scientist to rapidly deploy models into Azure DW and/or pipeline DW required data to Data Scientist requirements to build and test new models.
About us:
OTSI is a leading global technology company offering solutions, consulting, and managed services for businesses worldwide since 1999. OTSI serves clients from its 15 offices across 6 countries around the globe with a “Follow-the-Sun” model. Headquartered in Overland Park, Kansas, we have a strong presence in North America, Central America, and Asia-Pacific with a Global Delivery Center based in India. These strategic locations offer our customers the competitive advantages of onshore, nearshore, and offshore engagement and delivery options, with 24/7 support. OTSI works with 100+ enterprise customers, of which many are Fortune ranked, OTSI focuses on industry segments such as Banking, Financial Services & Insurance, Healthcare & Life Sciences, Energy & Utilities, Communications & Media Entertainment, Engineering & Telecom, Retail & Consumer Services, Hi-tech, Manufacturing, Engineering, transport logistics, Government, Defense & PSUs.
Our Center of Excellence:
Data & Analytics
Digital Transformation
QA & Automation
Enterprise Applications
Disruptive Technologies
Job Type: Contract
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: In person",4.6,"OTSi
4.6","Dallas, TX",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
497,Data Engineer,Employer Provided Salary:$150K,"Data Engineer
We are looking for a Data Engineer that will architect and implement systems handling the ingestion, cleaning, and structuring of data. They will be a member of the Data Science team and will have direct interaction with the people using the data ingested. The hire will also be responsible for managing our group’s core infrastructure including our region redundant pipeline orchestration servers (Airflow) and webserver stack (NGINX + Gunicorn + Django). The ideal candidate is an experienced data pipeline builder and core infrastructure guru.
Responsibilities:
Create and maintain an optimal data pipelines in Python + SQL
Build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python and SQL (Snowflake & SQL Server) in both on premise and cloud environments
Manage and improve the user facing server infrastructure (web + api) including authorization and load balancing
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that that provide actionable insights on both the user interactions and infrastructure loads
Technical Qualifications:
Python expert, notably savvy with data science stack (Pandas, NumPy, SciPy)
Relational SQL databases, Microsoft SQL Server and Postgres preferred
Snowflake cloud database experience
Pipeline and workflow management tools: Airflow, Luigi, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Athena, SQS, etc.
2+ professional years’ experience with Masters or 3+ professional years with Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Highly Valued:
Data Science/Analysis background
Computer science understanding, data structures, processes, threading, memory usage
Unix/Linux command-line experience
Broad understanding of equities, derivatives, futures, FX, or other financial-services instruments
Other Qualifications:
Excellent listening, and communication (both oral and written) skills
Self-starter and critical thinker, takes ownership of own projects and makes improvement suggestions for the entire infrastructure.
Proactive, assertive and attentive to details.
Can work independently and in a collaborative environment.
Can handle several projects with different priorities at the same time in a fast-paced environment.
Excellent self-management and problem-solving skills.
Results-oriented, can deliver quality code quickly
Millennium pays a total compensation package which includes a base salary, discretionary performance bonus, and a comprehensive benefits package. The estimated base salary range for this position is $150,000, which is specific to New York and may change in the future. When finalizing an offer, we take into consideration an individual’s experience level and the qualifications they bring to the role to formulate a competitive total compensation package.",3.8,"Millennium Management LLC
3.8","New York, NY",1001 to 5000 Employees,1989,Company - Private,Investment & Asset Management,Financial Services,Unknown / Non-Applicable
498,AWS Data Engineer,-1,"5+ years of data engineer experience in developing, implementing, delivering, and managing end-to-end data solutions using AWS Glue
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications
Proficiency in cloud data technologies, such as AWS S3, AWS Glue, EC2
Knowledge of Snowflake/AWS Redshift as a data warehousing solution
Advanced knowledge in designing, developing, implementing and managing data pipelines to deliver data or data insights for application, reporting, or analytics
Strong experience creating and maintaining functional and technical specifications documents
Strong experience creating test plans, test data sets, and automated testing ot ensure all components of the system meet specifications
Strong SQL technical experience such as linking IT applications to databases and creating and handling metadata
Strong programming skill in Python or Scala
Bonus skills:
Strong experience in NoSQL database (i.e., MongoDB)
Strong experience in streaming technology (i.e., Kafka, data bricks streaming)
Strong experience in working in the healthcare industry including PHI, HIPAA regulations, and BAA processes
?
AWS DATA ENGINEER RESPONSIBILITIES:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics
Support the creation of the new cloud infrastructure and data ecosystem in the cloud",5.0,"CEDENT
5.0",United States,1 to 50 Employees,-1,Contract,Computer Hardware Development,Information Technology,Less than $1 million (USD)
499,Data Engineer ll,Employer Provided Salary:$130K - $160K,"The Company
Have you ever found yourself or a loved one waiting hours and hours in a hospital Emergency Room to get care? Or have you ever had a surgery scheduled for months in the future that needed to happen sooner? Unfortunately, our healthcare system is full of these types of operational problems. Our work saves lives and helps hospitals cut tens of millions of dollars in operational costs, while improving the quality of care they’re able to deliver.
Qventus is a real-time decision making platform for hospital operations. Our mission is to simplify how healthcare operates, so that hospitals and caregivers can focus on delivering the best possible care to patients. We use artificial intelligence and machine learning to create products that help nurses, doctors, and hospital staff anticipate issues and make operational decisions proactively.
Qventus works with leading public, academic and community hospitals across the United States. The company was recognized by the 2019 Black Book Awards in healthcare for patient flow and by CB Insights as a 2019 top 100 Most Promising Company in Artificial Intelligence. Recently, Qventus won the Robert Wood Johnson Foundation Emergency Response for the Healthcare System Innovation Challenge through its work helping health systems across the country plan for and operate in the COVID pandemic.
The role
Qventus is looking for a Data Platform Engineer II to help scale our solutions, focusing on our analytical and data science needs. The Data Platform team acts as stewards for Qventus’ data. We stream hospital EMR to our core warehouses in real time, build out curated data layers to power our Healthcare AI & Analytical insights and overall ensure Qventus data users have the tools they need to explore and power the Qventus product at scale and cost to improve the lives of patients and doctors across the country.
As a Data Platform Engineer II, you will build and own significant components of the Qventus solution pipelines. You will be comfortable designing, building, and leading cross-functional initiatives with analytical and data science partners - from schema design, to pipeline design, to scaling services to support company expansion within the healthcare space (and HIPAA restrictions). You will have a strong passion for well designed data models and be motivated and excited to have an impact on the team and in the company and to improve the quality of healthcare operations.
Key Responsibilities
Work closely with core data users to understand product needs and design, build, tune and improve our core data assets and the overall end-to-end workflow of data users at Qventus (incl. designing data structures, building and scheduling data transformation pipelines, improving transparency etc.).
Automate & manage the lifecycle of data sets (schema development, deprecation, and iteration).
Improve the data quality and transparency of the pipelines (defining data requirements, identifying and implementing data observability tooling - lineage, sources, transformations).
Work closely with core team members to develop, test, deploy, and operate high quality, scalable software and raise engineering standards.
Key Qualifications
Demonstrated experience in data modeling / schema design and transformation pipeline implementation in collaboration with data science and analytics partners
Experience developing & coordinating execution in a fast paced dynamic environment across multiple technologies
Strong cross-functional communication - ability to break down complex technical components for technical and non-technical partners alike
Interest in mentoring and supporting new developers particularly in data modeling and analytical collaboration
3+ years of professional experience working with modern programming languages such as Java, C/C++, Python with a dedication to high code quality.
Nice to Have Skills
Degree in Computer Science, Engineering, or related field, or equivalent training / experience
Competence participating in technical architecture discussions to help drive high quality technical development within your team
Practical hands on experience with:
building large-scale, high complexity metrics and monitoring
ELK, DBT, Snowflake, AWS, Terraform, Looker, Ansible experience
Experience building and maintaining robust and efficient backend data systems with functional proficiency with AWS cloud services & modern data warehouse services (Snowflake)
We consider several factors when determining compensation, including location, experience, and other job-related factors.
Salary Range: $130,000 to $160,000 annually + equity + benefits- Qventus expects to hire for this position near the middle of the range. Only in truly rare or exceptional circumstances where a candidate's experience, credentials, or expertise far exceed those required or expected will we consider and offer at the top of the salary range.
Qventus offers a competitive benefits package including medical, dental, vision, paid time off, company holidays, and a stock option plan.
Qventus is an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
Candidate information will be treated in accordance with our candidate privacy notice which can be found here: https://qventus.com/ccpa-privacy-notice/
This position does not provide visa sponsorship.
Employment is contingent upon the satisfactory completion of our pre-employment background investigation and drug test.
#LI-REMOTE",4.3,"Qventus
4.3",Remote,51 to 200 Employees,2012,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
500,Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"Job title: Data Engineer ( USC GC )
Location: Dallas, TX ( Hybrid )
Client: Southwest Airlines
Looking for strong experience with Abinitio and AWS.
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Work Location: On the road",-1.0,Kommforcesolutions,"Dallas, TX",-1,-1,-1,-1,-1,-1
501,Data Engineer,Employer Provided Salary:$120K - $200K,"Verition Fund Management LLC (""Verition"") is a multi-strategy, multi-manager hedge fund founded in 2008. Verition focuses on global investment strategies including Global Credit, Global Convertible, Volatility & Capital Structure Arbitrage, Event-Driven Investing, Equity Long/Short & Capital Markets Trading, and Global Quantitative Trading. As a Data Engineer you would be responsible for building data pipelines and supporting Portfolio Managers and Risk teams.
Responsibilities:
Building data pipelines
Working closely with data vendors such as Bloomberg, Refinitiv, etc…
Taking this vendor data and normalizing/standardizing it or firm consumption
Taking the normalized data and customizing it to user specific needs.
Qualifications:
4+ years of experience in financial services
BS or MS in Computer Science or Computer Engineering
Strong technology/coding skills (Python)
Strong design skills to build extensible config/data-driven platforms
Good Financial data, specifically security master and/or knowledge of vendor datasets
Strong AWS data pipeline skills
Strong database skills – SQL and no-SQL
Strong problem solving skills
Ability work with datasets in Excel and other productivity tools
Nice to Have:
AWS Glue
Spark
Jupyter Notebook

Salary Range
$120,000—$200,000 USD",3.5,"Verition Group LLC
3.5","New York, NY",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
502,Senior Data Engineer,$87K - $114K (Glassdoor est.),"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.

This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration.
How will you make an impact?
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
What are we looking for?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework
You might be a good fit if you like to:
Leverage technology to deliver data solutions to drive business outcomes
Drive for continuous improvement of processes and solutions
Bring energy and commitment to excellence to drive delivery of high-quality solutions
Share knowledge and mentor team members on technologies and solutions
What you can learn on the job:
Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers
Grow practical understanding of agile work practices through application of the scrum framework to deliver work products
Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science
What does success look like?
First 30 days:
Understand the existing analytics data platform and technologies
Understand team’s work management processes
First 6 months:
Develop data solutions for the business including shared components and specific data sets
Learn business processes and develop necessary contacts throughout the organization
First year:
Continue to develop data solutions for the business
Become expert on the team for other data engineers to go to for mentoring in completing their work
Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,"AE Business Solutions
4.2","Milwaukee, WI",51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
503,"Data Engineer, Election Platforms (all-levels)",-1,"Job Description
The Washington Post is hiring a data engineer for our Election Platforms team, to work on our data pipelines, tools, and infrastructure ahead of the 2024 presidential election.
The Election Platforms team focuses on leveraging data, partnering with stakeholders, and pioneering serviceable tools to facilitate The Post’s election coverage. This role is part of Elections Engineering, a cross-functional group of full-stack engineers and data scientists, with varied technical and academic backgrounds, responsible for delivering live election results across The Post’s platforms.
The role will be focused on contributing to our technical stack and developing new technology to facilitate robust and lively election coverage. In particular, we’re looking for candidates who would be comfortable working on deploying high-impact code in production and have an interest in data and newsroom tooling. We also work on other data engineering projects related to elections, such as voter registration data and campaign finance, and occasionally write
open
source
software
. Most of our work is in Python and TypeScript/React/Node.
We typically work at a higher level of abstraction than a single story, and aim to create general purpose tools that are reusable and easy to deploy. Our ideal candidates are creative problem-solvers, who are comfortable writing production level code to solve real-world newsroom problems. This position is remote-friendly for work in the US.
Motivation
You have a desire to contribute to The Post’s journalism through technology.
You are passionate about refining and improving team process, culture, documentation and communication while tackling complex data science and engineering challenges.
You want to work at a deeper level of abstraction than a single story or beat.
You are excited about learning new technologies and applying them to the practice of journalism.
Responsibilities
Work on the software components of our live election architecture including deployment, observability, participating in code reviews, and more.
Work closely with our engineers and data scientists to develop and maintain pipelines and administrative apps/dashboards for election data.
Work on tooling to make data preparation easier and more efficient for our live election model.
Work with newsroom stakeholders to assess needs, build useful and reusable tooling, and contribute to storytelling.
Foster effective team collaboration and communication by improving and documenting internal tools, processes and guidelines.
Assist in attracting and hiring team members who want to work at the intersection of journalism and technology by actively participating in recruiting – and contributing feedback on our overall hiring process.
Qualifications
A minimum of one year of experience building and maintaining backend and data engineering applications.
Proficiency across some of the languages and frameworks used in our engineering stack such as Python, TypeScript/React/Node, and AWS services.
A desire to work on cutting edge, cross-functional tools to help with features, data science, and machine learning/big data work in a team setting.
Experience with election data is not necessary, but is a plus.
Experience developing and deploying newsroom tools and applications is not necessary, but is a plus.
Levels
If you have less than three years, but at least one year, we’d love to evaluate you for a data engineer role.
If you have more than three years but less than about seven years, we’d love to evaluate you for a senior data engineer role.
If you have more than about seven years, we’d love to talk to you about a principal data engineer role.
Wherever you are in your life or career, The Washington Post offers comprehensive and inclusive benefits for every step of your journey:
Competitive medical, dental and vision coverage
Company-paid pension and 401(k) match
Three weeks of vacation and up to three weeks of paid sick leave
Nine paid holidays and two personal days
20 weeks paid parental leave for any new parent
Robust mental health resources
Backup care and caregiver concierge services
Gender affirming services
Pet insurance
Free Post digital subscription
Leadership and career development programs
Benefits may vary based on the job, full-time or part-time schedule, location, and collectively bargained status.
The Post strives to provide its readers with high-quality, trustworthy news and information while constantly innovating. That mission is best served by a diverse, multi-generational workforce with varied life experiences and perspectives. All cultures and backgrounds are welcomed.
The innovation doesn’t end in the Newsroom – dozens of teams power The Washington Post. We are now hiring the next innovator – how will you Impact Tomorrow?
#washpostlife",4.2,"The Washington Post
4.2","Washington, DC",1001 to 5000 Employees,1877,Company - Private,Publishing,Media & Communication,Unknown / Non-Applicable
504,Data Engineer,$74K - $107K (Glassdoor est.),"Analyze Business Requirement Documents and Implement Technical Solutions for privacy related applications.
Develop ETL process for supporting Data Extraction, transformations and loading.
Perform data conversions and aggregations using different transformations such as Merge, Merge join, Union condition split, sort, order by. Derived columns convert and cast transformations and row count and lookup and fuzzy lookup transformations.
Develop UNIX scripts to load the data from Source server to Teradata and validate the files between different servers.
Develop new process to implement state level privacy regulations based on each state law in Big Data Platform.
Create Temperory/Fact tables, loading with data and writing Teradata and Spark SQL queries.
Optimize/tune ETL objects, indexing and partitioning for better performance and efficiency.
Validate the performance metrics and work on performance tuning for SQL, HQL and Spark SQL queries.
Perform testing and Provide test support for various level of testing phases like Unit, User Acceptance, Regression, Parallel and System testing.
Promote the components to production environment through CI/CD process by using Git hub .
Script task and execute SQL tasks to execute SQL code. Work on containers for loop and for each loop container to run a group of tasks into a single container and repeating tasks.
Create the data flow to extract data from sources to OLEDB Source, Excel, XML, flat files sources and destination is SQL data warehouse.
Minimum Education Required:- All the responsibilities mentioned above are in line with the professional background and requires an absolute minimum of a Bachelor’s degree in computer science, computer information systems, technology management, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.",4.9,"Wise Equation Solutions
4.9","La Vista, NE",51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
505,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
506,Data Engineer / Architect (Remote - US based),Employer Provided Salary:$101K - $216K,"Company Description

Due to the nature of this role, candidates must be geographically located, and authorized to work, in the United States.

Tidepool is a leading healthcare technology nonprofit company that is revolutionizing the way people with diabetes manage their condition. Our mission is to make diabetes data more accessible, actionable, and meaningful, empowering individuals and healthcare providers to make informed decisions and improve health outcomes. By leveraging cutting-edge technology and data-driven solutions, Tidepool is dedicated to making a positive impact on the lives of millions affected by diabetes.

Job Description

We are seeking a skilled and motivated Data Engineer / Architect to join our growing team. As a Data Engineer at Tidepool, you will play a pivotal role in designing, implementing, and optimizing our diabetes, product and business data infrastructure pipeline. You will collaborate closely with our Data Science and Data Analytics teams to ensure efficient data flow and enable advanced analytics and insights. Your expertise in data architecture, ETL processes, and database management will contribute to the development of innovative solutions that transform raw data into actionable information.

Essential Duties and Responsibilities
Design, develop, and maintain scalable and robust data pipelines, ensuring the efficient extraction, transformation, and loading (ETL) of data from various sources.
Collaborate with cross-functional teams, including Product, Engineering, to understand data requirements and develop data models that facilitate advanced analytics, machine learning, and predictive modeling. Work closely with Data Scientists and Data Analysts to ensure that our data pipeline architecture meets their needs.
Optimize data storage and retrieval systems to ensure high performance, scalability, and data integrity.
Implement and maintain data governance practices, including data quality monitoring, data lineage tracking, and metadata management.
Troubleshoot data-related issues and perform root cause analysis, ensuring timely resolution to minimize impact on data availability and accuracy.
Continuously evaluate and recommend improvements to existing data infrastructure, tools, and processes to enhance efficiency and reliability.
Stay up to date with emerging technologies, industry trends, and best practices in data engineering, and apply this knowledge to drive innovation within the team.

Qualifications
Bachelor's degree in Computer Science, Engineering, or a related field. Advanced degree is a plus.
Proven experience as a Data Engineer or similar role, with a strong understanding of data warehousing concepts, data modeling, and ETL processes.
Proficient in SQL and programming languages such as Python, Java, or Scala.
Experience with data processing tools and frameworks like Data Bricks or Apache Spark is highly desirable.
Solid knowledge of relational and NoSQL databases, data lakes, and data integration techniques.
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data services, such as S3, Redshift, BigQuery, or similar.
Experience in implementing data governance practices and ensuring data quality and integrity.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication skills with the ability to effectively articulate complex technical concepts to non-technical stakeholders.
Familiarity with diabetes or other types of health data is a plus
Understanding and knowledge of diabetes treatments, therapy and diabetes data is a big plus.

Additional Information

Salary range

$101,028 - $216,027 annually. To learn more about Tidepool's compensation philosophy please see Tidepool's Employee Handbook.
Benefits
Flexible PTO
Paid parental leave
Medical, Dental, and Vision coverage
Health and Childcare FSA
Flexible work schedule
Wellness and Productivity stipend
Continuing Education Reimbursement
Other Information
While many of Tidepool’s team members have a personal connection to diabetes, this is not a requirement. We ask that you have empathy for chronic conditions and you are prepared to learn about the diabetes experience.
This is a remote position. You’ll be working from home and interacting with a team of colleagues that works around the world. Learn more about working at Tidepool, including our approach to inclusion and diversity in this blog post.
Tidepool is an Equal Opportunity Employer. The company supports diversity and inclusion in its core values and does not discriminate against qualified employees or applicants because of race, color, religion, gender identity, sex, sexual preference, sexual identity, pregnancy, national origin, ancestry, citizenship, age, marital status, physical disability, mental disability, medical condition, military status, or any other characteristic protected by U.S. federal or state law or local ordinance. When necessary, the company will reasonably accommodate employees and applicants with disabilities if the person is otherwise qualified to safely perform all of the essential functions of the position.",-1.0,Tidepool,"Palo Alto, CA",1 to 50 Employees,2013,Nonprofit Organization,Health Care Services & Hospitals,Healthcare,$5 to $25 million (USD)
507,Data Engineer,$79K - $115K (Glassdoor est.),"Job Title :- Data Engineer
Location:- San Antonio, TX
Required:- Active Top Secret Clearance
We are looking for a talented Data Engineer to support the acquisition of mission-critical and mission-support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is performed mostly on customer site in Columbia, MD.
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.
Minimum Qualifications
Security Clearance - Must have a current Secret level security clearance and therefore all candidates must be a U.S. Citizen with a willingness to go to TS/SCI and take the CI poly after starting the position.
5 years experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.
Experience with programming languages such as Python and Java.
Proficiency with acquisition and understanding of network data and the associated metadata.
Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
Experience with Kibana and Elasticsearch.
Familiarity with various log formats such as JSON, XML, and others.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must have a Security+ or similar certification or the ability to obtain it immediately.
Must be able to work on customer site in Columbia, MD at least 4 days a week. (Subject to change)
Desired Skills (Optional)
Experience with NOSQL databases such as Accumulo desired
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
Job Type: Full-time
Schedule:
8 hour shift
Security clearance:
Top Secret (Preferred)
Work Location: In person",3.9,"Helm360
3.9","San Antonio, TX",201 to 500 Employees,1997,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
508,Data Insights Engineer,Employer Provided Salary:$85K - $95K,"Who We Are
We're purpose-driven. With every ride, we aim to redefine health and happiness. It's all about being more than a workout: SoulCycle is a mind-body-soul experience, built on community, love, respect, acceptance, and a lot of fun. It comes to life through the ride, the relationships, and the unparalleled hospitality. And all of that comes from our people. Join us—we'd love to have you.
Our Mission
To foster an open, diverse, & inclusive community—while embracing each unique individual exactly as they are. We empower each other by listening with an open mind, finding ways to learn and grow together, and always nurturing a sanctuary of trust. To make a real, lasting impact, we'll work nonstop to embrace and create change. Because nobody is equal until everyone is equal.
Job Description
The Data Insights Engineer will play a pivotal role in driving data-driven decisions at SoulCycle. You'll be responsible for building and maintaining the data infrastructure that supports all business functions, from marketing and operations to finance and customer experience, in addition to providing analysis to each of these teams. By leveraging your technical expertise and analytical skills, you will empower stakeholders to derive insights from data, enabling them to make strategic and informed decisions that positively impact the business.
Roles and Responsibilities
Insights and Recommendations: collaborate with cross-functional teams to understand business requirements, provide analytical support, and identify opportunities for data-driven improvements
Visualizations and Dashboarding: design and develop ad-hoc and recurring Looker reports; create and monitor business metrics; identify patterns, trends, and opportunities for performance improvement
Data Modeling: build, optimize, and document LookML data models that support quick and efficient analysis
Prediction: build predictive models that forecast business outcomes, customer behavior, and other relevant metrics
Qualifications
1-3 years of professional experience transforming and analyzing data across platforms such as Looker, Tableau, Mode, Jupyter Notebooks, Excel, and GCP/AWS. Looker/LookML experience is a plus.
Expert in SQL (able to write structured and efficient queries on large data sets) and familiarity with Python
Ability to identify patterns and trends in data and solve problems
Excellent communication skills to work with stakeholders to translate business needs and ideas into analyses and recommendations
Top-notch organizational skills and ability to manage projects in a fast-paced environment
Creative problem solving skills to find solutions to vague questions
Experience with Python data analysis and visualization packages is a plus (pandas, tensorflow, matplotlib, etc.)
Pay Range: $85,000 - $95,000 per year. This role is on-site 4 days a week.",3.9,"SoulCycle HQ
3.9","New York, NY",1001 to 5000 Employees,2006,Company - Private,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
509,Data Engineer,$67K - $96K (Glassdoor est.),"Massanutten Resort
Job Summary
The Data Engineer will play a crucial role in designing, developing, and maintaining the data infrastructure and systems that enable efficient and reliable data processing, storage, and analysis. The primary responsibility is to transform raw data into usable formats for various business applications and support data-informed decision-making processes. This role includes working in various Microsoft SQL Environments as well as supporting a Snowflake Data Warehouse. The Data Engineer is expected to have a broad understanding of data concepts and be avid about continuing education to keep up with the ever-changing data landscape.
Required
Degree in computer science or related file and 3+ years of experience with Python, SQL, and data visualization/exploration tools (in lieu of a degree, at least 5 years of experience in the above is equivalent)
Preferred
A degree in computer science or a related field
Certifications in database management and/or related technical certifications
Familiarity with advanced data concepts (i.e., AI and Machine Learning) and business intelligence/visualization tools
Typical Schedule
Days:
Monday-Friday, weekends and holidays as needed
Position will be required to be part of rotating on-call schedule
Hours:
8am-5pm, additional hours as needed
Core Responsibilities
Develop Data Pipeline: Design and implement scalable and efficient data pipelines, integrating diverse data sources and ensuring smooth data flow from ingestion to storage and processing
Transform and Model Data: Develop and maintain data transformation processes, such as ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform), to convert raw data into structured formats suitable for analysis and reporting. Create and manage data models to facilitate data integration and provide a consistent view of information across systems.
Implement and oversee Data Warehousing and Storage: Build and maintain data warehouses or data lakes, optimizing storage and retrieval processes to ensure high availability, reliability, and performance. Implement appropriate data governance and security measures to protect sensitive data.
Ensure Data Quality and Governance: Collaborate with data analysts and business stakeholders to understand data requirements and define data quality standards. Implement data validation, data profiling, and data cleansing techniques to ensure data accuracy, consistency, and integrity.
Optimize Performance: Identify and address performance bottlenecks in data processing and storage systems, leveraging optimization techniques and technologies to improve overall data engineering efficiency and reduce latency
Assist with Data Problem Solving: Support team with data related escalation issues, including help desk support tickets that are generated from our business users
Manage Data Integration and API Development: Establish connections and integrations between internal and external data sources, including APIs, databases, and third-party platforms. Develop and maintain data APIs to enable seamless data access and consumption by other applications and services.
Collaborate and Document: Work closely with cross-functional teams, including analysts, software engineers, and business stakeholders, to understand data requirements and ensure data solutions align with business objectives. Document data engineering processes, including data pipelines, transformations, and system architectures, to facilitate knowledge sharing and maintain a robust knowledge base.
Stay up-to-date with the latest advancements in data engineering, data management, and big data technologies. Evaluate and recommend new tools, frameworks, and methodologies to improve data engineering practices and enhance overall data infrastructure
Other duties as assigned by management
For more information, contact Ana at 540-289-4939.",3.6,"Great Eastern Resort Management, Inc.
3.6","McGaheysville, VA",1001 to 5000 Employees,-1,Self-employed,-1,-1,Unknown / Non-Applicable
510,"Manager, Data Engineer & ETL Processing",$83K - $118K (Glassdoor est.),"Company Description

About Spark Foundry:

Spark Foundry is a global media agency that exists to bring HEAT – Higher Engagement, Affinity, and Transactions – to brands. By combining flawless media fundamentals with aggressive innovation, Spark inspires consumers to pay more attention, to care more about our clients’ brands, and to buy more products and services from them.
Balancing the nimble spirit of a startup with the powerhouse soul of Publicis Media, Spark Foundry delivers the best of both worlds to a client roster that spans some of the world’s best and most beloved brands and companies. We combine boutique-caliber insights and service with the buying clout and first-look access of a global leader, bringing the heat to challenger brands that want to act like giants, and to giant brands that want to act like challengers.
With a bottom-up culture that celebrates diversity and aims for all voices to be heard, Spark has become a magnet for the industry’s best talent, with one of the best retention rates in the industry. And by applying a whole-person approach to professional and personal development, Spark develops a workforce that is well prepared for today’s challenges, and also poised to create meaningful careers in the years to come.
Because we know that heat arises the intersection of complementary forces, our professionals come from myriad disciplines and backgrounds: data, analytics, and insights, content and creative production, communications and strategy, finance and marketing, and sociology, psychology, and other liberal arts disciplines.

Job Description

Overview:
The Manager, Data Engineering & ETL Development is a key driver to build data platform leveraging best in class ETL practice and is also a strategic thinker and a talented data visualization expert with emphasis on dashboard reporting. The primary responsibility will be developing business intelligence platform to support media and marketing data for our clients. This position will allow you to be a significant contributor as part of the team to support easy access of data and visualizations to fuel stronger insights and media optimizations.
This position requires strong technical and tactical skill sets with an eye for numbers, intellectual curiosity, proficiency at problem solving, and a critical understanding of online media. The candidate must have a proven track record in managing ETL’s, APIs and business intelligence platforms. Candidate should be a team player. A “roll up the sleeves” approach is mandatory and a “get it done” attitude is a must. Specific responsibilities include coordination between the research, analytics & media teams ensuring high quality data projects are effectively delivered.
Successful candidates will be multi-dimensional ‘rising-stars’ who are able to employ complex problem solving skills, and are able to communicate these succinctly to a broad client and media stakeholder audience.
Role Objectives:
Responsible for loading and validating data into the data warehouse from various source systems
Analyze, develop, fix, test, review and deploy functionality, and bug fixes in ETL data pipelines
Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required
Building Data mappings between Source to Target systems
Provides support for technical issues and ensuring system availability
Work with business customers to identify and develop additional data and reporting needs
Understand how business intelligence platform/data technologies work and offer the ability to explain technical concepts in ordinary terms (be technically savvy - understand the opportunities and limitations)
Establish and manage data integrations utilizing a taxonomy nomenclature to make recommendations for data visualization to showcase media performance through the use of Datorama or Tableau
Perform regular quality assurance/quality control checks on assigned client campaigns to ensure the data is processing accurately
Design and build backend data streams and processes to automate reporting capability with data visualization tools
Contribute to client status and reporting calls, including presentation of reporting as required
Develop subject matter expertise in ETL, API development, and Business Intelligence platforms
Clearly define project deliverables, timelines, and dependencies for junior team members, internal stakeholders and clients
Collaborate on an inclusive team, where members openly communicate and collectively problem-solve
Strong ability to evaluate new technologies and present findings to team
Contribute to knowledge sharing efforts and mentorships
Complete other duties as assigned.

Qualifications
Bachelor’s degree or combination of education, and equivalent work experience is preferred in the field of computer science, management information systems or Information Technology
3+ years’ related experience ETL development and business intelligence platform management
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
3+ plus years’ development experience with database engines including Presto, Mongo db and Hadoop.
3+ years’ experience in ETL development, Strong Database (Modeling, SQL), SQL Server, Hadoop, AWS Redshift, Qubole.
Experience managing team of 2 or more associates/analysts preferred
Strong database modeling and SQL skills. Ability to write complex SQL statement, Procedures and data automation programs
Knowledge of ETL tools like Airflow, AWS Glue, Alteryx, SSIS, Qubole/Spark is a must
Knowledge of Python or similar programming languages required
Proficiency with Datorama, Tableau, or other data visualization tools is preferred
Experience working with AWS or other cloud technologies
Advanced user Microsoft Office Suite and reporting tools
Demonstrated expertise in core MS Excel functions (vlookup, pivot tables, data visualization)
Experience in designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.
Strong analytical and problem-solving skills
Strong verbal/written communication and interpersonal skills is required
Self-starter with ability to thrive in a fast-paced environment and able to function independently while providing status updates to a team of analysts
Cooperative, flexible, conscientious, dependable, resourceful, self-motivated, and team-oriented
Problem solving, time management, and critical thinking skills with a professional and positive attitude
Ability to work independently and as part of an agile team, participating in daily stand-ups, sprint planning and sprint review

Character:
The following qualities help drive success as member of the Spark Data and Analytics team:
Entrepreneurial, engaging, resourceful, curious, and self-directed spirit
Willing and easily roll sleeves up or down; love the nitty-gritty and the strategy
Collaborative approach to building cohesive, strong teams
Loving and living the intersections between brands, people, media, communications
Relentlessly passionate and resolute
Planning and time management excellence.
Embrace challenges
Proactive, especially in pushing for new opportunities, approaches, and ideas.
Keenly focused on action and solutions; thrives with deep critical thinking and analysis.
Pioneering insight attitude and research in-the-know.
Resourcefulness, flexibility and adaptability, strong ability to pivot when the need arises.
Inspired to be part of the insight journey/revolution with a growing, dedicated team

Additional Information

All your information will be kept confidential according to EEO guidelines.
23-2795",3.9,"Spark Foundry
3.9","Dallas, TX",51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
511,"Manager, Data Engineer & ETL Processing",$83K - $118K (Glassdoor est.),"Company Description

About Spark Foundry:

Spark Foundry is a global media agency that exists to bring HEAT – Higher Engagement, Affinity, and Transactions – to brands. By combining flawless media fundamentals with aggressive innovation, Spark inspires consumers to pay more attention, to care more about our clients’ brands, and to buy more products and services from them.
Balancing the nimble spirit of a startup with the powerhouse soul of Publicis Media, Spark Foundry delivers the best of both worlds to a client roster that spans some of the world’s best and most beloved brands and companies. We combine boutique-caliber insights and service with the buying clout and first-look access of a global leader, bringing the heat to challenger brands that want to act like giants, and to giant brands that want to act like challengers.
With a bottom-up culture that celebrates diversity and aims for all voices to be heard, Spark has become a magnet for the industry’s best talent, with one of the best retention rates in the industry. And by applying a whole-person approach to professional and personal development, Spark develops a workforce that is well prepared for today’s challenges, and also poised to create meaningful careers in the years to come.
Because we know that heat arises the intersection of complementary forces, our professionals come from myriad disciplines and backgrounds: data, analytics, and insights, content and creative production, communications and strategy, finance and marketing, and sociology, psychology, and other liberal arts disciplines.

Job Description

Overview:
The Manager, Data Engineering & ETL Development is a key driver to build data platform leveraging best in class ETL practice and is also a strategic thinker and a talented data visualization expert with emphasis on dashboard reporting. The primary responsibility will be developing business intelligence platform to support media and marketing data for our clients. This position will allow you to be a significant contributor as part of the team to support easy access of data and visualizations to fuel stronger insights and media optimizations.
This position requires strong technical and tactical skill sets with an eye for numbers, intellectual curiosity, proficiency at problem solving, and a critical understanding of online media. The candidate must have a proven track record in managing ETL’s, APIs and business intelligence platforms. Candidate should be a team player. A “roll up the sleeves” approach is mandatory and a “get it done” attitude is a must. Specific responsibilities include coordination between the research, analytics & media teams ensuring high quality data projects are effectively delivered.
Successful candidates will be multi-dimensional ‘rising-stars’ who are able to employ complex problem solving skills, and are able to communicate these succinctly to a broad client and media stakeholder audience.
Role Objectives:
Responsible for loading and validating data into the data warehouse from various source systems
Analyze, develop, fix, test, review and deploy functionality, and bug fixes in ETL data pipelines
Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required
Building Data mappings between Source to Target systems
Provides support for technical issues and ensuring system availability
Work with business customers to identify and develop additional data and reporting needs
Understand how business intelligence platform/data technologies work and offer the ability to explain technical concepts in ordinary terms (be technically savvy - understand the opportunities and limitations)
Establish and manage data integrations utilizing a taxonomy nomenclature to make recommendations for data visualization to showcase media performance through the use of Datorama or Tableau
Perform regular quality assurance/quality control checks on assigned client campaigns to ensure the data is processing accurately
Design and build backend data streams and processes to automate reporting capability with data visualization tools
Contribute to client status and reporting calls, including presentation of reporting as required
Develop subject matter expertise in ETL, API development, and Business Intelligence platforms
Clearly define project deliverables, timelines, and dependencies for junior team members, internal stakeholders and clients
Collaborate on an inclusive team, where members openly communicate and collectively problem-solve
Strong ability to evaluate new technologies and present findings to team
Contribute to knowledge sharing efforts and mentorships
Complete other duties as assigned.

Qualifications
Bachelor’s degree or combination of education, and equivalent work experience is preferred in the field of computer science, management information systems or Information Technology
3+ years’ related experience ETL development and business intelligence platform management
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
3+ plus years’ development experience with database engines including Presto, Mongo db and Hadoop.
3+ years’ experience in ETL development, Strong Database (Modeling, SQL), SQL Server, Hadoop, AWS Redshift, Qubole.
Experience managing team of 2 or more associates/analysts preferred
Strong database modeling and SQL skills. Ability to write complex SQL statement, Procedures and data automation programs
Knowledge of ETL tools like Airflow, AWS Glue, Alteryx, SSIS, Qubole/Spark is a must
Knowledge of Python or similar programming languages required
Proficiency with Datorama, Tableau, or other data visualization tools is preferred
Experience working with AWS or other cloud technologies
Advanced user Microsoft Office Suite and reporting tools
Demonstrated expertise in core MS Excel functions (vlookup, pivot tables, data visualization)
Experience in designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.
Strong analytical and problem-solving skills
Strong verbal/written communication and interpersonal skills is required
Self-starter with ability to thrive in a fast-paced environment and able to function independently while providing status updates to a team of analysts
Cooperative, flexible, conscientious, dependable, resourceful, self-motivated, and team-oriented
Problem solving, time management, and critical thinking skills with a professional and positive attitude
Ability to work independently and as part of an agile team, participating in daily stand-ups, sprint planning and sprint review

Character:
The following qualities help drive success as member of the Spark Data and Analytics team:
Entrepreneurial, engaging, resourceful, curious, and self-directed spirit
Willing and easily roll sleeves up or down; love the nitty-gritty and the strategy
Collaborative approach to building cohesive, strong teams
Loving and living the intersections between brands, people, media, communications
Relentlessly passionate and resolute
Planning and time management excellence.
Embrace challenges
Proactive, especially in pushing for new opportunities, approaches, and ideas.
Keenly focused on action and solutions; thrives with deep critical thinking and analysis.
Pioneering insight attitude and research in-the-know.
Resourcefulness, flexibility and adaptability, strong ability to pivot when the need arises.
Inspired to be part of the insight journey/revolution with a growing, dedicated team

Additional Information

All your information will be kept confidential according to EEO guidelines.
23-2795",3.9,"Spark Foundry
3.9","Dallas, TX",51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
512,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
513,AWS Data Engineer,-1,"5+ years of data engineer experience in developing, implementing, delivering, and managing end-to-end data solutions using AWS Glue
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications
Proficiency in cloud data technologies, such as AWS S3, AWS Glue, EC2
Knowledge of Snowflake/AWS Redshift as a data warehousing solution
Advanced knowledge in designing, developing, implementing and managing data pipelines to deliver data or data insights for application, reporting, or analytics
Strong experience creating and maintaining functional and technical specifications documents
Strong experience creating test plans, test data sets, and automated testing ot ensure all components of the system meet specifications
Strong SQL technical experience such as linking IT applications to databases and creating and handling metadata
Strong programming skill in Python or Scala
Bonus skills:
Strong experience in NoSQL database (i.e., MongoDB)
Strong experience in streaming technology (i.e., Kafka, data bricks streaming)
Strong experience in working in the healthcare industry including PHI, HIPAA regulations, and BAA processes
?
AWS DATA ENGINEER RESPONSIBILITIES:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics
Support the creation of the new cloud infrastructure and data ecosystem in the cloud",5.0,"CEDENT
5.0",United States,1 to 50 Employees,-1,Contract,Computer Hardware Development,Information Technology,Less than $1 million (USD)
514,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1.0,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
515,Sr. Data Engineer,$80K - $112K (Glassdoor est.),"Object Technology Solutions, Inc (OTSI) has an immediate opening for Sr. Data Engineer
This is an on-site position in Dallas, TX OR Kansas City, MO (Long term)
Skills & abilities required:
Minimum 5+ years of experience as Data Engineer.
Prefer to have Databricks experience
Prefer to have understanding of embedding Python into Azure environment (containerization)
Expectation is that this position works with Data Scientist to rapidly deploy models into Azure DW and/or pipeline DW required data to Data Scientist requirements to build and test new models.
About us:
OTSI is a leading global technology company offering solutions, consulting, and managed services for businesses worldwide since 1999. OTSI serves clients from its 15 offices across 6 countries around the globe with a “Follow-the-Sun” model. Headquartered in Overland Park, Kansas, we have a strong presence in North America, Central America, and Asia-Pacific with a Global Delivery Center based in India. These strategic locations offer our customers the competitive advantages of onshore, nearshore, and offshore engagement and delivery options, with 24/7 support. OTSI works with 100+ enterprise customers, of which many are Fortune ranked, OTSI focuses on industry segments such as Banking, Financial Services & Insurance, Healthcare & Life Sciences, Energy & Utilities, Communications & Media Entertainment, Engineering & Telecom, Retail & Consumer Services, Hi-tech, Manufacturing, Engineering, transport logistics, Government, Defense & PSUs.
Our Center of Excellence:
Data & Analytics
Digital Transformation
QA & Automation
Enterprise Applications
Disruptive Technologies
Job Type: Contract
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: In person",4.6,"OTSi
4.6","Dallas, TX",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
516,AWS Data Engineer Snowflake,-1,"Company Description

Orchestrated by adept technical architects with over fifty years of applied expertise, KYNITE is an advanced technology company specializing in the disciplines of: Blockchain, Cloud Services, Big Data & Analytics, Artificial Intelligence, Enterprise, Staff Augmentation and Managed Services
We are BigData Experts
We are Cloud Experts
We are Enterprise Architects
We are Artificial Intelligence Innovators
We are Technological Evangelists
We are Doers
We are Kynite

Additional Information

All your
This job is only for individuals residing in US
US Citizens, Green Card holders, EAD's can apply
W2, C2c,
Information will be kept confidential according to EEO guidelines.",-1.0,Kynite,"Myrtle Point, OR",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
517,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
518,Data Engineer,Employer Provided Salary:$120K - $200K,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!",-1.0,shaped.ai Inc.,"New York, NY",1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
519,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
520,Data Engineer- Hadoop,Employer Provided Salary:$63.00 - $66.00 Per Hour,"Position Summary:
Synthetic data engineer is the experienced professional who can bring together skills and technologies to be able design and develop solutions to deliver data required for training models/test applications. Candidate will be working with a team of resources using Python, Sqoop, pyspark to create the data in a most efficient manner. Candidate should have prior experience & knowledge in highly scalable production grade applications with a strong engineering acumen. Should be able to work in a collaborative, fast paced environment. Candidate must have knowledge and/or experience working with Hadoop ecosystem.
Skills:
• Advanced knowledge in Python Programming
• Familiarity with Faker module
• pyspark, Sqoop, Hadoop, Hive, Shell script, Hadoop Shell commands
• Good experience dealing with bulk data extracts and bulk data loads
• Ability to translate requirements from stake holders into the technical requirements and the code
• Experience on statistical distribution, agent to model , deep learning is a plus
• Background of MF environment is a plus
Job Type: Contract
Pay: $63.00 - $66.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
Hadoop: 7 years (Required)
Python: 5 years (Required)
ETL: 3 years (Preferred)
Work Location: Hybrid remote in Charlotte, NC 28203",3.9,"Data Inc
3.9","Charlotte, NC",501 to 1000 Employees,1983,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
521,Data Engineer,$77K - $104K (Glassdoor est.),"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Paid time off
Referral program
Vision insurance
Schedule:
Monday to Friday
No nights
No weekends
Ability to commute/relocate:
Atlanta, GA 30309: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 2 years (Required)
Language:
English (Required)
Work Location: Hybrid remote in Atlanta, GA 30309",3.7,"United Digestive
3.7","Atlanta, GA",501 to 1000 Employees,2018,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
522,Data Governance Engineer,Employer Provided Salary:$100K - $125K,"Performance Health is seeking a Data Governance Engineer to join our team. In this role, you will be responsible for leveraging advanced data analytics techniques to optimize our manufacturing operations, streamline supply chain logistics, enhance overall efficiency, and support master data governance initiatives.

Essential Job Duties & Responsibilities
Develop and implement predictive and prescriptive analytics models to forecast demand, optimize inventory levels, and improve production scheduling in alignment with healthcare industry standards.
Analyze and interpret large datasets from various sources, including production systems, distribution centers, and market trends, to identify opportunities for process improvement, cost reduction, and master data cleansing.
Collaborate with manufacturing, procurement, and distribution teams to define key performance indicators (KPIs), establish data-driven goals, and measure progress towards operational excellence.
Design and execute A/B tests to evaluate the impact of process enhancements and initiatives, continuously refining strategies for improved outcomes.
Utilize machine learning techniques for anomaly detection, fault prediction, and quality control to ensure compliance with regulatory standards and product quality assurance.
Develop data visualizations, dashboards, and reports to effectively communicate insights and recommendations to stakeholders at all levels.
Stay current with advancements in data science methodologies, tools, and technologies, and proactively identify opportunities to apply them to manufacturing and distribution challenges.
Collaborate with Operational Excellence and Data Governance and IT teams to ensure data accessibility, integrity, and security, and assist in data integration efforts.
Performs other duties as assigned

Job Qualifications
Bachelor’s degree in Data Analytics or related field
3-5 years of experience applying data science techniques to manufacturing, supply chain, or distribution challenges, preferably within the healthcare industry.
Proficiency in programming languages such as Python or R for data analysis and statistical modeling
Strong expertise in data manipulation, feature engineering, and data preprocessing techniques.
Proficiency in SQL for data querying and manipulation.
Experience with data visualization tools (e.g., Tableau, Power BI) to create clear and impactful visualizations.
Excellent problem-solving skills and the ability to work effectively in cross-functional teams.
Ability to travel 10% of the time, including overnight travel

Benefits
Our benefits include healthcare; insurance benefits; retirement programs; paid time off plans; family and parenting leaves; wellness programs; discount purchase programs.
This is a full-time position with a base salary range of $100,000-$125,000 plus benefits.

To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. The requirements listed above are representative of the knowledge, skills, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Performance Health is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to sex, gender, gender identity, sexual orientation, race, color, religion, national origin, disability status, protected Veteran status, age, genetic information, and any other characteristic protected by law.",2.7,"Performance Health Supply,LLC
2.7","Warrenville, IL",Unknown,-1,Company - Private,Health Care Products Manufacturing,Manufacturing,Unknown / Non-Applicable
523,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
524,Data Engineer,Employer Provided Salary:$100K - $145K,"Garner's mission is to transform the healthcare economy, delivering high quality and affordable care for all. By helping employers restructure their healthcare benefit to provide clear incentives and data-driven insights, we direct employees to higher quality and lower cost healthcare providers. The result is that patients get better health outcomes while doctors are rewarded for practicing well, not performing more procedures. We are backed by top-tier venture capital firms, are growing rapidly and looking to expand our team.
We are looking for a Data Engineer to support our technical teams by ensuring ease of access to data within our organization. The ideal candidate for this role will have strong technical skills, including Python, SQL, and AWS as well as a desire to be a hands-on contributor to building out a data platform from the ground up.
Main Responsibilities:
Build the data pipelines that power our business
Collaborate across disciplines to high-quality datasets
Protect our users' privacy and security through best practices
Support data pipelines in production
Our Tools:
Python, AWS, Snowflake, dbt, Terraform, Postgres
The ideal candidate has:
2+ years of experience building data pipelines in a fast-paced environment
Strong Python knowledge
Experience with Big Data technologies such as Snowflake, RedShift, BigQuery, or DataBricks
Ability to think in principles and frameworks to understand and decompose abstract problems
An aptitude to learn new technologies and tools quickly
Why You Should Join Our Team:
You are mission-driven and want to work at a company that can change the healthcare system
You want to be on a small, fast-paced team that nimbly moves to meet new challenges
You love ideating on new features and working with data to find new insights
You're excited about researching and working with the latest tools and technologies
The salary range for this position is $100,000 - $145,000. Compensation for this role will depend on a variety of factors including qualifications, skills and applicable laws. In addition to base compensation this role is eligible to participate in our equity incentive and competitive benefits plans.

Garner Health is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics.
Garner Health is committed to providing accommodations for qualified individuals with disabilities in our recruiting process. If you need assistance or an accommodation due to a disability, you may contact us at talent@getgarner.com.",3.5,"Garner Health
3.5","Dallas, TX",51 to 200 Employees,2019,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
525,Data Visualization Engineer,$77K - $101K (Glassdoor est.),"Passionate about precision medicine and advancing the healthcare industry?
Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.
With the advent of genomic sequencing, we can finally decode and process our genetic makeup. We now have more data than ever before but providers don't have the infrastructure or expertise to make sense of this data. We're on a mission to connect an entire ecosystem to redefine how genomic data is used in clinical settings.
What You'll Do:
Own the design, development and administration of advanced analytics dashboards and reporting and business intelligence tools. Beginning with requirements gathering across cross-functional teams, to the delivery of analytic solutions into production.
Collaborate with stakeholders, providing guidance, data and analytical support for developing new datasets, analyses and tools in support of strategic initiatives.
Perform deep-dive root cause and problem-solving analyses for and with laboratory, business operations and engineering teams.
Design and build data views to support advanced analytical reporting, self-service data analysis and exploration, utilizing multiple data and visualization technologies.
Work closely with technical and product teams to identify and align changes to internal systems in support of future data analysis needs and requirements.
Develop and facilitate data governance initiatives to assure data integrity across distributed systems.
Solve new and interesting problems.
Qualifications:
Bachelor's degree in Information Systems, Business Administration, Finance, Economics, Mathematics, Operations Research, Marketing or similar
3+ years of experience developing data sets for analysis use-cases
3+ years of experience with direct stakeholder management and ownership of full lifecycle of analytics/data projects and presenting analysis results to stakeholders
3+ years report-development and data visualization experience with BI tools (Looker, Tableau, etc.)
Ability to deliver both high-level explanations of complex problems while also able to deep dive into complex problems
Advanced SQL skills for querying and analysis of large, complex datasets
Must work well in a team environment and be able to partner with both business and technical team members and translate technical language to the non-technical need
Creative problem solver, ability to handle multiple projects, and strong work ethic needed
Excellent written and verbal communication skills
Highly organized and systematic, superb attention to detail
Able to complete tasks with a high degree of accuracy and an eye towards process improvement
Persistent and resilient person who thrives in a fast-paced, highly collaborative environment
Bonus points for:
Advanced analytics experience - classification/predictive modeling, optimization models, forecasting
Experience with Python for data engineering or advanced data analysis projects/use-cases
Healthcare domain knowledge and experience
Expert knowledge of database/data modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others
Experience building cloud-native applications and supporting technologies / patterns / practices including: GCP, AWS, Docker, CI/CD, DevOps, and microservices

#LI-EH1
#LI-Hybrid
We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.1,"Tempus
3.1","Chicago, IL",501 to 1000 Employees,2015,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
526,Data Engineer,$77K - $107K (Glassdoor est.),"Position Summary:
Reporting to the Director of Data Science, the Data Engineer will play a crucial role in designing, developing, and maintaining data pipelines and workflows to facilitate data integration, storage, and analysis. With the support of the Data Science team, this position will utilize proven expertise in Microsoft Azure cloud computing to implement and automate processes, ensure the health and efficiency of the data ecosystem, and bring unmatched value to our +1.5 million members across the country and across the world.

Duties and Responsibilities:
Lead architectural efforts for implementing the Data Science team's models and solutions.
Design, implement, and manage data pipelines using Azure Data Factory to extract, transform, and load data from various sources efficiently and securely.
Develop and optimize storage solutions on Microsoft Azure, ensuring data availability, scalability, and reliability.
Automate data-related jobs, processes, and workflows to streamline data delivery
Collaborate closely with cross-functional teams, including data scientists, analysts, and developers, to understand data requirements and support their data-related needs.
Design and maintain data models and architecture, enabling efficient data consumption by business applications and analytical tools.
Ensure data quality, integrity, and security throughout the data lifecycle.
Troubleshoot and promptly resolve data-related issues, performance bottlenecks, and system failures.
Research new data sources, third-party platforms, and data engineering best practices.
Stay current with the latest data engineering and cloud computing trends, particularly in the Azure ecosystem.
Assist Data Scientists with general analysis and dashboards, as needed.
Qualifications:
3-5+ years of relevant Data Engineering, Software Engineering, and/or Analytics experience.
Proved experience with Azure cloud solutions, preferably through your work portfolio and Azure certifications.
Strong proficiency in Azure Data Factory, Azure Storage, and other relevant Azure services.
Extensive experience designing and implementing automated data pipelines, ETL processes, and data workflows.
Advanced skills in SQL and Python for data manipulation, querying, and scripting tasks.
Familiarity with data warehousing concepts, modeling, and integration techniques.
Expert in integrating cloud solutions with external API sources and knowledgeable in creating in-house API solutions that meet our needs.
Excellent problem-solving abilities and a track record of handling complex data engineering challenges.
Strong communication skills and the ability to collaborate effectively with multidisciplinary teams.
Demonstrated ability to prioritize tasks, meet deadlines, and adapt to evolving project requirements.
Azure certifications (e.g., Azure Data Engineer Associate) will be a plus.
Willingness to travel up to 10%.
Organizational Overview:
The National Association of REALTORS® (NAR) is a team of professionals dedicated to providing world-class service to over 1.5 million REALTORS® working in the United States and around the world. The real estate industry is fast-paced and fast-changing--each year, our members participate in the sale, lease, and management of billions of dollars in real estate. As in every industry, our members' value proposition is constantly being challenged by innovation.

It is our mission to empower REALTORS® as they preserve, protect and advance the right to real property for all. We cannot do that without the ideas, passion, and commitment from our talented employees. As our greatest assets, employees are offered their pick of competitive benefits/perks and flexible work options. NAR is certified as a Great Place to Work®. Our most successful employees are committed to our Core Values, which are:

Putting members first
Leading change
Advancing Diversity and Inclusion
Giving respect
Collaborating
Communicating",3.7,"National Association of REALTORS®
3.7","Washington, DC",201 to 500 Employees,1908,Nonprofit Organization,Real Estate,Real Estate,$100 to $500 million (USD)
527,Data Engineer,$79K - $115K (Glassdoor est.),"Job Title :- Data Engineer
Location:- San Antonio, TX
Required:- Active Top Secret Clearance
We are looking for a talented Data Engineer to support the acquisition of mission-critical and mission-support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is performed mostly on customer site in Columbia, MD.
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.
Minimum Qualifications
Security Clearance - Must have a current Secret level security clearance and therefore all candidates must be a U.S. Citizen with a willingness to go to TS/SCI and take the CI poly after starting the position.
5 years experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.
Experience with programming languages such as Python and Java.
Proficiency with acquisition and understanding of network data and the associated metadata.
Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
Experience with Kibana and Elasticsearch.
Familiarity with various log formats such as JSON, XML, and others.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must have a Security+ or similar certification or the ability to obtain it immediately.
Must be able to work on customer site in Columbia, MD at least 4 days a week. (Subject to change)
Desired Skills (Optional)
Experience with NOSQL databases such as Accumulo desired
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
Job Type: Full-time
Schedule:
8 hour shift
Security clearance:
Top Secret (Preferred)
Work Location: In person",3.9,"Helm360
3.9","San Antonio, TX",201 to 500 Employees,1997,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
528,Data Engineer,Employer Provided Salary:$130K - $175K,"Rightworks is hiring a Data Engineer in Fort Worth for a Multi-Billion dollar private equity fund in Fort Worth. This position is fully in-office in Fort Worth (client will pay relocation if needed). Compensation is aggressive & flexible, typical 10% above your previous compensation level with tremendous growth opportunities. (Expected compensation range for this data engineer will fall somewhere between 130 & 180,000 per year depending on experience). 50 hours per week, paid lunch, casual office atmosphere.
-Looking for data engineer with multiple years of SQL Server maintenance experience.
-Optimally would like a data engineer with formal education in either MIS, Mathematics, Software Engineering, or Computer Science
-SQL maintenance, Data Extraction, Transforms, Macros
-Database maintenance
Job Type: Full-time
Pay: $130,000.00 - $175,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Tuition reimbursement
Vision insurance
Schedule:
10 hour shift
Ability to commute/relocate:
Fort Worth, TX 76102: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What compensation range are you looking for?
Are you willing to work in-person in Fort Worth, TX 50 hrs a week?
Experience:
Microsoft SQL Server: 1 year (Required)
Work Location: In person",4.8,"RightWorks Inc.
4.8","Fort Worth, TX",51 to 200 Employees,2006,Company - Private,HR Consulting,Human Resources & Staffing,$25 to $100 million (USD)
529,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
530,Sr. Data Engineer,$83K - $115K (Glassdoor est.),"Provides application design, analysis, programming, testing and documentation for the development, integration, enhancement, and maintenance of information systems solutions
Job Description:
BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Job Responsibilities
Formulates and defines system scope and objectives through research and fact-finding to design, develop, modify, or integrate complex information systems.
Devises or modifies application systems and procedures to optimize functional requirements including capacity, operating time, response time, and form of desired results.
Designs, codes, tests, debugs, and documents programs, subroutines, and scripts.
Develops and implements program/system test plans. Devises data verification methods and standard system procedures.
Responds to system failures and performance events by taking appropriate measures to reduce system downtime and eliminate recurrence of problems.
May conduct research on software and hardware products to justify recommendations and support management in budgeting and purchasing efforts.
Acts as expert technical resource to development staff in all phases of the development and implementation process.
Job Qualifications
Education
Bachelors Degree in Computer Science or equivalent work experience required. Equivalent years of experience are determined as one year of technical experience for every year of college requested.
Experience
1 year - Experience with information technology concepts, application development methodology, terminology, and standards required
5 years - Systems and programming experience required
1 year - Expert working knowledge of at least one programming language in use at BCBST.
Skills\Certifications
Proven skill with information technology in use at BCBST is required
Proven analysis, design, and coding skills, and demonstrated success in leading large and complex projects
Top level technical experience in one or more specialized areas of applications systems analysis and programming
Demonstrated ability to interpret and translate technical and/or or complex concepts into information meaningful to project team members and/or business personnel.
Be organized, reliable, and able to manage multiple tasks with exceptional work ethic.
Leadership skills
Must be able to communicate effectively with both technical and non-technical co-workers
Job Specific Requirements:
Preference for candidates with Cloud experience (GCP specifically)
Python knowledge/experience a plus
Preferred Skills:
Number of Openings Available:
1
Worker Type:
Employee
Worker Sub-Type:
Employee
Company:
BCBST BlueCross BlueShield of Tennessee, Inc.
Applying for this job indicates your acknowledgement and understanding of the following statements:
BCBST is an Equal Opportunity employer (EEO), and all employees and applicants will be entitled to equal employment opportunities when employment decisions are made. BCBST will take affirmative action to recruit, hire, train and promote individuals in all job classifications without regard to race, religion, color, age, sex, national origin, citizenship, pregnancy, veteran status, sexual orientation, physical or mental disability, gender identity, or any other characteristic protected by applicable law.
Further information regarding BCBST's EEO Policies/Notices may be found by reviewing the following page:
BCBST's EEO Policies/Notices
BlueCross BlueShield of Tennessee is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at BlueCross BlueShield of Tennessee via-email, the Internet or any other method without a valid, written Direct Placement Agreement in place for this position from BlueCross BlueShield of Tennessee HR/Talent Acquisition will not be considered. No fee will be paid in the event the applicant is hired by BlueCross BlueShield of Tennessee as a result of the referral or through other means.
All applicants will be advised that BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Tobacco-Free Hiring Statement
To further our mission of peace of mind through better health, effective 2017, BlueCross BlueShield of Tennessee and its subsidiaries no longer hire individuals who use tobacco or nicotine products (including but not limited to cigarettes, cigars, pipe tobacco, snuff, chewing tobacco, gum, patch, lozenges and electronic or smokeless cigarettes) in any form in Tennessee and where state law permits. A tobacco or nicotine free hiring practice is part of an effort to combat serious diseases, as well as to promote health and wellness for our employees and our community. All offers of employment will be contingent upon passing a background check which includes an illegal drug and tobacco/nicotine test. An individual whose post offer screening result is positive for illegal drugs or tobacco/nicotine and/or whose background check is verified to be unsatisfactory, will be disqualified from employment, the job offer will be withdrawn, and they may be disqualified from applying for employment for six (6) months from the date of the post offer screening results.
Resources to help individuals discontinue the use of tobacco/nicotine products include smokefree.gov or 1-800-QUIT-NOW.",3.8,"BlueCross BlueShield of Tennessee
3.8","Chattanooga, TN",5001 to 10000 Employees,1945,Nonprofit Organization,Insurance Carriers,Insurance,$5 to $10 billion (USD)
531,Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"Job title: Data Engineer ( USC GC )
Location: Dallas, TX ( Hybrid )
Client: Southwest Airlines
Looking for strong experience with Abinitio and AWS.
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Work Location: On the road",-1.0,Kommforcesolutions,"Dallas, TX",-1,-1,-1,-1,-1,-1
532,Data Engineer,Employer Provided Salary:$80K - $90K,"Job Title: Data Engineer
Classification: Exempt
Location: Washington DC

DUTIES:
Design and build new data solutions to support our data science platform, business intelligence tools and other core business applications.
Develop a variety of data workflows, pipelines, and ETL processes using cloud platform products and Indeed’s internal infrastructure and tools such as Snowflake, Redshift, Matillion, dbt, SSIS and Python.
Constantly improve our data platform architecture and software development processes to deliver greater software quality, pipeline transparency and team velocity.
Create new datasets to power our different systems or to be used in our visualization tools, like Tableau and Domo.
Build and maintain data that support our marketing, sales, finance, billing operations, and customer care partners.
Develop data quality and governance automations to ensure the accuracy and quality of the data through inspection, validation, processing, anomaly detection and auto-corrections.
Appropriately prioritize incoming and existing requests to ensure alignment across teams and the work being performed has the highest impact.
Utilize various database development and querying tools, including Python, SQL, database management systems, cloud columnar data warehousing platforms, data extraction and transformation tools, and data processing/data modeling tools for big data.
Other duties as assigned within area of responsibility.

REQUIREMENTS: Bachelor’s degree or foreign equivalent degree in Data Science, IT, Database Technology, or related technical field.2 years of hands-on data processing and data modeling experience in a big data environment.
Qualified candidate must possess two (2) years of experience with:
Programming in Python, SQL, and/or TSQL
Development in a Linux environment and Windows environment
Advanced SQL scripting
Database Management Systems (for example, Postgres, MySQL or MS SQLServer)
Cloud Columnar Data Warehouse Platforms (for example Redshift, Snowflake or Vertica)
Data extraction and transformation tools (for example Fivetran, Matillion, Airflow, Talend, dbt, SSIS or custom Python scripting)
Business Intelligence and Data Visualization Systems (for example, Domo, Tableau or Power BI)
Any amount of experience in or demonstrated knowledge* of:
Algorithms and data structures
Fast-paced, sales-force driven business models
CI/CD tools
Commerce Cloud platforms
ERD and data flow/exchange modeling
Knowledge may be demonstrated through education, training, and/or experience.
Full-time telecommuting available. Up to 5% travel required. SALARY: $80,000 - $90,000/year
LOCATION: 1055 Thomas Jefferson St NW, Washington, DC 20007
TO APPLY: Please apply online at https://cleanchoiceenergy.com/careers, reference CC101.

bURtzGDOtI",4.5,"CleanChoice Energy
4.5","Washington, DC",51 to 200 Employees,-1,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",$25 to $100 million (USD)
533,"Data Center Network Engineer, Infrastructure Network Engineering",-1,"What to Expect
Tesla is currently seeking a Network Engineer to join our Data Center team. This role will provide network design, implementation, and operational support for Tesla's Data Centers.
What You’ll Do
Help design, build and maintain new and existing Data Centers
Work closely with other team members on design and initiatives; maintain and grow existing data center networks.
Work with Tesla’s key application teams to support their growth, including Tesla Autopilot team.
Provide high availability & reliability to network
Requirements gathering, analyze, and propose solution to networking needs.
Monitor, analyze, and report metrics of network services.
Develop automation methods to rapidly deploy, configure, and update network equipment.
Assist with network troubleshooting.
Conduct product POC evaluation.
Document network knowledge base and operational “Run-Book.”
Must be able to work occasional weekends, after hours, and holidays.
Participate in on call rotation.
May require unscheduled after-hours work. 10-20% travel required as necessary.
What You’ll Bring
4+ years’ experience mid-large global enterprise networking infrastructure
Experience with mid/large-scale networks in a global environment
Juniper, Arista and Palo Alto Networks hardware
Experience in IP networking, L2/L3 network protocols (spanning-tree, OSPF, BGP), TCP/IP, DHCP, DNS, end to end QOS, VLAN, VRRP, LACP, MC-LAG, EVPN with VXLAN, ACL and infrastructure cabling.
Basic knowledge of AWS, Azure or GCP.
Experience with various tools such as Protocol Analyzer, SNMP, flow, IPAM, RADIUS, Splunk, network taps, and load/stress testing",3.6,"Tesla
3.6","Fremont, CA",10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$1 to $5 billion (USD)
534,Data Engineer,$84K - $119K (Glassdoor est.),"Job Description

Proficiency with major tools such as SQL, Python, R, NiFi and Git. Demonstrated experience with ETL, cleaning, management, optimizing performance and processing large volumes of data. Familiarity with industry best-practices for software-hardware optimization when processing large sets of data. Experience with ML, with statistical modeling, time-series forecasting, and/or geospatial analytics. Experience with Hadoop, Spark, or other parallel storage/computing processes ETL programming experience.

Required Skills

Experience with SQL database structures and mapping of data structures

Experience with Nifi working with a parallel storage (ie. Hadoop/Spark or other), and the Extract-Transform-Load processes (ETL)

Data performance optimization-how to optimize analytical workloads

1. Use case and concurrent user volumes (Hardware)
2. Appropriate or transactional or analytical (databases)
3. Data model is optimized for queries and report generation (Data Model)
4. Increase performance by Limit by Date and Use Data Aggregation (Data Volume)

Ability to create operating system level scripts to perform ETL opeations on SQL data bases

Test-driven development of software solutions for the extraction, transacting, and loading of data using efficient languages, e.g. Nifi, Java, Python, SQL, and R.

Demonstrated success in developing software applications through all stages, including requirements generation/gathering, data engineering (data extract/transform/load), web service implementation, and user interface.

Map-Reduce technology experience for data management

Open source framework tools for statistical modeling and statical modeling experience

Experience with link/graph analysis

Work with users to define their data requirements and to assist with ad hoc data analysis.

Troubleshoot complex problems and provide support for software systems and application issues.

Undergraduate or Master’s Degree in Computer Science, Electrical or Computer Engineering or related technical discipline, or the equivalent combination of education, technical training, or work experience.

Tools:

Workflow tools-NiFi, Airflow, Prefect, Apache Flink

NoSQl Databases-ElasticSearch, DynamoDB, Redis

Relational Database Management Systems-PostgresSQL, and MySQL

Data Management technology-EMR, Spark

About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1.0,The Swift Group,"Chantilly, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
535,Data Engineer,$88K - $120K (Glassdoor est.),"Data Engineer
Engineering
REMOTE OPPORTUNITY

About Catalytic Data Science (CDS):
Catalytic Data Science is a groundbreaking cloud R&D platform designed to integrate the volumes of scientific resources, data, and analytic tools while providing the ability to network with colleagues in one secure and scalable environment. By enabling R&D teams to work more collaboratively and improving productivity company-wide, the Catalytic platform helps teams achieve key R&D milestones faster and with greater accuracy. Our customers are passionate about making the world a better place, and we are inspired by the opportunity to help them.

The Role:
You are a Data Engineer with experience in processing terabytes of data. You have experience in creating and automating scalable, fault-tolerant and reproducible data pipelines using Amazon AWS technologies. You are interested in helping to create a platform completely built on top of AWS. You are eager to join a team of Life Scientists and Software Engineers that believe the brightest minds in research should have the best tools to drive innovation.

What You’ll Do:

Build & operate automated ETL pipelines that process terabytes of text data nightly
Develop service frontends around our various backend datastores (AWS Aurora MySQL, Elasticsearch, S3)
Perform technical analyses and requirements specification with our product team on data service integrations
Help customers bring their data to the platform

What You Know:

Must Haves:

Python 3 or Java programming experience, preferably both
Day-to-day experience using AWS technologies such as Lambda, ECS Fargate, SQS, & SNS
Experience building and operating cloud-native data pipelines
Experience extracting, processing, storing, and querying of petabyte-scale datasets
Familiarity with building and using containers
Familiarity with event-based microservices

Nice-to-Haves:

Prior experience with Elasticsearch (custom development and/or administration) is a huge plus
Prior work with text and natural-language processing
Knowledge of Graph databases

What do we love in team members?

Your specialization is less important than your ability to learn fast and adapt to shifting technologies. We’re especially fond of people who:

Focus on customer’s needs and our company’s goals, not just writing code
Iterate until customers love what you’ve built
Self-start and initiate
Self-organize
Strive to grow personally and professionally, beyond just expanding technical abilities
Love to experiment with new technology and share knowledge with the team

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.",4.2,"Catalytic Data Science
4.2","Boston, MA",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
536,Data Science Engineer,-1,"Build the solution that transforms the real estate industry!
We are looking for a Data Science Engineer superhero with a sixth sense for data who can ignite our day-to-day activities with their creativity.
Want to infuse a $30B+ sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? Looking for a fully remote startup culture supported by a profitable business model? Join Mashvisor and help us build an entirely new type of real estate model.

Our Values

Customer Obsessed – We always put our customers first.
Solution Driven – We solve problems that other people are afraid to.
Product led: We are always one step ahead of our customer's needs and create / add features they love every time
One Team – We believe inclusion and teamwork produce the best results.
Open and Direct – We communicate with honesty and respect to our colleagues, customers, and partners.

What You’ll Do
Designing, developing, and researching Machine Learning systems, models, and schemes
Studying, transforming, and converting data science prototypes
Searching and selecting appropriate data sets
Performing statistical analysis and using results to improve models
Training and retraining ML systems and models as needed
Identifying differences in data distribution that could affect model performance in real-world situations
Visualizing data for deeper insights
Analyzing the use cases of ML algorithms and ranking them by their success probability
Understanding when your findings can be applied to business decisions
Enriching existing ML frameworks and libraries
Verifying data quality and/or ensuring it via data cleaning

What You’ll Need
BS or Masters degree in Mathematics, Statistics, Economics, Data Science or another quantitative field
3+ years of hands-on experience utilizing data science to manage, enhance and develop models and deploy solutions to solve complex business problems
Expertise in SQL and programming in SQL, R, Python, C++, Java, and beneficial to know Lisp and Prolog
Strong organizational, interpersonal, and communication skills (both written and verbal)
A bias towards solving problems from a customer-centric lens and an intuitive sense for how the work aligns closely with business objectives
Solid experience with managing databases and datasets and structuring and optimizing the framework
A thorough understanding of SQL databases
Bonus: background in US real estate data, insurance or financial markets analysis
The ideal candidate will be a creative problem solver with an excellent work history on data analytics projects.

We want the work you do here to be the best work of your life.
Compensation: We offer a great salary with a yearly bonus based on performance.
Attitude: Work with a Can-Do team across the world.
Freedom: Work anywhere, anytime.
Time Off: Yearly vacations and sick leaves.
Responsibility: Ability to excel in a fully remote work environment.
Are you the one?",3.7,"Mashvisor Inc.
3.7",Remote,1 to 50 Employees,2015,Company - Private,Real Estate,Real Estate,$1 to $5 million (USD)
537,Data Pipeline Engineer,$76K - $106K (Glassdoor est.),"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
The position:
Design and develop scalable data pipeline processes (including ingestion, cleansing, curation, unification, etc.)
Automate the processing of inbound client data feeds
Design and develop tools and processes to support automated data profiling and data quality methodologies
Work with our data science team to assist with the development of feature store data including data prep, enrichment, and feature engineering for AI/ML
Write and maintain documentation on data pipelines
Provide periodic support to our customer success team Skills & Experience
BS / MS in Computer Science, Engineering, or applicable experience
3+ Year using Python (Pandas/NumPy) in a production environment
3+ Year using PowerShell in a production environment
Expertise with ETL/ELT and the development of automated validation and data pipelines
Understand database design and data manipulation and transformation methodologies
Keen understanding of EDW, master data management and other database design principles
Experience designing solutions using a range of AWS Services
Experience with data engineering and workflow management frameworks such as Airflow and dbt
Comfortable working with high volume data in a variety of formats
Experience with CI/CD such as Jenkins
Experience with version control tools: Git preferred
Excellent verbal and written communication
Familiarity with healthcare data is a plus
Familiarity with ML pipelines, principles and libraries is a plus
Experience with REST API is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation",5.0,"Decision Point Healthcare
5.0","Boston, MA",1 to 50 Employees,2013,Company - Public,-1,-1,Less than $1 million (USD)
538,Data Engineer,$67K - $96K (Glassdoor est.),"Massanutten Resort
Job Summary
The Data Engineer will play a crucial role in designing, developing, and maintaining the data infrastructure and systems that enable efficient and reliable data processing, storage, and analysis. The primary responsibility is to transform raw data into usable formats for various business applications and support data-informed decision-making processes. This role includes working in various Microsoft SQL Environments as well as supporting a Snowflake Data Warehouse. The Data Engineer is expected to have a broad understanding of data concepts and be avid about continuing education to keep up with the ever-changing data landscape.
Required
Degree in computer science or related file and 3+ years of experience with Python, SQL, and data visualization/exploration tools (in lieu of a degree, at least 5 years of experience in the above is equivalent)
Preferred
A degree in computer science or a related field
Certifications in database management and/or related technical certifications
Familiarity with advanced data concepts (i.e., AI and Machine Learning) and business intelligence/visualization tools
Typical Schedule
Days:
Monday-Friday, weekends and holidays as needed
Position will be required to be part of rotating on-call schedule
Hours:
8am-5pm, additional hours as needed
Core Responsibilities
Develop Data Pipeline: Design and implement scalable and efficient data pipelines, integrating diverse data sources and ensuring smooth data flow from ingestion to storage and processing
Transform and Model Data: Develop and maintain data transformation processes, such as ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform), to convert raw data into structured formats suitable for analysis and reporting. Create and manage data models to facilitate data integration and provide a consistent view of information across systems.
Implement and oversee Data Warehousing and Storage: Build and maintain data warehouses or data lakes, optimizing storage and retrieval processes to ensure high availability, reliability, and performance. Implement appropriate data governance and security measures to protect sensitive data.
Ensure Data Quality and Governance: Collaborate with data analysts and business stakeholders to understand data requirements and define data quality standards. Implement data validation, data profiling, and data cleansing techniques to ensure data accuracy, consistency, and integrity.
Optimize Performance: Identify and address performance bottlenecks in data processing and storage systems, leveraging optimization techniques and technologies to improve overall data engineering efficiency and reduce latency
Assist with Data Problem Solving: Support team with data related escalation issues, including help desk support tickets that are generated from our business users
Manage Data Integration and API Development: Establish connections and integrations between internal and external data sources, including APIs, databases, and third-party platforms. Develop and maintain data APIs to enable seamless data access and consumption by other applications and services.
Collaborate and Document: Work closely with cross-functional teams, including analysts, software engineers, and business stakeholders, to understand data requirements and ensure data solutions align with business objectives. Document data engineering processes, including data pipelines, transformations, and system architectures, to facilitate knowledge sharing and maintain a robust knowledge base.
Stay up-to-date with the latest advancements in data engineering, data management, and big data technologies. Evaluate and recommend new tools, frameworks, and methodologies to improve data engineering practices and enhance overall data infrastructure
Other duties as assigned by management
For more information, contact Ana at 540-289-4939.",3.6,"Great Eastern Resort Management, Inc.
3.6","McGaheysville, VA",1001 to 5000 Employees,-1,Self-employed,-1,-1,Unknown / Non-Applicable
539,Senior Data Engineer,Employer Provided Salary:$71.00 Per Hour,"Role: Application Developer/Senior Data Engineer-RID00005396
Location: Melbourne FL (Hybrid Onsite and remote)
Duration: 12 months
Must- Haves (Hard Skills):
· 4+ years’ experience developing high performance queries using Microsoft SQL Server & SQL Server Management Studio
· Hands on performance tuning
· Looking for a software developer not a Database Administrator.
Must- Haves (Soft Skills):
· Strong verbal and written communication skills
· Good team player
· Collaboration
Job Description:
1. Our Client is seeking a Software Engineer to join the Mission Networks Engineering team that provides modern, secure, reliable and resilient telecommunications networks and information management systems. Join the team developing critical communication capabilities for air traffic control, air-to-ground data communication, secure access to situational awareness data and secure information sharing to state agencies.
2. The Software Engineer will be responsible for the design and development of an enterprise Java application leveraging SQL procedures, functions, and packages. Work with SQL Server Management Studio to evaluate and improve performance of queries and stored procedures. Collaborate in a cross-functional team that includes Software Developers, DBAs, System Engineers and System Administrators to maintain applications supporting managed services for the FAA.
Essential Functions:
· Develop & maintain high performance queries for relational database management systems
· Design, develop & maintain Java application(s) for Operational Support Systems (OSS) & Business Support Systems (BSS)
· Troubleshoot issues reported by internal & external customers in support of operational systems
Qualifications:
· Bachelor’s Degree and minimum 6 years of prior relevant experience or a Graduate Degree and a minimum of 4 years of prior related experience.
· 4+ years’ experience developing high performance queries using Microsoft SQL Server & SQL Server Management Studio
· 3+ years’ hands-on experience developing applications with Java (or similar Object Oriented language) and with front-end technologies such as JavaScript, HTML and CSS
· Obtain and maintain an FAA public trust clearance.
Preferred Additional Skills:
· Database Design, query optimization, index design, stored procedure.
· Agile development experience including Scrum and DevOps.
· Experience with Git, Jenkins, CI/CD.
· Experience with JavaScript libraries like jQuery, D3, or similar libraries.
· Experience working in Linux environment like RHEL.
Job Types: Full-time, Contract, Permanent
Salary: Up to $71.00 per hour
Schedule:
8 hour shift
Day shift
Holidays
Monday to Friday
On call
Weekend availability
Education:
Bachelor's (Preferred)
Experience:
SSAS, SSRS, SSIS: 5 years (Preferred)
SQL Server Management Studio: 4 years (Preferred)
HTML, CSS, Javascript: 3 years (Preferred)
Work Location: Remote",4.4,"COMTEC INFORMATION SYSTEMS
4.4","Melbourne, FL",501 to 1000 Employees,1996,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
540,Senior Data Engineer (Remote),Employer Provided Salary:$50.00 - $70.00 Per Hour,"Company Overview: IT Giants INC is a dynamic and innovative technology company at the forefront of delivering advanced data solutions across various industries. We are seeking an experienced Data Engineer with expertise in cloud platforms, programming languages, database technologies, and analytics tools. If you also have a strong background in the airline domain, this is an opportunity to join our dedicated team and shape the future of data-driven decision-making in the aviation sector.
Position Overview: As a Data Engineer at IT Giants INC, you will play a pivotal role in designing, building, and maintaining sophisticated data pipelines and analytical solutions for our airline industry clients. Leveraging your deep understanding of both data engineering and the airline domain, you will empower our clients to harness the power of their data to improve operations and customer experiences.
Key Responsibilities:
Data Pipeline Development: Architect, develop, and maintain end-to-end data pipelines, integrating data from diverse sources, including cloud platforms (GCP, Azure, AWS), APIs, and various databases (Teradata, Oracle, PostgreSQL, NoSQL).
Data Transformation and Modeling: Apply your expertise in SQL, Python, and Alteryx to transform raw data into meaningful insights, while ensuring data integrity and accuracy.
Cloud Expertise: Utilize your proficiency in GCP, Azure, and AWS to design and implement scalable and cost-efficient cloud-based solutions that meet the unique demands of the airline industry.
Database Management: Manage relational and NoSQL databases to store and retrieve data efficiently, ensuring optimal performance and security.
Analytics and Reporting: Collaborate with data scientists and analysts to develop interactive dashboards and reports using tools like Tableau, QuickSight, and CloudWatch, enabling stakeholders to make informed decisions.
Data Quality and Governance: Implement data quality checks, validation processes, and adherence to industry regulations to ensure data accuracy and compliance.
Version Control and Collaboration: Utilize GitLab and other version control tools to collaborate with cross-functional teams on code management and deployment.
Real-time Monitoring: Develop real-time data processing solutions using technologies like CloudWatch to enable timely insights into operational metrics.
Linux Environment: Leverage your proficiency in Linux to optimize system performance and troubleshoot issues as they arise.
Domain Knowledge: Apply your deep understanding of the airline industry to structure data effectively and drive actionable insights from the data.
Qualifications:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
years of proven experience in data engineering, with a focus on cloud platforms (GCP, Azure, AWS), Python, SQL, and relevant database technologies.
Experience with Alteryx, Tableau, and data visualization tools for creating impactful dashboards.
Proficiency in managing databases such as Teradata, Oracle, PostgreSQL, and NoSQL databases.
Strong command of Linux environments and version control using GitLab.
Familiarity with CloudWatch, QuickSight, and real-time data processing is advantageous.
Domain expertise in the airline industry, including a solid understanding of aviation data, standards, and operational challenges.
Excellent problem-solving skills and the ability to troubleshoot complex data issues.
Effective communication skills to collaborate with technical and non-technical teams.
Proven ability to work independently and in a team, delivering high-quality solutions.
Join Our Team: If you are a driven and knowledgeable Data Engineer with a passion for both technology and the airline industry, IT Giants INC offers an exciting opportunity to contribute to transformative data solutions. Join us in our mission to revolutionize data analytics within the aviation sector and drive meaningful impact.
Job Types: Full-time, Contract
Pay: $50.00 - $70.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Work Location: Remote",5.0,"IT Giants
5.0",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
541,"Data Engineer (AWS, Python)",Employer Provided Salary:$50.00 - $52.00 Per Hour,"Title: Data Engineer (AWS, Python)
Location: Remote
Duration: 12+ Months
Job Description:
Typical task breakdown:-
Daily standup, collaborate with peers, work independently to complete sprint commitments, participate in agile ceremonies.
Interaction with team:-
Daily stand up meetings with team – working mainly with Helios Platform adoption team.
Work environment:
Education & Experience Required:
Bachelor's in computer science required plus 7+ Working experience .
Masters degree with 6+ years working expeirence acceptable as well.
Preferred: AWS cloud certifications
Technical Skills
(Required)
Exp with Object oriented programming - Python (4+ years)
AWS (3+ years overall exp) lambda and/or glue, EMR, S3
SQL (3+ years)
2+ years exp in test driven development, unit test, integration testing, regression testing
(Desired)
Event driven framework experience.
Kinesis or Kafka experience
Familiar with working in Agile methodology
Soft Skills
(Required)
Strong written and verbal communication, experience working as a member of agile engineering team, ability to commit to and deliver work within allocated sprint window.
Job Type: Contract
Pay: $50.00 - $52.00 per hour
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",5.0,"DSMH LLC
5.0",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
542,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1.0,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
543,Data Engineer,Employer Provided Salary:$45.00 Per Hour,"Job Title: GCP Data Engineer
Duration: 12+ Months
Location: Hartford, CT (Onsite Role)
Tax term : W2 & 1099
Job Description:
As a GCP Data Engineer, you will play a pivotal role in designing, implementing, and optimizing data solutions on the Google Cloud Platform (GCP). Your expertise will be essential in leveraging various technologies to build robust data pipelines and contribute to the overall success of data-driven initiatives.
Roles and Responsibilities:
Design, develop, and maintain efficient data pipelines using Python or PySpark on the Google Cloud Platform.
Seamlessly integrate data solutions using GCP services like Google Cloud Storage, BigQuery, Dataflow, and Pub/Sub.
Utilize expertise in the Hadoop ecosystem, including HDFS and Hive, to craft seamless data workflows.
Transform raw data into insights by creating complex SQL queries for data transformation and analysis.
Implement data quality checks and validation procedures to ensure accuracy and completeness.
Optimize data pipelines and processes for maximum performance and efficiency.
Collaborate with cross-functional teams to understand data requirements and deliver tailored solutions.
Create comprehensive documentation for pipelines, workflows, and processes.
Stay updated with the latest data engineering and cloud technology trends.
Tackle complex challenges with a proactive problem-solving mindset.
Required Qualifications:
Expertise in Python/PySpark and hands-on experience with Google Cloud Platform (GCP) for data engineering tasks.
Strong understanding of the Hadoop ecosystem, including HDFS and Hive.
Proficiency in SQL for effective data manipulation and analysis.
Experience working with cloud platforms, preferably Google Cloud Platform (GCP).
Strong problem-solving skills and analytical thinking.
Effective communication and collaboration abilities.
Adaptability to evolving data engineering technologies and practices.
Job Type: Contract
Salary: From $45.00 per hour
Experience level:
9 years
Application Question(s):
What is your work authorization?
Experience:
Data engineer: 9 years (Required)
Google Cloud Platform: 5 years (Required)
Hadoop: 5 years (Required)
Python: 4 years (Preferred)
SQL: 4 years (Preferred)
Work Location: On the road",-1.0,Conglomerate IT,"Hartford, CT",-1,-1,-1,-1,-1,-1
544,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
545,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
546,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
547,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
548,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
549,BI Data Engineer (Remote),Employer Provided Salary:$75.00 - $85.00 Per Hour,"Stefanini Group is hiring!
Exciting opportunity awaits, let us help you get started!
Click Apply now or you may call: (248) 728-2611 / email: palak.agrawal@stefanini.com for faster processing!

Job Description:
We require a reliable, talented, highly motivated BI/Data Engineer with excellent SQL, PowerBI, Python skills, The firm is putting a huge focus on data and reporting, with an increased demand of data related tasks.
As a global leader in Asset management, you will be working as part of an innovative, fast paced technology driven culture who are always looking at ways to enhance how technology is used within their organization.
You will be a valued member of an effective data team working to deliver growth against ambitious targets.
You will play a key role in extracting useful information from data, providing important insights to global technology teams.
The role will focus on creating best practice, high quality data solutions in collaboration with numerous business stakeholders and technology partners.
We are looking for someone who enjoys problem solving, who has an inquisitive nature, a team player and can work in a collaborative environment with good communication skills.
Ability to help extract and analyze data is required for the job.



Requirements:
We are looking for a good mix of the following skills:
Excellent experience in the Microsoft BI stack, Excel.
Excellent experience in building data pipelines and integrations (ETL/ELT).
Excellent knowledge of data warehousing and analysis concepts, data normalization, dimensional modelling, and associated practices.
Knowledge and experience of data manipulation languages such as DAX or MDX, Visualization or dashboarding experience.
Python experience is a must. Azure cloud experience would be a bonus.
We believe in building a culture supported by strong values, trust and independence. Within the team, we are highly collaborative, supportive, open and have high standards in the work we produce.

Education:
Bachelor's degree or equivalent experience.

*Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives*

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",3.3,"Stefanini, Inc
3.3","New York, NY",10000+ Employees,1987,Company - Private,Enterprise Software & Network Solutions,Information Technology,$1 to $5 billion (USD)
550,Sr. Data Engineer,Employer Provided Salary:$105K - $110K,"Role : Data Engineer
Location : Remote
Note : We are looking Someone who can be technical and also be able to
structure and manage transformation projects as well.
JD :
* Data Engineer Skill: Spark, Python and Databricks
* 12+ years of experience in Data pipeline engineering for both batch
and streaming applications.
* Experience with data ingestion process, creating data pipelines
(should be able to move data between stages in data lake, apply
transformation rules) and performance tuning. Must be hands-on coding using
Python with Spark.
* Exposure working with complex data types like XML or JSON, CI/CD
pipeline and scheduling tools like Airflow.
* Proven experience in DWH (type-2 dimensions, aggregations, star
schema, etc.) design and implementation
* Proficient in writing complex SQL queries
* Effective communication and collaboration skills to work with
cross-functional teams and stakeholders.
* Oversee and monitor the work of offshore according to projects
timelines and, if necessary, resolve any issues as they arise
Job Type: Full-time
Salary: $105,000.00 - $110,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
technical and able to structure and manage projects: 5 years (Required)
Data pipeline engineering for both batch and streaming apps.: 10 years (Required)
data ingestion process, creating data pipelines: 5 years (Required)
Python and Spark: 4 years (Required)
Exposure of XML or JSON, CI/CD: 4 years (Required)
scheduling tools like Airflow: 5 years (Required)
writing complex SQL queries: 5 years (Required)
Oversee and monitor the work of offshore: 3 years (Required)
working on fulltime role: 2 years (Required)
Work Location: Remote",4.3,"keasis Inc
4.3",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
551,Azure Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Required Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 9 years (Required)
Python: 8 years (Required)
Data lake: 7 years (Required)
Data Bricks: 7 years (Required)
Work Location: Remote",-1.0,TekValue IT Solutions,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
552,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
553,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
554,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
555,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
556,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1.0,Percept Health,Remote,-1,-1,-1,-1,-1,-1
557,Data Engineer,-1,"Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore.

Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office.
The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus.
Position Responsibilities
Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities
Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization
Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process
Qualifications & Desired Skills
Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance
Minimum 2 years' of development experience using either Python, C#/Java or C++
Minimum 2 years' years of database/SQL experience
Familiarity with object-oriented programming concepts
Must be comfortable with development across the application stack
Ability to complete complex projects independently
Detail-oriented with strong verbal and written communication skills
Ability to work effectively in a high-energy, time sensitive team environment
Interest or prior experience in the financial trading industry (particularly in fixed income) a plus
This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education.
Base Salary for this role is expected to be between:
$110,000—$150,000 USD",4.4,"Garda Capital Partners
4.4","New York, NY",51 to 200 Employees,-1,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
558,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1.0,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
559,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
560,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
561,Data Engineer (MongoDB),$80K - $114K (Glassdoor est.),"CrossCountry Mortgage (CCM) is one of the nation’s top 3 retail mortgage lenders with more than 7,000 employees operating nearly 600 branches across all 50 states. We have been recognized nine times on the Inc. 5000 list of America’s fastest-growing private businesses and have received countless awards for our standout culture. Through our dedication to getting it done, we make every mortgage feel like a win.

A culture where you can grow! CCM has created an exceptional culture driving employee engagement, exceeding employee expectations, and directly impacting company success. At our core, our entrepreneurial spirit empowers every employee to be who they are to help us move forward together. You’ll get unwavering support from all departments and total transparency from the top down.

CCM offers eligible employees a competitive compensation plan and a robust benefits package, including medical, dental, vision, as well as a 401K with an employer match. We also offer company-provided short-term disability, an employee assistance program, and a wellness program.
Position Overview:
We are looking for an experienced MongoDB developer to work with our development teams to integrate innovative features on our new products and applications, as well as maintain existing applications. The MongoDB developer will engineer scripts that automate daily tasks and ensure the security of data sets.
Job Responsibilities:
Upgrade databases through patches.
Understand and convert company needs into technical specifications and creating scalable solutions using MongoDB.
Optimize database schema designs, ensuring data integrity, and promoting best practices to enhance the overall performance and scalability of MongoDB solutions.
Assist developers in identifying and resolving performance issues by utilizing Mongo Profiler to profile and fine-tune database queries and operations.
Leverage performance optimization techniques by implementing proper indexes, including B-Tree, Geospatial, and Text indexes, to enhance query performance and overall system efficiency.
Implement the most effective backup and recovery methods to safeguard critical data and ensure business continuity in case of data loss or system failures.
Collaborate with cross-functional teams to understand business requirements.
Stay updated with the latest MongoDB advancements, features, and best practices.
Qualifications and Skills:
Bachelor’s/Master’s degree in software engineering or Information Technology.
3+ years of experience in executing data solutions using MongoDB.
Recommend and implement best practices for Rest API integration framework/model.
Analytical and problem-solving skills.
Configuring schema and MongoDB data modelling.
Experience with database security management.
Administration of MongoDB in Atlas.
Thorough understanding of MongoDB architecture.
Excellent communication skills.
This job description is intended to convey information essential to understanding the scope of the job and the general nature and level of work performed by job holders within this job. However, this job description is not intended to be an exhaustive list of qualifications, skills, efforts, duties, responsibilities or working conditions associated with the position.
CrossCountry Mortgage, LLC offers MORE than a job, we offer a career. Apply now to begin your path to success! https://crosscountrymortgage.com/about-us/careers/apply/
California residents: Please see CrossCountry’s privacy statement for information about how CrossCountry collects and uses personal information about California consumers.
CrossCountry Mortgage, LLC. (NMLS3029) is an Equal Opportunity Employer committed to workforce diversity. Qualified applicants will receive consideration without regard to race, religion, creed, color, orientation, gender, age, national origin, veteran status, disability status, marital status, sexual orientation, gender identity, or gender expression, or any other protected status in accordance with all applicable federal, state and local laws. The collective sum of the individual differences, life experiences, knowledge, inventiveness, innovation, self-expression, unique capabilities and talent that our employees invest in their work represents a significant part of not only our culture, but our reputation. The Company is committed to fostering, cultivating and preserving a culture that welcomes diversity and inclusion. Employment is contingent upon successful completion of a background investigation. CrossCountry Mortgage, LLC. is an FHA Approved Lending Institution and is not acting on behalf of or at the direction of HUD/FHA or the Federal government. To verify licensing, please visit www.NMLSConsumerAccess.org.",3.3,"CrossCountry Mortgage, LLC.
3.3","Charlotte, NC",1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
562,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
563,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department.
The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
564,Lead Azure Data Engineer,Employer Provided Salary:$135K - $140K,"Lead Data Engineer.
Location : New York, NY ( Hybrid)
Full Time
10+ years of experience in data engineering
5+ years of experience managing a team of data engineers
Strong understanding of SQL Server SSIS
Excellent communication and interpersonal skills
Ability to work independently and as part of a team
Ability to meet deadlines and work under pressure
Strong desire to be hands-on with the development and implementation of data pipelines and data warehouses
Experience working with stakeholders to develop product backlog grooming, sprint planning data engineering and
QA Testing
Migration of SQL Server to Azure is desired
Experience with Azure Data Factory, ADLS, and CI/CD highly desired
The Hands-On Data Engineering Lead is responsible for leading a team of data engineers in the design, development, and
implementation of data pipelines and data warehouses. The ideal candidate will have a strong understanding of data
engineering principles and practices, as well as experience with SQL Server SSIS. The Hands-On Data Engineering Lead will
also be responsible for managing the team's workload, mentoring junior engineers, and communicating with stakeholders.
Additionally, the Hands-On Data Engineering Lead will be expected to be hands-on with the development and
implementation of data pipelines and data warehouses, and to have experience working with stakeholders to develop
product backlog grooming, sprint planning data engineering and QA Testing.
Responsibilities:
Lead the design, development, and implementation of data pipelines and data warehouses
Develop and implement data pipelines and data warehouses using SQL Server SSIS
Troubleshoot data pipelines and data warehouses
Communicate with stakeholders to understand their needs and ensure that projects meet their requirements
Manage the team's workload and ensure that projects are completed on time and within budget
Mentor junior data engineers and help them develop their skills
Stay up-to-date on the latest data engineering technologies and best practices
Provide support to production teams and QA teams
Work with stakeholders to develop product backlog grooming, sprint planning data engineering and QA Testing
Working with other engineers to design and implement data pipelines and data warehouses using Azure Data
Factory, ADLS, and CI/CD
Experience in migrating SQL server to Azure
30% team management and 70% hands-on
Job Type: Full-time
Salary: $135,000.00 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 10 years (Required)
SQL: 10 years (Required)
Data warehouse: 10 years (Required)
SSIS: 10 years (Required)
Work Location: In person",-1.0,Rivago Infotech Inc,"New York, NY",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
565,GCP Data Engineer,Employer Provided Salary:$50.00 - $56.00 Per Hour,"JOB Description: -
ABOUT CONGLOMERATE
Conglomerate IT is a certified and a pioneer in providing premium end-to-end Global Workforce Solutions and IT Services to diverse clients across various domains. Visit us at ConglomerateIThttp://www.conglomerateit.comConglomerateIT
Conglomerate IT mission is to establish global cross culture human connections that further the careers of our employees and strengthen the businesses of our clients. We are driven to use the power of global network to connect business with the right people without bias. We provide Global Workforce Solutions with affability.
We offer a gamut of services across diverse domains, categories, skill sets with varying lengths of assignments /core capabilities included:
· Application Development Services
· Business Intelligence Services
· Enterprise Resource Planning
· Infrastructure Support Services
· Testing Services
· Program / Project Management
· Temporary & Permanent Staffing
Conglomerate IT is headquartered in Addison, Texas with branch offices across Canada, India, Sri Lanka, Argentina to further serve our clients.
JOB ROLE: GCP Data Engineer
Location: Job Type:Hartford, CT (Onsite Role)
Contract : 12+ Months
Work Authorization: w2 ,1099
Must have Skills: - Skills: Python/PySpark, GCP Cloud , HDFS, Hive, Big data
About Role: -
As a GCP Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and data architecture on the Google Cloud Platform. You will work closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and storage. Your expertise in GCP services and data engineering best practices will play a crucial role in maintaining a scalable, reliable, and high-performance data infrastructure.
Roles and Responsibilities:
· Design and implement data pipelines: Develop ETL (Extract, Transform, Load) processes to move data from various sources into GCP storage solutions such as BigQuery, Cloud Storage, and Datastore.
· Data modeling: Design and optimize data models to support data analytics, reporting, and machine learning initiatives. Ensure data accuracy, integrity, and appropriate access controls.
· Performance optimization: Fine-tune data pipelines and query performance to maximize efficiency and reduce latency, utilizing techniques like partitioning, indexing, and caching.
· GCP services utilization: Leverage GCP services like Dataflow, Dataproc, Pub/Sub, and Composer to create, schedule, and manage data processing and orchestration tasks.
· Data integration: Collaborate with cross-functional teams to integrate data from various sources, both internal and external, to enable comprehensive analysis and reporting.
· Data quality and governance: Implement data quality checks, monitoring, and validation processes to ensure data accuracy and consistency across the platform.
· Security and compliance: Implement security measures and access controls to protect sensitive data. Ensure compliance with industry standards and regulations.
· Documentation: Create and maintain technical documentation for data pipelines, workflows, and processes.
· Troubleshooting and support: Diagnose and resolve data-related issues, performance bottlenecks, and operational challenges in a timely manner.
Ideal Profile
· You have experience with Python Hive, Big data
· You have experience withYou GCP Cloud
· have strong experience with HDFS and other relevant tools/technologies.
· You have advanced python skills.
· You have PySparkknowledge.
· You have excellent communication and collaboration skills.
· You have attention to details.
· You are self-directed.
What's on Offer?
· Join a team of world class IT professionals
· Comprehensive salary package
· Great opportunity for growth and development
Job Type: Contract
Salary: $50.00 - $56.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Experience:
google cloud platform: 4 years (Required)
Work Location: In person",-1.0,CONGLOMERATE IT LLC,"Novi, MI",-1,-1,-1,-1,-1,-1
566,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
567,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1.0,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
568,Data Engineer,Employer Provided Salary:$120K - $130K,"Role – Data Engineer- DataBricks -
Location – O Fallon, Missouri (Initially Remote)
Yrs. of experience – 8+
Full-Time
Job Description :
o 5+ years in a customer facing Technical architecture, consulting role with the expertise in The following technology.
o Developing modern Data warehouse solutions using Databricks and AWS/Azure stack.
o Drive technical discussion with client architect and team members.
o Knowledge in Databricks DELTA lake for the Analytical data lake use case
o Hand on experience in create MLOPS data pipeline creation.
o AIML Models develop, train and implement for the AIML use cases.
o Knowledge in Banking domain on card and payment areas.
o Any relation database – Data classification/ Data profiling for MLOPS use cases
o Good experience in offshore onsite coordination.
o Experience in translating a customer’s business needs.
Job Type: Full-time
Salary: $120,000.00 - $130,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
O'Fallon, MO 63366: Reliably commute or planning to relocate before starting work (Required)
Experience:
DataBricks: 1 year (Required)
MLOPS: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: In person",-1.0,Sbase Technologies,"O Fallon, MO",-1,-1,-1,-1,-1,-1
569,Snowflake Data Engineer,Employer Provided Salary:$70.00 - $80.00 Per Hour,"Job: Snowflake Data Engineer – Contract (6+ Months) – Stillwater, MN
REMOTE - HAVE TO WORK ONISTE WHEN EVER CLIENT CALL. Preferably local to MN NEEDED
EAD – GC – H4- EAD G C - CITIZEN
W2 Contract
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Expected hours: 40 per week
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 7 years (Required)
Snowflake API: 4 years (Required)
Snowflake administration: 6 years (Required)
Snowflake RBAC, RLS, CLS: 5 years (Required)
Willingness to travel:
50% (Required)
Work Location: Remote",-1.0,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
570,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
571,Data Engineer I,Employer Provided Salary:$90K - $120K,"Interested in joining a growing company where you will work with talented colleagues, enhance a supportive and energetic culture, and be part of the climate solution? At Energy Solutions, we focus on the big impacts. And we believe that market-based programs can be a powerful force to deliver large-scale energy, carbon, and water-use savings. For 28 years, we've harnessed that power to offer proven, performance-based solutions for our utility, government, and institutional customers.
Energy Solutions (ES) is currently seeking a full-time Data Engineer I to join our Business Operations Team. The Data Engineer role is part of the Business Operations Team and will work on critical projects such as the Corporate Operational Data Warehouse (CODW) and the new ERP system. The CODW project is one of ES's strategic goals, to build and maintain a system that enables ES to centralize, integrate, and analyze internal data from various sources. The goal is to provide a single source of truth for ES internal data and enable data-driven decision-making across the organization. This role will potentially lead the design and implementation of the infrastructure for internal reporting needs, driving operational efficiencies and helping our company's transition to operating internationally with 1000+ employees.
Energy Solutions has a remote-friendly work environment for staff located throughout the United States. We also have offices in Oakland and Orange, California, Boston, Massachusetts, New York, and Chicago, IL, for those that wish to work from one of our offices.

Daily responsibilities include but are not limited to:
Collaborate with stakeholders to understand data requirements and develop data-driven solutions that support business goals
Analyze data sources and pipelines needed for the CODW project
Design and implement scalable data pipelines, ETL processes, and data integration solutions for the CODW project using Python, SQL, and AWS technologies
Develop and maintain a data warehouse and optimize data storage, processing, and retrieval
Build schema, populate the database, and automate reporting
Document and maintain technical specifications, ETL processes, data mappings, and dictionaries
Support the Data Science Center of Excellence (DSCOE) with other data engineering needs

Minimum Qualifications:
BS/BA in Computer Science, Physics/Engineering, Business or Mathematics
Experience in building ETL data pipelines, real-time pipelines are a plus
Strong programming skills in Python and SQL
Minimum 2 years of data warehouse development and strong fundamentals in dimensional data modeling
Minimum 2 years of experience creating SQL queries and ETL design, implementation, and maintenance
Minimum 2 years of experience developing data pipelines using Python
Minimum 2 years of experience with AWS, AWS Redshift, AWS Data Engineering and ML Ops tools and services (ie: S3, Redshift, RDS)
Proven ability to aggregate, normalize, munge, analyze, and summarize value from disparate datasets
Strong oral, written, and interpersonal communication skills
High degree of accuracy and attention to detail
Ability to establish priorities, work independently with minimum supervision
Preferred Qualifications:
Ability to manage and communicate data warehouse plans to a non-technical audience
Expertise in cloud services architecting and designing secure AWS environments
Experience using BI tools like Tableau and Looker
Experienced in analyzing and troubleshooting data quality issues
Self-driven, highly motivated, and able to learn quickly
Familiarity with large data sets, cloud-based development and deployment, open-source practices and frameworks, and intelligent applications is desirable
Proven success in migrating data from legacy systems

Salary DOE: $90k - $120k / Annually
Compensation is commensurate with experience and includes a generous retirement package. Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP).
To see this and our other open positions, please visit https://energy-solution.com/company/careers/.
We encourage candidates from diverse backgrounds to apply. For more information about Energy Solutions and our Diversity, Equity and Inclusion efforts, please visit us on our website at www.energy-solution.com.
Background Check Information
Information will be requested to perform the compulsory background check. A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment. Energy Solutions is an equal opportunity employer.
Reasonable Accommodations
Energy Solutions is committed to providing access and reasonable accommodation for individuals with disabilities. If you require an accommodation in completing this application, interviewing, and/or completing any pre-employment testing, or otherwise participating in the employee selection process, please email accommodation@energy-solution.com.
Privacy Notice for Job Applicants",3.9,"Energy Solutions
3.9",Remote,10000+ Employees,2008,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$25 to $100 million (USD)
572,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
573,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
574,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
575,2024 Data Engineer I,-1,"2024 Data Engineer I
316114

Textron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.

Responsibilities:
Analyze, manipulate, or process large sets of data using statistical software.
Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces
Identify business problems or management objectives that can be addressed through data analysis.
Document data standards, semantic layer mapping, data flows and conceptual flows.
Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.
Perform statistical analysis to provide recommendations on best courses of action to business leaders.
Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.
Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.
Collaborate with business users to ensure data is available for self-service analytics.
Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.


Qualifications:
Education: Bachelor’s degree in computer science, engineering, mathematics, or similar field required
Years of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience required
Software Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferred
Experience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)
EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

Recruiting Company Textron Specialized Vehicles
Primary Location US-Georgia-Augusta
Job Field Information Technology
Schedule Full-time
Job Level Individual Contributor
Shift First Shift
Job Posting 08/24/2023, 8:20:05 AM",3.8,"Textron Specialized Vehicles
3.8","Augusta, GA",10000+ Employees,1923,Company - Public,Aerospace & Defense,Aerospace & Defense,$10+ billion (USD)
576,Data Engineer,$73K - $110K (Glassdoor est.),"Data Engineer

Posting Date: Aug 23, 2023
Location: Houston, TX, US
Lake Charles, LA, US
Company: Sasol
A job at Sasol is a career made by you, with purpose, development opportunities, benefits to support every stage of life and a working culture that embraces flexibility, diversity, and inclusion. Learn more.

Purpose of Job
The Data Engineer works as part of an agile team, interacts with data scientists/translators and industry experts to understand how data needs to be converted, loaded and presented. Collects, structures, analyses data and ensures quality. Responsible for the definition, conception, construction, and ongoing support of big data platforms and solutions.
Recruitment Description / Key Accountabilities
Integrate IT (Information Technology) and OT (Operational Technology) Systems.
Use programming languages and big data technologies following best practices and standards
Understands latest analytics tools and technologies (open source/ proprietary/ vendor supported) and determines fit with current architecture
Building and maintaining data pipelines that move data between various systems.
Ensuring data quality, integrity and security
Monitoring and optimizing data performance
Collaborate with technical teams to develop and maintain a flexible and sustainable workflow.
Collaborate with Product Owner, Data Scientists, Data Translators, and other Business Stakeholders to implement and deploy scalable solutions in an agile way of working; actively participate in scrum process and meetings with team where required
Assist with communication, improve transparency, radiate information, and address challenges effectively
Formal Education
University Bachelors Degree in Computer Science & Inf Systems or Engineering: Computer
Min Experience
Experience: 5+ relevant years
Long Description
Excellent communication and written skills.
Experience in building complex data pipelines using Cloud Services, REST APIs, SQL Databases. Python and SQL skills are are required. Experience in Azure ADF, Synapse or Databricks are beneficial.
Knowlege of Cloud Computing and Infrastructure, Data Models and Standards, Database Administration, Plant Information Management Systems, ERP Systems, Networking and Protocols, Security and Compliance, Change Managemnt, Problem Solving and Trouble Shooting are a plus.

Sasol (USA) Corporation is an Equal Opportunity Employer and gives consideration for employment to qualified applicants without regard to race, color, religion, creed, age, sex, sexual orientation, gender identity, pregnancy, national origin, disability or protected veteran status, as well as any other characteristic protected by applicable law, regulation or local ordinance. For more information about your rights under the law, see http://www.eeoc.gov/employers/upload/poster_screen_reader_optimized.pdf

Sasol treats work-authorized job applicants equally in recruiting and hiring without regard to their citizenship status or national origin unless required by law, regulation, executive order, government contract, or determination by the Attorney General. Please note Sasol will not sponsor, obtain, and/or petition for temporary visa status (for example, E, F-1, H-1, H-2, L, B, J, or TN) for candidates for this position and/or personnel hired for this position.


Nearest Major Market: Houston",3.9,"Sasol
3.9","Houston, TX",10000+ Employees,1950,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$10+ billion (USD)
577,Senior Data Engineer,Employer Provided Salary:$115K - $140K,"ABOUT THE JOB
Data is one of the foundations of Everside Health and plays an integral role in delivering first-class healthcare services to our patient population. We utilize a wide variety of tools to produce valuable data and provide better patient care as a result.
As a Senior Data Engineer, you will be an established thought leader through close partnerships with expert resources to design, develop, and implement data assets for a wide range of new initiatives at Everside Health. The role involves heavy data exploration, proficiency with SQL, ETL, knowledge of service-based deployments and APIs, and the ability to discover and learn quickly through collaboration. There is a need to think analytically and outside of the box while questioning current processes and continuing to build your business acumen. There will be a combination of team collaboration and independent work efforts. This role involves interaction with the Analytics team as well as a wide range of business areas across Everside.
We seek candidates with a strong quantitative background and excellent analytical and problem-solving skills. This position combines business and technical skills involving interaction with business customers, Analytics partners, internal and external data suppliers, and information technology partners.
ESSENTIAL DUTIES & RESPONSIBILITIES
Deliver data warehouse and analytic solutions by ingesting, integrating, and curating data to deliver information to the business and stakeholders
Conduct ETL design, development, and maintenance including data extraction, manipulation, analysis, source-target mapping, change data capture, code performance
Ensure seamless integration of data across the enterprise and drive automation of common and repeated tasks
Conduct performance tuning and optimization of all processes executed across the data platform
Develop large scale data structures and pipelines to organize, collect, and standardize data that helps generate insights and addresses reporting needs
Collaborate closely with business users and stakeholders to define data for the design, development, and deployment of new solutions that support strategic business priorities
Provide coaching and training to junior and new team members on ETL architecture, standards, and documentation
QUALIFICATIONS
Bachelor’s degree in Computer Science or related field and 5+ years of Data Engineering work experience. Experience designing and implementing ETL pipelines, working with a variety of data warehousing models and design fundamentals, working with Matillion, Azure Data Factory, Data Bricks, or similar ETL tool. Experience working in Snowflake, Synapse, or similar MPP platform and experience in DataOps/DevOps and agile methodologies
DESIRED ATTRIBUTES
Experience with messaging streaming systems (e.g., Kafka, Azure Event Hubs)
Experience in hybrid data processing methods (batch and streaming)
Experience with AWS or Azure application deployment
Experience with API integration
Pay Range: $115,000 - $140,000/yr
The actual offer may vary dependent upon geographic location and the candidate’s years of experience and/or skill level.
Everside Benefits Summary
We believe in empowering teammates to do their best work and build better healthcare. Below are some of our benefit offerings. Eligibility is based on 24/hr week.
Health and Well-Being: Free Everside membership for in person and virtual care, employer paid life and disability insurance, and choice in medical/dental plans, vision, employer funded HSA, FSA, and voluntary illness, accident and hospitalization plans. Benefits are effective on the first of the month following date of hire.
Financial Support: Competitive compensation, 401k match, access to financial coaching through our Employee Assistance Program
Lifestyle: Paid time off for vacation, sick leave, and more, holiday schedule
Learn more at
https://www.eversidehealth.com/careers/",3.2,"Everside Health
3.2",Remote,1001 to 5000 Employees,2001,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
578,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
579,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
580,Apple Music - Data Engineer,-1,"Summary
Posted: Aug 18, 2023
Weekly Hours: 40
Role Number:200496819
This team is more than a group of engineers — it’s a group of music lovers. That passion has made Apple Music the world’s most complete music experience, with over 60 million songs, thousands of playlists, and daily selections from music experts for 115 countries. The team’s data-driven engineers focus relentlessly on the customer experience by running worldwide experiments and analyzing usage and latency, while collaborating with Apple’s product groups. As a result, someone can use the Shazam app to identify an intriguing song in a café in the morning, add it to their playlist from Apple Watch, listen to it through their AirPods on their commute, and share it with their family on HomePod at dinnertime. And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love. Apple Music is a huge part of Apple’s business because it’s a big part of people’s lives.
Key Qualifications
Expertise in designing, implementing and supporting highly scalable data systems and services in Scala
Experience with distributed computing technologies such as Hadoop MapReduce, Spark/Spark-SQL, YARN/MR2
Expertise building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Solid understanding of data-modeling and data-architecture optimized for big data patterns, such as efficient storage and query on HDFS Experience with distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues
Experience with low-latency NoSQL datastores, and traditional relational databases is a plus
Description
We are seeking a hardworking, performance-savvy, engineer to build out the big data platform and services, which power many of these customer features — existing and new. As a core member of the Data Engineering team you will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability. This will include crafting systems to model, ingest, process and compute large-scale, mission-critical data across Apple Music. High-throughput and reliability are essential. The computed datasets are produced for internal reporting used by executives and they are also shared across many teams within the ASE organization, such as the Search team, Recommendation team, AB team, Marketing team. You will also enjoy the benefits of working in a fast growing business where you are encouraged to “Think Different” to solve very interesting technical challenges and where your efforts play a key role in the success of Apple’s business. This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users. Come join us to help deliver the next amazing Apple product!
Education & Experience
BS CS or equivalent work experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $131,500 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",4.2,"Apple
4.2","Seattle, WA",10000+ Employees,1976,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
581,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
582,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
583,Data Engineer,-1,"SalesIntel is the top revenue intelligence platform on the market. Our unique approach to data collection, enhancement, verification and growth distinguishes us from the market, solidifying our position as the best B2B data partner.

The Data Engineer will be responsible for building and extending our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys working with big data and building systems from the ground up.

You will collaborate with our software engineers our software engineers, database architects, data analysts and data scientists to ensure our data delivery architecture is consistent throughout the platform. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What You’ll Be Doing:
Design and build parts of our data pipeline architecture for extraction, transformation, and loading of data from a wide variety of data sources using the latest Big Data technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with machine learning, data, and analytics experts to drive innovation, accuracy and greater functionality in our data system.
Qualifications:
Bachelor's degree in Engineering, Computer Science, or relevant field.
5+ years of experience in a Data Engineer role.
3+ years experience with Apache Spark and solid understanding of the fundamentals.
Deep understanding of Big Data concepts and distributed systems.
Strong coding skills with Scala, Python, Java and other languages and the ability to quickly switch between them with ease.
Advanced working SQL knowledge and experience working with a variety of relational databases such as Postgres and MySQL.
Experience working with data stored in many formats including TSV, CSV, JSON, Parquet.
Cloud Experience with DataBricks or similar service: S3, Athena, EC2, EMR, RDS, Redshift.
Comfortable working in a linux shell environment and writing scripts as needed.
Comfortable working in an Agile environment
Machine Learning knowledge is a plus.
Must be capable of working independently and delivering stable, efficient and reliable software.
Excellent written and verbal communication skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

This is a remote position.",-1.0,"SalesIntel Research, Inc.",Remote,51 to 200 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
584,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
585,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
586,Data Engineer (Contract),$77K - $111K (Glassdoor est.),"About RevOpsforce:
At RevOpsforce, our mission is to drive sustainable revenue growth for our clients by leveraging data-driven insights, technology, and process optimization. We strive to deliver measurable results that exceed expectations, while building strong partnerships with our clients and continuously improving our own capabilities. Our Expert Network is composed of the highest skilled and certified professionals in revenue operations. We leverage this network to solve our clients' most complex operational challenges.
Type: Contract
Job Description:
We are seeking a talented and experienced Data Engineer to join our growing team. As a Data Engineer at our company, you will be responsible for designing, building, and maintaining the data pipelines that power our data-driven applications. In this role, you will work closely with our data science and engineering teams to develop high-quality, efficient data pipelines that enable us to derive insights from vast amounts of data. You should be comfortable working with large volumes of structured and unstructured data, and have a strong understanding of SQL, Python, and other programming languages.
Responsibilities:
Design, build, and maintain data pipelines to support data-driven applications and analytics
Analyze data to identify trends and patterns
Collaborate with data scientists and engineers to develop data-driven solutions
Write and maintain documentation for data pipelines
Monitor and optimize data pipelines for performance and efficiency
Qualifications:
Bachelor's or Master's degree in a related field (e.g. Computer Science, Data Science, Engineering)
3+ years of experience in a data engineering role
Strong skills in SQL and at least one programming language (e.g. Python, Java, C++, etc.)
Experience with big data technologies (e.g. Hadoop, Spark, etc.) and cloud platforms (e.g. AWS, Azure, GCP)
Strong problem-solving and analytical skills
Excellent communication skills and ability to work in a team environment
Join our dynamic and forward-thinking team to make a significant impact on our clients' revenue growth journey!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Website is coming soon and will be located at www.revopsforce.com",-1.0,RevOpsforce,"Austin, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
587,Data Engineer II,$90K - $119K (Glassdoor est.),"Data Engineer II
Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What’s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.
Join us to do the best work of your career and make a profound social impact as a Data Engineer on our Growth Marketing team in Round Rock, Texas, Hopkinton, Massachusetts, or Remote United States
What you’ll achieve
As a Data Engineer II on a growing team, you will bring in your industry experience to develop, deploy, and maintain productionized data pipelines that support the training and productionization of machine-learning models.
You will:
Build, test, and implement data pipelines using tools like Apache Airflow, Spark, and Python for the training and inferencing of machine-learning models
Ensure data quality, monitor data pipelines, and troubleshoot production issues
Document metadata, data models, data pipelines and APIs to help data science teams understand and leverage data assets.
Work with data scientists and business analysts to understand data requirements
Take the first step toward your dream career
Every Dell Technologies team member brings something unique to the table. Here’s what we are looking for in this role:
Essential Requirements
U.S. citizen residing in the U.S.
Bachelor’s Degree in Computer Science, Engineering, IT, or equivalent professional experience
1 to 2 years experience developing production-quality Python and performant SQL, working with data warehouses, JSON, Apache Hive, Hadoop or Spark, or related technologies
1 to 2 years experience with orchestration tools, such as Apache AirFlow
Experience with monitoring tools and observability
Who we are

We believe that each of us has the power to make an impact. That’s why we put our team members at the center of everything we do. If you’re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we’re looking for you.

Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us.

Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. Read the full Equal Employment Opportunity Policy here.
Job ID:R227305

Dell’s Flexible & Hybrid Work Culture

At Dell Technologies, we believe our best work is done when flexibility is offered.

We know that freedom and flexibility are crucial to all our employees no matter where you are located and our flexible and hybrid work style allows team members to have the freedom to ideate, be innovative, and drive results their way. To learn more about our work culture, please visit our locations page.",4.2,"Dell Technologies
4.2","Round Rock, TX",10000+ Employees,1984,Company - Public,Information Technology Support Services,Information Technology,$10+ billion (USD)
588,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
589,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
590,Data Scientist / ML Engineer (REMOTE /Contract),-1,"Role: Data Scientist / Machine Learning Engineer
Location: REMOTE
Duration: 6+ months
Scheduling Interviews
Skills/Experience:
6+ years of experience working in Data Science and Machine Learning field.
Extensive experience in applying data preprocessing, statistical analysis, data analytics tools, predictive modelling, model deployment and evidence-based approaches to find lean, actionable solutions to various real-world enterprise business problems.
Experience in the application of Supervised and Unsupervised Learning algorithms, Naïve Bayes, Regression Analysis, Neural Networks/Deep Learning, Support Vector Machines (SVM), Random Forest, K-Means, Hierarchical, Spectral clustering, DBSCAN, Collaborative Filtering and other advanced machine learning techniques.
Used AWS Cloud Services Sagemaker, ECS, EKS, S3, Redshift, QuickSight
Experience with Google Cloud Platform (GCP) AutoML, Vertex AI, BigQuery, Colab, DataProc, Data Studio,
Exposure to Azure ML, PowerBI, Azure SQL and DataBricks
Design custom BI reporting dashboards or interactive data visualizations and widgets in R and Python using Shiny, Tableau, Ggplot2, Plotly, Matplotlib, and Seaborn.
Produce custom BI reporting dashboards in R and Python using Shiny, and Plotly for rapid dissemination of actionable, data driven insights.
Strong experience in Software Development Life Cycle (SDLC), MLOPS
Experience in working with relational databases (Teradata, Oracle) with advanced SQL skills.",3.5,"Glow Networks
3.5",Remote,51 to 200 Employees,2003,Subsidiary or Business Segment,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
591,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
592,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
593,AWS Data Engineer,Employer Provided Salary:$60.00 - $75.00 Per Hour,"Title: AWS Data Engineer
Location: Columbus, OH (Day 1 onsite / Hybrid Model)
Duration: Contract
Job Description:
Data Engineer: Open positions – 2
Skills:
Must Have:
BS/BA degree or equivalent experience
General: Strong organizational, problem-solving, and critical thinking skills; Strong documentation skills
Coding: Proficiency in Java
• Cluster Computing frameworks: Proficiency in Spark and Spark SQL
• AWS Data Services: Proficiency in Lake formation, Glue ETL (or) EMR, S3, Glue Catalog, Athena, Kinesis (or) MSK, Airflow (or) Lambda + Step Functions + Event Bridge
Data De/Serialization: Expertise in at least 2 of the formats: Parquet, Iceberg, AVRO, JSON-LD
AWS Data Security: Good Understanding of security concepts such as: Lake formation, IAM, Service roles, Encryption, KMS, Secrets Manager
Good to Have:
• Linux Scripting, Jenkins
• DevOps: Git, CI/CD, JIRA, TDD
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Ability to commute/relocate:
Columbus, OH 43085: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Required)
Java: 4 years (Required)
Data warehouse, Glue and SparkSQL: 8 years (Required)
Work Location: In person",4.5,"Synkriom Technology Pvt. Ltd.
4.5","Columbus, OH",201 to 500 Employees,2015,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
594,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
595,Sr. Azure Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"LeadStack Inc. is an award-winning, one of the nation's fastest-growing, certified minority-owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Position Details:
Title: Azure Data Engineer
Location: Remote
Duration: 6+ Months Contract
Top 3 skills: SQL, Snowflake, and Data Warehousing/Design
Its a W2 Role. No C2C.
Client is seeking a talented and motivated Data Engineer to join our dynamic team. As a Data Engineer, you will play a critical role in developing and maintaining our data infrastructure, ensuring the smooth flow of data for our analytics and insights projects. The ideal candidate will have a strong background in SQL, data warehousing, ETL processes, and a preference for Snowflake experience. Additional experience with dbt or semantic layer tools will be a bonus, and proficiency in Python will also be advantageous.
Bachelor's degree in Computer Science, Engineering, or a related field.
Solid experience with SQL and data warehousing concepts.
Strong understanding of ETL processes and tools.
Preferred experience with Snowflake data warehouse platform.
Bonus for experience with dbt or other semantic layer tools.
Proficiency in Python is a plus.
Excellent problem-solving and analytical skills.
Strong attention to detail and ability to work in a fast-paced environment.
Effective communication skills and ability to collaborate with cross-functional teams.
Key Responsibilities:
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.
Optimize data ingestion and transformation processes to ensure efficient and scalable data flows.
Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.
Develop and maintain data models, schemas, and data dictionaries for efficient data storage and retrieval.
Perform data validation, quality checks, and troubleshooting to identify and resolve data-related issues.
Monitor and optimize database performance, including query tuning and index optimization.
Collaborate with data scientists and analysts to implement data models and support their data requirements.
Stay up-to-date with industry trends and best practices in data engineering, and proactively recommend and implement improvements to our data infrastructure
To know more about current opportunities at LeadStack, please visit us at https://leadstackinc.com/careers/
Should you have any questions, feel free to call me on 510-480-1006 or send an email on tarique.aziz@leadstackinc.com
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Data Warehousing/Design: 8 years (Required)
Snowflake: 5 years (Required)
Azure: 5 years (Required)
Work Location: Remote",4.3,"Leadstack Inc
4.3",Remote,51 to 200 Employees,2016,Company - Private,HR Consulting,Human Resources & Staffing,Unknown / Non-Applicable
596,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
597,Data Engineer,$79K - $116K (Glassdoor est.),"Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
Sr. Consultant /Data Engineer – Data Architecture & Engineering
Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
A Senior Consultant in the Data and Analytics Practice is someone that looks forward to being challenged and continuously grow their technical and team collaboration skills. You'll provide technical guidance, as needed, to other team members for quality assurance, mentoring and growth opportunities. If this is a path that excites you, let's chat!
Responsibilities
Technical delivery in both Agile and Waterfall methodologies
Implement solutions using the Microsoft Power BI tool suite and related technologies
Leverage advanced database systems including data warehouses, data marts, and models needed for business, financial and operational analysis and/or reporting
Develop high performing, reliable and scalable solutions
Ability to clearly communicate technical details to business and management personnel
Perform data analysis that will support and enhance Information Management systems
Author, oversee and gain approval of design documents for projects assigned
Develop reports, dashboards & scorecards, data exploration and visual analytics using existing and emerging tools and technologies.
Work independently or as part of a team to design and develop solutions
Assist business development team with pre-sales activities and RFPs
Provide meaningful feedback and coaching of other team members to successfully overcome technological challenges
Leading and demonstrating emerging technologies and concepts to teams
Requirements
Bachelor's Degree desired in Computer Science, Information Technology, or related field
2+ years of experience with data visualization
Expert knowledge of Data Management, Business Intelligence and Analytics concepts including:
Extract, Transform, Load (ETL)
Data Warehousing, Data Lakes, Data Marts, Data Stores, and Cubes
Data Quality and Profiling
Data Governance, Master Data Management, and Metadata Management
Data Visualization, Dashboard, Reporting, Self Service Business Intelligence (BI), Key Performance Indicators (KPI), and Scorecards
Predictive, Prescription, and Descriptive Analytics
Expert knowledge of Microsoft Business Intelligence stack including Power BI, Excel and SQL Server (SSAS, SSRS, SSIS)
Minimum of 3-5 years of experience (of which 2-3 years in consulting)
Experience with Azure Data Estate (Azure SQL DB, Cosmos DB, Azure SQL DW, Azure Data Lake) is a plus
Azure ML Studio/Services experience a plus
Certifications are a plus
Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and tuning is a plus
Knowledge of relational and dimensional database structures, theories, principles, and practice is a plus
Experience with Data Science tools, technologies and techniques (R, Python, algorithms) is a plus
Passionate about learning new technologies
Ability to learn new concepts and software quickly
Analytical approach to problem-solving; ability to use technology to solve business problems
Familiarity with database-centric applications
Ability to communicate effectively in both a technical and non-technical manner
Ability to work in a fast-paced environment
Ability to travel up to 30%
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways.
At this time, we cannot sponsor applicants for work visas. Candidates should be based in the United States.
About 3Cloud
3Cloud is a ""born in the cloud"" Gold-Certified Microsoft Azure technology consulting firm and Azure Expert Managed Services Provider that provides cloud strategy, design, implementation, acquisition and ongoing managed services to its clients across various industries. To meet the needs of its clients, 3Cloud offers comprehensive services across Cloud Infrastructure, DevOps & Automation, Cloud Application Development and Data, Analytics and AI. Founded by former Microsoft technology leaders, 3Cloud uniquely offers: (1) highly experienced and proven cloud architects and technologists, (2) strong business strategy, financial acumen and operational proficiency and (3) deep relationship and network into Microsoft engineering and field leadership.
3Cloud Total Rewards Highlights Include:
Flexible work location with a virtual first approach to work!
401(K) with match up to 50% of your 6% contributions of eligible pay
Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days
Three medical plan options to allow you the choice to elect what works best for you!
Option for vision and dental coverage
100% employer premium coverage for STD and LTD
Paid leave for birth parents and non-birth parents
Option for FSA, HSA, HRA and Dependent Care
$67.00 monthly tech and home office allowance
Utilization and/or discretionary bonus eligibility based on role
Robust Employee Assistance Program to help with everyday challenges
3Cloud offers competitive compensation. In addition to base pay employees are eligible to receive an annual discretionary/utilization bonus. If you are hired at 3Cloud your final base salary is based on factors such as skills, education, experience and/or geographic location. Please keep in mind that the range mentioned above includes the full base salary range for the role. It is not typical for offers to be made at or near the top of the range.
Base Salary Range
$90,000—$183,000 USD
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway.
At this time, we cannot sponsor applicants for work visas.",4.3,"3Cloud
4.3","Downers Grove, IL",501 to 1000 Employees,2016,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
598,Senior Data Center Engineer,-1,"Marco is a one-stop shop for all things business tech. Our employees are “movers and shakers” and our company is always striving to do what’s right. Does this sound like a culture you want to be a part of? We’re hiring a new team member to help take Marco’s technology further – working full-time, Monday - Friday, 8am-5pm.
More about us. We do it all – from copy and print solutions to IT and managed services. We are an organization led by salespeople with 650+ engineers ready to fix any and all issues. We have offices in 12 states and service nationally.
Join our growing team. You won’t regret it.

The Senior Data Center Engineer is responsible for providing quality services and solutions to our internal clients while maintaining a high level of client satisfaction. This position works cooperatively with other teams in a coordinated effort to develop, design, implement and support Marco’s Data Centers, along with the IT environment that depends upon them. While not a leadership position, the Senior datacenter engineer displays leadership qualities and may be called upon to lead others in certain situations.

ESSENTIAL FUNCTIONS:
Lead the overall design and implementation of the technology stack within the Marco Data Centers
Maintain physical access to the Marco Data Centers and be a primary contact for essential services
Perform proactive maintenance and reactive support to the equipment within the Marco Data Centers
Create and maintain documentation for all equipment within the Marco Data Center. Review and assist others with their documentation, offering constructive feedback and pointing out deviations from our policies and standards where necessary.
Report on the health of the Marco Data Centers, perform capacity planning and proactively provide recommendations for future growth
Monitor and provide proactive recommendations for backup systems and disaster recovery processes
Review all systems and make technical/process recommendations for improving efficiency
Assist in defining standards for all systems and processes provided by the Data Center Services group
Administer and maintain a reliable, stable, secure and cost-effective hosting environment to deliver 24x7x365 operational service
Provide escalated remote technical support to Marco Data Center clients. Handle requests with due urgency, empathy, and a spirit of cooperation.
Document your work, including troubleshooting steps, time, and resolution in the service ticketing system.
Ensure your work and the work of others adheres to Marco internal policies and applicable regulatory requirements.
Create and maintain standard templates, documentation, images, policy, and automation that will aid in a consistent and rapid deployment of Marco Data Centers
Research, test and recommend new tools and products to enhance the offering and efficiency of Marco’s Data Centers
Document the procedures, infrastructure and environments and keep updated with changes to the environment
Implement and maintain Data Center system builds that are consistent, optimized for client experience, overall performance and supportability
Set deadlines for projects, then organize your time to meet these deadlines; be willing to put in extra time and effort to meet a time-bound goal when necessary
Mentor other staff while you assist with implementation or utilization within your area of technical specialty
Work multiple projects simultaneously and provide consistent, high-quality results
Assist with implementation and or migration of clients’ existing data and applications
Perform system and application testing to provide functional environment
Provide onsite and or remote technical support to Marco clients during onboarding process
Implement changes and work orders for existing Marco clients
Review customer hosted systems and make technical/process recommendations for improving efficiency
Stay up to date with relevant state-of-the-art technology, equipment, and/or systems
QUALIFICATIONS:
Bachelor’s Degree and 4+ years of experience or equivalent experience
Experience and current certifications or specializations aligning with the technologies used in Marco’s Data Center to include Cisco NX-OS, SAN-OS, IOS, UCS-B, ASAv, EMC VNX, Meraki MX, VMWare vSphere, Citrix XenApp, Zscaler and XenDesktop preferred.
Current high level industry recognized certifications including one or more of the following: CCNP, CCE, VCDX, EMCTA
Solid understanding of the administration of network operating systems and services found in a Data Center environment
Benefits:
We’re not just competitive when it comes to business tech – we’re also pretty proud of what we offer our employees. Our benefits include medical, dental, and vision insurance. We also have paid holidays and vacation, 401k with generous company match, flexible spending accounts, employee purchase program, employer-paid life insurance, voluntary-term life insurance, short and long-term disability, critical illness and accident benefits, and pet insurance. Yes, we care about your furry family too.

all benefits are dependent on employment status
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities
Applicant Labor Law Posters",4.0,"Marco Technologies LLC
4.0",Remote,1001 to 5000 Employees,1973,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
599,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
600,Data Engineer,$85K - $117K (Glassdoor est.),"At WHOOP, we're on a mission to unlock human performance. WHOOP empowers users to perform at a higher level through a deeper understanding of their bodies and daily lives.

WHOOP is seeking a talented and motivated Data Engineer I to join our impactful and growing team. As a Data Engineer I at WHOOP, you will play a crucial role in building and maintaining the infrastructure and data pipelines that support our data-driven applications and analytics platform. You’ll grow as an engineer as you learn new technologies and write high quality, testable, scalable code. Your work will ensure the availability of critical data, enhance the performance and scalability of our systems, and drive data-related initiatives which empower our users with insights that optimize their health and performance.
RESPONSIBILITIES:
Implement, optimize, and maintain ETL (Extract, Transform, Load) processes to move, transform, and store data from various sources into our data warehouse.
Collaborate with cross-functional teams, including data analysts, data scientists, and software engineers to understand data requirements and design efficient and scalable data pipelines.
Develop and maintain data models that facilitate data consistency, integrity, and efficiency. Design and implement data warehouse schemas, ensuring proper partitioning to support the performance needs of analytical queries.
Monitor the performance of data pipelines and databases, identifying bottlenecks and inefficiencies and implement optimizations to enhance system performance and stability.
Contribute to the development of Whoop’s modern data stack which is built on cloud computing platforms (e.g. AWS, Azure, GCP), relational & non-relational databases (e.g. PostgreSQL, MySQL), data lakes, distributed databases (e.g. Snowflake, Redshift, BigQuery), and more.
Reports to the Data Engineering Technical Lead
QUALIFICATIONS:
Bachelor's degree in Computer Science, Engineering, Data Science, or a related field
1+ year of relevant full time experience or equivalent
Proficiency in Python, Java, or Scala (Python preferred)
Proficiency in SQL
Experience with data processing frameworks such as Apache Spark is a plus
Experience with ETL tools and frameworks (Prefect, Apache Airflow, etc) is a plus
Strong analytical and problem-solving skills
Excellent verbal and written communication skills, with the ability to present technical concepts to both technical and non-technical stakeholders
A collaborative mindset, with the ability to work effectively in a team-oriented environment
This role is based in the WHOOP office located in Boston, MA. The successful candidate must be prepared to relocate if necessary to work out of the Boston, MA office.

Interested in the role, but don’t meet every qualification? We encourage you to still apply! At WHOOP, we believe there is much more to a candidate than what is written on paper, and we value character as much as experience. As we continue to build a diverse and inclusive environment, we encourage anyone who is interested in this role to apply.

WHOOP is an Equal Opportunity Employer and participates in E-verify to determine employment eligibility",3.5,"WHOOP
3.5","Boston, MA",501 to 1000 Employees,2012,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
601,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
602,AI/ML Data Automation Engineer,$88K - $125K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
603,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
604,Data Engineer II,$90K - $119K (Glassdoor est.),"Data Engineer II
Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What’s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.
Join us to do the best work of your career and make a profound social impact as a Data Engineer on our Growth Marketing team in Round Rock, Texas, Hopkinton, Massachusetts, or Remote United States
What you’ll achieve
As a Data Engineer II on a growing team, you will bring in your industry experience to develop, deploy, and maintain productionized data pipelines that support the training and productionization of machine-learning models.
You will:
Build, test, and implement data pipelines using tools like Apache Airflow, Spark, and Python for the training and inferencing of machine-learning models
Ensure data quality, monitor data pipelines, and troubleshoot production issues
Document metadata, data models, data pipelines and APIs to help data science teams understand and leverage data assets.
Work with data scientists and business analysts to understand data requirements
Take the first step toward your dream career
Every Dell Technologies team member brings something unique to the table. Here’s what we are looking for in this role:
Essential Requirements
U.S. citizen residing in the U.S.
Bachelor’s Degree in Computer Science, Engineering, IT, or equivalent professional experience
1 to 2 years experience developing production-quality Python and performant SQL, working with data warehouses, JSON, Apache Hive, Hadoop or Spark, or related technologies
1 to 2 years experience with orchestration tools, such as Apache AirFlow
Experience with monitoring tools and observability
Who we are

We believe that each of us has the power to make an impact. That’s why we put our team members at the center of everything we do. If you’re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we’re looking for you.

Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us.

Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. Read the full Equal Employment Opportunity Policy here.
Job ID:R227305

Dell’s Flexible & Hybrid Work Culture

At Dell Technologies, we believe our best work is done when flexibility is offered.

We know that freedom and flexibility are crucial to all our employees no matter where you are located and our flexible and hybrid work style allows team members to have the freedom to ideate, be innovative, and drive results their way. To learn more about our work culture, please visit our locations page.",4.2,"Dell Technologies
4.2","Round Rock, TX",10000+ Employees,1984,Company - Public,Information Technology Support Services,Information Technology,$10+ billion (USD)
605,Apple Music - Data Engineer,-1,"Summary
Posted: Aug 18, 2023
Weekly Hours: 40
Role Number:200496819
This team is more than a group of engineers — it’s a group of music lovers. That passion has made Apple Music the world’s most complete music experience, with over 60 million songs, thousands of playlists, and daily selections from music experts for 115 countries. The team’s data-driven engineers focus relentlessly on the customer experience by running worldwide experiments and analyzing usage and latency, while collaborating with Apple’s product groups. As a result, someone can use the Shazam app to identify an intriguing song in a café in the morning, add it to their playlist from Apple Watch, listen to it through their AirPods on their commute, and share it with their family on HomePod at dinnertime. And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love. Apple Music is a huge part of Apple’s business because it’s a big part of people’s lives.
Key Qualifications
Expertise in designing, implementing and supporting highly scalable data systems and services in Scala
Experience with distributed computing technologies such as Hadoop MapReduce, Spark/Spark-SQL, YARN/MR2
Expertise building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Solid understanding of data-modeling and data-architecture optimized for big data patterns, such as efficient storage and query on HDFS Experience with distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues
Experience with low-latency NoSQL datastores, and traditional relational databases is a plus
Description
We are seeking a hardworking, performance-savvy, engineer to build out the big data platform and services, which power many of these customer features — existing and new. As a core member of the Data Engineering team you will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability. This will include crafting systems to model, ingest, process and compute large-scale, mission-critical data across Apple Music. High-throughput and reliability are essential. The computed datasets are produced for internal reporting used by executives and they are also shared across many teams within the ASE organization, such as the Search team, Recommendation team, AB team, Marketing team. You will also enjoy the benefits of working in a fast growing business where you are encouraged to “Think Different” to solve very interesting technical challenges and where your efforts play a key role in the success of Apple’s business. This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users. Come join us to help deliver the next amazing Apple product!
Education & Experience
BS CS or equivalent work experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $131,500 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",4.2,"Apple
4.2","Seattle, WA",10000+ Employees,1976,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
606,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
607,2024 Data Engineer I,-1,"2024 Data Engineer I
316114

Textron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.

Responsibilities:
Analyze, manipulate, or process large sets of data using statistical software.
Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces
Identify business problems or management objectives that can be addressed through data analysis.
Document data standards, semantic layer mapping, data flows and conceptual flows.
Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.
Perform statistical analysis to provide recommendations on best courses of action to business leaders.
Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.
Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.
Collaborate with business users to ensure data is available for self-service analytics.
Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.


Qualifications:
Education: Bachelor’s degree in computer science, engineering, mathematics, or similar field required
Years of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience required
Software Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferred
Experience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)
EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

Recruiting Company Textron Specialized Vehicles
Primary Location US-Georgia-Augusta
Job Field Information Technology
Schedule Full-time
Job Level Individual Contributor
Shift First Shift
Job Posting 08/24/2023, 8:20:05 AM",3.8,"Textron Specialized Vehicles
3.8","Augusta, GA",10000+ Employees,1923,Company - Public,Aerospace & Defense,Aerospace & Defense,$10+ billion (USD)
608,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
609,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
610,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
611,Data Engineer (multiple openings) - IHM,Employer Provided Salary:$88K - $157K,"Discover. A brighter future.
With us, you’ll do meaningful work from Day 1. Our collaborative culture is built on three core behaviors: We Play to Win, We Get Better Every Day & We Succeed Together. And we mean it — we want you to grow and make a difference at one of the world's leading digital banking and payments companies. We value what makes you unique so that you have an opportunity to shine.

Come build your future, while being the reason millions of people find a brighter financial future with Discover.

Job Description:
Employer: DFS Corporate Services LLC

Job Title: Data Engineer (multiple openings)

Job Location: Riverwoods, Illinois

Job Type: Full Time

Duties: Responsible for designing, developing, testing, and maintaining complex data solutions for the product. Mentor and influence peers to achieve commitments on data solutions on time and with quality. Telecommuting and/or working from home may be permissible pursuant to company policies.

Requirements: Employer will accept a Bachelor's degree in Computer Science, Computer Engineering, or a related field and 3 years of experience in Senior Data Engineer; ODI/DW/Batch Developer or related occupation.

Position required skills: Three (3) years of experience in the job offered or related occupation: working with Agile software development methodology and Agile management tools, including Jira; utilizing modern engineering practices, including the design and development of ETL data integration solutions in data warehouse environments; working with relational databases and Cloud-based technologies; utilizing design and architecture experience on large-scale ETL solutions; and working with Enterprise Data warehouse data models and dimensional modeling concepts, source to target mapping, and data integration architecture.

Position eligible for incentives under Employee Referral Program.

Rate of Pay: The base pay for this position generally ranges between $88,150.00 to $157,000.00. Additional incentives may be provided as part of a market competitive total compensation package. Factors, such as but not limited to, geographical location, relevant experience, education, and skill level may impact the pay for this position. We also offer a range of benefits and programs based on eligibility. Learn more at MyDiscoverBenefits.com .

QUALIFIED APPLICANTS: Please apply directly through our website by clicking on “Apply Now.” No calls. Equal Opportunity Employer/disability/vet.

What are you waiting for? Apply today!

All Discover employees place our customers at the very center of our work. To deliver on our promises to our customers, each of us contribute every day to a culture that values compliance and risk management.

Discover is committed to a diverse and inclusive workplace. Discover is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or other legally protected status. (Know Your Rights)",3.9,"Discover Financial Services
3.9","Riverwoods, IL",10000+ Employees,1985,Company - Public,Banking & Lending,Financial Services,$1 to $5 billion (USD)
612,Data Engineer (Fully Remote),$56K - $91K (Glassdoor est.),"Position Description: Who We Are
NIP Group www.nipgroup.com is a rapidly growing insurance service provider of specialty programs for commercial insurance brokers and carriers providing underwriting, distribution, product management, administration, and risk management services primarily by acting as a managing underwriter (MGA) and a Reciprocal Services Manager (RSM).
Our culture is one that empowers and encourages employees to be innovative, collaborative, and forward-thinking. If you are interested in being a part of a growing, entrepreneurial spirited organization, wed love to hear from you!

About the Position
Reporting into the Senior Actuary, you will own the continuous improvement culture for designing, building, and maintaining the infrastructure and systems required for collecting, storing, processing, and analyzing large volumes of data. As a Data Engineer, you will play a crucial role in ensuring that data is readily available, accessible, and usable by other data professionals, analysts, and stakeholders within an organization. You will be responsible for managing a data Lake / warehouse and implementing efficient data integration processes with internal and external systems.

What Youll Do
Design and maintain a scalable and secure Azure data warehouse.
Build data pipelines: Create and manage data integration processes, including data extraction, transformation, and loading (ETL) from various sources into the data warehouse.
Develop and implement data management strategies to ensure data quality, consistency, and accuracy.
Collaborate with cross-functional teams, including to identify data requirements and develop data models.
Develop, adhere, and enforce data governance policies and procedures to ensure compliance.
Monitor data quality and develop response mechanisms to address problems.
Collaborate with IT teams to ensure the availability, reliability, and performance of data systems and infrastructure.
Develop documentation related to data management processes, procedures, and data dictionaries.
Stay up to date with industry trends and advancements in data management technologies and techniques.
What Were Looking For
Education/Experience
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Proven experience working as Data Engineer or similar role, preferably in the insurance or financial industry.
Strong knowledge of data management principles, data quality assurance, and data governance practices.
Proficiency in data warehousing concepts, and ETL processes.
Experience with data integration tools and technologies (e.g., SQL, Python, ETL frameworks).
Solid understanding of regulatory requirements related to data privacy and security (e.g., GDPR, HIPAA).
Excellent problem-solving and analytical skills, with the ability to analyze complex data sets.
Strong communication and interpersonal skills to collaborate with cross-functional teams and communicate complex ideas effectively.
Technical Competencies
Proficiency in Microsoft services in Azure, Data Factory, Data Lake/warehouse storage.
Ability to design and implement data integration solutions using Azure Data Factory.
Expertise in designing and optimizing data warehousing solutions Azure SQL Data Warehouse.
Familiarity with Azure Data Lake Storage and data processing using Azure Data Lake Analytics.
Data Governance and Security: Understanding of data governance principles and compliance requirements within Azure, including access control and data privacy.
Monitoring and Troubleshooting: Skills in monitoring, optimizing, and troubleshooting data pipelines within Azure Data Factory.
Proficiency in scripting languages like PowerShell or Python for automation tasks in Azure.
Strong communication and collaboration skills to work effectively with cross-functional teams.
Proactive in staying updated with the latest Azure data services and advancements.
What You'll Receive
At NIP Group, we recognize there are many factors that contribute to your overall satisfaction both at work, and in your personal life. For that reason, we provide a perfect mix of compensation, benefits, company culture, and resources to ensure your everyday happiness. Below are some benefits that youll receive.
Competitive compensation to reward you for your hard work every day.
Progressive Paid-Time Off program for you to enjoy time out of the office, including time off for volunteering and life events.
Group Medical, Dental, Vision and Life insurance to encourage a healthy lifestyle.
Pretax Health and Dependent Care Spending Accounts to ease taxes on spending.
Discounts in retail and entertainment.",3.0,"NIP Group, Inc.
3.0","Woodbridge, NJ",51 to 200 Employees,-1,Company - Private,Insurance Agencies & Brokerages,Insurance,Unknown / Non-Applicable
613,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1.0,Radiant System,Remote,-1,-1,-1,-1,-1,-1
614,Data Warehouse & Artificial Intelligence Engineer,$83K - $120K (Glassdoor est.),"Job Description:

Design, Implement and Maintain data warehouse.

Duties and Responsibilities:

Develop, test, and deploy extraction, transformation and loading (ETL) routines to load the data warehouse using SSIS, stored procedures, etc.
Develop analytics including reports, dashboards, and data visualizations
Troubleshoot BI-related problems and tune for performance
Implement and oversee a company-wide data governance program to support master data management and resolve data definition discrepancies among stakeholders
Plan the technical approach for implementing analytics strategy across multiple channels
Plan and implement activities to maintain data confidence
Provide technical direction and technical support for all member of the team
Architect and implement analytics solutions to meet data collection and reporting needs.
Work with team to recommend analytic software
Provide general troubleshooting assistance for analytic reports and break fixes
Wrangle data (a.k.a. data engineering) from a wide variety of complex data sets large volumes of data into normalized and enriched data sets.
Assemble and evaluate data such that new insights, solutions, and visualizations can be derived.
Perform data discoveries to understand data formats, source systems, file sizes, etc. and engage with internal and external business partners in this discovery process.
Deliver data acquisition, transformations, normalization, mapping, and loading of data into a variety of data models.
Collaborate extensively with Technology to design data ingestion, data models, and automated operational metrics for consistently high quality data
Supports the research, analysis and presentation of information by: producing reports, compiling and summarizing information, producing supporting documentation and exhibits, and verifying information received from external sources.
Assists other associates with preparation of reports and use of information systems, software and related sources of information. Trains other users on report preparation and database access.
Perform other duties as assigned.

Required Knowledge, skills and abilities:

Ability to manage change successfully in a fast-paced environment
A passion for learning and the ability to do so on the fly
Must be strategic, dynamic, and detail-oriented. The ideal candidate is an analytical thinker, a self-starter and a quick learner.
Ability to design and architect method of extracting data from its original source to another location (i.e. a data lake) to make that data consumable for a variety of downstream purposes such as applications, visualizations, analytics and other related data science activities.
Basic understanding of data science
Ability to transforming and map one ""raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.
Exceptional ability to clearly and effectively communicate with IT peers and users at all levels of technical and business acumen.
Strong time management skills and multi-tasking capabilities
Ability to understand business rules governing financial and budgeting process
Exceptional oral and written communication skills
Self-motivated, ability to set priorities and meet deadlines

Education and Experience:

Bachelor's degree in Computer Science, Business, Economics, Information Science, or Analytics
Minimum of 3 years of experience with data analytics across multiple channels
Experienced with the technologies required to implement Analytics across multiple channels, (for example, Data Layer, Custom JavaScript, JAVA, AJAX, HTML, Objective-C, Swift)
Knowledge of Tableau is a plus
Familiarity with Microsoft Power BI
Extensive experience with ETL and/or other Big Data processes.
Contract manufacturing experience is a plus.

Foxconn Assembly, LLC is an Equal Opportunity Employer (EOE). All qualified candidates will receive consideration without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or marital status in accordance with applicable federal, state and local laws.
Foxconn Assembly, LLC participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

NbXn0DyWY6",3.4,"Foxconn Corporation
3.4","Houston, TX",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$5 to $25 million (USD)
615,Data Engineer,-1,"Hummingbird is a remote-first, fully distributed team united by the shared mission of helping fight financial crime. Since our launch in 2017, we’ve helped major financial institutions and tech-savvy trailblazers alike (e.g. Stripe, Affirm, etc.) orchestrate their compliance programs through our thoughtfully designed, intuitive SaaS product. We believe finding and stopping financial crime is a problem rooted in code, language and design, so we built the product that the heroes doing this work deserve.
We are customer-obsessed, and we love building and shipping great products. We set a high bar, challenge our assumptions, seek diverse opinions, and support each other to do our best work.
We do our best to write inclusive, descriptive and accurate job descriptions, but we’re not always perfect. If you’re interested in the role, we’d love to hear from you even if you don’t feel like you meet everything we’re looking for. We’re always iterating and improving, and it’s possible that your experience is even more impactful than we could have imagined.

About the Role
We are looking for a driven data engineer to join our team and champion the use of data at Hummingbird. Data plays a crucial role in our mission to fight financial crime and you will help us find new and innovative ways to leverage it to provide powerful tools for our users, and allow us to better understand the usage of our product. Hummingbird is uniquely positioned at the intersection of financial technology, security, policy, and law enforcement and as such we have built up a one of a kind data set that we can now use to give our customers the edge in their efforts to stay ahead of criminals. As an employee at a small startup, you will have the opportunity to wear many hats, working from product discovery through implementation.
What you’re looking to do:
Level up our use of data to make better decisions, build more powerful features, and fight financial crime
Leverage Infrastructure as Code (IaC) to manage and deploy infrastructure that supports a variety of different projects, such as data replication and orchestration for machine learning workflows
Build new data pipelines for ingesting data into our data warehouse via both batch and streaming architectures
Work closely with data science to enable us to build products that benefit our customers while keeping compliance and security at the forefront
Achieve goals through a combination of independent building, educating your peers, and influencing others to contribute towards your vision
What we’re looking for:
A data engineer with a history of taking projects from the earliest stages through successful rollout to production
Someone who is excited by the prospect of pioneering data as a practice at a fast growing startup and who is unafraid to dig in to discover what is possible
A flexible self starter that will cut across organizational lines to understand the business and identify the most valuable work
An engineer who brings a pragmatic approach to problem solving, favoring simplicity and shortening delivery cycles
Experience building data pipelines for sensitive data, including best practices for de-identification and data security
Experience deploying infrastructure via terraform or a similar infrastructure as code tool
Expertise in SQL and one or more programming languages, especially python
What’s in it for you:
The chance to help build from the ground up. The hires we’re making now are foundational to our growth as a company, so you will have an opportunity to help shape the future of Hummingbird.
Competitive compensation including cash and equity.
Remote-first, fully distributed company with flexible working hours.
Awesome health, vision & dental benefits, and 401k.
Safe, respectful & comfortable work environment with colleagues and leadership who prioritize diversity, equity, inclusion, and belonging.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please don't hesitate to contact us to request accommodation.",-1.0,Hummingbird,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
616,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
617,Data Engineer - Data Transfer Project,$80K - $120K (Glassdoor est.),"Do you bring experience managing, modelling and extrapolating genomic or clinical data? Are you interested in supporting a major collaboration partnership?
As part of the SOPHiA GENETICS & MSK collaboration, we are looking for a new Data Engineer / Bioinformatics Technology Manager.
The collaboration agreement will blend the power of SOPHiA GENETICS’ large, global network and deep expertise in predictive algorithms with MSK’s clinical expertise in cancer genomics.
You will work on will work on and prioritize matters implementing, operationalizing, facilitating and managing secure and compliant access to data as part of the partnerships,
This is a remote role, in vicinity of Boston or New York.
Your Mission
Drive analysis of clinical and high-throughput biomedical data,
Query clinical DBs to extract cohort information by tumor type, treatment, and other meta data such as existence of genomic data or biological samples
Links disparate data such as genomic, imaging, or proteomics, to generate multi-modal data sets
Design and develop software, data ETL pipelines and scripts, leveraging databases and data lake technologies as part of bioinformatics and biomedical informatics data storage, visualization, processing, and analysis systems
Requirements
2-3 Years experience within Data Engineering
Direct exposure to clinical, genomic or biomedical metadata
Working knowledge of SQL
Previous experience with Data Extraction and Script creation
Benefits
The Process:

Apply now with your CV and any supporting information.
Suitably qualified candidates will be invited through an interview and screening process where you will speak with members of our Talent Acquisition Team, the hiring leader alongside key colleagues and stakeholders from across the business.
Shortlisting & Interviews from 4th September
Contract type: Permanent, Full-time",3.9,"SOPHiA GENETICS
3.9","New York, NY",201 to 500 Employees,2011,Company - Public,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
618,Data Engineer,-1,"Duration: 11+ months

Job Description:

Aviation connects the world and Connected Aviation Solutions (CAS) connects Aviation. Sustainably. Seamlessly. Securely. The Data Management & Data Science (DM&DS) team is tasked with the end to end responsibility to ensure that CAS data assets are managed with integrity and quality prior to consumption by our critical customer facing applications - whether via APIs, analytics and/or data visualizations. As a senior data engineer on the DM&DS team, you will be responsible for the design, development and maintenance of data processes and pipelines supporting critical CAS Strategic Business Unit (SBU) Data initiatives in support of the Digital Transformation of CAS as well as for the cross-SBU Connected Ecosystem. In this endeavor, you will be working closely with data architecture, data analytics & visualization teams leaders across CAS, SBU and Digital Technology (DT) teams to ensure the technical solutions are efficient, scalable and meet long term Connected Ecosystem needs.

Primary Responsibilities:
Design, develop and support the processes and pipelines for moving data throughout the CAS and cross SBU environments.
Develop automation and monitoring processes that support the data pipelines
Work closely with the architecture team to implement modern data repositories that support the CAS use cases (Pipelines, APIs, Data Science, Applications and Visualizations)
Work with internal business customers and software development teams to gather and document requirements for data publishing and data consumption via data warehouse, data lake, and analytics solutions
Support the operation of the CAS and Digital Technology owned Data Platforms, Data Warehouse and Data Lakes with a view to leveraging capabilities and resources over-time
Work with the CAS and DT Enterprise Data Architects to automate cloud deployments, as well as build CI/CD pipeline to support Cloud-Based workloads. Develop views, materialized views, and SQL scripts
Work with the CAS and DT Enterprise Data Architects to recommend investments or changes in technology, resources, procedures, equipment, systems, or other assets to improve the quality of the organizations projects.
May travel domestically and internationally up to 15%.

Qualifications / Required Skills:
Bachelors degree and 5 years of prior relevant experience OR Advanced Degree in a related technical field and minimum years 3 experience OR In absence of a degree, 10 years of relevant experience is required
3+ years of demonstrated engineering leadership in a relevant engineering function, such as software/service development and deployment, system design and integration, or data analytics.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.",3.8,"Capgemini
3.8",Remote,10000+ Employees,1967,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
619,Data QA Engineer,-1,"Who We Are:
The Data Sherpas are a team of highly skilled and motivated engineers that help our clients at every phase of their cloud journey. If it touches the cloud, involves data, or lives as an application, we have either worked on it or have the skills and expertise to accomplish it.
What We Are Looking For:
We are seeking a dedicated and technically skilled Data QA Engineer. The Engineer will work closely with development leads and data engineers to define analysis strategies and create test plans that ensure the highest quality of code delivery. The candidate will build, manage, and maintain a robust testing environment.
What You'll Do:
Collaborate with development leads and data engineers to define analysis strategy and test plans to ensure quality delivery.
Define, build, automate, and execute testing for code in an agile practice in accordance with sprint planning.
Build, manage, and maintain unit, integration, and regression tests using DBT, Vertex, and Colab.
Troubleshoot issues found during the development and testing phases, proposing and implementing efficient solutions.
Design, monitor, and maintain QA reports, KPIs, and quality trends to evaluate the effectiveness of QA processes.
Communicate accurately the status and risks for ongoing work and timelines to stakeholders.
What You Have:
Expert-level proficiency in SQL and Python.
Hands-on experience with DBT, Vertex, and Colab.
Strong experience with agile development methodologies.
Knowledge of software QA methodologies, tools, and processes.
Familiarity with CI/CD tools and methods.
Bachelor's degree in Computer Science, Engineering, related field, or equivalent experience.

This is a contract to the end of the year with an expectation to extend into 2024
We cannot work with third-party agencies at this time. Resumes submitted via unapproved agencies will be automatically rejected.",-1.0,The Data Sherpas,Remote,-1,-1,-1,-1,-1,-1
620,Data Engineer,-1,"Remote
Contract
Opened 4 months ago
Job Description
Data Engineer (with Healthcare experience) Required Skills: SQL Databricks data engineering Snowflake data engineering QA experience for ETL experienced with data acquisition and ingestion using API and/or batch channels Experience with Healthcare",3.3,"Crackajack Solutions
3.3",Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
621,Data Engineer,Employer Provided Salary:$30.05 - $44.13 Per Hour,"AMAZING BENEFITS PACKAGE!
Medical, Dental, & Vision Insurance
Paid Time Off (PTO) – up to 6 weeks of PTO per year
Childcare Stipend – contribution of $5,000 per year to the employee’s dependent care spending account
Loan Reimbursement - reimbursement for student loan payments up to $5,250 per year
Flexible Work Arrangements - opportunity to work a modified schedule, work part-time, or work from home
Retirement Plans - organization matches the employee’s contribution, up to 6% of gross wages
above benefits dependent upon eligibility criteria
Click here for more detail about the benefits package!
Pay Range: $30.05 - $44.13 USD hourly.

Purpose of Job
Improves the overall health of the communities we serve by providing superior customer service and advanced technical support to ensure quality patient care and user satisfaction as follows:
Essential Duties and Responsibilities
Designs and implements databases, ETL routines (using SSIS), stored procedures, and OLAP solutions in addition to providing support and enhancements to existing data solutions.
Designs and builds normalized and denormalized database solutions using healthcare industry best practices for data warehousing for specific client requirements, using Microsoft SQL Server Database programming (stored procedures and database design).
Works on multiple concurrent deadline-driven projects while ensuring data quality and meeting service level agreements.
Builds and maintains cubes (internal and client-facing).
Optimizes database schemas, queries, cubes, and reports, implementing complex logic requirements.
Conducts tuning reviews / assessments (DB and SQL tuning), reporting, and query monitoring.
Determines, enforces, and documents database policies, procedures, and standards.
Performs other duties as assigned, including supporting the CHAS Health Mission and Core Values.
Qualifications
Education/Experience: Associate’s degree or commensurate experience in a technical field required. Prior experience in database development using Microsoft SQL server, SSIS and SSAS preferred; understanding performance, deployment, configuration, security, migration, and troubleshooting; as well as proficiency in building and optimizing SQL queries preferred. Previous experience building and supporting business intelligence/data warehousing solutions; and knowledge of healthcare data models preferred.
Skills: Excellent logical and problem-solving abilities, verbal and written communication skills required. Ability to work independently in a self-directed environment, contribute to a team, maintain a positive attitude, demonstrate very high attention to detail, and a commitment to quality while working in a high availability environment is required. Commitment to supporting a safe, respectful, equitable, and inclusive environment required. Valid drivers’ license and insurance required.
Physical Demands
Required to stand, sit, and be mobile up to two-thirds of the time. Required to read from text and computer screen over two-thirds of the time. Climbing or balancing, stooping, kneeling, or crouching occurs less than one-third of the time. Communicating occurs constantly throughout the day. Lifting occurs about half the time up to 10 lbs. and less than one-third of the day from 25-40 lbs. Rarely is there a need to lift more than 41 lbs.",3.8,"CHAS Health
3.8","Spokane, WA",1001 to 5000 Employees,1994,Nonprofit Organization,Health Care Services & Hospitals,Healthcare,$25 to $100 million (USD)
622,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
623,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
624,Data Engineer,-1,"ArchWell Health is a new, innovative healthcare provider devoted to improving the lives of our senior members. We deliver best-in-class care at comfortable, accessible neighborhood clinics where seniors can feel at home and become part of a vibrant, wellness-focused community. Our members experience greater continuity of care, as well as the comfort of knowing they will be treated with respect by people who genuinely care about them, their families, and their communities.

Duties/Responsibilities:
Build data integrations from internal and external sources to centralize data into a Data Warehouse environment.
Monitor data integration operations, data quality, troubleshoot, and resolve problems.
Profile data sources and map to target table formats.
Develop and monitor data quality processes and address problems.
Develop, unit test and system test integration components.
Create support documentation describing the functionality of the integrations.
Participating in technical design & requirements gathering meetings.
Participate in planning and implementing data integration and data migration activities.
Perform QA tests to ensure data integrity and quality.
Research data issues between source systems and the data warehouse.
Required Skills/Experience:
Bachelor’s degree required; Master's degree (in data science, computer science or MIS, mathematics, engineering, or related field) preferred.
5+ years of prior experience in Data Management / ETL / ELT / Data Warehousing
Experience in writing Data Quality routines for cleansing of data and capturing confidence score
Experience with master data management
Strong knowledge of Structured Query Language (SQL) and Transact-SQL (T-SQL)
Experience using scripting languages such as JavaScript or Python
Experience Healthcare data models, datasets, and source systems (e.g. EHR, claims, labs, etc.)
Experience with healthcare reference data (ICD, CPT etc.)
Experience with agile delivery methodologies
Data Modeling experience preferred.
Strong organizational, administrative, and analytical skills required.
Experience managing and working in cloud environments such as Amazon Web Services or Azure
Knowledge of HIPAA; ability to implement systems and processes in accordance with regulations
Excellent interpersonal communication skills, both written and verbal
ArchWell Health is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to their race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected classification.",4.0,"ArchWell Health
4.0",Remote,Unknown,-1,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
625,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
626,Senior Data Engineer,-1,"CoreTrust is the market leading commercial Group Purchasing Organization (GPO), leveraging the combined purchasing volume of its 3,000+ members to negotiate preferential pricing and terms across more than 80 indirect spend categories. Strategically aligned with private equity portfolios and large independent companies, we complement sourcing bandwidth and improve supply chain efforts with our industry-leading national contracts.
Recently acquired by Blackstone Private Equity, CoreTrust is growing rapidly and we’re looking for a passionate Senior Data Engineer.
Reporting to the Director of Data, you will be part of the data solutions team and be responsible for building our data platforms, enabling the use of advanced analytics to drive continued evolution and growth.
You will create and manage data pipelines to feed and curate our data lake solution and help develop our data roadmap. The ideal candidate has a great understanding of various data / tech solutions (e.g., data modeling tools, data pipeline, data catalogs, cloud databases) and a record of using them to bring tangible dollar impact. You should be excited to seek out and capitalize on a wide variety of opportunities to use data to create value across the organization.

Responsibilities
Lead data projects to build innovative and highly available solutions while ensuring adherence to budget, schedule, and scope of project
Mentor other members in the data solutions team
Develop and assist with oversight on the data tech infrastructure
Drive data & analytics solutions from conception to deployment/delivery with clear ROI impact
Develop and maintain relationships with all relevant business and tech stakeholders and functions
Provide input to proposals for assigned projects including project objectives, technologies, systems, information specifications, timelines, and staffing
Communicate timely status updates to affected internal or external customers and stakeholders
Collect, analyze, and summarize information and trends as needed to prepare project status reports
Assist in developing a culture of data-driven decision-making, including adoption of business intelligence, analysis, and advanced analytics globally
Perform other related duties as assigned

Qualifications
Bachelor’s degree in computer or information science or relevant experience
9+ years of relevant experience in a data-driven professional setting
Ability to assist with the vision of the team (e.g., mission, priorities, engagement model, tooling)
A record of accomplishment of successfully managing complex cross-functional projects under tight deadlines
Strong technical background – familiarity with Python, SQL, cloud technologies like Azure and AWS, statistics / machine learning, Snowflake, DBT, FiveTran, Data Visualization Software
Exceptional communication and presentation skills, particularly in the context of engaging senior management teams
A successful history of manipulating, processing, and extracting value from large, disconnected datasets
Build processes supporting data transformation, data structures, metadata, dependency, and workload management
Working knowledge of creating and leveraging API or Stream based data extraction processes such as Salesforce API
Strong command of databases and SQL
Proficiency with Python or R, especially for data manipulation and analysis, and ability to build, maintain and deploy sequences of automated processes with these tools
Ability to motivate groups of people to complete a project in a timely manner
Excellent analytical, logical thinking, and problem-solving skills
Thorough understanding of project management principles and planning
Thorough understanding of information technology procedures and practices
Proficient with, or able to quickly become proficient with, a range of general and specialized applications, software, and hardware used in the organization and the industry

Benefits
Competitive compensation package
Free individual employee medical coverage
Company subsidized dental and vision coverage
Dollar for dollar 401(k) match up to 6% of your salary with immediate vesting
Company-paid Short-Term and Long-Term Disability coverage
Employee Assistance Program to support your wellbeing and mental health
$1500 annual stipend for undergraduate/graduate college courses; $500 annual stipend for continuing education courses/certifications
Free snacks and beverages on-site
Brand new, state-of-the-art, tech-enabled work environment in downtown Nashville
Flexible/hybrid work culture",2.8,"CoreTrust
2.8",Remote,Unknown,-1,Subsidiary or Business Segment,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
627,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
628,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
629,"Data Engineer (AWS, Python)",Employer Provided Salary:$50.00 - $55.00 Per Hour,"Title: Data Engineer
Duration: 12+ Months
Technical Skills
(Required)
Four years or more of experience in data engineering and/or software
development.
Three years or more of experience with development, operations, or
architecture in AWS using Python.
Experience with development and delivery of microservices using serverless
AWS services (S3, RDS, Aurora, DynamoDB, Lambda, SNS, SQS, Kinesis,
IAM)
Strong competency in testing, including unit testing to coverage standards and
integration testing.
(Desired)
Software development experience with object-oriented development and design
patterns
Experience with serverless design in AWS
AWS technical certifications (Developer Associate, Solution Architect Associate)
Familiarity with machine learning and data design to support machine learning
Experience with productionizing machine learning workloads
Experience with ElasticSearch/ELK, Kibana, Grafana, and/or Prometheus
Experience with OpenTelemetry
Soft Skills
(Required)
Verbal and written communication skills, problem solving skills, customer
service and interpersonal skills.
Basic ability to work independently and manage one’s time.
(Desired)
Ability to work collaboratively in a complex, rapidly changing, and culturally
diverse environment.
Ability to clearly communicate complex technical ideas.
Comfortable working in a dynamic environment where digital is still evolving as
a core offering.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",5.0,"DSMH LLC
5.0",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
630,Senior Data Engineer (Remote),Employer Provided Salary:$50.00 - $70.00 Per Hour,"Company Overview: IT Giants INC is a dynamic and innovative technology company at the forefront of delivering advanced data solutions across various industries. We are seeking an experienced Data Engineer with expertise in cloud platforms, programming languages, database technologies, and analytics tools. If you also have a strong background in the airline domain, this is an opportunity to join our dedicated team and shape the future of data-driven decision-making in the aviation sector.
Position Overview: As a Data Engineer at IT Giants INC, you will play a pivotal role in designing, building, and maintaining sophisticated data pipelines and analytical solutions for our airline industry clients. Leveraging your deep understanding of both data engineering and the airline domain, you will empower our clients to harness the power of their data to improve operations and customer experiences.
Key Responsibilities:
Data Pipeline Development: Architect, develop, and maintain end-to-end data pipelines, integrating data from diverse sources, including cloud platforms (GCP, Azure, AWS), APIs, and various databases (Teradata, Oracle, PostgreSQL, NoSQL).
Data Transformation and Modeling: Apply your expertise in SQL, Python, and Alteryx to transform raw data into meaningful insights, while ensuring data integrity and accuracy.
Cloud Expertise: Utilize your proficiency in GCP, Azure, and AWS to design and implement scalable and cost-efficient cloud-based solutions that meet the unique demands of the airline industry.
Database Management: Manage relational and NoSQL databases to store and retrieve data efficiently, ensuring optimal performance and security.
Analytics and Reporting: Collaborate with data scientists and analysts to develop interactive dashboards and reports using tools like Tableau, QuickSight, and CloudWatch, enabling stakeholders to make informed decisions.
Data Quality and Governance: Implement data quality checks, validation processes, and adherence to industry regulations to ensure data accuracy and compliance.
Version Control and Collaboration: Utilize GitLab and other version control tools to collaborate with cross-functional teams on code management and deployment.
Real-time Monitoring: Develop real-time data processing solutions using technologies like CloudWatch to enable timely insights into operational metrics.
Linux Environment: Leverage your proficiency in Linux to optimize system performance and troubleshoot issues as they arise.
Domain Knowledge: Apply your deep understanding of the airline industry to structure data effectively and drive actionable insights from the data.
Qualifications:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
years of proven experience in data engineering, with a focus on cloud platforms (GCP, Azure, AWS), Python, SQL, and relevant database technologies.
Experience with Alteryx, Tableau, and data visualization tools for creating impactful dashboards.
Proficiency in managing databases such as Teradata, Oracle, PostgreSQL, and NoSQL databases.
Strong command of Linux environments and version control using GitLab.
Familiarity with CloudWatch, QuickSight, and real-time data processing is advantageous.
Domain expertise in the airline industry, including a solid understanding of aviation data, standards, and operational challenges.
Excellent problem-solving skills and the ability to troubleshoot complex data issues.
Effective communication skills to collaborate with technical and non-technical teams.
Proven ability to work independently and in a team, delivering high-quality solutions.
Join Our Team: If you are a driven and knowledgeable Data Engineer with a passion for both technology and the airline industry, IT Giants INC offers an exciting opportunity to contribute to transformative data solutions. Join us in our mission to revolutionize data analytics within the aviation sector and drive meaningful impact.
Job Types: Full-time, Contract
Pay: $50.00 - $70.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Work Location: Remote",5.0,"IT Giants
5.0",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
631,"Data Engineer (AWS, Python)",Employer Provided Salary:$50.00 - $52.00 Per Hour,"Title: Data Engineer (AWS, Python)
Location: Remote
Duration: 12+ Months
Job Description:
Typical task breakdown:-
Daily standup, collaborate with peers, work independently to complete sprint commitments, participate in agile ceremonies.
Interaction with team:-
Daily stand up meetings with team – working mainly with Helios Platform adoption team.
Work environment:
Education & Experience Required:
Bachelor's in computer science required plus 7+ Working experience .
Masters degree with 6+ years working expeirence acceptable as well.
Preferred: AWS cloud certifications
Technical Skills
(Required)
Exp with Object oriented programming - Python (4+ years)
AWS (3+ years overall exp) lambda and/or glue, EMR, S3
SQL (3+ years)
2+ years exp in test driven development, unit test, integration testing, regression testing
(Desired)
Event driven framework experience.
Kinesis or Kafka experience
Familiar with working in Agile methodology
Soft Skills
(Required)
Strong written and verbal communication, experience working as a member of agile engineering team, ability to commit to and deliver work within allocated sprint window.
Job Type: Contract
Pay: $50.00 - $52.00 per hour
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",5.0,"DSMH LLC
5.0",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
632,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1.0,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
633,Data Engineer,Employer Provided Salary:$45.00 Per Hour,"Job Title: GCP Data Engineer
Duration: 12+ Months
Location: Hartford, CT (Onsite Role)
Tax term : W2 & 1099
Job Description:
As a GCP Data Engineer, you will play a pivotal role in designing, implementing, and optimizing data solutions on the Google Cloud Platform (GCP). Your expertise will be essential in leveraging various technologies to build robust data pipelines and contribute to the overall success of data-driven initiatives.
Roles and Responsibilities:
Design, develop, and maintain efficient data pipelines using Python or PySpark on the Google Cloud Platform.
Seamlessly integrate data solutions using GCP services like Google Cloud Storage, BigQuery, Dataflow, and Pub/Sub.
Utilize expertise in the Hadoop ecosystem, including HDFS and Hive, to craft seamless data workflows.
Transform raw data into insights by creating complex SQL queries for data transformation and analysis.
Implement data quality checks and validation procedures to ensure accuracy and completeness.
Optimize data pipelines and processes for maximum performance and efficiency.
Collaborate with cross-functional teams to understand data requirements and deliver tailored solutions.
Create comprehensive documentation for pipelines, workflows, and processes.
Stay updated with the latest data engineering and cloud technology trends.
Tackle complex challenges with a proactive problem-solving mindset.
Required Qualifications:
Expertise in Python/PySpark and hands-on experience with Google Cloud Platform (GCP) for data engineering tasks.
Strong understanding of the Hadoop ecosystem, including HDFS and Hive.
Proficiency in SQL for effective data manipulation and analysis.
Experience working with cloud platforms, preferably Google Cloud Platform (GCP).
Strong problem-solving skills and analytical thinking.
Effective communication and collaboration abilities.
Adaptability to evolving data engineering technologies and practices.
Job Type: Contract
Salary: From $45.00 per hour
Experience level:
9 years
Application Question(s):
What is your work authorization?
Experience:
Data engineer: 9 years (Required)
Google Cloud Platform: 5 years (Required)
Hadoop: 5 years (Required)
Python: 4 years (Preferred)
SQL: 4 years (Preferred)
Work Location: On the road",-1.0,Conglomerate IT,"Hartford, CT",-1,-1,-1,-1,-1,-1
634,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
635,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
636,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
637,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
638,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
639,BI Data Engineer (Remote),Employer Provided Salary:$75.00 - $85.00 Per Hour,"Stefanini Group is hiring!
Exciting opportunity awaits, let us help you get started!
Click Apply now or you may call: (248) 728-2611 / email: palak.agrawal@stefanini.com for faster processing!

Job Description:
We require a reliable, talented, highly motivated BI/Data Engineer with excellent SQL, PowerBI, Python skills, The firm is putting a huge focus on data and reporting, with an increased demand of data related tasks.
As a global leader in Asset management, you will be working as part of an innovative, fast paced technology driven culture who are always looking at ways to enhance how technology is used within their organization.
You will be a valued member of an effective data team working to deliver growth against ambitious targets.
You will play a key role in extracting useful information from data, providing important insights to global technology teams.
The role will focus on creating best practice, high quality data solutions in collaboration with numerous business stakeholders and technology partners.
We are looking for someone who enjoys problem solving, who has an inquisitive nature, a team player and can work in a collaborative environment with good communication skills.
Ability to help extract and analyze data is required for the job.



Requirements:
We are looking for a good mix of the following skills:
Excellent experience in the Microsoft BI stack, Excel.
Excellent experience in building data pipelines and integrations (ETL/ELT).
Excellent knowledge of data warehousing and analysis concepts, data normalization, dimensional modelling, and associated practices.
Knowledge and experience of data manipulation languages such as DAX or MDX, Visualization or dashboarding experience.
Python experience is a must. Azure cloud experience would be a bonus.
We believe in building a culture supported by strong values, trust and independence. Within the team, we are highly collaborative, supportive, open and have high standards in the work we produce.

Education:
Bachelor's degree or equivalent experience.

*Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives*

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",3.3,"Stefanini, Inc
3.3","New York, NY",10000+ Employees,1987,Company - Private,Enterprise Software & Network Solutions,Information Technology,$1 to $5 billion (USD)
640,Sr. Data Engineer,Employer Provided Salary:$105K - $110K,"Role : Data Engineer
Location : Remote
Note : We are looking Someone who can be technical and also be able to
structure and manage transformation projects as well.
JD :
* Data Engineer Skill: Spark, Python and Databricks
* 12+ years of experience in Data pipeline engineering for both batch
and streaming applications.
* Experience with data ingestion process, creating data pipelines
(should be able to move data between stages in data lake, apply
transformation rules) and performance tuning. Must be hands-on coding using
Python with Spark.
* Exposure working with complex data types like XML or JSON, CI/CD
pipeline and scheduling tools like Airflow.
* Proven experience in DWH (type-2 dimensions, aggregations, star
schema, etc.) design and implementation
* Proficient in writing complex SQL queries
* Effective communication and collaboration skills to work with
cross-functional teams and stakeholders.
* Oversee and monitor the work of offshore according to projects
timelines and, if necessary, resolve any issues as they arise
Job Type: Full-time
Salary: $105,000.00 - $110,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
technical and able to structure and manage projects: 5 years (Required)
Data pipeline engineering for both batch and streaming apps.: 10 years (Required)
data ingestion process, creating data pipelines: 5 years (Required)
Python and Spark: 4 years (Required)
Exposure of XML or JSON, CI/CD: 4 years (Required)
scheduling tools like Airflow: 5 years (Required)
writing complex SQL queries: 5 years (Required)
Oversee and monitor the work of offshore: 3 years (Required)
working on fulltime role: 2 years (Required)
Work Location: Remote",4.3,"keasis Inc
4.3",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
641,Data Engineer,$85K - $117K (Glassdoor est.),"At WHOOP, we're on a mission to unlock human performance. WHOOP empowers users to perform at a higher level through a deeper understanding of their bodies and daily lives.

WHOOP is seeking a talented and motivated Data Engineer I to join our impactful and growing team. As a Data Engineer I at WHOOP, you will play a crucial role in building and maintaining the infrastructure and data pipelines that support our data-driven applications and analytics platform. You’ll grow as an engineer as you learn new technologies and write high quality, testable, scalable code. Your work will ensure the availability of critical data, enhance the performance and scalability of our systems, and drive data-related initiatives which empower our users with insights that optimize their health and performance.
RESPONSIBILITIES:
Implement, optimize, and maintain ETL (Extract, Transform, Load) processes to move, transform, and store data from various sources into our data warehouse.
Collaborate with cross-functional teams, including data analysts, data scientists, and software engineers to understand data requirements and design efficient and scalable data pipelines.
Develop and maintain data models that facilitate data consistency, integrity, and efficiency. Design and implement data warehouse schemas, ensuring proper partitioning to support the performance needs of analytical queries.
Monitor the performance of data pipelines and databases, identifying bottlenecks and inefficiencies and implement optimizations to enhance system performance and stability.
Contribute to the development of Whoop’s modern data stack which is built on cloud computing platforms (e.g. AWS, Azure, GCP), relational & non-relational databases (e.g. PostgreSQL, MySQL), data lakes, distributed databases (e.g. Snowflake, Redshift, BigQuery), and more.
Reports to the Data Engineering Technical Lead
QUALIFICATIONS:
Bachelor's degree in Computer Science, Engineering, Data Science, or a related field
1+ year of relevant full time experience or equivalent
Proficiency in Python, Java, or Scala (Python preferred)
Proficiency in SQL
Experience with data processing frameworks such as Apache Spark is a plus
Experience with ETL tools and frameworks (Prefect, Apache Airflow, etc) is a plus
Strong analytical and problem-solving skills
Excellent verbal and written communication skills, with the ability to present technical concepts to both technical and non-technical stakeholders
A collaborative mindset, with the ability to work effectively in a team-oriented environment
This role is based in the WHOOP office located in Boston, MA. The successful candidate must be prepared to relocate if necessary to work out of the Boston, MA office.

Interested in the role, but don’t meet every qualification? We encourage you to still apply! At WHOOP, we believe there is much more to a candidate than what is written on paper, and we value character as much as experience. As we continue to build a diverse and inclusive environment, we encourage anyone who is interested in this role to apply.

WHOOP is an Equal Opportunity Employer and participates in E-verify to determine employment eligibility",3.5,"WHOOP
3.5","Boston, MA",501 to 1000 Employees,2012,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
642,Azure Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Required Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 9 years (Required)
Python: 8 years (Required)
Data lake: 7 years (Required)
Data Bricks: 7 years (Required)
Work Location: Remote",-1.0,TekValue IT Solutions,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
643,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
644,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
645,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
646,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
647,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1.0,Percept Health,Remote,-1,-1,-1,-1,-1,-1
648,Data Engineer,-1,"Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore.

Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office.
The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus.
Position Responsibilities
Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities
Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization
Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process
Qualifications & Desired Skills
Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance
Minimum 2 years' of development experience using either Python, C#/Java or C++
Minimum 2 years' years of database/SQL experience
Familiarity with object-oriented programming concepts
Must be comfortable with development across the application stack
Ability to complete complex projects independently
Detail-oriented with strong verbal and written communication skills
Ability to work effectively in a high-energy, time sensitive team environment
Interest or prior experience in the financial trading industry (particularly in fixed income) a plus
This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education.
Base Salary for this role is expected to be between:
$110,000—$150,000 USD",4.4,"Garda Capital Partners
4.4","New York, NY",51 to 200 Employees,-1,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
649,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1.0,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
650,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
651,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
652,Data Engineer (MongoDB),$80K - $114K (Glassdoor est.),"CrossCountry Mortgage (CCM) is one of the nation’s top 3 retail mortgage lenders with more than 7,000 employees operating nearly 600 branches across all 50 states. We have been recognized nine times on the Inc. 5000 list of America’s fastest-growing private businesses and have received countless awards for our standout culture. Through our dedication to getting it done, we make every mortgage feel like a win.

A culture where you can grow! CCM has created an exceptional culture driving employee engagement, exceeding employee expectations, and directly impacting company success. At our core, our entrepreneurial spirit empowers every employee to be who they are to help us move forward together. You’ll get unwavering support from all departments and total transparency from the top down.

CCM offers eligible employees a competitive compensation plan and a robust benefits package, including medical, dental, vision, as well as a 401K with an employer match. We also offer company-provided short-term disability, an employee assistance program, and a wellness program.
Position Overview:
We are looking for an experienced MongoDB developer to work with our development teams to integrate innovative features on our new products and applications, as well as maintain existing applications. The MongoDB developer will engineer scripts that automate daily tasks and ensure the security of data sets.
Job Responsibilities:
Upgrade databases through patches.
Understand and convert company needs into technical specifications and creating scalable solutions using MongoDB.
Optimize database schema designs, ensuring data integrity, and promoting best practices to enhance the overall performance and scalability of MongoDB solutions.
Assist developers in identifying and resolving performance issues by utilizing Mongo Profiler to profile and fine-tune database queries and operations.
Leverage performance optimization techniques by implementing proper indexes, including B-Tree, Geospatial, and Text indexes, to enhance query performance and overall system efficiency.
Implement the most effective backup and recovery methods to safeguard critical data and ensure business continuity in case of data loss or system failures.
Collaborate with cross-functional teams to understand business requirements.
Stay updated with the latest MongoDB advancements, features, and best practices.
Qualifications and Skills:
Bachelor’s/Master’s degree in software engineering or Information Technology.
3+ years of experience in executing data solutions using MongoDB.
Recommend and implement best practices for Rest API integration framework/model.
Analytical and problem-solving skills.
Configuring schema and MongoDB data modelling.
Experience with database security management.
Administration of MongoDB in Atlas.
Thorough understanding of MongoDB architecture.
Excellent communication skills.
This job description is intended to convey information essential to understanding the scope of the job and the general nature and level of work performed by job holders within this job. However, this job description is not intended to be an exhaustive list of qualifications, skills, efforts, duties, responsibilities or working conditions associated with the position.
CrossCountry Mortgage, LLC offers MORE than a job, we offer a career. Apply now to begin your path to success! https://crosscountrymortgage.com/about-us/careers/apply/
California residents: Please see CrossCountry’s privacy statement for information about how CrossCountry collects and uses personal information about California consumers.
CrossCountry Mortgage, LLC. (NMLS3029) is an Equal Opportunity Employer committed to workforce diversity. Qualified applicants will receive consideration without regard to race, religion, creed, color, orientation, gender, age, national origin, veteran status, disability status, marital status, sexual orientation, gender identity, or gender expression, or any other protected status in accordance with all applicable federal, state and local laws. The collective sum of the individual differences, life experiences, knowledge, inventiveness, innovation, self-expression, unique capabilities and talent that our employees invest in their work represents a significant part of not only our culture, but our reputation. The Company is committed to fostering, cultivating and preserving a culture that welcomes diversity and inclusion. Employment is contingent upon successful completion of a background investigation. CrossCountry Mortgage, LLC. is an FHA Approved Lending Institution and is not acting on behalf of or at the direction of HUD/FHA or the Federal government. To verify licensing, please visit www.NMLSConsumerAccess.org.",3.3,"CrossCountry Mortgage, LLC.
3.3","Charlotte, NC",1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
653,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
654,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department.
The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
655,Lead Azure Data Engineer,Employer Provided Salary:$135K - $140K,"Lead Data Engineer.
Location : New York, NY ( Hybrid)
Full Time
10+ years of experience in data engineering
5+ years of experience managing a team of data engineers
Strong understanding of SQL Server SSIS
Excellent communication and interpersonal skills
Ability to work independently and as part of a team
Ability to meet deadlines and work under pressure
Strong desire to be hands-on with the development and implementation of data pipelines and data warehouses
Experience working with stakeholders to develop product backlog grooming, sprint planning data engineering and
QA Testing
Migration of SQL Server to Azure is desired
Experience with Azure Data Factory, ADLS, and CI/CD highly desired
The Hands-On Data Engineering Lead is responsible for leading a team of data engineers in the design, development, and
implementation of data pipelines and data warehouses. The ideal candidate will have a strong understanding of data
engineering principles and practices, as well as experience with SQL Server SSIS. The Hands-On Data Engineering Lead will
also be responsible for managing the team's workload, mentoring junior engineers, and communicating with stakeholders.
Additionally, the Hands-On Data Engineering Lead will be expected to be hands-on with the development and
implementation of data pipelines and data warehouses, and to have experience working with stakeholders to develop
product backlog grooming, sprint planning data engineering and QA Testing.
Responsibilities:
Lead the design, development, and implementation of data pipelines and data warehouses
Develop and implement data pipelines and data warehouses using SQL Server SSIS
Troubleshoot data pipelines and data warehouses
Communicate with stakeholders to understand their needs and ensure that projects meet their requirements
Manage the team's workload and ensure that projects are completed on time and within budget
Mentor junior data engineers and help them develop their skills
Stay up-to-date on the latest data engineering technologies and best practices
Provide support to production teams and QA teams
Work with stakeholders to develop product backlog grooming, sprint planning data engineering and QA Testing
Working with other engineers to design and implement data pipelines and data warehouses using Azure Data
Factory, ADLS, and CI/CD
Experience in migrating SQL server to Azure
30% team management and 70% hands-on
Job Type: Full-time
Salary: $135,000.00 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 10 years (Required)
SQL: 10 years (Required)
Data warehouse: 10 years (Required)
SSIS: 10 years (Required)
Work Location: In person",-1.0,Rivago Infotech Inc,"New York, NY",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
656,GCP Data Engineer,Employer Provided Salary:$50.00 - $56.00 Per Hour,"JOB Description: -
ABOUT CONGLOMERATE
Conglomerate IT is a certified and a pioneer in providing premium end-to-end Global Workforce Solutions and IT Services to diverse clients across various domains. Visit us at ConglomerateIThttp://www.conglomerateit.comConglomerateIT
Conglomerate IT mission is to establish global cross culture human connections that further the careers of our employees and strengthen the businesses of our clients. We are driven to use the power of global network to connect business with the right people without bias. We provide Global Workforce Solutions with affability.
We offer a gamut of services across diverse domains, categories, skill sets with varying lengths of assignments /core capabilities included:
· Application Development Services
· Business Intelligence Services
· Enterprise Resource Planning
· Infrastructure Support Services
· Testing Services
· Program / Project Management
· Temporary & Permanent Staffing
Conglomerate IT is headquartered in Addison, Texas with branch offices across Canada, India, Sri Lanka, Argentina to further serve our clients.
JOB ROLE: GCP Data Engineer
Location: Job Type:Hartford, CT (Onsite Role)
Contract : 12+ Months
Work Authorization: w2 ,1099
Must have Skills: - Skills: Python/PySpark, GCP Cloud , HDFS, Hive, Big data
About Role: -
As a GCP Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and data architecture on the Google Cloud Platform. You will work closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and storage. Your expertise in GCP services and data engineering best practices will play a crucial role in maintaining a scalable, reliable, and high-performance data infrastructure.
Roles and Responsibilities:
· Design and implement data pipelines: Develop ETL (Extract, Transform, Load) processes to move data from various sources into GCP storage solutions such as BigQuery, Cloud Storage, and Datastore.
· Data modeling: Design and optimize data models to support data analytics, reporting, and machine learning initiatives. Ensure data accuracy, integrity, and appropriate access controls.
· Performance optimization: Fine-tune data pipelines and query performance to maximize efficiency and reduce latency, utilizing techniques like partitioning, indexing, and caching.
· GCP services utilization: Leverage GCP services like Dataflow, Dataproc, Pub/Sub, and Composer to create, schedule, and manage data processing and orchestration tasks.
· Data integration: Collaborate with cross-functional teams to integrate data from various sources, both internal and external, to enable comprehensive analysis and reporting.
· Data quality and governance: Implement data quality checks, monitoring, and validation processes to ensure data accuracy and consistency across the platform.
· Security and compliance: Implement security measures and access controls to protect sensitive data. Ensure compliance with industry standards and regulations.
· Documentation: Create and maintain technical documentation for data pipelines, workflows, and processes.
· Troubleshooting and support: Diagnose and resolve data-related issues, performance bottlenecks, and operational challenges in a timely manner.
Ideal Profile
· You have experience with Python Hive, Big data
· You have experience withYou GCP Cloud
· have strong experience with HDFS and other relevant tools/technologies.
· You have advanced python skills.
· You have PySparkknowledge.
· You have excellent communication and collaboration skills.
· You have attention to details.
· You are self-directed.
What's on Offer?
· Join a team of world class IT professionals
· Comprehensive salary package
· Great opportunity for growth and development
Job Type: Contract
Salary: $50.00 - $56.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Experience:
google cloud platform: 4 years (Required)
Work Location: In person",-1.0,CONGLOMERATE IT LLC,"Novi, MI",-1,-1,-1,-1,-1,-1
657,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
658,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1.0,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
659,Data Engineer,Employer Provided Salary:$120K - $130K,"Role – Data Engineer- DataBricks -
Location – O Fallon, Missouri (Initially Remote)
Yrs. of experience – 8+
Full-Time
Job Description :
o 5+ years in a customer facing Technical architecture, consulting role with the expertise in The following technology.
o Developing modern Data warehouse solutions using Databricks and AWS/Azure stack.
o Drive technical discussion with client architect and team members.
o Knowledge in Databricks DELTA lake for the Analytical data lake use case
o Hand on experience in create MLOPS data pipeline creation.
o AIML Models develop, train and implement for the AIML use cases.
o Knowledge in Banking domain on card and payment areas.
o Any relation database – Data classification/ Data profiling for MLOPS use cases
o Good experience in offshore onsite coordination.
o Experience in translating a customer’s business needs.
Job Type: Full-time
Salary: $120,000.00 - $130,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
O'Fallon, MO 63366: Reliably commute or planning to relocate before starting work (Required)
Experience:
DataBricks: 1 year (Required)
MLOPS: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: In person",-1.0,Sbase Technologies,"O Fallon, MO",-1,-1,-1,-1,-1,-1
660,Data Engineer I,Employer Provided Salary:$90K - $120K,"Interested in joining a growing company where you will work with talented colleagues, enhance a supportive and energetic culture, and be part of the climate solution? At Energy Solutions, we focus on the big impacts. And we believe that market-based programs can be a powerful force to deliver large-scale energy, carbon, and water-use savings. For 28 years, we've harnessed that power to offer proven, performance-based solutions for our utility, government, and institutional customers.
Energy Solutions (ES) is currently seeking a full-time Data Engineer I to join our Business Operations Team. The Data Engineer role is part of the Business Operations Team and will work on critical projects such as the Corporate Operational Data Warehouse (CODW) and the new ERP system. The CODW project is one of ES's strategic goals, to build and maintain a system that enables ES to centralize, integrate, and analyze internal data from various sources. The goal is to provide a single source of truth for ES internal data and enable data-driven decision-making across the organization. This role will potentially lead the design and implementation of the infrastructure for internal reporting needs, driving operational efficiencies and helping our company's transition to operating internationally with 1000+ employees.
Energy Solutions has a remote-friendly work environment for staff located throughout the United States. We also have offices in Oakland and Orange, California, Boston, Massachusetts, New York, and Chicago, IL, for those that wish to work from one of our offices.

Daily responsibilities include but are not limited to:
Collaborate with stakeholders to understand data requirements and develop data-driven solutions that support business goals
Analyze data sources and pipelines needed for the CODW project
Design and implement scalable data pipelines, ETL processes, and data integration solutions for the CODW project using Python, SQL, and AWS technologies
Develop and maintain a data warehouse and optimize data storage, processing, and retrieval
Build schema, populate the database, and automate reporting
Document and maintain technical specifications, ETL processes, data mappings, and dictionaries
Support the Data Science Center of Excellence (DSCOE) with other data engineering needs

Minimum Qualifications:
BS/BA in Computer Science, Physics/Engineering, Business or Mathematics
Experience in building ETL data pipelines, real-time pipelines are a plus
Strong programming skills in Python and SQL
Minimum 2 years of data warehouse development and strong fundamentals in dimensional data modeling
Minimum 2 years of experience creating SQL queries and ETL design, implementation, and maintenance
Minimum 2 years of experience developing data pipelines using Python
Minimum 2 years of experience with AWS, AWS Redshift, AWS Data Engineering and ML Ops tools and services (ie: S3, Redshift, RDS)
Proven ability to aggregate, normalize, munge, analyze, and summarize value from disparate datasets
Strong oral, written, and interpersonal communication skills
High degree of accuracy and attention to detail
Ability to establish priorities, work independently with minimum supervision
Preferred Qualifications:
Ability to manage and communicate data warehouse plans to a non-technical audience
Expertise in cloud services architecting and designing secure AWS environments
Experience using BI tools like Tableau and Looker
Experienced in analyzing and troubleshooting data quality issues
Self-driven, highly motivated, and able to learn quickly
Familiarity with large data sets, cloud-based development and deployment, open-source practices and frameworks, and intelligent applications is desirable
Proven success in migrating data from legacy systems

Salary DOE: $90k - $120k / Annually
Compensation is commensurate with experience and includes a generous retirement package. Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP).
To see this and our other open positions, please visit https://energy-solution.com/company/careers/.
We encourage candidates from diverse backgrounds to apply. For more information about Energy Solutions and our Diversity, Equity and Inclusion efforts, please visit us on our website at www.energy-solution.com.
Background Check Information
Information will be requested to perform the compulsory background check. A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment. Energy Solutions is an equal opportunity employer.
Reasonable Accommodations
Energy Solutions is committed to providing access and reasonable accommodation for individuals with disabilities. If you require an accommodation in completing this application, interviewing, and/or completing any pre-employment testing, or otherwise participating in the employee selection process, please email accommodation@energy-solution.com.
Privacy Notice for Job Applicants",3.9,"Energy Solutions
3.9",Remote,10000+ Employees,2008,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$25 to $100 million (USD)
661,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
662,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
663,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Job Role: Data Engineer with Snowflake
Location: REMOTE, Stillwater, MN.
HAVE TO WORK ONISTE WHEN EVER CLIENT CALL.
Duration: 6+ Months Contract
Visas: USC, GC, EAD-GC, H4-EAD
W2 Requirement
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Expected hours: 40 per week
Experience level:
7 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
Snowflake: 4 years (Required)
Work Location: Remote",-1.0,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
664,2024 Data Engineer I,-1,"2024 Data Engineer I
316114

Textron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.

Responsibilities:
Analyze, manipulate, or process large sets of data using statistical software.
Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces
Identify business problems or management objectives that can be addressed through data analysis.
Document data standards, semantic layer mapping, data flows and conceptual flows.
Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.
Perform statistical analysis to provide recommendations on best courses of action to business leaders.
Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.
Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.
Collaborate with business users to ensure data is available for self-service analytics.
Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.


Qualifications:
Education: Bachelor’s degree in computer science, engineering, mathematics, or similar field required
Years of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience required
Software Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferred
Experience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)
EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

Recruiting Company Textron Specialized Vehicles
Primary Location US-Georgia-Augusta
Job Field Information Technology
Schedule Full-time
Job Level Individual Contributor
Shift First Shift
Job Posting 08/24/2023, 8:20:05 AM",3.8,"Textron Specialized Vehicles
3.8","Augusta, GA",10000+ Employees,1923,Company - Public,Aerospace & Defense,Aerospace & Defense,$10+ billion (USD)
665,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
666,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1.0,Radiant System,Remote,-1,-1,-1,-1,-1,-1
667,Data Engineer (Onsite),Employer Provided Salary:$85K - $100K,"If you are seeking a career that has tremendous impact on manufacturing operations, then come be a part of Tokai Carbon GE's team to design, develop and implement our new manufacturing execution system. You'll play a critical role in our organization's success, providing easier access to data that engineers, analysts and decision-makers need to do their jobs. You will set the foundation of data for growth, relying on your programming and problem-solving skills to create scalable solutions. You will be part of the team building BI and Analytics tools that will take our operations to the next level. If you are excited about applying your knowledge and skills to meaningful challenges, apply today!
JOB SUMMARY
The Data Engineer will be responsible for designing, implementing, and supporting solutions used to collect, transform, store, and analyze data from a variety of company applications. They analyze and organize raw data, build data systems and pipelines, review and analyze data infrastructure, plan, and implement solutions to store and manage data. Also, they create reports, dashboards, and other analytics solutions. They will work closely with the lines of business and external partners, to ensure successful development and delivery of BI and data analytics solutions. This is a hands-on development and support position with ownership responsibility for data architecture and engineering for BI and analytics purposes.
ESSENTIAL FUNCTIONS
Work with business users and other technology teams to understand their data requirements, identify appropriate data sources, design, and develop data pipelines and design appropriate data structures to store the required data.
Design data pipelines using SQL, other programming languages and data integration tools.
Gather and combine data from a variety of sources to support actionable decision making. Prepare data for reporting, and predictive and prescriptive tools.
Address BI/EDW technical development working with operations, sales, supply chain, technical services, ERP applications, analytics, and finance & accounting teams.
Contribute to our suite of reports and data products while collaborating with business analysts, business process owners, and other team members, including UAT, and support for technical testing.
Work closely with senior team members defining data architecture, optimizing EDW performance, and updating EDW documentation, including outlining options to improve resilience and data quality monitoring.
Maintain the Enterprise Data Warehouse (EDW) and influence data quality and reliability.

COMPETENCIES
Data modeling, relational databases, data warehousing, business intelligence, stream processing.
SQL programming skills, including writing efficient and optimized SQL queries for data retrieval and manipulation.
ETL/ELT processes and data integration.
Strong analytical skills, development experience, and excellent verbal and written communications.
Manage all stages of the development lifecycle, including planning, requirements gathering, designing, developing, documenting, testing, training, deployment, and support.
Own individual and cross-functional team deliverables on small to large projects.
Project and change management methodologies including Agile/Scrum.
Willingness and availability to travel, and perform other duties as needed.
Must be a self-starter, manage workload autonomously, and balance competing priorities.
Ability and desire to continuously learn about recent technologies and Analytics trends.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Computer Engineering, Analytics, Information Systems, or equivalent experience.
1-3 years of experience working with SQL and relational databases (MS SQL Server preferred).
1-3 years of experience working with Microsoft Azure, particularly Azure Data Factory, Azure Synapse and Power Platform.
1-3 years of experience preferred in reporting, dashboards, and BI tools (Power BI preferred) for ERP and MES platforms.
1-3 years of experience preferred with Visual Studio, SSRS.
Experience with MS D365 and AVEVA or similar ERP and MES solutions preferred.
Proficient in coding and scripting in Python preferred.
Experience in Manufacturing, Supply Chain, Logistics, and Finance and Accounting solutions preferred.
How to stand out: Microsoft Azure Data Engineer Associate Certification
COMPENSATION AND BENEFITS
Available on your first day: Full medical, vision, dental benefits, short-term disability, and long-term disability
Paid time off
Monthly wellness reimbursement
Health Savings, Flexible Spending and Dependent Care accounts
401(k) retirement savings plan with employer match
Annual bonus",2.6,"Tokai Carbon GE LLC
2.6","Charlotte, NC",1001 to 5000 Employees,-1,Company - Public,Chemical Manufacturing,Manufacturing,$1 to $5 billion (USD)
668,Big Data Engineer,Employer Provided Salary:$48.00 - $52.00 Per Hour,"Greetings from KonnectingTree Inc.,
We are actively looking for a candidate with below mentioned experience,
Role : Bigdata Engineer
Location : Hybrid ( Bloomington, MN)
Experience : 3-5 years
Job Description:
* Good experience in build and deployment of Big data applications using Pyspark
* Good experience in Python.
* Experience in Hadoop file structure.
* Good Communication and team player
Job Type: Contract
Salary: $48.00 - $52.00 per hour
Experience level:
3 years
4 years
Schedule:
8 hour shift
Ability to commute/relocate:
Bloomington, MN 55425: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",-1.0,KONNECTINGTREE INC,"Bloomington, MN",-1,-1,-1,-1,-1,-1
669,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
670,Energy Data Engineer - Mid Level,$81K - $121K (Glassdoor est.),"About the Organization
Now is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.

The Data Engineer will support Redhorse in its execution of specific task areas related to the Federal Government Carbon Pollution Free Electricity (CFE) Technical and Strategy Support contract. The program will establish technical frameworks and analytical tools and methods to inform CFE procurement decisions for the entirety of the federal government. Work will be conducted primarily at Redhorse office locations (hybrid remote work) and on government client site as required. The ideal candidate will live in the DC/Virginia/Maryland area and willing to undergo a background check to obtain a Public Trust.
Primary Duties and Responsibilities for this position include
Review and analyze the US Federal Government Executive Agencies electric load data
Develop and test modeling techniques to understand an agency’s hourly load profile to inform the agency’s hourly CFE procurement requirement as an off taker from a new CFE technology source.
Support the configuration and ingestion of designated structured, unstructured, and semi-structured data repositories into capabilities that satisfy client requirements and support a data analytics pipeline to drive rapid delivery of functionality to the client.
Maintain all operational aspects of data transfers while accounting for the security posture of the underlying infrastructure and the systems and applications that are supported and monitoring the health of the environment through a variety of health tracking capabilities.
Automate configuration management, leverage tools, and stay current on data extract, transfer, and load (ETL) technologies and services.
Work under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.
Apply comprehension of data engineering-specific technologies and services, leverage expertise in databases and a variety of approaches to structuring and retrieving of data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically using vendor consoles.
Engage with multiple functional groups to comprehend client challenges, prototype new ideas and new technologies, help to create solutions to drive the next wave of innovation, and design, implement, schedule, test, and deploy full features and components of solutions.
Identify and implement scalable and efficient coding solutions.
Minimum Requirements for skills, Experience, and Credentials
Master’s Degree in Engineering, Computer Science, Mathematics or Statistics
Four (4) years of experience in enterprise data analysis with a preferred focus on utility data, to include advanced metering infrastructure (AMI) electricity data
Local to Washington D.C. metro area preferred, remote work is an option
Willingness to travel in support of on-site customer meetings
Willingness to obtain a Public Trust
Desired Basic Requirements for Skills, Experience, and Credentials include
Knowledge of U.S. Federal Government Energy Legislation and Executive Orders (especially EO 14057)
Experience in or knowledge of utility interval data and electricity pricing
Executive Order Requirement:
While currently on HOLD, Redhorse recognizes that we may need to comply with Executive Order 14042 which requires Federal Contractors to ensure all U.S. new hires be fully vaccinated for COVID-19. As required by the Executive Order, Redhorse will work in coordination with applicable contract agencies to consider requests for Reasonable Accommodations.
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.
EOE/M/F/Vet/Disabled",3.5,"Redhorse
3.5","Arlington, VA",51 to 200 Employees,2008,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
671,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
672,Data Engineer (Contract),$77K - $111K (Glassdoor est.),"About RevOpsforce:
At RevOpsforce, our mission is to drive sustainable revenue growth for our clients by leveraging data-driven insights, technology, and process optimization. We strive to deliver measurable results that exceed expectations, while building strong partnerships with our clients and continuously improving our own capabilities. Our Expert Network is composed of the highest skilled and certified professionals in revenue operations. We leverage this network to solve our clients' most complex operational challenges.
Type: Contract
Job Description:
We are seeking a talented and experienced Data Engineer to join our growing team. As a Data Engineer at our company, you will be responsible for designing, building, and maintaining the data pipelines that power our data-driven applications. In this role, you will work closely with our data science and engineering teams to develop high-quality, efficient data pipelines that enable us to derive insights from vast amounts of data. You should be comfortable working with large volumes of structured and unstructured data, and have a strong understanding of SQL, Python, and other programming languages.
Responsibilities:
Design, build, and maintain data pipelines to support data-driven applications and analytics
Analyze data to identify trends and patterns
Collaborate with data scientists and engineers to develop data-driven solutions
Write and maintain documentation for data pipelines
Monitor and optimize data pipelines for performance and efficiency
Qualifications:
Bachelor's or Master's degree in a related field (e.g. Computer Science, Data Science, Engineering)
3+ years of experience in a data engineering role
Strong skills in SQL and at least one programming language (e.g. Python, Java, C++, etc.)
Experience with big data technologies (e.g. Hadoop, Spark, etc.) and cloud platforms (e.g. AWS, Azure, GCP)
Strong problem-solving and analytical skills
Excellent communication skills and ability to work in a team environment
Join our dynamic and forward-thinking team to make a significant impact on our clients' revenue growth journey!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Website is coming soon and will be located at www.revopsforce.com",-1.0,RevOpsforce,"Austin, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
673,Data Engineer II,$90K - $119K (Glassdoor est.),"Data Engineer II
Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What’s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.
Join us to do the best work of your career and make a profound social impact as a Data Engineer on our Growth Marketing team in Round Rock, Texas, Hopkinton, Massachusetts, or Remote United States
What you’ll achieve
As a Data Engineer II on a growing team, you will bring in your industry experience to develop, deploy, and maintain productionized data pipelines that support the training and productionization of machine-learning models.
You will:
Build, test, and implement data pipelines using tools like Apache Airflow, Spark, and Python for the training and inferencing of machine-learning models
Ensure data quality, monitor data pipelines, and troubleshoot production issues
Document metadata, data models, data pipelines and APIs to help data science teams understand and leverage data assets.
Work with data scientists and business analysts to understand data requirements
Take the first step toward your dream career
Every Dell Technologies team member brings something unique to the table. Here’s what we are looking for in this role:
Essential Requirements
U.S. citizen residing in the U.S.
Bachelor’s Degree in Computer Science, Engineering, IT, or equivalent professional experience
1 to 2 years experience developing production-quality Python and performant SQL, working with data warehouses, JSON, Apache Hive, Hadoop or Spark, or related technologies
1 to 2 years experience with orchestration tools, such as Apache AirFlow
Experience with monitoring tools and observability
Who we are

We believe that each of us has the power to make an impact. That’s why we put our team members at the center of everything we do. If you’re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we’re looking for you.

Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us.

Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. Read the full Equal Employment Opportunity Policy here.
Job ID:R227305

Dell’s Flexible & Hybrid Work Culture

At Dell Technologies, we believe our best work is done when flexibility is offered.

We know that freedom and flexibility are crucial to all our employees no matter where you are located and our flexible and hybrid work style allows team members to have the freedom to ideate, be innovative, and drive results their way. To learn more about our work culture, please visit our locations page.",4.2,"Dell Technologies
4.2","Round Rock, TX",10000+ Employees,1984,Company - Public,Information Technology Support Services,Information Technology,$10+ billion (USD)
674,Data Engineer,$73K - $110K (Glassdoor est.),"Data Engineer

Posting Date: Aug 23, 2023
Location: Houston, TX, US
Lake Charles, LA, US
Company: Sasol
A job at Sasol is a career made by you, with purpose, development opportunities, benefits to support every stage of life and a working culture that embraces flexibility, diversity, and inclusion. Learn more.

Purpose of Job
The Data Engineer works as part of an agile team, interacts with data scientists/translators and industry experts to understand how data needs to be converted, loaded and presented. Collects, structures, analyses data and ensures quality. Responsible for the definition, conception, construction, and ongoing support of big data platforms and solutions.
Recruitment Description / Key Accountabilities
Integrate IT (Information Technology) and OT (Operational Technology) Systems.
Use programming languages and big data technologies following best practices and standards
Understands latest analytics tools and technologies (open source/ proprietary/ vendor supported) and determines fit with current architecture
Building and maintaining data pipelines that move data between various systems.
Ensuring data quality, integrity and security
Monitoring and optimizing data performance
Collaborate with technical teams to develop and maintain a flexible and sustainable workflow.
Collaborate with Product Owner, Data Scientists, Data Translators, and other Business Stakeholders to implement and deploy scalable solutions in an agile way of working; actively participate in scrum process and meetings with team where required
Assist with communication, improve transparency, radiate information, and address challenges effectively
Formal Education
University Bachelors Degree in Computer Science & Inf Systems or Engineering: Computer
Min Experience
Experience: 5+ relevant years
Long Description
Excellent communication and written skills.
Experience in building complex data pipelines using Cloud Services, REST APIs, SQL Databases. Python and SQL skills are are required. Experience in Azure ADF, Synapse or Databricks are beneficial.
Knowlege of Cloud Computing and Infrastructure, Data Models and Standards, Database Administration, Plant Information Management Systems, ERP Systems, Networking and Protocols, Security and Compliance, Change Managemnt, Problem Solving and Trouble Shooting are a plus.

Sasol (USA) Corporation is an Equal Opportunity Employer and gives consideration for employment to qualified applicants without regard to race, color, religion, creed, age, sex, sexual orientation, gender identity, pregnancy, national origin, disability or protected veteran status, as well as any other characteristic protected by applicable law, regulation or local ordinance. For more information about your rights under the law, see http://www.eeoc.gov/employers/upload/poster_screen_reader_optimized.pdf

Sasol treats work-authorized job applicants equally in recruiting and hiring without regard to their citizenship status or national origin unless required by law, regulation, executive order, government contract, or determination by the Attorney General. Please note Sasol will not sponsor, obtain, and/or petition for temporary visa status (for example, E, F-1, H-1, H-2, L, B, J, or TN) for candidates for this position and/or personnel hired for this position.


Nearest Major Market: Houston",3.9,"Sasol
3.9","Houston, TX",10000+ Employees,1950,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$10+ billion (USD)
675,Apple Music - Data Engineer,-1,"Summary
Posted: Aug 18, 2023
Weekly Hours: 40
Role Number:200496819
This team is more than a group of engineers — it’s a group of music lovers. That passion has made Apple Music the world’s most complete music experience, with over 60 million songs, thousands of playlists, and daily selections from music experts for 115 countries. The team’s data-driven engineers focus relentlessly on the customer experience by running worldwide experiments and analyzing usage and latency, while collaborating with Apple’s product groups. As a result, someone can use the Shazam app to identify an intriguing song in a café in the morning, add it to their playlist from Apple Watch, listen to it through their AirPods on their commute, and share it with their family on HomePod at dinnertime. And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love. Apple Music is a huge part of Apple’s business because it’s a big part of people’s lives.
Key Qualifications
Expertise in designing, implementing and supporting highly scalable data systems and services in Scala
Experience with distributed computing technologies such as Hadoop MapReduce, Spark/Spark-SQL, YARN/MR2
Expertise building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Solid understanding of data-modeling and data-architecture optimized for big data patterns, such as efficient storage and query on HDFS Experience with distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues
Experience with low-latency NoSQL datastores, and traditional relational databases is a plus
Description
We are seeking a hardworking, performance-savvy, engineer to build out the big data platform and services, which power many of these customer features — existing and new. As a core member of the Data Engineering team you will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability. This will include crafting systems to model, ingest, process and compute large-scale, mission-critical data across Apple Music. High-throughput and reliability are essential. The computed datasets are produced for internal reporting used by executives and they are also shared across many teams within the ASE organization, such as the Search team, Recommendation team, AB team, Marketing team. You will also enjoy the benefits of working in a fast growing business where you are encouraged to “Think Different” to solve very interesting technical challenges and where your efforts play a key role in the success of Apple’s business. This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users. Come join us to help deliver the next amazing Apple product!
Education & Experience
BS CS or equivalent work experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $131,500 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",4.2,"Apple
4.2","Seattle, WA",10000+ Employees,1976,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
676,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
677,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
678,Data Scientist / ML Engineer (REMOTE /Contract),-1,"Role: Data Scientist / Machine Learning Engineer
Location: REMOTE
Duration: 6+ months
Scheduling Interviews
Skills/Experience:
6+ years of experience working in Data Science and Machine Learning field.
Extensive experience in applying data preprocessing, statistical analysis, data analytics tools, predictive modelling, model deployment and evidence-based approaches to find lean, actionable solutions to various real-world enterprise business problems.
Experience in the application of Supervised and Unsupervised Learning algorithms, Naïve Bayes, Regression Analysis, Neural Networks/Deep Learning, Support Vector Machines (SVM), Random Forest, K-Means, Hierarchical, Spectral clustering, DBSCAN, Collaborative Filtering and other advanced machine learning techniques.
Used AWS Cloud Services Sagemaker, ECS, EKS, S3, Redshift, QuickSight
Experience with Google Cloud Platform (GCP) AutoML, Vertex AI, BigQuery, Colab, DataProc, Data Studio,
Exposure to Azure ML, PowerBI, Azure SQL and DataBricks
Design custom BI reporting dashboards or interactive data visualizations and widgets in R and Python using Shiny, Tableau, Ggplot2, Plotly, Matplotlib, and Seaborn.
Produce custom BI reporting dashboards in R and Python using Shiny, and Plotly for rapid dissemination of actionable, data driven insights.
Strong experience in Software Development Life Cycle (SDLC), MLOPS
Experience in working with relational databases (Teradata, Oracle) with advanced SQL skills.",3.5,"Glow Networks
3.5",Remote,51 to 200 Employees,2003,Subsidiary or Business Segment,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
679,Sr. Azure Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"LeadStack Inc. is an award-winning, one of the nation's fastest-growing, certified minority-owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Position Details:
Title: Azure Data Engineer
Location: Remote
Duration: 6+ Months Contract
Top 3 skills: SQL, Snowflake, and Data Warehousing/Design
Its a W2 Role. No C2C.
Client is seeking a talented and motivated Data Engineer to join our dynamic team. As a Data Engineer, you will play a critical role in developing and maintaining our data infrastructure, ensuring the smooth flow of data for our analytics and insights projects. The ideal candidate will have a strong background in SQL, data warehousing, ETL processes, and a preference for Snowflake experience. Additional experience with dbt or semantic layer tools will be a bonus, and proficiency in Python will also be advantageous.
Bachelor's degree in Computer Science, Engineering, or a related field.
Solid experience with SQL and data warehousing concepts.
Strong understanding of ETL processes and tools.
Preferred experience with Snowflake data warehouse platform.
Bonus for experience with dbt or other semantic layer tools.
Proficiency in Python is a plus.
Excellent problem-solving and analytical skills.
Strong attention to detail and ability to work in a fast-paced environment.
Effective communication skills and ability to collaborate with cross-functional teams.
Key Responsibilities:
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.
Optimize data ingestion and transformation processes to ensure efficient and scalable data flows.
Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.
Develop and maintain data models, schemas, and data dictionaries for efficient data storage and retrieval.
Perform data validation, quality checks, and troubleshooting to identify and resolve data-related issues.
Monitor and optimize database performance, including query tuning and index optimization.
Collaborate with data scientists and analysts to implement data models and support their data requirements.
Stay up-to-date with industry trends and best practices in data engineering, and proactively recommend and implement improvements to our data infrastructure
To know more about current opportunities at LeadStack, please visit us at https://leadstackinc.com/careers/
Should you have any questions, feel free to call me on 510-480-1006 or send an email on tarique.aziz@leadstackinc.com
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Data Warehousing/Design: 8 years (Required)
Snowflake: 5 years (Required)
Azure: 5 years (Required)
Work Location: Remote",4.3,"Leadstack Inc
4.3",Remote,51 to 200 Employees,2016,Company - Private,HR Consulting,Human Resources & Staffing,Unknown / Non-Applicable
680,AI/ML Data Automation Engineer,$88K - $125K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
681,Data Warehouse & Artificial Intelligence Engineer,$83K - $120K (Glassdoor est.),"Job Description:

Design, Implement and Maintain data warehouse.

Duties and Responsibilities:

Develop, test, and deploy extraction, transformation and loading (ETL) routines to load the data warehouse using SSIS, stored procedures, etc.
Develop analytics including reports, dashboards, and data visualizations
Troubleshoot BI-related problems and tune for performance
Implement and oversee a company-wide data governance program to support master data management and resolve data definition discrepancies among stakeholders
Plan the technical approach for implementing analytics strategy across multiple channels
Plan and implement activities to maintain data confidence
Provide technical direction and technical support for all member of the team
Architect and implement analytics solutions to meet data collection and reporting needs.
Work with team to recommend analytic software
Provide general troubleshooting assistance for analytic reports and break fixes
Wrangle data (a.k.a. data engineering) from a wide variety of complex data sets large volumes of data into normalized and enriched data sets.
Assemble and evaluate data such that new insights, solutions, and visualizations can be derived.
Perform data discoveries to understand data formats, source systems, file sizes, etc. and engage with internal and external business partners in this discovery process.
Deliver data acquisition, transformations, normalization, mapping, and loading of data into a variety of data models.
Collaborate extensively with Technology to design data ingestion, data models, and automated operational metrics for consistently high quality data
Supports the research, analysis and presentation of information by: producing reports, compiling and summarizing information, producing supporting documentation and exhibits, and verifying information received from external sources.
Assists other associates with preparation of reports and use of information systems, software and related sources of information. Trains other users on report preparation and database access.
Perform other duties as assigned.

Required Knowledge, skills and abilities:

Ability to manage change successfully in a fast-paced environment
A passion for learning and the ability to do so on the fly
Must be strategic, dynamic, and detail-oriented. The ideal candidate is an analytical thinker, a self-starter and a quick learner.
Ability to design and architect method of extracting data from its original source to another location (i.e. a data lake) to make that data consumable for a variety of downstream purposes such as applications, visualizations, analytics and other related data science activities.
Basic understanding of data science
Ability to transforming and map one ""raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.
Exceptional ability to clearly and effectively communicate with IT peers and users at all levels of technical and business acumen.
Strong time management skills and multi-tasking capabilities
Ability to understand business rules governing financial and budgeting process
Exceptional oral and written communication skills
Self-motivated, ability to set priorities and meet deadlines

Education and Experience:

Bachelor's degree in Computer Science, Business, Economics, Information Science, or Analytics
Minimum of 3 years of experience with data analytics across multiple channels
Experienced with the technologies required to implement Analytics across multiple channels, (for example, Data Layer, Custom JavaScript, JAVA, AJAX, HTML, Objective-C, Swift)
Knowledge of Tableau is a plus
Familiarity with Microsoft Power BI
Extensive experience with ETL and/or other Big Data processes.
Contract manufacturing experience is a plus.

Foxconn Assembly, LLC is an Equal Opportunity Employer (EOE). All qualified candidates will receive consideration without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or marital status in accordance with applicable federal, state and local laws.
Foxconn Assembly, LLC participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

NbXn0DyWY6",3.4,"Foxconn Corporation
3.4","Houston, TX",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$5 to $25 million (USD)
682,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
683,Data Movement Engineer C2H,Employer Provided Salary:$55.00 Per Hour,"Client required information:
***They need Someone who has strong Talend Experience minimum of 5 years***.
Talend API is Desirable, But any API is also fine like snowflake as a last option.
They are working on Data Base Movement from Talend to ETL/ELT.
Formally they need ETL/ELT Developers or Talend Developers with ETL/ELT data movement experience.
Implementation and Configuration Talend is Mandate.
They also need to take a an on call support after work hours for ongoing production Support team, this might happen on weekends too.
They are going to implement more API’s, so having extra API experience is plus.
They need to understand the baseline of Insurance( The Basic work or Knowledge on Insurance Modules).
They can Work Remote, But need only from EST Zones.
Manager is not going to accept any profiles without Talend Experience.
Skill Qualifications Required:
· Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
· Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
· Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
· Some experience with cloud-based database technologies required
· Working knowledge of data warehousing concepts, structures and ETL best practices
· Experience using query tools (e.g. AQT, MS Query)
· Ability to problem solve using analytical thinking skills
· Must work well independently - must be inquisitive and seek answers to complex questions without being prompted
· Strong organizational and time management skills
· Strong communication skills including verbal and written to communicate effectively with clients and management
· Strong project management skills to ensure that projects get done on time and within budget
· Effectively participates in teams and moves the team toward completion of goals
Job Types: Full-time, Contract
Salary: From $55.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Experience:
Talend: 5 years (Required)
ETL: 5 years (Required)
SQL query: 3 years (Required)
data warehousing: 3 years (Required)
business intelligence tools: 3 years (Required)
Work Location: Remote",-1.0,Technovant inc,"New Haven, CT",-1,-1,-1,-1,-1,-1
684,Senior Data Engineer,Employer Provided Salary:$87K - $180K,"Oakland, California; Atlanta, Georgia; Irving, Texas
Regular Employee Full-Time
R-2023-08-81

Who We Are:
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners - in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
Pandora
Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. Pandora provides consumers with a uniquely-personalized music and podcast listening experience with its proprietary Music Genome Project® and Podcast Genome Project® technology. Pandora is available through its mobile app, the web, and integrations with more than 2,000 connected products.
How you’ll make an impact:
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What you’ll do:
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What you’ll need:
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to pay attention to details and be organized.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $87,200 to $180,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.

Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-08-81",3.6,"Adswizz
3.6","Oakland, CA",51 to 200 Employees,2007,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
685,Data Engineer,-1,"SalesIntel is the top revenue intelligence platform on the market. Our unique approach to data collection, enhancement, verification and growth distinguishes us from the market, solidifying our position as the best B2B data partner.

The Data Engineer will be responsible for building and extending our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys working with big data and building systems from the ground up.

You will collaborate with our software engineers our software engineers, database architects, data analysts and data scientists to ensure our data delivery architecture is consistent throughout the platform. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What You’ll Be Doing:
Design and build parts of our data pipeline architecture for extraction, transformation, and loading of data from a wide variety of data sources using the latest Big Data technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with machine learning, data, and analytics experts to drive innovation, accuracy and greater functionality in our data system.
Qualifications:
Bachelor's degree in Engineering, Computer Science, or relevant field.
5+ years of experience in a Data Engineer role.
3+ years experience with Apache Spark and solid understanding of the fundamentals.
Deep understanding of Big Data concepts and distributed systems.
Strong coding skills with Scala, Python, Java and other languages and the ability to quickly switch between them with ease.
Advanced working SQL knowledge and experience working with a variety of relational databases such as Postgres and MySQL.
Experience working with data stored in many formats including TSV, CSV, JSON, Parquet.
Cloud Experience with DataBricks or similar service: S3, Athena, EC2, EMR, RDS, Redshift.
Comfortable working in a linux shell environment and writing scripts as needed.
Comfortable working in an Agile environment
Machine Learning knowledge is a plus.
Must be capable of working independently and delivering stable, efficient and reliable software.
Excellent written and verbal communication skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

This is a remote position.",-1.0,"SalesIntel Research, Inc.",Remote,51 to 200 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
686,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
687,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
688,Senior Software Engineer (Data),Employer Provided Salary:$160K - $200K,"About Juniper Square
Our mission is to unlock the full potential of private markets. Privately owned assets like commercial real estate, private equity, and venture capital make up half of our financial ecosystem yet remain inaccessible to most people. We are digitizing these markets, and as a result, bringing efficiency, transparency, and access to one of the most productive corners of our financial ecosystem. If you care about making the world a better place by making markets work better through technology – all while contributing as a member of a values-driven organization – we want to hear from you.
Juniper Square offers employees a variety of ways to work, ranging from a fully remote experience to working full-time in one of our physical offices. We invest heavily in digital-first operations, allowing our teams to collaborate effectively across most US states, 2 Canadian Provinces, and Mexico. We also have physical offices in San Francisco, CA and Austin, TX, for employees who prefer to work in an office some or all of the time.
GP Experience
Juniper Square serves two sides of the private capital markets, the investment managers (GPs) and the investors (LPs). The GP eXperience team (i.e., GPX) is responsible for Juniper Square’s product offering for General Partners (GPs). This is our core product that enables all other innovation at Juniper Square as we unlock and improve the world’s private capital markets. Our platform handles billions of dollars of transactions each month and we are actively expanding into additional private asset classes such as Venture Capital & Private Equity. Come help us innovate in fundraising, reporting, asset-ownership mapping, and more.
The Team
The Data Engineering team is responsible for Data pipelines that serve multiple types of customers including internal Juniper Square users for Business Intelligence and GPs for Analytics on their data. We support the ability for these customers to create and manage their custom dashboards. We also support the ability for other Product Engineering teams to add metrics to track product usage for the features they launch into production.
About your role
Juniper Square is growing rapidly, and our data needs are growing even faster, so we’re growing our Data Engineering Team. As a Senior Data Engineer your role will be pivotal to evolving our existing data and reporting experiences. You’ll build out pipelines to gather data from multiple sources and make it available for analysis. You will shape both internal and external analytics products to help guide business-critical decisions, enhance their workflows, and improve decision-making.
What you’ll do
Design and implement sophisticated data models in SQL.
Work closely with the other Software Engineers to ensure sound, scalable implementation.
Act as a technical expert on our team regarding all things data, especially as the data team grows and evolves.
Introduce new technologies to evolve and enhance our data pipeline capabilities.
Document data models, architectural decisions and data dictionaries to enable collaboration, maintainability and usability of our analytics platforms and code.
Assist with governance, guidance, code reviews, and access controls so that we maintain consistency, quality, and business confidentiality as we scale analytics access across the company and to customers.
Externally: learn our application data schema, and develop a fluency in how to transform it to enhance customer’s decision-making with data.
Internally: guide product and development teams, advising on instrumentation and laying development foundations for product usage reporting.
Fulfill projects with minimal guidance but with an appropriate sense of when and how to collaborate with others.
Build scalable, highly performant infrastructure for delivering clear business insights from a variety of raw data sources.
Qualifications
Bachelor's degree in Computer Science, or equivalent work experience
4+ years of experience building ETL (Extraction Transform Load) or ELT (Extraction Load Transform) pipelines from scratch
Strong command of relational databases (Postgresql preferred), data modeling and database design
Strong command of Python and experience building production web applications using Python
Experience with cloud based services (AWS RDS preferred)
Experience developing on (or administering) BI / data visualization platforms (ex. Looker, Tableau, PowerBI, Mode, Data Studio, Domo, QlikView etc.).
Basic understanding of data warehouses such as Amazon Redshift, Google BigQuery, Snowflake etc.
Demonstrated history of translating data into clear and actionable narratives and communicating opportunities and challenges relevant to stakeholders.
You must be flexible and adaptable—you will be operating in a fast-paced startup environment.
At Juniper Square, we believe building a diverse workforce and an inclusive culture makes us a better company. If you think this job sounds like a fit, we encourage you to apply even if you don’t meet all the qualifications.
Benefits
Compensation for this position includes a base salary, equity, and a variety of benefits. The U.S. base salary range for this role is $160,000 - $200,000. Actual base salaries will be based on candidate-specific factors, including experience, skillset, and location, and local minimum pay requirements as applicable. We are actively hiring for this role in Canada, and offer competitive local pay and benefits. Your recruiter can provide further details.
Competitive salary and meaningful equity
Health, dental, and vision care for you and your family
Unlimited vacation policy and paid holidays
Generous paid family leave, medical leave, and bereavement leave policies
401k retirement savings plan
Healthcare FSA and commuter benefits programs
Freedom to customize your work and technology setup as you see fit
Professional development stipend
Monthly work from home wellness stipend while we're all remote
Mental wellness coverage including live coaching and therapy sessions
Home office productivity allowance to help create an ideal work from home setup
#LI-AD1
#LI-Remote",4.0,"Juniper Square
4.0",Illinois,201 to 500 Employees,2014,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
689,"Data Engineer (Snowflake, Python, Power BI)",Employer Provided Salary:$40.00 - $45.00 Per Hour,"Title: Data Engineer (Snowflake, Python, Power BI)
Location: Morton, IL
Duration: 12+ Months
Job Description:
The main function of a data engineer is to ensure that the data assets of an organization are supported by an architecture that supports the organization in achieving its strategic goal. A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data.
Position’s Contributions to Work Group:
The PSLD Transformation Analytics group engages with various stakeholders across the organization to help solve their business problems. The individual will run the entire project end to end, so strong skills in gathering/understanding customer requirements, creating and maintaining optimal data pipeline architecture, choosing appropriate tools/techniques and delivering actionable insights are must. They will also learn all relevant BU specific data and often combine them with relevant enterprise data domains to bring insights that are not possible if BU data alone is analyzed.
Specific responsibilities are –
Extract large, complex data sets that meet business requirements. Work to build the on-prem /cloud infrastructure for optimal extraction, transformation, and loading of a wide variety of complex business data from on-prem/cloud databases.
Identify ways to improve data reliability, efficiency, and quality.
Work with internal and external stakeholders to assist with data-related technical issues and support data needs.
Own the design and development of ongoing business metrics/KPI, reports and dashboards to drive key business decisions.
Prepare data for predictive and prescriptive modeling.
Typical Day:
Work is typically directed by a direct supervisor, project or team lead. Decisions on routine, medium risk issues that may affect the project team, suppliers or internal customers may be made by this position. Challenges include meeting expectations in delivering results, learning to refine solutions to better fit complex situations, making timely decisions, and communicating effectively with all project stakeholders.
Education/Experience:
Associate's degree in computer programming or a relevant field required. Bachelor's degree preferred.
Critical Technical Skills:
Familiarity with database such as Snowflake, DB2, SQL Server, Oracle (2-3 of these are required)
Programming languages - SQL(required), Python(required) and SAS(preferred)
Experience working with large data sets, preferably in several GB or millions of transactions.
Visualization - PowerBI(required), Tableau(preferred)
Experience working with platform integration tool like Snaplogic is preferred
Experience working with AWS (required)
Soft Skills Required:
Communication, Team-work, Problem Solving, Customer Focus
Job Type: Contract
Pay: $40.00 - $45.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
Snowflake: 3 years (Required)
Power BI: 4 years (Required)
Python: 4 years (Required)
Work Location: In person",5.0,"DSMH LLC
5.0","Morton, IL",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
690,Apple Music - Data Engineer,-1,"Summary
Posted: Aug 18, 2023
Weekly Hours: 40
Role Number:200496819
This team is more than a group of engineers — it’s a group of music lovers. That passion has made Apple Music the world’s most complete music experience, with over 60 million songs, thousands of playlists, and daily selections from music experts for 115 countries. The team’s data-driven engineers focus relentlessly on the customer experience by running worldwide experiments and analyzing usage and latency, while collaborating with Apple’s product groups. As a result, someone can use the Shazam app to identify an intriguing song in a café in the morning, add it to their playlist from Apple Watch, listen to it through their AirPods on their commute, and share it with their family on HomePod at dinnertime. And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love. Apple Music is a huge part of Apple’s business because it’s a big part of people’s lives.
Key Qualifications
Expertise in designing, implementing and supporting highly scalable data systems and services in Scala
Experience with distributed computing technologies such as Hadoop MapReduce, Spark/Spark-SQL, YARN/MR2
Expertise building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Solid understanding of data-modeling and data-architecture optimized for big data patterns, such as efficient storage and query on HDFS Experience with distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues
Experience with low-latency NoSQL datastores, and traditional relational databases is a plus
Description
We are seeking a hardworking, performance-savvy, engineer to build out the big data platform and services, which power many of these customer features — existing and new. As a core member of the Data Engineering team you will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability. This will include crafting systems to model, ingest, process and compute large-scale, mission-critical data across Apple Music. High-throughput and reliability are essential. The computed datasets are produced for internal reporting used by executives and they are also shared across many teams within the ASE organization, such as the Search team, Recommendation team, AB team, Marketing team. You will also enjoy the benefits of working in a fast growing business where you are encouraged to “Think Different” to solve very interesting technical challenges and where your efforts play a key role in the success of Apple’s business. This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users. Come join us to help deliver the next amazing Apple product!
Education & Experience
BS CS or equivalent work experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $131,500 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",4.2,"Apple
4.2","Seattle, WA",10000+ Employees,1976,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
691,2024 Data Engineer I,-1,"2024 Data Engineer I
316114

Textron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.

Responsibilities:
Analyze, manipulate, or process large sets of data using statistical software.
Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces
Identify business problems or management objectives that can be addressed through data analysis.
Document data standards, semantic layer mapping, data flows and conceptual flows.
Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.
Perform statistical analysis to provide recommendations on best courses of action to business leaders.
Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.
Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.
Collaborate with business users to ensure data is available for self-service analytics.
Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.


Qualifications:
Education: Bachelor’s degree in computer science, engineering, mathematics, or similar field required
Years of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience required
Software Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferred
Experience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)
EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

Recruiting Company Textron Specialized Vehicles
Primary Location US-Georgia-Augusta
Job Field Information Technology
Schedule Full-time
Job Level Individual Contributor
Shift First Shift
Job Posting 08/24/2023, 8:20:05 AM",3.8,"Textron Specialized Vehicles
3.8","Augusta, GA",10000+ Employees,1923,Company - Public,Aerospace & Defense,Aerospace & Defense,$10+ billion (USD)
692,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
693,AI/ML Data Automation Engineer,$88K - $125K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
694,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
695,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
696,Data Engineer II,$90K - $119K (Glassdoor est.),"Data Engineer II
Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What’s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.
Join us to do the best work of your career and make a profound social impact as a Data Engineer on our Growth Marketing team in Round Rock, Texas, Hopkinton, Massachusetts, or Remote United States
What you’ll achieve
As a Data Engineer II on a growing team, you will bring in your industry experience to develop, deploy, and maintain productionized data pipelines that support the training and productionization of machine-learning models.
You will:
Build, test, and implement data pipelines using tools like Apache Airflow, Spark, and Python for the training and inferencing of machine-learning models
Ensure data quality, monitor data pipelines, and troubleshoot production issues
Document metadata, data models, data pipelines and APIs to help data science teams understand and leverage data assets.
Work with data scientists and business analysts to understand data requirements
Take the first step toward your dream career
Every Dell Technologies team member brings something unique to the table. Here’s what we are looking for in this role:
Essential Requirements
U.S. citizen residing in the U.S.
Bachelor’s Degree in Computer Science, Engineering, IT, or equivalent professional experience
1 to 2 years experience developing production-quality Python and performant SQL, working with data warehouses, JSON, Apache Hive, Hadoop or Spark, or related technologies
1 to 2 years experience with orchestration tools, such as Apache AirFlow
Experience with monitoring tools and observability
Who we are

We believe that each of us has the power to make an impact. That’s why we put our team members at the center of everything we do. If you’re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we’re looking for you.

Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us.

Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. Read the full Equal Employment Opportunity Policy here.
Job ID:R227305

Dell’s Flexible & Hybrid Work Culture

At Dell Technologies, we believe our best work is done when flexibility is offered.

We know that freedom and flexibility are crucial to all our employees no matter where you are located and our flexible and hybrid work style allows team members to have the freedom to ideate, be innovative, and drive results their way. To learn more about our work culture, please visit our locations page.",4.2,"Dell Technologies
4.2","Round Rock, TX",10000+ Employees,1984,Company - Public,Information Technology Support Services,Information Technology,$10+ billion (USD)
697,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
698,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
699,Data Engineer (multiple openings) - IHM,Employer Provided Salary:$88K - $157K,"Discover. A brighter future.
With us, you’ll do meaningful work from Day 1. Our collaborative culture is built on three core behaviors: We Play to Win, We Get Better Every Day & We Succeed Together. And we mean it — we want you to grow and make a difference at one of the world's leading digital banking and payments companies. We value what makes you unique so that you have an opportunity to shine.

Come build your future, while being the reason millions of people find a brighter financial future with Discover.

Job Description:
Employer: DFS Corporate Services LLC

Job Title: Data Engineer (multiple openings)

Job Location: Riverwoods, Illinois

Job Type: Full Time

Duties: Responsible for designing, developing, testing, and maintaining complex data solutions for the product. Mentor and influence peers to achieve commitments on data solutions on time and with quality. Telecommuting and/or working from home may be permissible pursuant to company policies.

Requirements: Employer will accept a Bachelor's degree in Computer Science, Computer Engineering, or a related field and 3 years of experience in Senior Data Engineer; ODI/DW/Batch Developer or related occupation.

Position required skills: Three (3) years of experience in the job offered or related occupation: working with Agile software development methodology and Agile management tools, including Jira; utilizing modern engineering practices, including the design and development of ETL data integration solutions in data warehouse environments; working with relational databases and Cloud-based technologies; utilizing design and architecture experience on large-scale ETL solutions; and working with Enterprise Data warehouse data models and dimensional modeling concepts, source to target mapping, and data integration architecture.

Position eligible for incentives under Employee Referral Program.

Rate of Pay: The base pay for this position generally ranges between $88,150.00 to $157,000.00. Additional incentives may be provided as part of a market competitive total compensation package. Factors, such as but not limited to, geographical location, relevant experience, education, and skill level may impact the pay for this position. We also offer a range of benefits and programs based on eligibility. Learn more at MyDiscoverBenefits.com .

QUALIFIED APPLICANTS: Please apply directly through our website by clicking on “Apply Now.” No calls. Equal Opportunity Employer/disability/vet.

What are you waiting for? Apply today!

All Discover employees place our customers at the very center of our work. To deliver on our promises to our customers, each of us contribute every day to a culture that values compliance and risk management.

Discover is committed to a diverse and inclusive workplace. Discover is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or other legally protected status. (Know Your Rights)",3.9,"Discover Financial Services
3.9","Riverwoods, IL",10000+ Employees,1985,Company - Public,Banking & Lending,Financial Services,$1 to $5 billion (USD)
700,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
701,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Job Role: Data Engineer with Snowflake
Location: REMOTE, Stillwater, MN.
HAVE TO WORK ONISTE WHEN EVER CLIENT CALL.
Duration: 6+ Months Contract
Visas: USC, GC, EAD-GC, H4-EAD
W2 Requirement
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Expected hours: 40 per week
Experience level:
7 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
Snowflake: 4 years (Required)
Work Location: Remote",-1.0,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
702,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
703,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
704,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
705,Data Engineer I - People Analytics,$73K - $111K (Glassdoor est.),"POSITION SUMMARY Our People Analytics team is expanding! The Data Engineer I is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.

This position can work in a flexible, hybrid model with 3 days on-site in Appleton, WI and 2 days remote.

JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.

QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming. Preferred experience in Python applied to data engineering and transformation.
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated

#LI-Hybid

Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101 .",4.1,"US Venture
4.1","Appleton, WI",1001 to 5000 Employees,1951,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
706,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
707,Data Engineer,Employer Provided Salary:$77K - $113K,"Eaton’s Electrical division is currently seeking a Business Analyst to work out of our Raleigh, NC location.

Relocation within the United States will be offered.

The expected annual salary range for this role is $77,249 - $113,299 a year.
Please note the salary information shown above is a general guideline only. Salaries are based upon candidate skills, experience, and qualifications, as well as market and business considerations.
What you’ll do:

The Data Engineer will collaborate with cross-functional teams to understand data requirements, design and implement efficient data pipelines, and ensure the availability, reliability, and scalability of our data solutions. This position necessitates proficiency in data manipulation, data analysis, SQL (writing queries), data modeling, and visualization skills.

Job responsibilities
Gather data from various sources. Design and write SQL queries to retrieve and analyze data stored in various databases.
Develop ETL processes to clean, transform, and enrich data in PowerQuery, ensuring data quality, consistency, and accuracy for use in the PowerBI's Data model.
Design and develop data model, transform raw data into meaningful insights, create relationships, hierarchies, and calculated measures in PowerBI’s Data Model
Document data integration and transformation processes for future reference and troubleshooting.
Assists in the implementation of data governance processes and systems.
Qualifications:
Basic Qualifications:
Bachelor's degree from an accredited educational institution
Minimum three (3) years of SQL and Power BI experience
Must be legally authorized to work in the United States without company sponsorship now or in the future

Preferred Qualifications:
Bachelor’s degree in computer science or software engineering
PowerBI (PowerQuery, Modelling, DAX, creating interactive visuals), Relational databases, Data Governance, Visualization
Problem-solving, Communication, Attention to detail
#LI-JM3
We are committed to ensuring equal employment opportunities for all job applicants and employees. Employment decisions are based upon job-related reasons regardless of an applicant's race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, marital status, genetic information, protected veteran status, or any other status protected by law.
Eaton considers qualified applicants regardless of criminal histories, consistent with local laws. To request a disability-related reasonable accommodation to assist you in your job search, application or interview process, please call us at 1-800-836-6345 to discuss your specific need. Only accommodation requests will be accepted by this phone number.
We know that good benefit programs are important to employees and their families. Eaton provides various Health and Welfare benefits as well as Retirement benefits, and several programs that provide for paid and unpaid time away from work. Click here for more detail: Eaton Benefits Overview. Please note that specific programs and options available to an employee may depend on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.",3.9,"Eaton
3.9","Raleigh, NC",10000+ Employees,1911,Company - Public,Electronics Manufacturing,Manufacturing,$10+ billion (USD)
708,Data Engineer,Employer Provided Salary:$30.05 - $44.13 Per Hour,"AMAZING BENEFITS PACKAGE!
Medical, Dental, & Vision Insurance
Paid Time Off (PTO) – up to 6 weeks of PTO per year
Childcare Stipend – contribution of $5,000 per year to the employee’s dependent care spending account
Loan Reimbursement - reimbursement for student loan payments up to $5,250 per year
Flexible Work Arrangements - opportunity to work a modified schedule, work part-time, or work from home
Retirement Plans - organization matches the employee’s contribution, up to 6% of gross wages
above benefits dependent upon eligibility criteria
Click here for more detail about the benefits package!
Pay Range: $30.05 - $44.13 USD hourly.

Purpose of Job
Improves the overall health of the communities we serve by providing superior customer service and advanced technical support to ensure quality patient care and user satisfaction as follows:
Essential Duties and Responsibilities
Designs and implements databases, ETL routines (using SSIS), stored procedures, and OLAP solutions in addition to providing support and enhancements to existing data solutions.
Designs and builds normalized and denormalized database solutions using healthcare industry best practices for data warehousing for specific client requirements, using Microsoft SQL Server Database programming (stored procedures and database design).
Works on multiple concurrent deadline-driven projects while ensuring data quality and meeting service level agreements.
Builds and maintains cubes (internal and client-facing).
Optimizes database schemas, queries, cubes, and reports, implementing complex logic requirements.
Conducts tuning reviews / assessments (DB and SQL tuning), reporting, and query monitoring.
Determines, enforces, and documents database policies, procedures, and standards.
Performs other duties as assigned, including supporting the CHAS Health Mission and Core Values.
Qualifications
Education/Experience: Associate’s degree or commensurate experience in a technical field required. Prior experience in database development using Microsoft SQL server, SSIS and SSAS preferred; understanding performance, deployment, configuration, security, migration, and troubleshooting; as well as proficiency in building and optimizing SQL queries preferred. Previous experience building and supporting business intelligence/data warehousing solutions; and knowledge of healthcare data models preferred.
Skills: Excellent logical and problem-solving abilities, verbal and written communication skills required. Ability to work independently in a self-directed environment, contribute to a team, maintain a positive attitude, demonstrate very high attention to detail, and a commitment to quality while working in a high availability environment is required. Commitment to supporting a safe, respectful, equitable, and inclusive environment required. Valid drivers’ license and insurance required.
Physical Demands
Required to stand, sit, and be mobile up to two-thirds of the time. Required to read from text and computer screen over two-thirds of the time. Climbing or balancing, stooping, kneeling, or crouching occurs less than one-third of the time. Communicating occurs constantly throughout the day. Lifting occurs about half the time up to 10 lbs. and less than one-third of the day from 25-40 lbs. Rarely is there a need to lift more than 41 lbs.",3.8,"CHAS Health
3.8","Spokane, WA",1001 to 5000 Employees,1994,Nonprofit Organization,Health Care Services & Hospitals,Healthcare,$25 to $100 million (USD)
709,"Data Engineer (Snowflake, Python, Power BI)",Employer Provided Salary:$40.00 - $45.00 Per Hour,"Title: Data Engineer (Snowflake, Python, Power BI)
Location: Morton, IL
Duration: 12+ Months
Job Description:
The main function of a data engineer is to ensure that the data assets of an organization are supported by an architecture that supports the organization in achieving its strategic goal. A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data.
Position’s Contributions to Work Group:
The PSLD Transformation Analytics group engages with various stakeholders across the organization to help solve their business problems. The individual will run the entire project end to end, so strong skills in gathering/understanding customer requirements, creating and maintaining optimal data pipeline architecture, choosing appropriate tools/techniques and delivering actionable insights are must. They will also learn all relevant BU specific data and often combine them with relevant enterprise data domains to bring insights that are not possible if BU data alone is analyzed.
Specific responsibilities are –
Extract large, complex data sets that meet business requirements. Work to build the on-prem /cloud infrastructure for optimal extraction, transformation, and loading of a wide variety of complex business data from on-prem/cloud databases.
Identify ways to improve data reliability, efficiency, and quality.
Work with internal and external stakeholders to assist with data-related technical issues and support data needs.
Own the design and development of ongoing business metrics/KPI, reports and dashboards to drive key business decisions.
Prepare data for predictive and prescriptive modeling.
Typical Day:
Work is typically directed by a direct supervisor, project or team lead. Decisions on routine, medium risk issues that may affect the project team, suppliers or internal customers may be made by this position. Challenges include meeting expectations in delivering results, learning to refine solutions to better fit complex situations, making timely decisions, and communicating effectively with all project stakeholders.
Education/Experience:
Associate's degree in computer programming or a relevant field required. Bachelor's degree preferred.
Critical Technical Skills:
Familiarity with database such as Snowflake, DB2, SQL Server, Oracle (2-3 of these are required)
Programming languages - SQL(required), Python(required) and SAS(preferred)
Experience working with large data sets, preferably in several GB or millions of transactions.
Visualization - PowerBI(required), Tableau(preferred)
Experience working with platform integration tool like Snaplogic is preferred
Experience working with AWS (required)
Soft Skills Required:
Communication, Team-work, Problem Solving, Customer Focus
Job Type: Contract
Pay: $40.00 - $45.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
Snowflake: 3 years (Required)
Power BI: 4 years (Required)
Python: 4 years (Required)
Work Location: In person",5.0,"DSMH LLC
5.0","Morton, IL",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
710,Data Engineer,Employer Provided Salary:$80K - $85K,"Simatree is searching for a Data Engineer Consultant to support our client’s operational requirements on a mission focused program within a challenging, dynamic setting. Successful candidates in this position will collaborate with a diverse group of individuals across the homeland security community and directly contribute to management and analysis of operational data using data science tools. This role supports a US Government contract. An US Citizenship is required.
The Role
The Data Engineer will work alongside a team to generate management tools that effectively control programs and generate reports while supporting the programmatic development of operational applications of the tools or analysis that are focused on operations research and document findings. They will conduct research into management, organizational, and business practices to implement improvements to existing processes. They will utilize commercial or open-source software to manage tasks and present findings to a broad audience, including agency executives. They will maintain a database to produce monthly financial reports, tracks processes, distribute deliverables, and monitor comment status. The Data Engineer will be required to use Microsoft Office to perform queries, analyses, and produce reports. Other responsibilities include:
Use R and Python to perform analyses and develop new tools using popular data visualization libraries such as ggplot2, plotly, matplotlib, or bokeh.
Use Databricks to Extract, Transform, and Load (ETL) data from different sources and transform it into a usable, reliable resource.
Manage data collection and transformation from federal law enforcement and other databases to be used in a wide spectrum of analyses including statistical modeling, performance metrics analysis, crosschecking data with other federal agencies, and risk analysis.
Use ArcGIS to perform analyses and develop new web map applications that summarize results from deep learning/AI algorithms.
Maintain and enhance business intelligence (BI) dashboards using best practices in UI/UX design
Develop and test new methodologies and algorithms which would increase the efficiency and accuracy of existing models, including pilot deployments of automated scripts
Requirements
This role supports a US Government contract. A US citizenship is required.
Strong Experience with SQL programming
Experience with R statistical programming and Python programming
Experience with BI software such as Qlik, Tableau or PowerBI
Demonstrated experience in performing data analysis and modeling on an independent basis
Demonstrated experience in developing and applying quantitative methods to find patterns and relationships in large data sets using statistical and graphical packages
Ability to translate complex, technical, or analytic findings into an easily understood narrative—tell a story with graphical, verbal or written form
3 years experience in supporting quantitative and analytical efforts
Bachelors degree in relevant fields such as statistics, mathematics, computer science, physical science, economics or engineering
Job Type: Full-time
Pay: $80,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Experience level:
1 year
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This question MUST be answered for consideration: This position supports a US Government contract which requires a US Citizenship. Are you a US Citizen?
Work Location: Hybrid remote in Washington, DC 20001",5.0,"Simatree
5.0","Washington, DC",1 to 50 Employees,2018,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
711,Data Warehouse & Artificial Intelligence Engineer,$83K - $120K (Glassdoor est.),"Job Description:

Design, Implement and Maintain data warehouse.

Duties and Responsibilities:

Develop, test, and deploy extraction, transformation and loading (ETL) routines to load the data warehouse using SSIS, stored procedures, etc.
Develop analytics including reports, dashboards, and data visualizations
Troubleshoot BI-related problems and tune for performance
Implement and oversee a company-wide data governance program to support master data management and resolve data definition discrepancies among stakeholders
Plan the technical approach for implementing analytics strategy across multiple channels
Plan and implement activities to maintain data confidence
Provide technical direction and technical support for all member of the team
Architect and implement analytics solutions to meet data collection and reporting needs.
Work with team to recommend analytic software
Provide general troubleshooting assistance for analytic reports and break fixes
Wrangle data (a.k.a. data engineering) from a wide variety of complex data sets large volumes of data into normalized and enriched data sets.
Assemble and evaluate data such that new insights, solutions, and visualizations can be derived.
Perform data discoveries to understand data formats, source systems, file sizes, etc. and engage with internal and external business partners in this discovery process.
Deliver data acquisition, transformations, normalization, mapping, and loading of data into a variety of data models.
Collaborate extensively with Technology to design data ingestion, data models, and automated operational metrics for consistently high quality data
Supports the research, analysis and presentation of information by: producing reports, compiling and summarizing information, producing supporting documentation and exhibits, and verifying information received from external sources.
Assists other associates with preparation of reports and use of information systems, software and related sources of information. Trains other users on report preparation and database access.
Perform other duties as assigned.

Required Knowledge, skills and abilities:

Ability to manage change successfully in a fast-paced environment
A passion for learning and the ability to do so on the fly
Must be strategic, dynamic, and detail-oriented. The ideal candidate is an analytical thinker, a self-starter and a quick learner.
Ability to design and architect method of extracting data from its original source to another location (i.e. a data lake) to make that data consumable for a variety of downstream purposes such as applications, visualizations, analytics and other related data science activities.
Basic understanding of data science
Ability to transforming and map one ""raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.
Exceptional ability to clearly and effectively communicate with IT peers and users at all levels of technical and business acumen.
Strong time management skills and multi-tasking capabilities
Ability to understand business rules governing financial and budgeting process
Exceptional oral and written communication skills
Self-motivated, ability to set priorities and meet deadlines

Education and Experience:

Bachelor's degree in Computer Science, Business, Economics, Information Science, or Analytics
Minimum of 3 years of experience with data analytics across multiple channels
Experienced with the technologies required to implement Analytics across multiple channels, (for example, Data Layer, Custom JavaScript, JAVA, AJAX, HTML, Objective-C, Swift)
Knowledge of Tableau is a plus
Familiarity with Microsoft Power BI
Extensive experience with ETL and/or other Big Data processes.
Contract manufacturing experience is a plus.

Foxconn Assembly, LLC is an Equal Opportunity Employer (EOE). All qualified candidates will receive consideration without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or marital status in accordance with applicable federal, state and local laws.
Foxconn Assembly, LLC participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

NbXn0DyWY6",3.4,"Foxconn Corporation
3.4","Houston, TX",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$5 to $25 million (USD)
712,Data Engineer,-1,"ArchWell Health is a new, innovative healthcare provider devoted to improving the lives of our senior members. We deliver best-in-class care at comfortable, accessible neighborhood clinics where seniors can feel at home and become part of a vibrant, wellness-focused community. Our members experience greater continuity of care, as well as the comfort of knowing they will be treated with respect by people who genuinely care about them, their families, and their communities.

Duties/Responsibilities:
Build data integrations from internal and external sources to centralize data into a Data Warehouse environment.
Monitor data integration operations, data quality, troubleshoot, and resolve problems.
Profile data sources and map to target table formats.
Develop and monitor data quality processes and address problems.
Develop, unit test and system test integration components.
Create support documentation describing the functionality of the integrations.
Participating in technical design & requirements gathering meetings.
Participate in planning and implementing data integration and data migration activities.
Perform QA tests to ensure data integrity and quality.
Research data issues between source systems and the data warehouse.
Required Skills/Experience:
Bachelor’s degree required; Master's degree (in data science, computer science or MIS, mathematics, engineering, or related field) preferred.
5+ years of prior experience in Data Management / ETL / ELT / Data Warehousing
Experience in writing Data Quality routines for cleansing of data and capturing confidence score
Experience with master data management
Strong knowledge of Structured Query Language (SQL) and Transact-SQL (T-SQL)
Experience using scripting languages such as JavaScript or Python
Experience Healthcare data models, datasets, and source systems (e.g. EHR, claims, labs, etc.)
Experience with healthcare reference data (ICD, CPT etc.)
Experience with agile delivery methodologies
Data Modeling experience preferred.
Strong organizational, administrative, and analytical skills required.
Experience managing and working in cloud environments such as Amazon Web Services or Azure
Knowledge of HIPAA; ability to implement systems and processes in accordance with regulations
Excellent interpersonal communication skills, both written and verbal
ArchWell Health is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to their race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected classification.",4.0,"ArchWell Health
4.0",Remote,Unknown,-1,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
713,Data Engineer,$60K - $85K (Glassdoor est.),"NVR is looking for a talented Data Enginneer to work onsite in Frederick, MD. Join our industry leading technology organization and be a part of making someone’s dream home a reality!
As a Data Engineer, you will be responsible for the day-to-day operations of data-dependent systems to ensure data is properly processed and securely transferred to its appropriate location, in a timely manner. If you are a recent graduate or an engineer early in your career, this is a great opportunity for you to leverage your computer knowledge and analytical skills by getting involved in projects and gaining exposure to cloud technologies, data and analytics!
Key Job Responsibilities:
Keeping the data flowing by working with SQL, SQL Server, SSRS and SSIS
Managing, manipulating, storing, and parsing data on premises and in the cloud.
Collaborate with the team to solve support issues.
Interact with business and IT teams of various applications to understand their data, database, and flow within the systems.
Create logical mapping of source data into target data models, based on business process and data/reporting requirements.
Job Qualifications:
0-3 years of experience in data management (integration, modeling, optimization, and quality).
Experience or coursework with databases and queries
Excellent written and verbal communication skills, interpersonal and collaborative skills.
Must have strong problem-solving and analytical skills.
High degree of initiative and be well organized.
Ability to manage multiple projects with strict timelines.
Here’s what will put you ahead of the pack:
Understanding of data modeling, structured and unstructured data, and data transformation techniques.
Experience or coursework using visualization tools like Tableau or Power BI.
Microsoft Azure Certifications.
About Us & Life at NVR
NVR has been helping families build their happily ever after since 1948. As a Top 5 US homebuilder, we’re committed to quality and to our customers and we take pride in the over 500,000 new homes we have sold and built across the country. Working in the homebuilding industry is tangible and rewarding, but not every job at NVR requires a hard hat. We don’t just sell and build new homes; we also manage teams, acquire land, manufacture materials, provide mortgages to our customers, and provide corporate support to NVR’s multi-billion dollar business operations.
At NVR, we value our teams and provide opportunities to learn new technologies and skills to grow your career. Your desire to excel is matched by our commitment to your success and we’ll give you the tools and industry knowledge you need. Our management team is tenured and talented, nearly 80% of them promoted from within, so you’ll find mentors who can share their knowledge, provide career guidance and encourage your success.
NVR also offers benefits among the best in the industry that reflect the strong commitment we have to all of our employees.
Competitive Compensation
Home Purchase Discount
Mortgage and Settlement Services Discounts
Comprehensive Health, Life and Disability Insurance
401(k) (Full-time employees are eligible to contribute immediately)
Employee Stock Ownership Program
Vacation and Holidays
In addition to the traditional benefits, we offer all our employees stock ownership through a profit sharing trust as part of our retirement savings package. NVR has had the highest Earnings Per Share growth rate in the homebuilding industry for the past 10 years, so as we grow financially, so do you.
We are an Equal Opportunity Employer.
Drug Testing and Credit Check are required.
Applicants must be legally entitled to work in the United States, as NVR does not provide visa sponsorships.",3.7,"NVR, Inc
3.7","Frederick, MD",5001 to 10000 Employees,1948,Company - Public,Construction,"Construction, Repair & Maintenance Services",$10+ billion (USD)
714,Data Engineer,-1,"Hummingbird is a remote-first, fully distributed team united by the shared mission of helping fight financial crime. Since our launch in 2017, we’ve helped major financial institutions and tech-savvy trailblazers alike (e.g. Stripe, Affirm, etc.) orchestrate their compliance programs through our thoughtfully designed, intuitive SaaS product. We believe finding and stopping financial crime is a problem rooted in code, language and design, so we built the product that the heroes doing this work deserve.
We are customer-obsessed, and we love building and shipping great products. We set a high bar, challenge our assumptions, seek diverse opinions, and support each other to do our best work.
We do our best to write inclusive, descriptive and accurate job descriptions, but we’re not always perfect. If you’re interested in the role, we’d love to hear from you even if you don’t feel like you meet everything we’re looking for. We’re always iterating and improving, and it’s possible that your experience is even more impactful than we could have imagined.

About the Role
We are looking for a driven data engineer to join our team and champion the use of data at Hummingbird. Data plays a crucial role in our mission to fight financial crime and you will help us find new and innovative ways to leverage it to provide powerful tools for our users, and allow us to better understand the usage of our product. Hummingbird is uniquely positioned at the intersection of financial technology, security, policy, and law enforcement and as such we have built up a one of a kind data set that we can now use to give our customers the edge in their efforts to stay ahead of criminals. As an employee at a small startup, you will have the opportunity to wear many hats, working from product discovery through implementation.
What you’re looking to do:
Level up our use of data to make better decisions, build more powerful features, and fight financial crime
Leverage Infrastructure as Code (IaC) to manage and deploy infrastructure that supports a variety of different projects, such as data replication and orchestration for machine learning workflows
Build new data pipelines for ingesting data into our data warehouse via both batch and streaming architectures
Work closely with data science to enable us to build products that benefit our customers while keeping compliance and security at the forefront
Achieve goals through a combination of independent building, educating your peers, and influencing others to contribute towards your vision
What we’re looking for:
A data engineer with a history of taking projects from the earliest stages through successful rollout to production
Someone who is excited by the prospect of pioneering data as a practice at a fast growing startup and who is unafraid to dig in to discover what is possible
A flexible self starter that will cut across organizational lines to understand the business and identify the most valuable work
An engineer who brings a pragmatic approach to problem solving, favoring simplicity and shortening delivery cycles
Experience building data pipelines for sensitive data, including best practices for de-identification and data security
Experience deploying infrastructure via terraform or a similar infrastructure as code tool
Expertise in SQL and one or more programming languages, especially python
What’s in it for you:
The chance to help build from the ground up. The hires we’re making now are foundational to our growth as a company, so you will have an opportunity to help shape the future of Hummingbird.
Competitive compensation including cash and equity.
Remote-first, fully distributed company with flexible working hours.
Awesome health, vision & dental benefits, and 401k.
Safe, respectful & comfortable work environment with colleagues and leadership who prioritize diversity, equity, inclusion, and belonging.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please don't hesitate to contact us to request accommodation.",-1.0,Hummingbird,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
715,Data Engineer - Mid Level,-1,"Why USAA?
At USAA, we have an important mission: facilitating the financial security of millions of U.S. military members and their families. Not all of our employees served in our nation’s military, but we all share in the mission to give back to those who did. We’re working as one to build a great experience and make a real impact for our members.

We believe in our core values of honesty, integrity, loyalty and service. They’re what guides everything we do – from how we treat our members to how we treat each other. Come be a part of what makes us so special!
The Opportunity
We are seeking a dedicated Data Engineer – Mid Level for our Plano, TX Office. The candidate selected for this position is going to work with the Enterprise Data Engineering Services Certified Metrics team. They will work with data engineering technologies and support the team responsible for developing and maintaining USAA’s digital reporting and analytical environment in Snowflake. The resource should be experienced in building analytical applications in the cloud. SQL skills are a must and knowledge of Snowflake, dbt, and other cloud-based data movement tools would be highly beneficial.
This position is a hybrid work type and will be based in Plano, TX. Hybrid roles help employees gain the best of both worlds – collaborating in-person in the office and working from home when needed to achieve focused results.
What you'll do:
Independently conducts work on the full life cycle of data engineering to include analysis, solution design, data pipeline engineering, testing, deployment, scheduling, and production support.
Designs and implements complex technical solutions for data engineering and analytic systems.
Identifies and solves significant technical problems and architecture deficiencies to include design, security, and performance.
Collaborates on design reviews by providing feedback on trends and makes recommendations for solutions.
Breaks down business features and into technical stories and approaches. Influences stakeholders in technical adoption for best solutions.
Creates proof of concepts and prototypes that drive the vision and the outcome for complex initiatives to be delivered.
Collaborates with the team and other engineers on new technologies and alternatives to plan and execute complex assignments and tasks.
Helps on-board entry level engineers. May begin mentoring junior engineers.
Acquires data from multiple data sources and maintains resulting databases, data warehouses, and/or data lakes.
Assists to develop, maintain, and enforce the company’s data development tools and standards. Conducts code reviews on a regular basis to improve quality and ensure compliance.
Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled in accordance with risk and compliance policies and procedures.
What you have:
Bachelor’s degree; OR 4 years of related experience (in addition to the minimum years of experience required) may be substituted in lieu of degree.
4 years of data engineering, data analysis or software development experience implementing data solutions with at least 1 year of data engineering or data management experience.
Extensive knowledge and working experience in SQL and Relational Databases.
Strong analytical and problem-solving skills.
Basic understanding of cloud technologies and tools.
What sets you apart:
Experience with Snowflake
Experience with AWS
Visualization experience with Tableau or similar tool
Experience with DBT
Experience with Python
Experience providing technical leadership, guidance, or mentorship to other engineers
The above description reflects the details considered necessary to describe the principal functions of the job and should not be construed as a detailed description of all the work requirements that may be performed in the job.
What we offer:
Compensation: USAA has an effective process for assessing market data and establishing ranges to ensure we remain competitive. You are paid within the salary range based on your experience and market data of the position. The actual salary for this role may vary by location. The salary range for this position is: $86,520.00 – $165,340.00.
Employees may be eligible for pay incentives based on overall corporate and individual performance and at the discretion of the USAA Board of Directors.
Benefits: At USAA our employees enjoy best-in-class benefits to support their physical, financial, and emotional wellness. These benefits include comprehensive medical, dental and vision plans, 401(k), pension, life insurance, parental benefits, adoption assistance, paid time off program with paid holidays plus 16 paid volunteer hours, and various wellness programs. Additionally, our career path planning and continuing education assists employees with their professional goals.
For more details on our outstanding benefits, please visit our benefits page on USAAjobs.com.
Relocation assistance is not available for this position.
USAA is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",3.4,"USAA
3.4","Plano, TX",10000+ Employees,1922,Company - Private,Insurance Carriers,Insurance,$25 to $100 million (USD)
716,Data Engineer,-1,"Highlights
Experience
0 - 1 Year
Joining Date
Immediate or max 30 days’ Notice Period
Qualification
Bachelor's Degree or Post Graduation in Computer Science, IT, or a related field
Job Description
Beinex is looking for excellent Data Engineers who can make life easier for the league of extraordinary Data Analysts and Data Scientists of Beinex’s esteemed client line up. Out there, you get to be a part of an awesome work culture and robust work ethics. You will find yourself immersed in the latest trends and technologies expanding the frontiers of Data Engineering as we know it currently. And no one can beat the pay/ perks package either! Get to take the right decision, for you and for us! Read on and join us!
Responsibilities
Perform analytical and technical tasks to complete special and ongoing projects requiring extensive research, data collection, and detailed analysis, following departmental guidelines, policies, and procedures.
Need to provide complex analytical, technical, and administrative support to facilitate the day to-day operations.
Respond to complex inquiries from administrators
Exercise independent judgment to troubleshoot and resolve issues
Key Skills Requirements
0-1 year of experience in a similar role
Strong desire to learn cloud technologies, Python, and Spark to build resilient data pipelines
SQL experience would be a plus
Experience in cloud platforms is an added advantage
Solid analytical and problem-solving skills involving sound decision making and effective resolutions
Keen attention to detail
Strong planning and organizational skills involving the ability to manage multiple work streams effectively
Ability to work within a team to meet established project goals
Ability to communicate professionally, concisely and effectively, both verbally and in writing, to internal and external stakeholders
Self-starter with the ability to work independently while supporting a team environment
Data analysis and mapping skills with strong attention to detail and concern for data accuracy
Intermediate knowledge of SQL commonly used concepts, practices and procedures related to relational databases
Bachelor's Degree or Post Graduation in Computer Science, IT, or a related field

Perks: Careers at Beinex
Comprehensive Health Plans
Learning and development
Workation and outdoor training
Hybrid working environment
On-site travel Opportunity
Beinex Branded Merchandise",3.6,"Beinex
3.6",Remote,201 to 500 Employees,2017,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
717,Data Engineer,$92K - $128K (Glassdoor est.),"Become a Part of the NIKE, Inc. Team

\r

\r

NIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it's about each person bringing skills and passion to a challenging and constantly evolving game.

Data Engineer-NIKE USA, Inc., Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.

Employer will accept a Bachelor's degree in Data Science, Computer Engineering, Computer Science, Computer Information Systems and 5 years of progressive post-baccalaureate experience in the job offered or in an engineering-related position.

#LI-DNI

NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.

NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.



At NIKE, Inc. we promise to provide a premium, inclusive, compelling and authentic candidate experience. Delivering on this promise means we allow you to be at your best - and to do that, you need to understand how the hiring process works. Transparency is key.




Whether it's transportation or financial health, we continually invest in our employees to help them achieve greatness - inside and outside of work. All who work here should be able to realize their full potential.",4.2,"Nike
4.2","Beaverton, OR",10000+ Employees,1972,Company - Public,Consumer Product Manufacturing,Manufacturing,$10+ billion (USD)
718,2024 Technology Early Career Development Program (Software or Data Engineer),-1,"About this role:

Wells Fargo is seeking talent to join the 2024 Technology Analyst Program. Learn more about the career areas and lines of business at wellsfargojobs.com

Program Overview:

The Wells Fargo Technology Analyst Program is a strategically designed, 2 year program that provides analysts with the necessary knowledge, skills, experiences, and exposure to build a career in Wells Fargo Technology. Analysts are full time, permanent employees and will continue to be employees after completing the program.

Program components include a mentoring relationship, division and function specific training, employee engagement activities, professional development opportunities, opportunities to meet and network with executives, community service projects, and overall program projects. You will receive support from a dedicated program manager to help navigate through the program and the transition to corporate life. This program provides opportunities to be a part of a team that develops business solutions through innovation.

In this role, you will:

Be placed in one of the following roles where your responsibilities could include, but are not limited to:
Software Engineer - Responsible for application design, development, testing, readiness, and integration which can include developing enhancements or resolving bugs in existing applications, developing new applications to support the sunset of older applications and working with ancillary technologies to support application integration and data integrity. Accountable for the technical evolution of our services, working in close partnership with enterprise architecture, product management and partner teams. Typically, will work within an Agile (Scrum / Kanban) setting.
Typical Tech Stacks: Java, .Net Core/ASP.Net, UI Frameworks (ReactJS, Node.js, Angular JS, JQuery, Java Script), Spring Boot, GitHub, TDD/BDD, API, Microservices, Jenkins, Docker, Cloud Technologies (PCF, Azure, Google)
Data Engineer - Create and support reusable frameworks, data warehouses and analytics environments for data engineering and leverage common design patterns across traditional and bigdata solutions. Contribute to the data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. Build and maintain optimized and highly available data pipelines that facilitate deeper analysis and reporting. Resolve moderately complex issues to meet data engineering deliverables while leveraging data information policies, procedures, and compliance requirements.
Typical Tech Stacks: Python, MongoDB, MySQL, SQL, AI/ML, Tableau/PowerBI
**Roles for our Information and Cyber Security teams are posted separately.

Program Start Dates: February 2024 or July 2024

Ideal candidate for this role must have the following:
Energetic self-starter who proactively takes initiative, remains curious and has a genuine interest in learning and growth
Ability to learn from and collaborate with a diverse set of employees in a fast paced, change driven and dynamic environment
Required Qualifications, US:
6+ months of work experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education.
Desired Qualifications:
Must be pursuing a bachelor's degree in Computer Science, Computer Engineering or other Technology STEM related majors with an expected graduation date between December 2023-June 2024
Demonstrated knowledge, interest and/or experience with: Java, Cloud technology, .NET, Python, SQL or related technical skills
Involvement in extracurricular enrichment activities through one or more of the following: volunteerism, student organization involvement, study abroad program(s), leadership position(s), non-profit involvement
Program Locations* :
Charlotte, North Carolina
Minneapolis Market Area
New York Market Area
Phoenix Market Area
San Francisco Bay Market Area
St. Louis, Missouri
Locations subject to change and contingent on business needs

Pay Range:
AZ: $48.08 Hourly
CA: $60.10 Hourly
MN: $48.08 Hourly
MO: $48.08 Hourly
NC: $48.08 Hourly
NJ: $60.10 Hourly
NY: $60.10 Hourly
Wells Fargo will only consider candidates who are presently authorized to work for any employer in the United States and who will not require work visa sponsorship from Wells Fargo now or in the future in order to retain their authorization to work in the United States.

Based on the volume of applications received, this job posting may be removed prior to the indicated close date. If you do not apply prior to the closing of this posting, we encourage you to apply for other opportunities with Wells Fargo. After submitting your application, please monitor your e-mail for future communications.

We Value Diversity

At Wells Fargo, we believe in diversity, equity and inclusion in the workplace; accordingly, we welcome applications for employment from all qualified candidates, regardless of race, color, gender, national origin, religion, age, sexual orientation, gender identity, gender expression, genetic information, individuals with disabilities, pregnancy, marital status, status as a protected veteran or any other status protected by applicable law.

Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit's risk appetite and all risk and compliance program requirements.

Candidates applying to job openings posted in US: All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.

Drug and Alcohol Policy

Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.",3.7,"Wells Fargo
3.7","San Francisco, CA",10000+ Employees,1852,Company - Public,Banking & Lending,Financial Services,$10+ billion (USD)
719,Data Engineer,-1,"Remote
Contract
Opened 4 months ago
Job Description
Data Engineer (with Healthcare experience) Required Skills: SQL Databricks data engineering Snowflake data engineering QA experience for ETL experienced with data acquisition and ingestion using API and/or batch channels Experience with Healthcare",3.3,"Crackajack Solutions
3.3",Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
720,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
721,Data Engineer,$76K - $101K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
The Data Governance/Data Quality Subject Matter Expert (SME) is a professional responsible for designing, implementing, and managing the client’s data governance strategy and ensuring high data quality standards across data-centric operations and decision-making.
The SME is in charge of developing and maintaining robust data pipelines, ensuring data observability, managing data within the Microsoft Azure environment, and implementing data governance principles in alignment with industry best practices and regulatory standards.
Primary Responsibilities and Accountabilities:
Data Governance & Quality: This includes developing and implementing data governance policies and procedures, monitoring data quality, managing data within the Microsoft Azure environment, and ensuring compliance with data privacy regulations.
Data Pipeline & Workflow Development: Designing and implementing efficient and secure data pipelines in Azure Data Factory and workflows, ensuring optimal performance and incorporating data quality principals.
Data Integration & Observability: Designing data integration strategies, integrating data from multiple sources, ensuring data quality and observability through tools like Azure Synapse.
Data Security: Defining and implementing data security policies and procedures, including access control and data encryption within the Azure environment.
Performance Tuning: Optimizing data access, indexing, and query performance within Azure Data Factory to ensure scalable and reliable data infrastructure.
Monitoring & Troubleshooting: Monitoring data systems, troubleshooting issues, identifying performance bottlenecks, and collaborating with team members to resolve problems.
Collaboration & Communication: Working closely with stakeholders to understand their data needs and develop solutions adhering to industry best practices and the needs of the business.
Automation & Tooling: Automating repetitive tasks and maintaining tools necessary for data processing and storage within the Azure environment.
Qualifications
A Bachelor's degree in a related field such as computer science, mathematics, engineering, or information systems.
3+ years of hands-on experience programming in SQL.
2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes within the Microsoft Azure environment.
Experience in Data Governance OR Data Quality Management is a must.
Skills
Competencies:
Expertise in Microsoft Azure data tools, with a focus on Azure Data Factory and Azure Synapse for data processing, storage, and analysis.
Strong SQL skills and knowledge of data warehousing concepts and ETL (Extract, Transform, Load) processes within the Azure environment.
Proficiency in programming languages such as Python, and experience with big data technologies such as Hadoop, Spark, and Kafka.
Ability to design, build, and maintain data pipelines, with a particular focus on Azure Data Factory.
Strong analytical and problem-solving skills, with the ability to identify and resolve data-related issues effectively.
Experience with data security and privacy regulations, and understanding of HIPAA compliance.
Strong experience with data governance, data quality, and data observability principles.
Excellent written and verbal communication skills, ability to explain complex technical concepts to non-technical stakeholders.
Excellent collaboration and teamwork skills, with the ability to work in a team involving client data engineers, data scientists, and data analysts.
Strong attention to detail and commitment to ensuring high data quality.
Continuous learning mindset and ability to stay up-to-date with new developments and technologies in data governance and data quality
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
722,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
723,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
724,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1.0,Percept Health,Remote,-1,-1,-1,-1,-1,-1
725,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
726,Cloud Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"We are seeking a highly experienced Principal Data Engineer with a deep understanding of PySpark using Databricks or AWS Glue or AWS EMR and cloud-based databases such as Snowflake. Proficiency in workflow management tools like Airflow is essential. Healthcare industry experience is a significant advantage. The ideal candidate will be responsible for designing, implementing, and maintaining data pipelines while ensuring the highest performance, security, and data quality.
Responsibilities:
Design, develop, and maintain scalable, reliable, and secure data pipelines to process large volumes of structured and unstructured healthcare data using PySpark and cloud-based databases.
Collaborate with data architects, data scientists, and analysts to understand data requirements and implement solutions that meet business and technical objectives.
Leverage AWS or Azure cloud services for data storage, processing, and analytics, optimizing cost and performance.
Utilize tools like Airflow for workflow management and Kubernetes for container orchestration to ensure seamless deployment, scaling, and management of data processing applications.
Develop and implement data ingestion, transformation, and validation processes to ensure data quality, consistency, and reliability across various healthcare datasets.
Monitor and troubleshoot data pipelines, proactively identifying and resolving issues to minimize downtime and ensure optimal performance.
Establish and enforce data engineering best practices, ensuring compliance with data privacy and security regulations specific to the healthcare industry.
Continuously evaluate and adopt new tools, technologies, and frameworks to improve the data infrastructure and drive innovation.
Mentor and guide junior data engineers, fostering a culture of collaboration, learning, and growth within the team.
Collaborate with cross-functional teams to align data engineering efforts with broader organizational goals and strategies.
Is familiar with SOC 2 compliance and its impact on company policies and processes.
Understands the importance of adhering to SOC 2 requirements and maintains an effort to do so.
Reviews and understands the Employee Handbook, and internal policies that define individual security responsibilities, and maintains segregation of duties in accordance with their role requirements.
Requirements:
Bachelor's or Master’s degree in Computer Science, Engineering, or a related field.
8+ years of experience in data engineering, with a strong background in Apache Spark and cloud-based databases such as Snowflake.
Strong Knowledge in Big Data Technologies, Databricks, AWS Services, and PySpark, thorough in one or more programming languages like Python.
Proven experience with AWS or Azure cloud services for data storage, processing, and analytics.
Expertise in workflow management tools like Airflow and container orchestration systems such as Kubernetes.
Strong knowledge of SQL and NoSQL databases, as well as data modeling and schema design principles.
Familiarity with healthcare data standards, terminologies, and regulations, such as HIPAA and GDPR, is highly desirable.
Excellent problem-solving, communication, and collaboration skills, with the ability to work effectively in cross-functional teams.
Demonstrated ability to manage multiple projects, prioritize tasks, and meet deadlines in a fast-paced environment.
A strong desire to learn, adapt, and contribute to a rapidly evolving data landscape.
Benefits:
401(k).
Dental Insurance.
Health insurance.
Vision insurance.
We are an equal-opportunity employer and value diversity, equality, inclusion, and respect for people.
The salary will be determined based on several factors including, but not limited to, location, relevant education, qualifications, experience, technical skills, and business needs.
Additional Responsibilities:
Participate in OrangePeople monthly team meetings, and participate in team-building efforts.
Contribute to OrangePeople technical discussions, peer reviews, etc.
Contribute content and collaborate via the OP-Wiki/Knowledge Base.
Provide status reports to OP Account Management as requested.
About us:
OrangePeople is an Enterprise Architecture and Project Management solutions company. Our most valuable asset is our people: dynamic, creative thinkers, who are passionate about doing quality work. As a member of the OrangePeople team, you will have access to industry-leading consulting practices, strategies & technologies, innovative training & education. An ideal Orange Person is a technology leader with a proven track record of technical achievements and a strong process/methodology orientation.
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Application Question(s):
Do you require sponsorship for this job?
Work Location: Remote",4.1,"OrangePeople
4.1",Remote,51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
727,Data Engineer,$100K - $144K (Glassdoor est.),"Position: Data Engineer
Duration: Contract
Location: Fremont, California
Skills:
Strong in SQL and Python
Data warehousing concepts , Data Modeling and RDBMS concepts
Good to have exposure to big data technologies – Apache Spark, Kafka, Airflow, NoSQL
Good to have exposure on Insurance & Finance business processes
Regards
Tejash Gupta
Recruiter
tejash@swifttechinc.com
Job Type: Contract
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Python: 7 years (Required)
SQL: 7 years (Required)
Data modeling: 6 years (Required)
RDBMS: 6 years (Required)
Work Location: In person",4.1,"Swift Technologies Inc
4.1","Fremont, CA",1 to 50 Employees,-1,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
728,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
729,AWS Data Engineer II,$77K - $105K (Glassdoor est.),"Overview:
Shift4 (NYSE: FOUR) is boldly redefining commerce by simplifying complex payments ecosystems across the world. As the leader in commerce-enabling technology, Shift4 powers billions of transactions annually for hundreds of thousands of businesses in virtually every industry. For more information, visit www.shift4.com.

Shift4 Payments is seeking a Business Intelligence Engineer with an AWS Focus to join our growing internal Information Technology team. As a BI Engineer, you will help design and maintain the flow of information and analytics to the entire organization utilizing AWS capabilities to store, move and report on data. The position requires at least 3 years of prior experience as a Business Intelligence AWS engineer. We are looking for individuals that are extremely self-sufficient, available to work flexible hours and hold themselves to the highest standards of professionalism.Maintain and expand BI AWS architecture and content.
Responsibilities:
Support of BI AWS architecture and content up to and including off hours support when necessary.
Work on necessary bugs, enhancements and new system developments utilizing our Software Development Life Cycle (SDLC) standards.
Complete root cause analysis and provide recommendations to leadership to address opportunities.
Contribute to and enforce the use of the organization's Business Intelligence solution.
Develop software solutions that will provide solid foundations for the growth of the company.
Support key metrics and shared goals that determine success for a particular area.
Qualifications:
3-5+ years of professional experience in business intelligence AWS development or a related field.
Advanced experience working with AWS: S3, Glue, Lambda, Step Functions, Redshift, Athena.
Advanced experience working with various data sources such as AWS S3 and Redshift, Postgres, SQL Server, Amazon Athena, Excel, Flat Files, etc.
Advanced experience working with large data sets from sources such as AWS S3 and Redshift, Postgres, SQL Server, etc.
Strong PySpark, Python, CLI proficiency
Strong SQL proficiency (T-SQL, MySQL, and PostgreSQL).
Experience working with an Enterprise Data Warehouse and Dimensional Databases is a plus.
Advanced experience of the SDLC and how Development processes fit into KANBAN and Scrum Frameworks.
Experience with Jira and Confluence is a plus.
Ability to prioritize multiple tasks and easily adjust to changing priorities.
Ability to identify problems, initiate solutions, and effectively collaborate and communicate with team members.
Have excellent verbal and written communication skills.

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.",3.5,"Shift4 Payments
3.5","Allentown, PA",1001 to 5000 Employees,1999,Company - Public,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
730,AWS Data Engineer,-1,"5+ years of data engineer experience in developing, implementing, delivering, and managing end-to-end data solutions using AWS Glue
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications
Proficiency in cloud data technologies, such as AWS S3, AWS Glue, EC2
Knowledge of Snowflake/AWS Redshift as a data warehousing solution
Advanced knowledge in designing, developing, implementing and managing data pipelines to deliver data or data insights for application, reporting, or analytics
Strong experience creating and maintaining functional and technical specifications documents
Strong experience creating test plans, test data sets, and automated testing ot ensure all components of the system meet specifications
Strong SQL technical experience such as linking IT applications to databases and creating and handling metadata
Strong programming skill in Python or Scala
Bonus skills:
Strong experience in NoSQL database (i.e., MongoDB)
Strong experience in streaming technology (i.e., Kafka, data bricks streaming)
Strong experience in working in the healthcare industry including PHI, HIPAA regulations, and BAA processes
?
AWS DATA ENGINEER RESPONSIBILITIES:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics
Support the creation of the new cloud infrastructure and data ecosystem in the cloud",5.0,"CEDENT
5.0",United States,1 to 50 Employees,-1,Contract,Computer Hardware Development,Information Technology,Less than $1 million (USD)
731,Data Engineer,$85K - $117K (Glassdoor est.),"At WHOOP, we're on a mission to unlock human performance. WHOOP empowers users to perform at a higher level through a deeper understanding of their bodies and daily lives.

WHOOP is seeking a talented and motivated Data Engineer I to join our impactful and growing team. As a Data Engineer I at WHOOP, you will play a crucial role in building and maintaining the infrastructure and data pipelines that support our data-driven applications and analytics platform. You’ll grow as an engineer as you learn new technologies and write high quality, testable, scalable code. Your work will ensure the availability of critical data, enhance the performance and scalability of our systems, and drive data-related initiatives which empower our users with insights that optimize their health and performance.
RESPONSIBILITIES:
Implement, optimize, and maintain ETL (Extract, Transform, Load) processes to move, transform, and store data from various sources into our data warehouse.
Collaborate with cross-functional teams, including data analysts, data scientists, and software engineers to understand data requirements and design efficient and scalable data pipelines.
Develop and maintain data models that facilitate data consistency, integrity, and efficiency. Design and implement data warehouse schemas, ensuring proper partitioning to support the performance needs of analytical queries.
Monitor the performance of data pipelines and databases, identifying bottlenecks and inefficiencies and implement optimizations to enhance system performance and stability.
Contribute to the development of Whoop’s modern data stack which is built on cloud computing platforms (e.g. AWS, Azure, GCP), relational & non-relational databases (e.g. PostgreSQL, MySQL), data lakes, distributed databases (e.g. Snowflake, Redshift, BigQuery), and more.
Reports to the Data Engineering Technical Lead
QUALIFICATIONS:
Bachelor's degree in Computer Science, Engineering, Data Science, or a related field
1+ year of relevant full time experience or equivalent
Proficiency in Python, Java, or Scala (Python preferred)
Proficiency in SQL
Experience with data processing frameworks such as Apache Spark is a plus
Experience with ETL tools and frameworks (Prefect, Apache Airflow, etc) is a plus
Strong analytical and problem-solving skills
Excellent verbal and written communication skills, with the ability to present technical concepts to both technical and non-technical stakeholders
A collaborative mindset, with the ability to work effectively in a team-oriented environment
This role is based in the WHOOP office located in Boston, MA. The successful candidate must be prepared to relocate if necessary to work out of the Boston, MA office.

Interested in the role, but don’t meet every qualification? We encourage you to still apply! At WHOOP, we believe there is much more to a candidate than what is written on paper, and we value character as much as experience. As we continue to build a diverse and inclusive environment, we encourage anyone who is interested in this role to apply.

WHOOP is an Equal Opportunity Employer and participates in E-verify to determine employment eligibility",3.5,"WHOOP
3.5","Boston, MA",501 to 1000 Employees,2012,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
732,Senior Data Engineer,-1,"JOB DESCRIPTION:
Need to be expertise Coding on PySpark.
Supply Chain and Azure.
Experience in working with end-to-end technology implementation project, preferable those related to Data warehousing/Analytics/Reporting
Strong communication skills with the ability to collaborate effectively with business owners/users and technical teams.
Proven ability to gather product requirements from stakeholders and prioritize feature implementations based on business needs and product strategy.
Strong problem-solving skills and attention to detail.
Good to have experience with CPG/ Retail industry (preferable CPG companies such as Nestle, Unilever, P&G, Coca coal, PepsiCo etc)
Bachelor's/master's degree in business administration, computer science, engineering, or a related field.
Job Type: Contract
Experience:
PySpark: 5 years (Preferred)
SQL: 10 years (Preferred)
Data analytics: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Azure: 5 years (Preferred)
Data Bricks: 5 years (Preferred)
Data Lake: 5 years (Preferred)
Synapse: 5 years (Preferred)
Retail Domain: 8 years (Preferred)
CPG: 7 years (Preferred)
Work Location: Remote",4.0,"Quadrant Resource
4.0",Remote,1001 to 5000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
733,Data Engineer,Employer Provided Salary:$120K - $130K,"Role – Data Engineer- DataBricks -
Location – O Fallon, Missouri (Initially Remote)
Yrs. of experience – 8+
Full-Time
Job Description :
o 5+ years in a customer facing Technical architecture, consulting role with the expertise in The following technology.
o Developing modern Data warehouse solutions using Databricks and AWS/Azure stack.
o Drive technical discussion with client architect and team members.
o Knowledge in Databricks DELTA lake for the Analytical data lake use case
o Hand on experience in create MLOPS data pipeline creation.
o AIML Models develop, train and implement for the AIML use cases.
o Knowledge in Banking domain on card and payment areas.
o Any relation database – Data classification/ Data profiling for MLOPS use cases
o Good experience in offshore onsite coordination.
o Experience in translating a customer’s business needs.
Job Type: Full-time
Salary: $120,000.00 - $130,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
O'Fallon, MO 63366: Reliably commute or planning to relocate before starting work (Required)
Experience:
DataBricks: 1 year (Required)
MLOPS: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: In person",-1.0,Sbase Technologies,"O Fallon, MO",-1,-1,-1,-1,-1,-1
734,ETL Data Pipeline Engineer,Employer Provided Salary:$60.00 Per Hour,"ETL Data Pipeline Engineer
We do not work with 3rd party employers. Visa Sponsorship NOT available.
We are seeking a ETL DATA Pipeline Engineer for a consulting engagement with a major entertainment and media company. This person will be hands-on-date engineering development across multiple projects.
Required Skills:
10+ years of experience as Data Engineer with Large Data Pipelines
Strong SQL skills
Distributed Systems (Spark, Hadoop)
Cloud experience
STRONG ETL Experience
Python/Bash
Agile/Scrum
----------------------------------------
ABOUT MOORECROFT
A quality oriented national consulting firm. For more than two decades, providing expert IT professionals to our clients around the nation. Our core values are professionalism, honesty and integrity. We are only interested in working with professionals that subscribe to the same values and are driven to truly serve clients.
Job Type: Contract
Pay: From $60.00 per hour
Experience level:
10 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Santa Monica, CA 90404: Reliably commute or planning to relocate before starting work (Required)
Work Location: Hybrid remote in Santa Monica, CA 90404",-1.0,Moorecroft Systems,"Santa Monica, CA",Unknown,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
735,Data Engineer,-1,"Clear Demand Company Overview
Clear Demand is the leader in Intelligent Price Management and Optimization (IPMO) for retail. We were the first company to deliver an omni-channel lifecycle pricing solution that synchronizes prices, promotions, and markdowns online and in-store to produce a consistent brand and shopping experience. Clear Demand is the leading innovator in retail pricing solutions with patented science that analyzes historical sales to understand shoppers’ sensitivity to price and generate price and promotion strategies that account for pricing rules, cost changes, and competitor prices to achieve profit and revenue goals. Architected on big data and delivered through Software-as-a-Service (SaaS), Clear Demand’s Intelligent IPMO solution can be administered from a public or private cloud. Clear Demand’s innovations in retail science simplify adoption and use, while allowing retailers to see value in just weeks with more transparency and minimal disruption to existing business.
Clear Demand offers a competitive salary with stock options and a benefits package, including medical, dental, 401k.
Job Description – Data Engineer
This is a permanent position with tremendous potential for growth. The successful candidate will be exceptionally talented and hardworking---a self-starter able to multi-task and deliver results in a fast-paced environment. We are looking for a team player with experience developing high-performance applications for large enterprises. The software developer will be experienced in agile product-development methodologies. The ideal candidate will have a proven track record showing commitment to and sense of urgency for project timelines. This position will report to the Director of Engineering.
Primary Responsibilities
Designing, developing, testing, deploying, and maintaining applications to support business requirements.
Develop and improve data solutions for
- Designing relational schemas for persisting complex business objects
- ETL of customer data into our solution.
- Performing validation and mitigation strategies to handle invalid incoming data.
- Exchanging data between CDI’s user facing application and backend pricing optimization science solutions.
- Versioning database schemas, stored procedures
- Visualizing analytics and reporting
Working closely with both the Engineering and Science departments to build cohesive data-centric solutions.
Providing development expertise on migrating from a document store to a relational database.
Resolving technical issues through debugging and troubleshooting is also required.
Estimating level of effort for user stories and tasks.
Participate in Agile/SCRUM processes and ceremonies.
Required Skills
5+ years of experience in software development
Team player and effective communicator
Knowledge, experience, and proficiency with:
- Agile Development Methodology
- Relational Databases (SQL Server, Postgres)
- Document stores (i.e., MongoDB)
- Postgres
- Python
- Database query performance optimization
- Database versioning and migration
- ETL
- Git
- Object-Relational Mapping (ORM)
Performance optimization and debugging
Takes initiative to identify and address technology issues and opportunities, and proactively contributes to the business.
Good interpersonal, written, and oral communication skills.
Experience working in a team-oriented, collaborative environment.
Technical documentation skills.
Desired Skills
Google Cloud
CI/CD build and release pipeline
Jira
C#
HTML, JavaScript
Security/SSO/SSL
SaaS (Software-as-a-Service)
Experience with the Retail industry
Education
College degree in Computer Science or equivalent.
Clear Demand offers a competitive salary with stock options and a benefits package, including medical, dental, 401(k).
To apply, please your send resume to HumanResources@ClearDemand.com.
To learn more about Clear Demand, visit http://ClearDemand.com.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Employee discount
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Retirement plan
Vision insurance
Experience level:
5 years
Application Question(s):
Would you be willing to do a coding exercise as part of this application process?
Experience:
Agile: 3 years (Preferred)
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Scottsdale, AZ 85258",-1.0,Clear Demand,"Scottsdale, AZ",-1,-1,-1,-1,-1,-1
736,Data Engineer,Employer Provided Salary:$115K - $135K,"**This position is a majority Remote role with occasional in-office meetings on an as-needed basis. Because of this, interested applicants must live within a reasonable driving distance of Symmetry Lending's office in Anaheim, CA.**
Job Description
This role will play a pivotal role in assisting the IT team with the evolution of the company data architecture. In addition to supporting extensions to the data warehouse design, this role will design and develop ETL required to onboard new types of business data. Finally, this role will also provide support for Analytics assignments as needed.
Responsibilities
Display sense of ownership over assigned work, requiring minimal direction and driving to completion in a sometimes fuzzy and uncharted environment.
Designing and building new data pipelines that support business requirements.
Work with IT and other business partners to support the evolution of the enterprise data architecture.
Work with IT to build new ETLs to take data from various operational systems and extend existing data warehouse data model for analytics and reporting.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business constituents.
Support the development of the company BI infrastructure, including the construction of scalable analytic solutions, and on-premises reporting tools.
Utilize Power BI development to build scalable reporting models to serve BI reporting needs.
Demonstrate the ability to gather requirements, extract and manipulate data residing in multiple disparate databases, and articulate solutions to support the business.
Design, develop, and maintain performant data models in Power BI utilizing best practices.
Serve as a subject matter expert for all data warehouse and enterprise systems
Use new or existing technologies to produce analytics solutions (in the form of excel spreadsheets, dashboards, etc.).
Work with IT and Analytics team to architect and build data pipelines to optimize for performance, data quality, scalability, ease of future development, and cost.
Gather requirements, assess gaps and build roadmaps to help the analytics driven organization achieve its goals.
Develop data-related Proof of Concepts in order to demonstrate feasibility and value to Symmetry business constituents.
Qualifications / Requirements
Bachelor’s degree in Information Systems, Computer Science, Finance, or similar education from an accredited college
2-3 years Accounting and Finance background preferred
Strong skillset in Microsoft Excel (2-3 years, financial formulas & VBA is a plus) and familiarity with Microsoft Office Productivity Suite (Excel, Word, etc.)
5+years’ experience with advanced SQL concepts and writing SQL statements (SQL Server or similar).
Prefer 2 years’ experience with Microsoft BI Suite, PowerBI, Power Query, PowerPivot, Reporting Service (SSRS), SharePoint
Experience in mentoring other team members in development best practice, and methodologies.
You are passionate about data quality control and know how and where to anticipate potential errors.
Knowledge of the software development lifecycle, agile methodologies, and structured software development methodologies.
Experience performing analysis with large datasets in a cloud-based environment.
Ability to work effectively with stakeholders at all levels within the organization
Strong communication and time management skills and a self-motivated approach
Ability to work independently, detail-oriented, and execution focused
Highly collaborative and team oriented
Tenacious (doesn’t give up easily)
Genuine passion for clean and reliable data
At least 5 years of work experience
About Symmetry
Symmetry Lending, specializes in providing mortgage fulfillment services to include origination, servicing, and capital markets needs to various Lenders across the country with whom we partner. We have offices in Atlanta, GA, Eden Prairie, MN, Denver, CO, Orlando, FL, and Anaheim, CA, and we do business from coast to coast. We take great pride in building a diverse team of motivated professionals that contribute to an exciting work atmosphere. We provide a competitive benefits package including medical, dental, and vision plan options, paid time off, and more.
California Disclosure - Employee Notice at Collection
This disclosure is intended to comply with the California Consumer Privacy Act (CCPA), which gives California residents who are applicants, employees, or contractors of Symmetry Lending (“Symmetry”) the right to know what categories of personal information Symmetry collects about them and the purposes for which Symmetry uses that information. As used in this Privacy Notice, “Personal Information” means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a particular individual or household. Personal Information includes, but is not limited to, the categories of personal information identified below if such information identifies, relates to, describes, is reasonably capable of being associated with, or could be reasonably linked, directly or indirectly, with a particular individual or household.
The following is a list of the categories of Personal Information that we may collect about consumers:
Identifiers. This may include a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, Social Security number, driver's license number, passport number, or other similar identifiers.
Personal information described in the California Customer Records Statute (Cal. Civ. Code § 1798.80(e)). This may include a name, signature, Social Security number, physical characteristics or description, address, telephone number, passport number, driver's license or state identification card number, insurance policy number, education, employment, employment history, bank account number, or any other financial information, medical information, or health insurance information.
Characteristics of Protected Classification under California or Federal Law. This may include age, race, color, ancestry, national or ethnic origin, citizenship status, religion or belief, marital status, a childbirth or related medical condition, physical or mental disability, sex (including gender, gender identity, gender expression, pregnancy or childbirth, and related medical conditions), sexual orientation, veteran or military status.
Biometric information. This may include voice and video recordings.
Sensory data. This may include audio, electronic, visual, or similar information, including photos.
Professional or employment-related information. This may include current or past job history, compensation data, performance evaluations, or employee benefits.
Beneficiaries, dependents, and emergency contact information. This may include the name, gender, phone number, and relationship of beneficiaries, dependents, and emergency contacts.
We may use the categories of Personal Information for the following business or commercial purposes:
To perform background checks necessary to comply with licensing requirements, to perform reference checks, to verify eligibility to work in the United States, for contact purposes, to assess your qualification for employment, to conduct performance evaluations, for payrolls and budgeting purposes, for implementation of employee benefits, for internal organizational purposes to establish proper accommodations for sick time, PTO, leaves of absences, or emergency situations, and to conduct health screenings as allowed by OSHA and the CDC to protect the safety of our employee during pandemic situations.
For internal use, such as tracking access into buildings, timekeeping, activity logs, etc.
To comply with laws and regulations, including but not limited to applicable tax, health and safety, anti-discrimination, immigration, labor and employment, and social welfare laws.
For security or the prevention, detection, or investigation of fraud, suspected or actual illegal activity, violations of company policy or rules, or other misconduct.
To comply with civil, criminal, judicial, or regulatory inquiries, investigations, subpoenas, or summons.
To comply with all licensing requirements necessary for our business operations, including state exam audits.
To exercise or defend the legal rights of Symmetry and its employees, affiliates, customers, contractors, and agents.
To seek advice from lawyers, auditors, or other professional advisors.
If Symmetry uses Personal Information of an applicant, employee, or contractor for a purpose materially different than those disclosed in this notice, Symmetry will notify the employee and obtain explicit consent from the employee to use the Personal Information for this new purpose.
Job Type: Full-time
Pay: $115,000.00 - $135,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Paid time off
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
Accounting and Finance: 2 years (Preferred)
advanced SQL concepts and writing SQL statements: 5 years (Required)
Work Location: Hybrid remote in Anaheim, CA 92807",3.9,"Symmetry Lending
3.9","Anaheim, CA",51 to 200 Employees,-1,Company - Public,Banking & Lending,Financial Services,$5 to $25 million (USD)
737,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
738,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1.0,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
739,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
740,Data Engineer,Employer Provided Salary:$55.00 - $75.00 Per Hour,"Senior Data Engineer - 10+ Years of Total Experience Required
Location: Dallas, TX and Remote
Job Description:
Slesha inc is looking for a Data Engineer to join our team in our new location in Dallas, TX. This role will be responsible for the following:
Data Engineer
Responsibilities
· Writes ETL (Extract / Transform / Load) processes, designs database systems and, develops tools for real-time and offline analytic processing.
· Troubleshoots software and processes for data consistency and integrity. Integrates complex and large scale data from a variety of sources for business partners to generate insight and make decisions.
· Translates business specifications into design specifications and code. Responsible for writing complex programs, ad hoc queries, and reports. Ensures that all code is well structured, includes sufficient documentation, and is easy to maintain and reuse.
· Partners with internal clients to gain an expert understanding of business functions and informational needs. Works closely with other technical and data analytics experts across the business to implement data solutions.
· Leads all phases of solution development. Explains technical considerations at related meetings, including those with internal clients and less experienced team members.
· Assesses data quality and tests code thoroughly for accuracy of intended purpose. Provides data analysis guidance and serves as a technical consultant for the client.
· Educates and develops junior data engineers on the team while applying quality control to their work. Develops data engineering standards and contributes expertise to other data expert teams across Vanguard.
· Tests and implements new software releases through regression testing. Identifies issues and engages with vendors to resolve and elevate software into production.
· Participates in special projects and performs other duties as assigned.
Qualifications
· Deep technical knowledge – including proficiency in at least two of Python, SQL, Hive, Spark, Amazon Web Services / cloud computing (e.g., Elastic MapReduce, EC2, S3), Bash shell scripting
· Experience writing production quality code to create data products
· Ability to effectively communicate technical concepts to non-technical audiences
Job Type: Contract
Pay: $55.00 - $75.00 per hour
Compensation package:
Hourly pay
Experience level:
10 years
Schedule:
8 hour shift
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
AWS: 3 years (Required)
ETL: 3 years (Required)
Data warehouse: 3 years (Required)
10 key typing: 9 years (Required)
Work Location: Remote",-1.0,Slesha inc,"Dallas, TX",-1,-1,-1,-1,-1,-1
741,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
742,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
743,Data Engineer,-1,"At MNTN, we've built a culture based on quality, trust, ambition, and accountability – but most importantly, we really enjoy working here. We pride ourselves on our self-service platform, originally coded by our President and CEO, and are constantly seeking to improve the user experience for our customers and scale for efficiency. Our startup spirit powers our growth mindset and supports our teammates as they build the future of ConnectedTV. We're looking for people who naturally want to do more, own more, and make an impact in their careers – and we're seeking someone to be part of our next stage of growth.
As a Senior Data Engineer on the Data team, you will help build the platform to generate, track, manage and triage key business and client success metrics. The goal is to have rapid insights across all available information to mitigate issues and identify opportunities for a smooth marketing experience.
You will:
Become the expert on the MNTN platforms, UI, data infrastructure, and data processes
Extract meaningful business metrics from raw data using SQL and other tools
Create and manage ETL/ELT workflows that transform our billions of raw data points daily into quickly accessible information across our databases and data warehouses
Organize data and metrics for measurable and trackable confidence in reporting and client performance to fulfill agreed-upon quality standards
Organize visualizations, reporting, and alerting necessary to rapidly illustrate performance, data quality, trends and opportunities
Investigate critical incidents and otherwise ensure that any issues reach resolution by the relevant parties
You have:
5+ years of experience related to data engineering, analysis and modeling complex data
Strong experience in SQL, data modeling, and manipulating and extracting large data sets.
Hands-on experience working with data warehouse technologies. Familiarity with building data pipelines and architectures and designing ETL flows.
Experience with programming languages such as Python, Java, or shell scripting. Familiarity with algorithms.
Familiarity with software processes and tools such as Git, CI/CD pipelines, Linux, and Airflow
Experience with working in a cloud computing environment such as AWS, Azure, or GCP
Familiarity in a business intelligence tool such as Domo, Looker or Tableau
Written and verbal communication skills to convey complex technical topics to non-technical audiences across the organization
MNTN Perks:
100% remote
Open-ended vacation policy with an annual vacation allowance
Three-day weekend every month of the year
Competitive compensation
100% healthcare coverage
401k plan
Flexible Spending Account (FSA) for dependent, medical, and dental care
Access to coaching, therapy, and professional development
About MNTN:
MNTN provides advertising software for brands to reach their audience across Connected TV, web, and mobile. MNTN Performance TV has redefined what it means to advertise on television, transforming Connected TV into a direct-response, performance marketing channel. Our web retargeting has been leveraged by thousands of top brands for over a decade, driving billions of dollars in revenue.
Our solutions give advertisers total transparency and complete control over their campaigns – all with the fastest go-live in the industry. As a result, thousands of top brands have partnered with MNTN, including, Petsmart, Build with Ferguson Master, Simplisafe, Yieldstreet and National University.
#Li-Remote",3.9,"MNTN
3.9",Remote,201 to 500 Employees,2009,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
744,Data Engineer,Employer Provided Salary:$91K - $116K,"A reasonable salary expectation is between $91,000.00 and $116,022.40, based upon experience and internal equity.
This position is on a hybrid work schedule. (Mon & Fri - remote, Tues - Thurs onsite in Rancho Cucamonga, CA)
Position Summary/Position
The Data Engineer II assists in the implementation of methods to improve data reliability and quality. This role is responsible for combining raw information from different sources to create consistent and machine-readable formats. The Data Engineer II must also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling. The Data Engineer II will focus on data accessibility, which will enable the organization to utilize data for performance evaluation and optimization. The data Engineer II is also responsible for managing the entire back-end development life cycle for the company's enterprise data warehouse. In this role the incumbent will handle tasks associated with the implementation of ETL procedures, building warehouse databases, database performance management, and dimensional modeling and design of the table structures.
Major Functions (Duties and Responsibilities)
1. Design and develop data warehouse Extraction, Transformation and Loading (ETL) solutions using Microsoft SQL Server Integration Services (SSIS), Azure Data Factory, Synapse Analytics, Az Data Bricks, PySpark ETL.
2. Develop and implement data collection processes in conjunction with the data warehouse. Source data from legacy systems supporting a centralized data warehouse and reporting platform.
3. Develop technical solutions to meet the requirements for Data Warehouse, BI & Analytics
4. Work closely with the data engineering and BI & Analytics teams to design data and analytics solutions to increase the usability, completeness, and accuracy of enterprise data
5. Analyze user requirements and translate into database requirements and implement in database code
6. Create and maintain the optimal data pipeline architectures based on micro services based on platform and application requirements
7. Assemble large, complex data sets that meet functional / non-functional business requirements
8. Identify, design, and implement process improvements: automating manual pipeline processes, optimizing data ingestion and consumption, re-designing infrastructure for greater scalability, micro services, etc
9. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
10. Work closely with Data Warehouse Architect and Data Systems Architect to design data and analytics solutions to increase the usability, completeness, and accuracy of enterprise data
11. Create, maintain, and optimize SQL queries and routines
12. Analyze potential data quality issues to determine the root cause and create effective solutions.
13. Develop, adopt, and enforce Data Warehouse and ETL standards and architecture
14. Monitor and support ETL processes ensuring integrity and proper integration of all data sources
15. Create high throughput historical and incremental ETL jobs
16. Facilitate problem management, and communication among data architects, managers, informaticists and analysts
17. Provide detailed analysis of data issues; data mapping; and the process for automation and enhancement of data quality
18. Perform development activities such as source to target mapping validations, identify, document and execute unit test cases/scripts, peer and lead code reviews per code review checklist and document test and review results.
19. Collaborate and contribute to data integration strategies and visions
20. Provide ongoing proactive technical support for ETL and data warehouse system to ensure business continuity.
21. Work with Informaticists and Analysts to translate analytic requirements into technical solutions.
Experience Qualifications
Four (4) years of relevant work experience. Experience and knowledge in logical, rational, dimensional, and physical data modeling. Background in database systems along with a strong knowledge of SQL. Experience with Orchestration tools, Azure DevOps, and CI/CD. Intermediate experience with the following tools and technologies:
a. Azure Data Catalogue / Purview
b. Azure Cloud
c. Databricks
d. Power BI Dataflows
e. Power Query
f. Azure Cosmos
g. Azure Monitor
h. PowerShell
i. Python
Preferred Experience
Development experience using PySpark, Spark, Hadoop, Kubernetes, and RDMIS is highly desired.
Education Qualifications
Bachelor's degree from an accredited institution required.
Preferred Education
Master’s degree from an accredited institution preferred.
Professional Certification
Azure Data Engineering Certification is preferred.
Knowledge Requirement
Multi-server environment knowledge such as linked servers, data replication, backup/restore with MS SQL Server 2008+. Knowledge of applicable data privacy practices and laws.
Skills Requirement
Highly skilled in developing and optimizing T-SQL (DDL, DML, DCL) queries, stored procedures, functions, and views for various applications that involve numerous database tables and complex business logic. Good written and oral communication skills. Strong technical documentation skills. Good interpersonal skills.
Abilities Requirement
Highly self-motivated and directed. Keen attention to detail. Proven analytical and problem-solving abilities. Ability to effectively prioritize and execute tasks in a high-pressure environment.
Commitment to Team Culture
The IEHP Team environment requires a Team Member to participate in the IEHP Team Culture. A Team Member demonstrates support of the Culture by developing professional and effective working relationships that include elements of respect and cooperation with Team Members, Members and associates outside of our organization.
Working Conditions
Word processing and programming involving computer keyboard and screens.
Position is eligible for Hybrid work location upon completing the necessary steps and receiving HR approval. All IEHP positions approved for telecommute or hybrid work locations may periodically be required to report to IEHP’s main campus for mandatory in-person meetings or for other business needs as determined by IEHP leadership.
Inland Empire Health Plan (IEHP) is the largest not-for-profit Medi-Cal and Medicare health plan in the Inland Empire. We are also one of the largest employers in the region, designated as “Great Place to Work.” With a provider network of more than 5,000 and a team of more than 3,000 employees, IEHP provides quality, accessible healthcare services to more than 1.5 million members. And our Mission, Vision, and Values help guide us in the development of innovative programs and the creation of an award-winning workplace. As the healthcare landscape is transformed, we’re ready to make a difference today and in the years to come. Join our Team and make a difference with us! IEHP offers a competitive salary and stellar benefit package with a value estimated at 35% of the annual salary, including medical, dental, vision, team bonus, and state pension plan.
Job Types: Full-time, Permanent
Pay: $91,000.00 - $116,022.40 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Parental leave
Retirement plan
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Rancho Cucamonga, CA 91730: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
data engineering: 4 years (Required)
Work Location: Hybrid remote in Rancho Cucamonga, CA 91730",3.7,"Inland Empire Health Plan
3.7","Rancho Cucamonga, CA",1001 to 5000 Employees,1996,Company - Public,Health Care Services & Hospitals,Healthcare,$1 to $5 billion (USD)
745,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
746,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
747,Data Engineer,$88K - $124K (Glassdoor est.),"Recently awarded one of Crain's Best Places to Work in Chicago®, Premier International is a privately held and private equity backed software and technology consulting firm headquartered in downtown Chicago, serving large enterprise consulting and Fortune 500 firms deploying large-scale systems implementations.
You will work in a fast-paced environment that exposes you to diverse project experiences as we collaborate to solve our clients' biggest data challenges.
The Opportunity:
Premier International is hiring an experienced Data Engineer to join our growing Data Governance Practice. You will work in a fast-paced environment that exposes you to diverse project experiences, leading-edge technologies, and continuous learning experiences that will grow your career while solving clients' biggest challenges.
Our Data Governance Practice delivers end-to-end business advisory services, implementation, and technical solutions for the Data Governance Lifecycle including Consulting, Metadata Integration, Reference Data Management, Sensitive Data Management, Tool Evaluation, and Product Implementation.
What You'll Be Doing:
Designing, implementing, and maintaining Data Warehouse environments
Creating and maintaining comprehensive documentation of data engineering processes, pipelines, and workflows
Collaborating effectively with cross-functional teams, including data scientists and analysts, to understand their data needs and support data-related initiatives
What You'll Bring to the Team:
Bachelor's degree in Data Science, Computer Science, Statistics, or a related field
8+ years of relevant experience in data migration
Proficient in Python and Spark development/programming with a focus on performance and scalability
Experience with version control systems, GitHub, for code integrity
Strong analytical mindset and the ability to derive actionable insights from data
Excellent communication and presentation skills, capable of conveying complex information in a clear and concise manner
Ability to work independently and collaboratively in a fast-paced and dynamic environment
Premier Perks & Benefits:
Highly competitive compensation with annual bonus incentive
401K plan with company match
Company paid individual health, dental, vision, disability, and life insurance coverage
Four weeks of paid time off
Nine company paid holidays
Employee referral bonuses
Much more at one of Chicago's Best and Brightest Companies to Work For®!
Premier has been named one of The Best and Brightest Companies to Work For® in Chicago (2019, 2020 & 2021), one of Crain's top 100 Best Places to Work in Chicago (2020 & 2021) and recently made the 2021 Inc. 5000 list of America's Fastest-Growing Private Companies. While we are relentlessly client-focused, we are proud to have our culture and company recognized by others.
Premier is an EEO Employer and provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",4.2,"Premier International
4.2","Chicago, IL",1 to 50 Employees,1985,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
748,Data Engineer I - People Analytics,$73K - $111K (Glassdoor est.),"POSITION SUMMARY Our People Analytics team is expanding! The Data Engineer I is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.

This position can work in a flexible, hybrid model with 3 days on-site in Appleton, WI and 2 days remote.

JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.

QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming. Preferred experience in Python applied to data engineering and transformation.
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated

#LI-Hybid

Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101 .",4.1,"US Venture
4.1","Appleton, WI",1001 to 5000 Employees,1951,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
749,Senior Data Engineer,Employer Provided Salary:$190K,"Welcome to the MOMENTUM Family!
MOMENTUM is not just our company name; it is the highest value we deliver to our customers. We are a rapidly growing technology solutions company delivering innovative technology, engineering, and intelligence solutions across the DoD sector. The efforts of our high-capacity team ultimately strengthen our Nation and the warfighter.

Our team is dispersed throughout the US, which means we value the diversity and unique collaboration fostered throughout our team. We work incredibly hard for our customers and believe deeply in our core values. We're a high-energy, high-growth team and we love to win.

Data Engineer
The Data Engineer provides engineering support to the data science and software engineering team members. Includes augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments. Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, to include hardware and software support to existing servers.

In this role, you will:
Support ad-hoc data analysis requirements defined by the client's Leadership.
Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives.
Knowledge of business intelligence reporting tools and data visualization software including Tableau.
Will work on Data cleaning and transformation efforts in delivery of CDRLs.
Experienced in extracting and aggregating structured and unstructured data.
Experienced in data programming languages and tools such as Python and R.
Experience with SQL or similar database language.
Experience designing and implementing data models to enable, sustain and enhance the value of information they contain.
Strong analytical and critical thinking skills.
Ability to work collaboratively and effectively in a team environment.
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers
Make sure pedigree and provenance of the data is maintained such that the access to data is protected
Clean and preprocess data to enable analytic access
Collaborate with enterprise working groups to advance the state of data standards
Collaborate with the engineering team, data stewards, and mission partners to aid in processes getting actionable value out of the data holdings architects complex, repeatable ETL
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic and others
Supports Experience with Targeting using Sponsor Tools, Reverse Engineering
Develop API connectors to enable ingest of new data catalog entries from databases and files to ensure that data mappings will provide the best performance for expected user experience
Supports Deliverables and Reports

If you're suitable for this role, you have:
Top Secret SCI with FULL SCOPE POLY REQUIRED
9+ Years of verifiable experience


To learn more about us, check out our website at www.gomomentum.tech!

MOMENTUM is an EEO/M/F/Veteran/Disabled Employer:
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The qualifications listed above are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions.

Accommodations:
Consistent with the Americans with Disabilities Act (ADA) and Alabama civil rights law, it is the policy of Momentum to provide reasonable accommodation when requested by a qualified applicant or employee with a disability, unless such accommodation would cause an undue hardship. The policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If reasonable accommodation is needed, please include a request when applying.",3.6,"Momentum
3.6","Chantilly, VA",501 to 1000 Employees,1987,Company - Public,Advertising & Public Relations,Media & Communication,$100 to $500 million (USD)
750,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
751,Data Engineer,$80K - $115K (Glassdoor est.),"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",3.6,"AppsIntegration.Inc
2.9","Sturgis, MI",5001 to 10000 Employees,1947,Company - Private,Home Furniture & Housewares Stores,Retail & Wholesale,$1 to $5 billion (USD)
752,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
753,"Manager, Data Engineer & ETL Processing",$83K - $118K (Glassdoor est.),"Company Description

About Spark Foundry:

Spark Foundry is a global media agency that exists to bring HEAT – Higher Engagement, Affinity, and Transactions – to brands. By combining flawless media fundamentals with aggressive innovation, Spark inspires consumers to pay more attention, to care more about our clients’ brands, and to buy more products and services from them.
Balancing the nimble spirit of a startup with the powerhouse soul of Publicis Media, Spark Foundry delivers the best of both worlds to a client roster that spans some of the world’s best and most beloved brands and companies. We combine boutique-caliber insights and service with the buying clout and first-look access of a global leader, bringing the heat to challenger brands that want to act like giants, and to giant brands that want to act like challengers.
With a bottom-up culture that celebrates diversity and aims for all voices to be heard, Spark has become a magnet for the industry’s best talent, with one of the best retention rates in the industry. And by applying a whole-person approach to professional and personal development, Spark develops a workforce that is well prepared for today’s challenges, and also poised to create meaningful careers in the years to come.
Because we know that heat arises the intersection of complementary forces, our professionals come from myriad disciplines and backgrounds: data, analytics, and insights, content and creative production, communications and strategy, finance and marketing, and sociology, psychology, and other liberal arts disciplines.

Job Description

Overview:
The Manager, Data Engineering & ETL Development is a key driver to build data platform leveraging best in class ETL practice and is also a strategic thinker and a talented data visualization expert with emphasis on dashboard reporting. The primary responsibility will be developing business intelligence platform to support media and marketing data for our clients. This position will allow you to be a significant contributor as part of the team to support easy access of data and visualizations to fuel stronger insights and media optimizations.
This position requires strong technical and tactical skill sets with an eye for numbers, intellectual curiosity, proficiency at problem solving, and a critical understanding of online media. The candidate must have a proven track record in managing ETL’s, APIs and business intelligence platforms. Candidate should be a team player. A “roll up the sleeves” approach is mandatory and a “get it done” attitude is a must. Specific responsibilities include coordination between the research, analytics & media teams ensuring high quality data projects are effectively delivered.
Successful candidates will be multi-dimensional ‘rising-stars’ who are able to employ complex problem solving skills, and are able to communicate these succinctly to a broad client and media stakeholder audience.
Role Objectives:
Responsible for loading and validating data into the data warehouse from various source systems
Analyze, develop, fix, test, review and deploy functionality, and bug fixes in ETL data pipelines
Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required
Building Data mappings between Source to Target systems
Provides support for technical issues and ensuring system availability
Work with business customers to identify and develop additional data and reporting needs
Understand how business intelligence platform/data technologies work and offer the ability to explain technical concepts in ordinary terms (be technically savvy - understand the opportunities and limitations)
Establish and manage data integrations utilizing a taxonomy nomenclature to make recommendations for data visualization to showcase media performance through the use of Datorama or Tableau
Perform regular quality assurance/quality control checks on assigned client campaigns to ensure the data is processing accurately
Design and build backend data streams and processes to automate reporting capability with data visualization tools
Contribute to client status and reporting calls, including presentation of reporting as required
Develop subject matter expertise in ETL, API development, and Business Intelligence platforms
Clearly define project deliverables, timelines, and dependencies for junior team members, internal stakeholders and clients
Collaborate on an inclusive team, where members openly communicate and collectively problem-solve
Strong ability to evaluate new technologies and present findings to team
Contribute to knowledge sharing efforts and mentorships
Complete other duties as assigned.

Qualifications
Bachelor’s degree or combination of education, and equivalent work experience is preferred in the field of computer science, management information systems or Information Technology
3+ years’ related experience ETL development and business intelligence platform management
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
3+ plus years’ development experience with database engines including Presto, Mongo db and Hadoop.
3+ years’ experience in ETL development, Strong Database (Modeling, SQL), SQL Server, Hadoop, AWS Redshift, Qubole.
Experience managing team of 2 or more associates/analysts preferred
Strong database modeling and SQL skills. Ability to write complex SQL statement, Procedures and data automation programs
Knowledge of ETL tools like Airflow, AWS Glue, Alteryx, SSIS, Qubole/Spark is a must
Knowledge of Python or similar programming languages required
Proficiency with Datorama, Tableau, or other data visualization tools is preferred
Experience working with AWS or other cloud technologies
Advanced user Microsoft Office Suite and reporting tools
Demonstrated expertise in core MS Excel functions (vlookup, pivot tables, data visualization)
Experience in designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.
Strong analytical and problem-solving skills
Strong verbal/written communication and interpersonal skills is required
Self-starter with ability to thrive in a fast-paced environment and able to function independently while providing status updates to a team of analysts
Cooperative, flexible, conscientious, dependable, resourceful, self-motivated, and team-oriented
Problem solving, time management, and critical thinking skills with a professional and positive attitude
Ability to work independently and as part of an agile team, participating in daily stand-ups, sprint planning and sprint review

Character:
The following qualities help drive success as member of the Spark Data and Analytics team:
Entrepreneurial, engaging, resourceful, curious, and self-directed spirit
Willing and easily roll sleeves up or down; love the nitty-gritty and the strategy
Collaborative approach to building cohesive, strong teams
Loving and living the intersections between brands, people, media, communications
Relentlessly passionate and resolute
Planning and time management excellence.
Embrace challenges
Proactive, especially in pushing for new opportunities, approaches, and ideas.
Keenly focused on action and solutions; thrives with deep critical thinking and analysis.
Pioneering insight attitude and research in-the-know.
Resourcefulness, flexibility and adaptability, strong ability to pivot when the need arises.
Inspired to be part of the insight journey/revolution with a growing, dedicated team

Additional Information

All your information will be kept confidential according to EEO guidelines.
23-2795",3.9,"Spark Foundry
3.9","Dallas, TX",51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
754,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
755,AWS Data Engineer Snowflake,-1,"Company Description

Orchestrated by adept technical architects with over fifty years of applied expertise, KYNITE is an advanced technology company specializing in the disciplines of: Blockchain, Cloud Services, Big Data & Analytics, Artificial Intelligence, Enterprise, Staff Augmentation and Managed Services
We are BigData Experts
We are Cloud Experts
We are Enterprise Architects
We are Artificial Intelligence Innovators
We are Technological Evangelists
We are Doers
We are Kynite

Additional Information

All your
This job is only for individuals residing in US
US Citizens, Green Card holders, EAD's can apply
W2, C2c,
Information will be kept confidential according to EEO guidelines.",-1.0,Kynite,"Myrtle Point, OR",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
756,Data Insights Engineer,Employer Provided Salary:$85K - $95K,"Who We Are
We're purpose-driven. With every ride, we aim to redefine health and happiness. It's all about being more than a workout: SoulCycle is a mind-body-soul experience, built on community, love, respect, acceptance, and a lot of fun. It comes to life through the ride, the relationships, and the unparalleled hospitality. And all of that comes from our people. Join us—we'd love to have you.
Our Mission
To foster an open, diverse, & inclusive community—while embracing each unique individual exactly as they are. We empower each other by listening with an open mind, finding ways to learn and grow together, and always nurturing a sanctuary of trust. To make a real, lasting impact, we'll work nonstop to embrace and create change. Because nobody is equal until everyone is equal.
Job Description
The Data Insights Engineer will play a pivotal role in driving data-driven decisions at SoulCycle. You'll be responsible for building and maintaining the data infrastructure that supports all business functions, from marketing and operations to finance and customer experience, in addition to providing analysis to each of these teams. By leveraging your technical expertise and analytical skills, you will empower stakeholders to derive insights from data, enabling them to make strategic and informed decisions that positively impact the business.
Roles and Responsibilities
Insights and Recommendations: collaborate with cross-functional teams to understand business requirements, provide analytical support, and identify opportunities for data-driven improvements
Visualizations and Dashboarding: design and develop ad-hoc and recurring Looker reports; create and monitor business metrics; identify patterns, trends, and opportunities for performance improvement
Data Modeling: build, optimize, and document LookML data models that support quick and efficient analysis
Prediction: build predictive models that forecast business outcomes, customer behavior, and other relevant metrics
Qualifications
1-3 years of professional experience transforming and analyzing data across platforms such as Looker, Tableau, Mode, Jupyter Notebooks, Excel, and GCP/AWS. Looker/LookML experience is a plus.
Expert in SQL (able to write structured and efficient queries on large data sets) and familiarity with Python
Ability to identify patterns and trends in data and solve problems
Excellent communication skills to work with stakeholders to translate business needs and ideas into analyses and recommendations
Top-notch organizational skills and ability to manage projects in a fast-paced environment
Creative problem solving skills to find solutions to vague questions
Experience with Python data analysis and visualization packages is a plus (pandas, tensorflow, matplotlib, etc.)
Pay Range: $85,000 - $95,000 per year. This role is on-site 4 days a week.",3.9,"SoulCycle HQ
3.9","New York, NY",1001 to 5000 Employees,2006,Company - Private,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
757,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
758,Senior Data Engineer,$87K - $114K (Glassdoor est.),"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.

This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration.
How will you make an impact?
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
What are we looking for?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework
You might be a good fit if you like to:
Leverage technology to deliver data solutions to drive business outcomes
Drive for continuous improvement of processes and solutions
Bring energy and commitment to excellence to drive delivery of high-quality solutions
Share knowledge and mentor team members on technologies and solutions
What you can learn on the job:
Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers
Grow practical understanding of agile work practices through application of the scrum framework to deliver work products
Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science
What does success look like?
First 30 days:
Understand the existing analytics data platform and technologies
Understand team’s work management processes
First 6 months:
Develop data solutions for the business including shared components and specific data sets
Learn business processes and develop necessary contacts throughout the organization
First year:
Continue to develop data solutions for the business
Become expert on the team for other data engineers to go to for mentoring in completing their work
Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,"AE Business Solutions
4.2","Milwaukee, WI",51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
759,Sr. Data Engineer( ETL testing experience),Employer Provided Salary:$50.00 - $60.00 Per Hour,"Key Skills to evaluate – Python (advanced level), Pyspark, data flow pipeline in AWS, distributed system, Snowflake, Redshift, ETL testing, QE knowledge
JD:
· Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Job Type: Full-time
Pay: $50.00 - $60.00 per hour
Experience level:
8 years
Experience:
python advanced: 10 years (Preferred)
pyspark: 10 years (Preferred)
aws: 10 years (Preferred)
snowflake: 10 years (Preferred)
Work Location: Remote",-1.0,Sana Pivot Inc,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
760,Sr. ETL DEV/Data Engineer,Employer Provided Salary:$96K - $159K,"Data Engineers will be responsible for transformation and modernization of enterprise data solutions on Cloud Platforms integrating Azure services and 3rd party data technologies. Data Engineer will work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions.
ESSENTIAL FUNCTIONS
Reasonable Accommodations Statement
To accomplish this job successfully, an individual must be able to perform, with or without reasonable accommodation, each essential function satisfactorily. Reasonable accommodations may be made to help enable qualified individuals with disabilities to perform the essential functions.
Essential Functions Statement
As a Data Engineer, you will be responsible for assisting our clients envision, design, and deploy data engineering workloads as part of our solutions. As part of a small, dynamic team, you will have the opportunity to contribute to multiple phases of the solution life cycle including designing and implementing models and processes for large-scale datasets used for descriptive, diagnostic, predictive, and prescriptive purposes
Work with the team to evaluate business needs and priorities, liaise with key business partners and address team needs related to data systems and management.
Translate business requirements into technical specifications; establish and define details, definitions, and requirements of applications, components and enhancements.
Generate design, development, test plans, detailed functional specifications documents, user interface design, and process flow charts for execution of programming.
Develop data pipelines / APIs using Python, SQL, potentially Spark and AWS, Azure or GCP Methods.
Build large-scale batch and real-time data pipelines with data processing frameworks in Azure cloud platform.
Assist in the migration from on-prem SQL Server data analytics platform to MS Azure cloud platform.
Work as part of a team to build upon ingestion framework to intake new data sources.
Analyze, design, code and test multiple components of application code across one or more clients.
Perform maintenance, enhancements and/or development work

Qualifications
BA/BS in computer science, mathematics, information management, business, or equivalent experience
6+ years of experience in SQL
4+ years of experience in Cloud Platforms: Azure or AWS or GCP
4+ years of experience in Python and Pyspark
4+ years of experience in Synapse highly preferred
Experience using SQL, dB Visualizer, AWS, Azure, Cloud technologies
Experience with Power BI or similar data visualization tools
knowledge of HL7 v2, HL7 CDA and FHIR interface mapping highly preferred
Exposure to non-relational databases and tools, such as Cassandra, JSON, JAVA, Python, and Spark
In-depth knowledge of healthcare interoperability and patient data aggregation
Ability to effectively communicate, at times in a non-technical language, with customers at all levels of the organization.",3.7,"Gold Coast Health Plan
3.7","Camarillo, CA",Unknown,-1,Self-employed,Insurance Agencies & Brokerages,Insurance,Unknown / Non-Applicable
761,Data Engineer,$77K - $107K (Glassdoor est.),"Position Summary:
Reporting to the Director of Data Science, the Data Engineer will play a crucial role in designing, developing, and maintaining data pipelines and workflows to facilitate data integration, storage, and analysis. With the support of the Data Science team, this position will utilize proven expertise in Microsoft Azure cloud computing to implement and automate processes, ensure the health and efficiency of the data ecosystem, and bring unmatched value to our +1.5 million members across the country and across the world.

Duties and Responsibilities:
Lead architectural efforts for implementing the Data Science team's models and solutions.
Design, implement, and manage data pipelines using Azure Data Factory to extract, transform, and load data from various sources efficiently and securely.
Develop and optimize storage solutions on Microsoft Azure, ensuring data availability, scalability, and reliability.
Automate data-related jobs, processes, and workflows to streamline data delivery
Collaborate closely with cross-functional teams, including data scientists, analysts, and developers, to understand data requirements and support their data-related needs.
Design and maintain data models and architecture, enabling efficient data consumption by business applications and analytical tools.
Ensure data quality, integrity, and security throughout the data lifecycle.
Troubleshoot and promptly resolve data-related issues, performance bottlenecks, and system failures.
Research new data sources, third-party platforms, and data engineering best practices.
Stay current with the latest data engineering and cloud computing trends, particularly in the Azure ecosystem.
Assist Data Scientists with general analysis and dashboards, as needed.
Qualifications:
3-5+ years of relevant Data Engineering, Software Engineering, and/or Analytics experience.
Proved experience with Azure cloud solutions, preferably through your work portfolio and Azure certifications.
Strong proficiency in Azure Data Factory, Azure Storage, and other relevant Azure services.
Extensive experience designing and implementing automated data pipelines, ETL processes, and data workflows.
Advanced skills in SQL and Python for data manipulation, querying, and scripting tasks.
Familiarity with data warehousing concepts, modeling, and integration techniques.
Expert in integrating cloud solutions with external API sources and knowledgeable in creating in-house API solutions that meet our needs.
Excellent problem-solving abilities and a track record of handling complex data engineering challenges.
Strong communication skills and the ability to collaborate effectively with multidisciplinary teams.
Demonstrated ability to prioritize tasks, meet deadlines, and adapt to evolving project requirements.
Azure certifications (e.g., Azure Data Engineer Associate) will be a plus.
Willingness to travel up to 10%.
Organizational Overview:
The National Association of REALTORS® (NAR) is a team of professionals dedicated to providing world-class service to over 1.5 million REALTORS® working in the United States and around the world. The real estate industry is fast-paced and fast-changing--each year, our members participate in the sale, lease, and management of billions of dollars in real estate. As in every industry, our members' value proposition is constantly being challenged by innovation.

It is our mission to empower REALTORS® as they preserve, protect and advance the right to real property for all. We cannot do that without the ideas, passion, and commitment from our talented employees. As our greatest assets, employees are offered their pick of competitive benefits/perks and flexible work options. NAR is certified as a Great Place to Work®. Our most successful employees are committed to our Core Values, which are:

Putting members first
Leading change
Advancing Diversity and Inclusion
Giving respect
Collaborating
Communicating",3.7,"National Association of REALTORS®
3.7","Washington, DC",201 to 500 Employees,1908,Nonprofit Organization,Real Estate,Real Estate,$100 to $500 million (USD)
762,Data Engineer- Hadoop,Employer Provided Salary:$63.00 - $66.00 Per Hour,"Position Summary:
Synthetic data engineer is the experienced professional who can bring together skills and technologies to be able design and develop solutions to deliver data required for training models/test applications. Candidate will be working with a team of resources using Python, Sqoop, pyspark to create the data in a most efficient manner. Candidate should have prior experience & knowledge in highly scalable production grade applications with a strong engineering acumen. Should be able to work in a collaborative, fast paced environment. Candidate must have knowledge and/or experience working with Hadoop ecosystem.
Skills:
• Advanced knowledge in Python Programming
• Familiarity with Faker module
• pyspark, Sqoop, Hadoop, Hive, Shell script, Hadoop Shell commands
• Good experience dealing with bulk data extracts and bulk data loads
• Ability to translate requirements from stake holders into the technical requirements and the code
• Experience on statistical distribution, agent to model , deep learning is a plus
• Background of MF environment is a plus
Job Type: Contract
Pay: $63.00 - $66.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
Hadoop: 7 years (Required)
Python: 5 years (Required)
ETL: 3 years (Preferred)
Work Location: Hybrid remote in Charlotte, NC 28203",3.9,"Data Inc
3.9","Charlotte, NC",501 to 1000 Employees,1983,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
763,Data center infrastructure Engineer,Employer Provided Salary:$60.00 - $68.00 Per Hour,"Requirements and Responsibilities:
Must have Bachelor’s degree in IS or equivalent.
Must have experience with VMWARE, DATA CENTER OPERATIONS, IT INFRASTRUCTURE, and VIRTUALIZATION.
Experience working with Linux (CentOS/RHEL/Rocky) .
Experience working with VMWare 6.x / 7.x and storage area network (SAN).
Manage and support Microsoft domain / Active Directory Services, RBAC, Group Policy, Windows Servers, desktops and services automate tasks through scripting technologies Support execution of Information
Support and manage the VMWare virtualization and storage (SAN - Dell Extreme IO & Pure Storage) infrastructure to meet and exceed development operations growth expectations.
Job Type: Contract
Pay: $60.00 - $68.00 per hour
Benefits:
Vision insurance
Experience:
Data center operations: 4 years (Required)
IT infrastructure: 3 years (Required)
Work Location: In person",3.0,"BJ Services
3.0","Exton, PA",1001 to 5000 Employees,1872,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",$1 to $5 billion (USD)
764,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
765,Junior Data Engineer,$83K - $119K (Glassdoor est.),"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a junior data engineer, you will be part of a team that is responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.
The position:
Develop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Support various components of the data pipelines, including ingestion, validation, cleansing and curation
Manage and ensure the success of ongoing data pipeline routines
Collaborate with our implementation team to assist with the identification and reconciliation of data anomalies
Create and maintain documentation on data pipelines
Engage with our software engineering team to ensure precise data points per application specifications
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering, or applicable experience
Expertise with SQL, database design and data manipulation methodologies
Expertise with ETL/ELT and the development of automated validation and data pipelines
Strong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW, master data management and other database design principles
Comfortable working with high volume data in a variety of formats
Excellent verbal and written communication
Self-motivated
Passionate at learning
Familiarity with healthcare data is a plus
Experience with CI/CD and version control tools is a plus
Experience working within hybrid cloud environment such as AWS is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation
Social outings",5.0,"Decision Point Healthcare
5.0","Boston, MA",1 to 50 Employees,2013,Company - Public,-1,-1,Less than $1 million (USD)
766,"Data Engineer - Spark, Python, AWS",$83K - $122K (Glassdoor est.),"FlexIT client has an immediate need forData Engineer - Spark, Python,AWS12 months Remote contract in Portland, Oregon.
Top Skills:Python, Spark, PySpark, AWS, Machine Learning/Data Science, CI/CD integration
Job Description:
We are looking for a motivated and experienced Software Data Engineer to join our clients Advanced Analytics & Machine Learning team.
This is a unique opportunity to enhance & upgrade our next generation Data Science Platform that supports a dynamic team of data scientists, data engineers, and data analysts, all working on Demand Sensing related projects.
Job Duties:
We are looking for strong experience in Python, AWS, Machine Learning/Data Science, CI/CD integration and the ability work with cross functional team.
The work will also involve building and incorporate automated unit & integration tests into the Data science platform",4.0,"FlexIT Inc
4.0","Beaverton, OR",1 to 50 Employees,-1,Company - Private,-1,-1,$5 to $25 million (USD)
767,Data Science Engineer,-1,"Build the solution that transforms the real estate industry!
We are looking for a Data Science Engineer superhero with a sixth sense for data who can ignite our day-to-day activities with their creativity.
Want to infuse a $30B+ sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? Looking for a fully remote startup culture supported by a profitable business model? Join Mashvisor and help us build an entirely new type of real estate model.

Our Values

Customer Obsessed – We always put our customers first.
Solution Driven – We solve problems that other people are afraid to.
Product led: We are always one step ahead of our customer's needs and create / add features they love every time
One Team – We believe inclusion and teamwork produce the best results.
Open and Direct – We communicate with honesty and respect to our colleagues, customers, and partners.

What You’ll Do
Designing, developing, and researching Machine Learning systems, models, and schemes
Studying, transforming, and converting data science prototypes
Searching and selecting appropriate data sets
Performing statistical analysis and using results to improve models
Training and retraining ML systems and models as needed
Identifying differences in data distribution that could affect model performance in real-world situations
Visualizing data for deeper insights
Analyzing the use cases of ML algorithms and ranking them by their success probability
Understanding when your findings can be applied to business decisions
Enriching existing ML frameworks and libraries
Verifying data quality and/or ensuring it via data cleaning

What You’ll Need
BS or Masters degree in Mathematics, Statistics, Economics, Data Science or another quantitative field
3+ years of hands-on experience utilizing data science to manage, enhance and develop models and deploy solutions to solve complex business problems
Expertise in SQL and programming in SQL, R, Python, C++, Java, and beneficial to know Lisp and Prolog
Strong organizational, interpersonal, and communication skills (both written and verbal)
A bias towards solving problems from a customer-centric lens and an intuitive sense for how the work aligns closely with business objectives
Solid experience with managing databases and datasets and structuring and optimizing the framework
A thorough understanding of SQL databases
Bonus: background in US real estate data, insurance or financial markets analysis
The ideal candidate will be a creative problem solver with an excellent work history on data analytics projects.

We want the work you do here to be the best work of your life.
Compensation: We offer a great salary with a yearly bonus based on performance.
Attitude: Work with a Can-Do team across the world.
Freedom: Work anywhere, anytime.
Time Off: Yearly vacations and sick leaves.
Responsibility: Ability to excel in a fully remote work environment.
Are you the one?",3.7,"Mashvisor Inc.
3.7",Remote,1 to 50 Employees,2015,Company - Private,Real Estate,Real Estate,$1 to $5 million (USD)
768,Senior Data Engineer,$98K - $131K (Glassdoor est.),"Brivo is looking for a passionate and skilled Senior Data Engineer to design, build, and operate Brivo’s data platforms that support our mission critical transactional and warehouses.
For this role, we are seeking candidates that are open to traveling to our Bethesda, MD office 1-2 times a quarter (every three months).
In this role you are expected to:
Create, monitor, and maintain data pipelines across multiple applications and environments
Proactively monitor, tune, and report on the performance of the platform: databases, tooling, and infrastructure
Build expert-level knowledge and understanding of the applications and their underlying data platform whilst providing 24/7 support for production workloads (including oncall rotation) to ensure the highest standards of availability, resilience, integrity, security, and performance required by our business systems
Strategize and implement the next generation of the data platform with a focus on building analytical datasets to facilitate BI and ML applications
Manage, maintain and monitor disaster recovery strategies, and security controls in accordance with company policies, procedures, and processes
Design, develop and maintain appropriate levels of documentation for the data platform, processes and procedures
About the Team
The Data Engineering team is responsible for driving the company’s Data Strategy through ownership of the underlying data platform and data pipelines supporting the company’s SaaS IoT Security products, with a strong focus on maintaining reliability, resilience, and continual improvement. We develop data solutions that power Brivo. Join our team to help us build the best platform for Smart Spaces.
About You
3+ years of work experience with coding in one of mainstream programming languages: Python, Java, Scala, Node.js etc.
3+ years of experience operating production workloads on PostgreSQL (preferred), MySQL databases
3+ years of experience working on data warehouses such as Snowflake, Redshift or similar
3+ years of experience with open source technologies (Spark, Kafka, Kinesis, Flink, Kafka, etc.)
3+ years of experience managing ETL pipelines with tools such as Fivetran, Airflow, Talend, etc.
2+ years of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale
2+ years of experience with AWS EMR, Athena, Lambda, Kafka, S3, EC2, Kubernetes
Familiarity with Agile principles including but not limited to Scrum meetings and mechanics
Intellectually curious about emerging technologies with the ability to turn that into reasonable working solutions
Excellent communication skills and dynamic team player
Deep understanding of RDBMS technologies, specifically as it pertains to monitoring and tuning performance, including the use of logging and monitoring tools such as CloudWatch
Knowledge of leveraging the core building blocks within AWS to properly build and secure data for SaaS applications (ex. RDS, Lambda, EMR, S3, IAM, etc.)
Ability to work in the EST time zone
Optional experience to make you stand out:
AWS certifications
Experience in batch and stream processing technologies, Python, Java or Scala
Knowledge of or experience with DataOps
About Us
Brivo is the global leader in mobile, cloud-based access control for commercial real estate, multifamily residential, and large distributed enterprises. Our comprehensive product ecosystem and open API provide businesses with powerful digital tools to increase security automation, elevate employee and tenant experience, and improve the safety of all people and assets in the built environment. Having created the category over twenty years ago, our building access platform is now the digital foundation for the largest collection of customer facilities in the world, trusted by more than 25 million users occupying over 300M square feet of secured space in 42 countries.
Our dedication to simply better security means providing the best technology and support to property owners, managers, and tenants as they look for more from buildings where they live, work, and play. Our comprehensive product suite includes access control, smart readers, touchless mobile credentials, visitor management, occupancy monitoring, health and safety features, and integrated video surveillance, smart locks, and intercoms. Valued for its simple installation, high-reliability backbone, and rich API partner network, Brivo also has the longest track record of cybersecurity audits and privacy protections in the industry.
Brivo is privately held and headquartered in Bethesda, Maryland. Learn more at www.Brivo.com
Brivo is an Equal Opportunity/Affirmative Action Employer
MGvYwLkYgX",4.4,"Brivo
4.4","Bethesda, MD",51 to 200 Employees,1999,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
769,Data Visualization Engineer,$77K - $101K (Glassdoor est.),"Passionate about precision medicine and advancing the healthcare industry?
Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.
With the advent of genomic sequencing, we can finally decode and process our genetic makeup. We now have more data than ever before but providers don't have the infrastructure or expertise to make sense of this data. We're on a mission to connect an entire ecosystem to redefine how genomic data is used in clinical settings.
What You'll Do:
Own the design, development and administration of advanced analytics dashboards and reporting and business intelligence tools. Beginning with requirements gathering across cross-functional teams, to the delivery of analytic solutions into production.
Collaborate with stakeholders, providing guidance, data and analytical support for developing new datasets, analyses and tools in support of strategic initiatives.
Perform deep-dive root cause and problem-solving analyses for and with laboratory, business operations and engineering teams.
Design and build data views to support advanced analytical reporting, self-service data analysis and exploration, utilizing multiple data and visualization technologies.
Work closely with technical and product teams to identify and align changes to internal systems in support of future data analysis needs and requirements.
Develop and facilitate data governance initiatives to assure data integrity across distributed systems.
Solve new and interesting problems.
Qualifications:
Bachelor's degree in Information Systems, Business Administration, Finance, Economics, Mathematics, Operations Research, Marketing or similar
3+ years of experience developing data sets for analysis use-cases
3+ years of experience with direct stakeholder management and ownership of full lifecycle of analytics/data projects and presenting analysis results to stakeholders
3+ years report-development and data visualization experience with BI tools (Looker, Tableau, etc.)
Ability to deliver both high-level explanations of complex problems while also able to deep dive into complex problems
Advanced SQL skills for querying and analysis of large, complex datasets
Must work well in a team environment and be able to partner with both business and technical team members and translate technical language to the non-technical need
Creative problem solver, ability to handle multiple projects, and strong work ethic needed
Excellent written and verbal communication skills
Highly organized and systematic, superb attention to detail
Able to complete tasks with a high degree of accuracy and an eye towards process improvement
Persistent and resilient person who thrives in a fast-paced, highly collaborative environment
Bonus points for:
Advanced analytics experience - classification/predictive modeling, optimization models, forecasting
Experience with Python for data engineering or advanced data analysis projects/use-cases
Healthcare domain knowledge and experience
Expert knowledge of database/data modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others
Experience building cloud-native applications and supporting technologies / patterns / practices including: GCP, AWS, Docker, CI/CD, DevOps, and microservices

#LI-EH1
#LI-Hybrid
We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.1,"Tempus
3.1","Chicago, IL",501 to 1000 Employees,2015,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
770,Data Communications Engineer - III,Employer Provided Salary:$91K - $181K,"Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","San Francisco, CA",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
771,Sr. Data Engineer,$83K - $115K (Glassdoor est.),"Provides application design, analysis, programming, testing and documentation for the development, integration, enhancement, and maintenance of information systems solutions
Job Description:
BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Job Responsibilities
Formulates and defines system scope and objectives through research and fact-finding to design, develop, modify, or integrate complex information systems.
Devises or modifies application systems and procedures to optimize functional requirements including capacity, operating time, response time, and form of desired results.
Designs, codes, tests, debugs, and documents programs, subroutines, and scripts.
Develops and implements program/system test plans. Devises data verification methods and standard system procedures.
Responds to system failures and performance events by taking appropriate measures to reduce system downtime and eliminate recurrence of problems.
May conduct research on software and hardware products to justify recommendations and support management in budgeting and purchasing efforts.
Acts as expert technical resource to development staff in all phases of the development and implementation process.
Job Qualifications
Education
Bachelors Degree in Computer Science or equivalent work experience required. Equivalent years of experience are determined as one year of technical experience for every year of college requested.
Experience
1 year - Experience with information technology concepts, application development methodology, terminology, and standards required
5 years - Systems and programming experience required
1 year - Expert working knowledge of at least one programming language in use at BCBST.
Skills\Certifications
Proven skill with information technology in use at BCBST is required
Proven analysis, design, and coding skills, and demonstrated success in leading large and complex projects
Top level technical experience in one or more specialized areas of applications systems analysis and programming
Demonstrated ability to interpret and translate technical and/or or complex concepts into information meaningful to project team members and/or business personnel.
Be organized, reliable, and able to manage multiple tasks with exceptional work ethic.
Leadership skills
Must be able to communicate effectively with both technical and non-technical co-workers
Job Specific Requirements:
Preference for candidates with Cloud experience (GCP specifically)
Python knowledge/experience a plus
Preferred Skills:
Number of Openings Available:
1
Worker Type:
Employee
Worker Sub-Type:
Employee
Company:
BCBST BlueCross BlueShield of Tennessee, Inc.
Applying for this job indicates your acknowledgement and understanding of the following statements:
BCBST is an Equal Opportunity employer (EEO), and all employees and applicants will be entitled to equal employment opportunities when employment decisions are made. BCBST will take affirmative action to recruit, hire, train and promote individuals in all job classifications without regard to race, religion, color, age, sex, national origin, citizenship, pregnancy, veteran status, sexual orientation, physical or mental disability, gender identity, or any other characteristic protected by applicable law.
Further information regarding BCBST's EEO Policies/Notices may be found by reviewing the following page:
BCBST's EEO Policies/Notices
BlueCross BlueShield of Tennessee is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at BlueCross BlueShield of Tennessee via-email, the Internet or any other method without a valid, written Direct Placement Agreement in place for this position from BlueCross BlueShield of Tennessee HR/Talent Acquisition will not be considered. No fee will be paid in the event the applicant is hired by BlueCross BlueShield of Tennessee as a result of the referral or through other means.
All applicants will be advised that BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Tobacco-Free Hiring Statement
To further our mission of peace of mind through better health, effective 2017, BlueCross BlueShield of Tennessee and its subsidiaries no longer hire individuals who use tobacco or nicotine products (including but not limited to cigarettes, cigars, pipe tobacco, snuff, chewing tobacco, gum, patch, lozenges and electronic or smokeless cigarettes) in any form in Tennessee and where state law permits. A tobacco or nicotine free hiring practice is part of an effort to combat serious diseases, as well as to promote health and wellness for our employees and our community. All offers of employment will be contingent upon passing a background check which includes an illegal drug and tobacco/nicotine test. An individual whose post offer screening result is positive for illegal drugs or tobacco/nicotine and/or whose background check is verified to be unsatisfactory, will be disqualified from employment, the job offer will be withdrawn, and they may be disqualified from applying for employment for six (6) months from the date of the post offer screening results.
Resources to help individuals discontinue the use of tobacco/nicotine products include smokefree.gov or 1-800-QUIT-NOW.",3.8,"BlueCross BlueShield of Tennessee
3.8","Chattanooga, TN",5001 to 10000 Employees,1945,Nonprofit Organization,Insurance Carriers,Insurance,$5 to $10 billion (USD)
772,Data Engineer,$88K - $120K (Glassdoor est.),"Data Engineer
Engineering
REMOTE OPPORTUNITY

About Catalytic Data Science (CDS):
Catalytic Data Science is a groundbreaking cloud R&D platform designed to integrate the volumes of scientific resources, data, and analytic tools while providing the ability to network with colleagues in one secure and scalable environment. By enabling R&D teams to work more collaboratively and improving productivity company-wide, the Catalytic platform helps teams achieve key R&D milestones faster and with greater accuracy. Our customers are passionate about making the world a better place, and we are inspired by the opportunity to help them.

The Role:
You are a Data Engineer with experience in processing terabytes of data. You have experience in creating and automating scalable, fault-tolerant and reproducible data pipelines using Amazon AWS technologies. You are interested in helping to create a platform completely built on top of AWS. You are eager to join a team of Life Scientists and Software Engineers that believe the brightest minds in research should have the best tools to drive innovation.

What You’ll Do:

Build & operate automated ETL pipelines that process terabytes of text data nightly
Develop service frontends around our various backend datastores (AWS Aurora MySQL, Elasticsearch, S3)
Perform technical analyses and requirements specification with our product team on data service integrations
Help customers bring their data to the platform

What You Know:

Must Haves:

Python 3 or Java programming experience, preferably both
Day-to-day experience using AWS technologies such as Lambda, ECS Fargate, SQS, & SNS
Experience building and operating cloud-native data pipelines
Experience extracting, processing, storing, and querying of petabyte-scale datasets
Familiarity with building and using containers
Familiarity with event-based microservices

Nice-to-Haves:

Prior experience with Elasticsearch (custom development and/or administration) is a huge plus
Prior work with text and natural-language processing
Knowledge of Graph databases

What do we love in team members?

Your specialization is less important than your ability to learn fast and adapt to shifting technologies. We’re especially fond of people who:

Focus on customer’s needs and our company’s goals, not just writing code
Iterate until customers love what you’ve built
Self-start and initiate
Self-organize
Strive to grow personally and professionally, beyond just expanding technical abilities
Love to experiment with new technology and share knowledge with the team

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.",4.2,"Catalytic Data Science
4.2","Boston, MA",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
773,Senior Data Analytics Engineer,Employer Provided Salary:$75.00 Per Hour,"JOB PROFILE: NCDHHS-Senior Data Analytics Engineer (714485)
JOB LOCATION: 820 s Boylan Ave ,Raleigh, NC, 27603
JOB TYPE: Hybrid
CONTRACT TENNURE: (1 YEAR WITH POSSIBLE EXTENTION)
PAY RATE: $75.00/hr on W2
Candidate must have US Citizenship or Green Card/Permanent Residency in the US to be considered for this position.
Responsibilities:
Provide strategic insights using SAS/SQL, Tableau, and Congas to analyze complex data and assist DHB management in informed decision-making.
Engage directly with clients to understand data needs, translating requirements into effective solutions.
Expertly manage technical aspects, ensuring data quality control and accurate implementation of specifications.
Propose and implement operational healthcare reporting solutions for improved decision-making.
Aggregate, analyze, and report on complex healthcare data from sources like MMIS and claims data.
Develop operational reports and dashboards aligned with healthcare management goals.
Collaborate with staff to assess needs, design solutions, and conduct statistical analysis for insights.
Create advanced analytics based on technical specifications, aiding healthcare program oversight.
Efficiently manage client data requests, ensuring timely delivery and effective communication.
Skills Required:
SQL Proficiency.
Cognos Proficiency.
Tableau Expertise.
Data Quality Control.
Data Aggregation and Analysis.
Technical Specifications.
Local Candidate Consideration.
Job Type: Full-time
Pay: $75.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 5 years (Required)
Tableau: 5 years (Required)
Cognos: 5 years (Required)
data quality control: 5 years (Required)
Data aggregation with MMIS: 5 years (Required)
SAS and SQL coding: 3 years (Required)
technical specifications to develop reports: 5 years (Required)
Work Location: In person",-1.0,"Changing Technologies, Inc.","Raleigh, NC",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
774,Data Governance Engineer,Employer Provided Salary:$100K - $125K,"Performance Health is seeking a Data Governance Engineer to join our team. In this role, you will be responsible for leveraging advanced data analytics techniques to optimize our manufacturing operations, streamline supply chain logistics, enhance overall efficiency, and support master data governance initiatives.

Essential Job Duties & Responsibilities
Develop and implement predictive and prescriptive analytics models to forecast demand, optimize inventory levels, and improve production scheduling in alignment with healthcare industry standards.
Analyze and interpret large datasets from various sources, including production systems, distribution centers, and market trends, to identify opportunities for process improvement, cost reduction, and master data cleansing.
Collaborate with manufacturing, procurement, and distribution teams to define key performance indicators (KPIs), establish data-driven goals, and measure progress towards operational excellence.
Design and execute A/B tests to evaluate the impact of process enhancements and initiatives, continuously refining strategies for improved outcomes.
Utilize machine learning techniques for anomaly detection, fault prediction, and quality control to ensure compliance with regulatory standards and product quality assurance.
Develop data visualizations, dashboards, and reports to effectively communicate insights and recommendations to stakeholders at all levels.
Stay current with advancements in data science methodologies, tools, and technologies, and proactively identify opportunities to apply them to manufacturing and distribution challenges.
Collaborate with Operational Excellence and Data Governance and IT teams to ensure data accessibility, integrity, and security, and assist in data integration efforts.
Performs other duties as assigned

Job Qualifications
Bachelor’s degree in Data Analytics or related field
3-5 years of experience applying data science techniques to manufacturing, supply chain, or distribution challenges, preferably within the healthcare industry.
Proficiency in programming languages such as Python or R for data analysis and statistical modeling
Strong expertise in data manipulation, feature engineering, and data preprocessing techniques.
Proficiency in SQL for data querying and manipulation.
Experience with data visualization tools (e.g., Tableau, Power BI) to create clear and impactful visualizations.
Excellent problem-solving skills and the ability to work effectively in cross-functional teams.
Ability to travel 10% of the time, including overnight travel

Benefits
Our benefits include healthcare; insurance benefits; retirement programs; paid time off plans; family and parenting leaves; wellness programs; discount purchase programs.
This is a full-time position with a base salary range of $100,000-$125,000 plus benefits.

To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. The requirements listed above are representative of the knowledge, skills, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Performance Health is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to sex, gender, gender identity, sexual orientation, race, color, religion, national origin, disability status, protected Veteran status, age, genetic information, and any other characteristic protected by law.",2.7,"Performance Health Supply,LLC
2.7","Warrenville, IL",Unknown,-1,Company - Private,Health Care Products Manufacturing,Manufacturing,Unknown / Non-Applicable
775,Data Engineer,$67K - $96K (Glassdoor est.),"Massanutten Resort
Job Summary
The Data Engineer will play a crucial role in designing, developing, and maintaining the data infrastructure and systems that enable efficient and reliable data processing, storage, and analysis. The primary responsibility is to transform raw data into usable formats for various business applications and support data-informed decision-making processes. This role includes working in various Microsoft SQL Environments as well as supporting a Snowflake Data Warehouse. The Data Engineer is expected to have a broad understanding of data concepts and be avid about continuing education to keep up with the ever-changing data landscape.
Required
Degree in computer science or related file and 3+ years of experience with Python, SQL, and data visualization/exploration tools (in lieu of a degree, at least 5 years of experience in the above is equivalent)
Preferred
A degree in computer science or a related field
Certifications in database management and/or related technical certifications
Familiarity with advanced data concepts (i.e., AI and Machine Learning) and business intelligence/visualization tools
Typical Schedule
Days:
Monday-Friday, weekends and holidays as needed
Position will be required to be part of rotating on-call schedule
Hours:
8am-5pm, additional hours as needed
Core Responsibilities
Develop Data Pipeline: Design and implement scalable and efficient data pipelines, integrating diverse data sources and ensuring smooth data flow from ingestion to storage and processing
Transform and Model Data: Develop and maintain data transformation processes, such as ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform), to convert raw data into structured formats suitable for analysis and reporting. Create and manage data models to facilitate data integration and provide a consistent view of information across systems.
Implement and oversee Data Warehousing and Storage: Build and maintain data warehouses or data lakes, optimizing storage and retrieval processes to ensure high availability, reliability, and performance. Implement appropriate data governance and security measures to protect sensitive data.
Ensure Data Quality and Governance: Collaborate with data analysts and business stakeholders to understand data requirements and define data quality standards. Implement data validation, data profiling, and data cleansing techniques to ensure data accuracy, consistency, and integrity.
Optimize Performance: Identify and address performance bottlenecks in data processing and storage systems, leveraging optimization techniques and technologies to improve overall data engineering efficiency and reduce latency
Assist with Data Problem Solving: Support team with data related escalation issues, including help desk support tickets that are generated from our business users
Manage Data Integration and API Development: Establish connections and integrations between internal and external data sources, including APIs, databases, and third-party platforms. Develop and maintain data APIs to enable seamless data access and consumption by other applications and services.
Collaborate and Document: Work closely with cross-functional teams, including analysts, software engineers, and business stakeholders, to understand data requirements and ensure data solutions align with business objectives. Document data engineering processes, including data pipelines, transformations, and system architectures, to facilitate knowledge sharing and maintain a robust knowledge base.
Stay up-to-date with the latest advancements in data engineering, data management, and big data technologies. Evaluate and recommend new tools, frameworks, and methodologies to improve data engineering practices and enhance overall data infrastructure
Other duties as assigned by management
For more information, contact Ana at 540-289-4939.",3.6,"Great Eastern Resort Management, Inc.
3.6","McGaheysville, VA",1001 to 5000 Employees,-1,Self-employed,-1,-1,Unknown / Non-Applicable
776,"Data Engineer (I, II, III)",$75K - $110K (Glassdoor est.),"Significance is a woman-owned consulting firm serving the federal government. We are known for building trusted relationships within our teams and with our clients and hiring the highest-level experts who implement innovative solutions. We also like to have fun! Our focus on culture has contributed to Significance being named a Washington Business Journal Best Place to Work each of the last five years.

We are seeking a Data Engineer (I, II, III) to join our program in support of AmeriCorps. Duties include:
Designing and implementing data pipelines using Azure Synapse Analytics.
Collaborating with cross-functional teams to identify and solve complex data problems.
Developing and maintaining ETL processes.
Monitoring and optimizing data pipelines for performance and reliability.
Ensuring data quality and integrity.
Working with large datasets and designing scalable data solutions.

Required Skills:
Bachelor's Degree in a related technical discipline
5+ years of experience as a Data Engineer.
Strong SQL skills
Strong Python skills and experience with Python data libraries, e.g., Pandas, PySpark
Experience working with Azure Synapse Analytics

Desired Skills:
Experience with big data technologies such as Parquet, Hadoop, or Spark.
Experience with AmeriCorps
Significance, Inc. is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, gender, color, age, sexual orientation, gender identification, national origin, religion, marital status, ancestry, disability, protected veteran status, or any other factor prohibited by applicable law.
We are an E-Verify Employer
https://e-verify.uscis.gov/web/media/resourcesContents/E-Verify_Participation_Poster.pdf
https://www.e-verify.gov/sites/default/files/everify/posters/IER_RighttoWorkPoster.pdf

#LI-MH1",4.0,"Significance, Inc.
4.0","Washington, DC",1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
777,Senior Data Engineer,Employer Provided Salary:$136K - $167K,"Note: Contractors (C2C, C2H) that directly apply will not be considered. Individual applicants only

Spokeo is a people search engine and identity platform that enlightens and empowers our customers. With nearly 15 billion records and 18 million monthly visitors, we reconnect friends, reunite families, prevent fraud, and more.

As a Senior Data Engineer at Spokeo, you will develop, optimize, and maintain the ETL data pipeline. This involves working with infrastructure built in AWS, including Airflow, PySpark, EMR, S3, DynamoDB, and more. This role will help build and improve automation platform features, analytical software packages, and data pipeline orchestration tools.

Deliverables - include an estimated time of how much an average week is spent on each item. This is subject to change:
40% - Build infrastructure and data automation pipeline for extracting, preparing, and loading data from various sources. Automate and integrate new components into the data pipeline.
30% - Implement robust ETL processes to efficiently execute product vision and strategy in alignment with organizational goals and priorities.
10% - Create unit and stress test components to monitor technical performance and ensure identified issues are resolved.
10% - Develop data analysis tools to provide data insights and capture key metrics.
10% - Research solutions and maintain technical documentation.
Follow best practices for data governance, quality, cleansing, and ETL-related activities.

Requirements:
7+ years of development experience in data engineering.
5+ years of hands-on programming experience with Python.
5+ years of professional experience working in big data ecosystems, preferably with Spark
3+ years experience with SQL, schema design, and dimensional data modeling.
2+ years of professional experience working with dataflow management tools, such as Airflow
2+ years of development experience in highly scalable, distributed systems and cluster architectures using AWS.
2+ years experience with non-relational databases (e.g., DynamoDB, Elasticsearch, etc.)
Prior experience working with large data sets (>100M+ records)
B.S. in Computer Science, Information Systems, or related fields

Spokeo offers a bonus program, equity plans, and 401K matching for qualified roles. Twice a year, we do discretionary, merit-based salary increases. Additional benefits include; 100% medical/dental/vision coverage for all employees and unlimited PTO.

Spokeo extends written offers to candidates who successfully complete their selection process. Spokeo’s offers include a base salary, participation in a company bonus program, stock options, and comprehensive benefits. A final offer will depend on several factors, including, but not limited to, marketplace competition, job leveling, the candidate’s experience, skills, etc.

Privacy Notice for Candidates: https://www.spokeo.com/recruiting-policy

Spokeo is an equal-opportunity employer. Applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, or protected veteran status. Spokeo fosters a business culture where ideas and decisions from all people help us grow, innovate, create the best products, and be relevant in a rapidly changing world.

Recruiters or staffing agencies: Spokeo is not obligated to compensate any external recruiter or search firm who presents a candidate or their resume or profile to a Spokeo employee without 1) a current, fully-executed agreement on file, and 2) being assigned to the open position (as a search) via our applicant tracking solution.

#LI-Remote

This is a remote position.",4.0,"Spokeo
4.0","Pasadena, CA",201 to 500 Employees,2006,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
778,Quantitative Data Engineer,$102K - $135K (Glassdoor est.),"This position is part of our Data Technology team and will help implement, enhance, and manage our quantitative models. Primary responsibilities include researching, designing, coding, testing, and deploying projects while working in a fast-paced environment and improving proprietary data repository and financial data platforms. The Quantitative Data Engineer will work closely with quantitative research and portfolio management professionals to implement new ideas. The successful candidate must possess strong knowledge of financial equity data (e.g., Compustat, Bloomberg, Thomson Reuters), have solid coding skills (in SQL, Python, Julia, and C++), and experience working with large datasets. MS/PhD degree in Computer Science or related field required.


We are seeking a Quantitative Data Engineer to design and implement our proprietary quantitative investment systems. You will be a key player in the Technology team and will research, design, code, test and deploy projects while working in a fast-paced environment.

Responsibilities include:
Implement, enhance, and manage quantitative models
Design and improve proprietary data repository and financial data platforms
Automate and support the Extract, Transform, and Load (ETL) processes from various market data vendors
Develop and manage reporting and performance analytics platforms



Requirements include:
MS/PhD in Computer Science, Engineering, Statistics, or related discipline with excellent academic credentials
Strong knowledge of financial equity data, a plus with experience in Bloomberg, Thomson Reuters, Compustat, and CapIQ data
Broad knowledge of database concepts with proficiency in SQL and stored procedures, preferably with Microsoft SQL Server
2+ years of solid coding experience in Python, Julia, C++, C#
Experience in processing large and complex datasets
An advanced knowledge of math and statistics

For immediate and confidential consideration, please email your cover letter and resume to careers@jlem.com. Please indicate the position for which you are applying.
Equal Opportunity Employer",4.5,"Jacobs Levy Equity Management
4.5","Florham Park, NJ",1 to 50 Employees,-1,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
779,Sr. Data Engineer I,$82K - $109K (Glassdoor est.),"Pax8 is the leading value-added cloud-based SaaS distributor, simplifying the cloud journey for our partners by integrating technology, business intelligence and proactive service to deliver an unparalleled experience. Serving thousands of partners through the indirect sales channel, our mission is to be the world's favorite place to buy cloud products. We are a fast-growing, dynamic and high-energy startup organization, allowing you to make a meaningful impact on the business. Culture is important to us, and at Pax8, it's business, and it IS personal. We are passionate, creative and humorously offbeat. We work hard, keep it fun, and expect the best.
We Elev8 each other. We Advoc8 for our partners. We Innov8 continuously. We Celebr8 life.
No matter who you are, Pax8 is a place you can call home. We know there's no such thing as a ""perfect"" candidate, so we don't look for the right ""fit"" – instead, we look for the add. We encourage you to apply for a role at Pax8 even if you don't meet 100% of the bullet points. We believe in cultivating an environment with a diversity of perspectives, in hopes that we can all thrive in an inclusive environment.
We are only as great as our people. And we have great people all over the world. No matter where you live and work, you're a part of the Pax8 team. This means embracing hybrid- and remote-work whenever possible.
Position Summary:
The Sr. Data Engineer I designs, develops, tests, deploys, maintains, and improves systems that collect, transform, store, and manage data for end users. They deliver individual projects based upon deadlines and required deliverables. The Sr. Engineer builds complex features independently and collaborates with other teams to conduct design and code reviews. They develop and/or provide technical leadership in the development of data systems involving the application of new technologies with significant technical risk. The Sr. Engineer prepares detailed plans, which may span over a year for complex projects. They determine test philosophy, goals, and objectives, and participate in the formation of project goals, scope, and schedule.
Essential Responsibilities:
Builds pipelines to ingest new data sources
Transforms data to support varied use cases
Includes testing in all aspects of the development process
Mentors junior and mid-level Engineers
Optimizes existing data pipelines and improves existing code quality
Makes updates and improvements to deployment processes
Participates in project planning and architecture discussions
Analyzes potential problems and finds solutions to pressing data issues
Participates in on-call rotation
Ideal Skills, Experience, and Competencies:
At least Four (4) years of relevant data engineering experience
Advanced experience with Python
Expert experience with SQL
Intermediate experience with a JVM language
Exposure to other software development languages
Advanced experience with Apache Spark or other distributed processing engines
Advanced experience with Apache Kafka or other stream processing frameworks
Intermediate experience with Terraform, Docker, Kubernetes, or other similar infrastructure tooling
Advanced experience with cloud data tools such as S3, Glue, and Athena
Intermediate experience with building CI/CD pipelines
Effective problem solving and troubleshooting abilities
Ability to consistently achieve results, even under tough circumstances
Effective technical leadership abilities
Excellent verbal and written communication skills
Experience with innovative application design and implementation
Ability to make sense of complex, high quantity, and sometimes contradictory information to effectively solve problems
Required Education & Certifications:
B.A./B.S. in related field or equivalent work experience
M.S./M.A. in related field or equivalent work experience
Compensation:
Qualified candidates can expect a salary beginning at $140,000 or more depending on experience
#LI-Remote #LI-JF1 #Dice-J #BI-Remote

Note: Compensation is benchmarked on local Denver Metro area market rates. Qualified candidates in other locations can expect a salary package that may be adjusted based off applicable cost of wages in their respective location.
At Pax8 we believe that your Total Rewards should include a benefits package that shows how much we value our greatest assets. All FTE Pax8 people enjoy the following benefits:
Non-Commissioned Bonus Plans or Variable Commission
401(k) plan with employer match
Medical, Dental & Vision Insurance
Employee Assistance Program
Employer Paid Short & Long Term Disability, Life and AD&D Insurance
Flexible, Open Vacation
Paid Sick Time Off
Extended Leave for Life events
RTD Eco Pass (For local Colorado Employees)
Career Development Programs
Stock Option Eligibility
Employee-led Resource Groups
Pax8 is an EEOC Employer.",4.1,"Pax8
4.1","Greenwood Village, CO",501 to 1000 Employees,2012,Company - Private,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
780,Junior Data Engineer,$83K - $119K (Glassdoor est.),"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a junior data engineer, you will be part of a team that is responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.
The position:
Develop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Support various components of the data pipelines, including ingestion, validation, cleansing and curation
Manage and ensure the success of ongoing data pipeline routines
Collaborate with our implementation team to assist with the identification and reconciliation of data anomalies
Create and maintain documentation on data pipelines
Engage with our software engineering team to ensure precise data points per application specifications
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering, or applicable experience
Expertise with SQL, database design and data manipulation methodologies
Expertise with ETL/ELT and the development of automated validation and data pipelines
Strong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW, master data management and other database design principles
Comfortable working with high volume data in a variety of formats
Excellent verbal and written communication
Self-motivated
Passionate at learning
Familiarity with healthcare data is a plus
Experience with CI/CD and version control tools is a plus
Experience working within hybrid cloud environment such as AWS is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation
Social outings",5.0,"Decision Point Healthcare
5.0","Boston, MA",1 to 50 Employees,2013,Company - Public,-1,-1,Less than $1 million (USD)
781,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
782,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
783,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
784,Data Engineer,$78K - $112K (Glassdoor est.),"Translate business requirements and functional specifications into logical program designs and to deliver code modules, stable application systems, and software solutions.
Develop, configure, or modify complex integrated business and/or enterprise application solutions within various computing environments.
Implement and maintain complex business and enterprise software solutions to ensure successful deployment of released applications.
Translate complex cross-functional business requirements and functional specifications into logical program designs, code modules, stable application systems, and software solutions.
Partner with Product Team to understand business needs and functional specifications. .
Collaborate with cross-functional teams to ensure specifications are converted into flexible, scalable, and maintainable solution designs.
Evaluate project deliverables to ensure they meet specifications and architectural standards.
Coordinate, execute, and participate in component integration (CIT) scenarios, systems integration testing (SIT), and user acceptance testing (UAT) to identify application errors and to ensure quality software deployment.
Participate in all software development end-to-end product lifecycle phases by applying and sharing an in-depth understanding of complex company and industry methodologies, policies, standards, and controls.
Architect software design patterns and approaches, application level software architecture and make technical trade-off decisions at application level.
Automate and simplify team development, test, and operations processes.
Develop detailed architecture plans for large scale enterprise architecture projects and drives the plans to fruition.
Solve complex architecture/design and business problems; solutions are extensible; work to simplify, optimize, remove bottlenecks, etc.
Minimum Education Required:- All the responsibilities mentioned above are in line with the professional background and requires an absolute minimum of bachelor’s degree in computer science, computer information systems, information technology or a combination of education and experience equating to the U.S. equivalent of a bachelor's degree in one of the aforementioned subjects.",4.9,"Wise Equation Solutions
4.9","Mooresville, NC",51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
785,"Data Center Network Engineer, Infrastructure Network Engineering",-1,"What to Expect
Tesla is currently seeking a Network Engineer to join our Data Center team. This role will provide network design, implementation, and operational support for Tesla's Data Centers.
What You’ll Do
Help design, build and maintain new and existing Data Centers
Work closely with other team members on design and initiatives; maintain and grow existing data center networks.
Work with Tesla’s key application teams to support their growth, including Tesla Autopilot team.
Provide high availability & reliability to network
Requirements gathering, analyze, and propose solution to networking needs.
Monitor, analyze, and report metrics of network services.
Develop automation methods to rapidly deploy, configure, and update network equipment.
Assist with network troubleshooting.
Conduct product POC evaluation.
Document network knowledge base and operational “Run-Book.”
Must be able to work occasional weekends, after hours, and holidays.
Participate in on call rotation.
May require unscheduled after-hours work. 10-20% travel required as necessary.
What You’ll Bring
4+ years’ experience mid-large global enterprise networking infrastructure
Experience with mid/large-scale networks in a global environment
Juniper, Arista and Palo Alto Networks hardware
Experience in IP networking, L2/L3 network protocols (spanning-tree, OSPF, BGP), TCP/IP, DHCP, DNS, end to end QOS, VLAN, VRRP, LACP, MC-LAG, EVPN with VXLAN, ACL and infrastructure cabling.
Basic knowledge of AWS, Azure or GCP.
Experience with various tools such as Protocol Analyzer, SNMP, flow, IPAM, RADIUS, Splunk, network taps, and load/stress testing",3.6,"Tesla
3.6","Fremont, CA",10000+ Employees,2003,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$1 to $5 billion (USD)
786,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1.0,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
787,AWS Data Engineer Snowflake,-1,"Company Description

Orchestrated by adept technical architects with over fifty years of applied expertise, KYNITE is an advanced technology company specializing in the disciplines of: Blockchain, Cloud Services, Big Data & Analytics, Artificial Intelligence, Enterprise, Staff Augmentation and Managed Services
We are BigData Experts
We are Cloud Experts
We are Enterprise Architects
We are Artificial Intelligence Innovators
We are Technological Evangelists
We are Doers
We are Kynite

Additional Information

All your
This job is only for individuals residing in US
US Citizens, Green Card holders, EAD's can apply
W2, C2c,
Information will be kept confidential according to EEO guidelines.",-1.0,Kynite,"Myrtle Point, OR",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
788,Data Analytics Engineer,Employer Provided Salary:$57K - $113K,"Abbott is a global healthcare leader that helps people live more fully at all stages of life. Our portfolio of life-changing technologies spans the spectrum of healthcare, with leading businesses and products in diagnostics, medical devices, nutritionals and branded generic medicines. Our 115,000 colleagues serve people in more than 160 countries.
Job Description – Software Engineer II - Big Data
About Abbott
Abbott is a global healthcare leader, creating breakthrough science to improve people’s health. We’re always looking towards the future, anticipating changes in medical science and technology.

Working at Abbott
At Abbott, you can do work that matters, grow, and learn, care for yourself and family, be your true self and live a full life. You’ll also have access to:
Career development with an international company where you can grow the career you dream of.
Free medical coverage for employees* via the Health Investment Plan (HIP) PPO
An excellent retirement savings plan with high employer contribution
Tuition reimbursement, the Freedom 2 Save student debt program and FreeU education benefit - an affordable and convenient path to getting a bachelor’s degree.
A company recognized as a great place to work in dozens of countries around the world and named one of the most admired companies in the world by Fortune.
A company that is recognized as one of the best big companies to work for as well as a best place to work for diversity, working mothers, female executives, and scientists.

The Opportunity
At Abbott, we believe people with diabetes should have the freedom to enjoy active lives. That’s why we’re focused on helping people with diabetes manage their health more effectively and comfortably, with life-changing products that provide accurate data to drive better-informed decisions. We’re revolutionizing the way people monitor their glucose levels with our new sensing technology.

Our location in Alameda, CA currently has an opportunity for a Data Analytics Engineer
Interested in applying your wealth of technical knowledge and experience towards an opportunity in the medical field and improving the lives of people with diabetes? The candidate will be responsible for big data engineering, data wrangling, and data analysis in the Cloud. The role will also contribute to defining and implementing Big Data Strategy for the organization along with driving implementation of IT solutions for the business. Candidate will be working with other data engineers and data scientists to focus on applying data engineering, data science and machine learning approaches to solve business problems.

Candidate should be able to work on a distributed project team along with data scientists to develop data pipelines capable of handling complex data sets quickly and securely as well as operationalize data science solutions. Candidate will be working in a technology driven environment utilizing the latest tools and techniques such as Redshift, S3, Lambda, DynamoDB, Sparc and Python.
#Software
Job locations: Alameda, CA, Bend, OR; Orlando, FL
What You’ll Work On
Collect and process raw data at scale for a variety of projects and initiatives.
Create and maintain optimal data pipeline architecture by designing and implementing data ingestion solutions on AWS using AWS native services.
Design and optimize data models on AWS Cloud using AWS data stores such as Redshift, RDS, S3.
Integrate and assemble large, complex data sets that meet a broad range of business requirements.
Design and develop data applications using selected tools and frameworks as required and requested for a variety of projects and initiatives.
Read, extract, transform, stage and load data to selected tools and frameworks as required and requested.
Monitoring and optimizing data performance.
Customizing and managing integration tools, databases, warehouses, and analytical systems
Process unstructured data into a form suitable for analysis and assist in analysis of the processed data.
Working directly with the technology and engineering teams to integrate data processing and business objectives.
Ensure performance, uptime, and scale, maintaining high standards of code quality and thoughtful design.
Required Qualifications
Bachelors Degree in Computer Science, Information Technology or other relevant field.
At least 1 to 3 years of recent experience in Software Engineering, Data Engineering or Big Data.
Ability to work effectively within a team in a fast-paced changing environment.
Software development experience, ideally in Python, Spark, Kafka or Go, and a willingness to learn new software development languages to meet goals and objectives.
Knowledge of strategies for processing large amounts of structured and unstructured data, including integrating data from multiple sources.
Knowledge of data cleaning, wrangling, visualization and reporting.
Ability to explore new alternatives or options to solve data mining issues, and utilize a combination of industry best practices, data innovations and experience.
Exposure to databases, BI applications, data quality and performance tuning.
Excellent written, verbal and listening communication skills.
Preferred Qualifications
Knowledge of or direct experience with the following AWS Services desired S3, RDS, Redshift, DynamoDB, EMR, Spark, and Lambda.
Knowledge of or direct experience with Hadoop, Snowflake or BigQuery.
Experience working in an agile environment.
Practical Knowledge of Linux.
Apply Now

Participants who complete a short wellness assessment qualify for FREE coverage in our HIP PPO medical plan. Free coverage applies in the next calendar year.

Learn more about our health and wellness benefits, which provide the security to help you and your family live full lives: www.abbottbenefits.com

Follow your career aspirations to Abbott for diverse opportunities with a company that can help you build your future and live your best life. Abbott is an Equal Opportunity Employer, committed to employee diversity.

Connect with us at www.abbott.com, on Facebook at www.facebook.com/Abbott and on Twitter @AbbottNews and @AbbottGlobal.

The base pay for this position is $56,700.00 – $113,300.00. In specific locations, the pay range may vary from the range posted.",3.8,"Abbott Laboratories
3.8","Orlando, FL",10000+ Employees,1888,Company - Public,Health Care Services & Hospitals,Healthcare,$10+ billion (USD)
789,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
790,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
791,Sr. Data Engineer,$83K - $115K (Glassdoor est.),"Provides application design, analysis, programming, testing and documentation for the development, integration, enhancement, and maintenance of information systems solutions
Job Description:
BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Job Responsibilities
Formulates and defines system scope and objectives through research and fact-finding to design, develop, modify, or integrate complex information systems.
Devises or modifies application systems and procedures to optimize functional requirements including capacity, operating time, response time, and form of desired results.
Designs, codes, tests, debugs, and documents programs, subroutines, and scripts.
Develops and implements program/system test plans. Devises data verification methods and standard system procedures.
Responds to system failures and performance events by taking appropriate measures to reduce system downtime and eliminate recurrence of problems.
May conduct research on software and hardware products to justify recommendations and support management in budgeting and purchasing efforts.
Acts as expert technical resource to development staff in all phases of the development and implementation process.
Job Qualifications
Education
Bachelors Degree in Computer Science or equivalent work experience required. Equivalent years of experience are determined as one year of technical experience for every year of college requested.
Experience
1 year - Experience with information technology concepts, application development methodology, terminology, and standards required
5 years - Systems and programming experience required
1 year - Expert working knowledge of at least one programming language in use at BCBST.
Skills\Certifications
Proven skill with information technology in use at BCBST is required
Proven analysis, design, and coding skills, and demonstrated success in leading large and complex projects
Top level technical experience in one or more specialized areas of applications systems analysis and programming
Demonstrated ability to interpret and translate technical and/or or complex concepts into information meaningful to project team members and/or business personnel.
Be organized, reliable, and able to manage multiple tasks with exceptional work ethic.
Leadership skills
Must be able to communicate effectively with both technical and non-technical co-workers
Job Specific Requirements:
Preference for candidates with Cloud experience (GCP specifically)
Python knowledge/experience a plus
Preferred Skills:
Number of Openings Available:
1
Worker Type:
Employee
Worker Sub-Type:
Employee
Company:
BCBST BlueCross BlueShield of Tennessee, Inc.
Applying for this job indicates your acknowledgement and understanding of the following statements:
BCBST is an Equal Opportunity employer (EEO), and all employees and applicants will be entitled to equal employment opportunities when employment decisions are made. BCBST will take affirmative action to recruit, hire, train and promote individuals in all job classifications without regard to race, religion, color, age, sex, national origin, citizenship, pregnancy, veteran status, sexual orientation, physical or mental disability, gender identity, or any other characteristic protected by applicable law.
Further information regarding BCBST's EEO Policies/Notices may be found by reviewing the following page:
BCBST's EEO Policies/Notices
BlueCross BlueShield of Tennessee is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at BlueCross BlueShield of Tennessee via-email, the Internet or any other method without a valid, written Direct Placement Agreement in place for this position from BlueCross BlueShield of Tennessee HR/Talent Acquisition will not be considered. No fee will be paid in the event the applicant is hired by BlueCross BlueShield of Tennessee as a result of the referral or through other means.
All applicants will be advised that BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Tobacco-Free Hiring Statement
To further our mission of peace of mind through better health, effective 2017, BlueCross BlueShield of Tennessee and its subsidiaries no longer hire individuals who use tobacco or nicotine products (including but not limited to cigarettes, cigars, pipe tobacco, snuff, chewing tobacco, gum, patch, lozenges and electronic or smokeless cigarettes) in any form in Tennessee and where state law permits. A tobacco or nicotine free hiring practice is part of an effort to combat serious diseases, as well as to promote health and wellness for our employees and our community. All offers of employment will be contingent upon passing a background check which includes an illegal drug and tobacco/nicotine test. An individual whose post offer screening result is positive for illegal drugs or tobacco/nicotine and/or whose background check is verified to be unsatisfactory, will be disqualified from employment, the job offer will be withdrawn, and they may be disqualified from applying for employment for six (6) months from the date of the post offer screening results.
Resources to help individuals discontinue the use of tobacco/nicotine products include smokefree.gov or 1-800-QUIT-NOW.",3.8,"BlueCross BlueShield of Tennessee
3.8","Chattanooga, TN",5001 to 10000 Employees,1945,Nonprofit Organization,Insurance Carriers,Insurance,$5 to $10 billion (USD)
792,Data Visualization Engineer,$77K - $101K (Glassdoor est.),"Passionate about precision medicine and advancing the healthcare industry?
Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.
With the advent of genomic sequencing, we can finally decode and process our genetic makeup. We now have more data than ever before but providers don't have the infrastructure or expertise to make sense of this data. We're on a mission to connect an entire ecosystem to redefine how genomic data is used in clinical settings.
What You'll Do:
Own the design, development and administration of advanced analytics dashboards and reporting and business intelligence tools. Beginning with requirements gathering across cross-functional teams, to the delivery of analytic solutions into production.
Collaborate with stakeholders, providing guidance, data and analytical support for developing new datasets, analyses and tools in support of strategic initiatives.
Perform deep-dive root cause and problem-solving analyses for and with laboratory, business operations and engineering teams.
Design and build data views to support advanced analytical reporting, self-service data analysis and exploration, utilizing multiple data and visualization technologies.
Work closely with technical and product teams to identify and align changes to internal systems in support of future data analysis needs and requirements.
Develop and facilitate data governance initiatives to assure data integrity across distributed systems.
Solve new and interesting problems.
Qualifications:
Bachelor's degree in Information Systems, Business Administration, Finance, Economics, Mathematics, Operations Research, Marketing or similar
3+ years of experience developing data sets for analysis use-cases
3+ years of experience with direct stakeholder management and ownership of full lifecycle of analytics/data projects and presenting analysis results to stakeholders
3+ years report-development and data visualization experience with BI tools (Looker, Tableau, etc.)
Ability to deliver both high-level explanations of complex problems while also able to deep dive into complex problems
Advanced SQL skills for querying and analysis of large, complex datasets
Must work well in a team environment and be able to partner with both business and technical team members and translate technical language to the non-technical need
Creative problem solver, ability to handle multiple projects, and strong work ethic needed
Excellent written and verbal communication skills
Highly organized and systematic, superb attention to detail
Able to complete tasks with a high degree of accuracy and an eye towards process improvement
Persistent and resilient person who thrives in a fast-paced, highly collaborative environment
Bonus points for:
Advanced analytics experience - classification/predictive modeling, optimization models, forecasting
Experience with Python for data engineering or advanced data analysis projects/use-cases
Healthcare domain knowledge and experience
Expert knowledge of database/data modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others
Experience building cloud-native applications and supporting technologies / patterns / practices including: GCP, AWS, Docker, CI/CD, DevOps, and microservices

#LI-EH1
#LI-Hybrid
We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.1,"Tempus
3.1","Chicago, IL",501 to 1000 Employees,2015,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
793,Power BI/Data Engineer,$73K - $112K (Glassdoor est.),"ClinDCast is seeking a skilled and motivated Power BI/Data Engineer to join our dynamic team. This role involves designing, developing, and maintaining data pipelines, transforming raw data into meaningful insights using Power BI, and contributing to data engineering efforts. The ideal candidate should possess a strong background in data processing, ETL (Extract, Transform, Load) processes, and visualization using Power BI. This role requires a blend of technical expertise, creativity, and a strong commitment to delivering high-quality data solutions.
Data Pipeline Development: Design, implement, and maintain robust data pipelines to extract, transform, and load data from various sources into target data repositories.
Data Transformation: Develop ETL processes to clean, transform, and enrich data, ensuring data quality, consistency, and accuracy.
Data Modeling: Design and optimize data models for efficient data storage, retrieval, and analysis in collaboration with data scientists and analysts.
Power BI Development: Create interactive and insightful Power BI dashboards and reports that provide actionable insights to various stakeholders.
Data Integration: Integrate data from different systems and sources, ensuring seamless data flow and integration points between various applications.
Performance Tuning: Identify and resolve performance bottlenecks in data pipelines, ETL processes, and Power BI reports to ensure optimal performance.
Data Governance: Implement data governance practices to ensure data privacy, security, compliance, and adherence to data quality standards.
Collaboration: Work closely with cross-functional teams including data scientists, analysts, business stakeholders, and IT teams to understand requirements and deliver solutions that meet business objectives.
Documentation: Maintain clear and concise documentation for data pipelines, transformations, and Power BI reports to facilitate knowledge sharing and future maintenance.",4.0,"ClinDCast LLC
4.0","Tampa, FL",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
794,Sr. Data Engineer- Contractor- Remote Work Eligible,-1,"Notes to applicants:
This position is eligible for full-time remote work in Texas, or North Carolina, or, in the alternative, to work in accordance with Dimensional’s best-of-both hybrid working model, which involves working in the office on Tuesdays, Wednesdays and Thursdays, and choosing to work in the office or remotely on Mondays and Fridays.
Resumes and portfolios (when applicable) are required as part of your application. When applying from a mobile device or tablet, you may not be able to attach a resume. If you cannot include an attachment at the time of your application, you will receive a follow up email asking you to attach your resume from a computer.
Here at Dimensional, we strive to be an inclusive workplace for all. Even if you do not match every qualification listed, if you are interested in who we are, what we do, and why we do it, we suggest and encourage you to apply.

Job Description:
About Dimensional:
Dimensional was built around a set of ideas bigger than the firm itself. With a confidence in markets, deep connections to the academic community, and a focus on implementation, we go where the science leads, and continue to pursue new insights, both large and small, that can benefit our clients.
The Technology Department at Dimensional leverages the rapidly evolving state of the art to engineer scalable, innovative, and research driven solutions to improve our client’s financial lives.
Software Engineers at Dimensional participate in the design and development of software solutions across an array of domains from Research and Investments to Sales and Marketing; collaboratively developing MVPs to test their ideas and rapidly iterate with constant feedback from users. Dimensional invests heavily in developer tools, platforms, paradigms and experience enabling teams to provide modern solutions that contribute profoundly to our client’s success.
We are looking for a Python Data Engineer to join our team and translate our customers’ goals into working software throughout the stack from automated configurations to model definitions, calculation APIs, and building robust data pipelines. The most important qualifications are a passion for quality software and enthusiasm for learning new technologies and approaches. The level of seniority for this position is negotiable based on experience.
You may be a fit for this role if you:
Are open-minded, curious, and resourceful
Are passionate about/stay current with modern technologies
Solve problems systematically and transparently
Share ideas, solicit/integrate feedback, design and solve collaboratively
Take a software engineering approach and demonstrate automation and security mindsets
What you might work on:
As a Data engineer at Dimensional, you will have the opportunity to understand the users’ needs and solve problems at all levels of the stack from automating infrastructure and deployments to building complex data pipelines to designing user friendly data applications.
Collaborate with subject matter experts in a variety of areas to drive the success of our clients
Perform software and data architecture and design
Develop complex software solutions using ETL and/or back-end technologies
Demonstrate and mentor software engineering best practices and participate in code reviews
Develop configurations and automations to enable testing, infrastructure and deployments
The successful candidate will be self-motivated and have a strong drive for learning and self-improvement.
Qualifications:
Bachelor’s degree in a technical field or equivalent practical experience.
5-10+ years of software development experience in a professional and/or academic setting (seniority of the role is negotiable).
5+ years of hands-on experience in developing ETL solutions using python.
Working knowledge of DevOps concepts, tools, and continuous delivery pipelines such as Octopus, TeamCity, Stash, Bitbucket, Jira, GIT, etc.
Advanced SQL knowledge and experience working with relational databases and working familiarity with various cloud data warehouses.
Experience in building processes supporting data transformation, data structures, metadata, dependency, and workload management.
Experience with data pipeline and workflow management tools: Airflow, etc.
Preferred Competencies:
Interest and ability to learn other coding languages as needed
Ability to write in English fluently and idiomatically
Advanced degree or equivalent experience in engineering, computer science or other technical related field
Experience with agile/scrum methodologies
Financial services industry experience
Experience with any of the following:
Redis, Postgresql, MongoDB, SQLServer
Airflow, Kafka, AWS, serverless/microservice architecture
TDD, BDD, Numpy/Scipy/Pandas, Ansible

#LI-Remote

Dimensional offers a variety of programs to help take care of you, your family, and your career, including comprehensive benefits, educational initiatives, and special celebrations of our history, culture, and growth.
It is the policy of the Company to provide equal employment opportunity for all applicants and employees. The Company does not unlawfully discriminate on the basis of race, color, religion, creed, sex, gender, gender identity, gender expression, national origin, age, disability, genetic information, ancestry, medical condition, marital status, covered veteran status, citizenship status, sexual orientation, or any other protected status. This policy applies to all areas of employment including recruitment, hiring, training, job assignment, promotion, compensation, benefits, transfer, discipline, termination, and social and recreational programs.",3.7,"Dimensional Fund Advisors
3.7",Texas,1001 to 5000 Employees,1981,Company - Private,Investment & Asset Management,Financial Services,$1 to $5 billion (USD)
795,Data Engineer,$77K - $104K (Glassdoor est.),"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Paid time off
Referral program
Vision insurance
Schedule:
Monday to Friday
No nights
No weekends
Ability to commute/relocate:
Atlanta, GA 30309: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 2 years (Required)
Language:
English (Required)
Work Location: Hybrid remote in Atlanta, GA 30309",3.7,"United Digestive
3.7","Atlanta, GA",501 to 1000 Employees,2018,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
796,Data Engineer / Architect (Remote - US based),Employer Provided Salary:$101K - $216K,"Company Description

Due to the nature of this role, candidates must be geographically located, and authorized to work, in the United States.

Tidepool is a leading healthcare technology nonprofit company that is revolutionizing the way people with diabetes manage their condition. Our mission is to make diabetes data more accessible, actionable, and meaningful, empowering individuals and healthcare providers to make informed decisions and improve health outcomes. By leveraging cutting-edge technology and data-driven solutions, Tidepool is dedicated to making a positive impact on the lives of millions affected by diabetes.

Job Description

We are seeking a skilled and motivated Data Engineer / Architect to join our growing team. As a Data Engineer at Tidepool, you will play a pivotal role in designing, implementing, and optimizing our diabetes, product and business data infrastructure pipeline. You will collaborate closely with our Data Science and Data Analytics teams to ensure efficient data flow and enable advanced analytics and insights. Your expertise in data architecture, ETL processes, and database management will contribute to the development of innovative solutions that transform raw data into actionable information.

Essential Duties and Responsibilities
Design, develop, and maintain scalable and robust data pipelines, ensuring the efficient extraction, transformation, and loading (ETL) of data from various sources.
Collaborate with cross-functional teams, including Product, Engineering, to understand data requirements and develop data models that facilitate advanced analytics, machine learning, and predictive modeling. Work closely with Data Scientists and Data Analysts to ensure that our data pipeline architecture meets their needs.
Optimize data storage and retrieval systems to ensure high performance, scalability, and data integrity.
Implement and maintain data governance practices, including data quality monitoring, data lineage tracking, and metadata management.
Troubleshoot data-related issues and perform root cause analysis, ensuring timely resolution to minimize impact on data availability and accuracy.
Continuously evaluate and recommend improvements to existing data infrastructure, tools, and processes to enhance efficiency and reliability.
Stay up to date with emerging technologies, industry trends, and best practices in data engineering, and apply this knowledge to drive innovation within the team.

Qualifications
Bachelor's degree in Computer Science, Engineering, or a related field. Advanced degree is a plus.
Proven experience as a Data Engineer or similar role, with a strong understanding of data warehousing concepts, data modeling, and ETL processes.
Proficient in SQL and programming languages such as Python, Java, or Scala.
Experience with data processing tools and frameworks like Data Bricks or Apache Spark is highly desirable.
Solid knowledge of relational and NoSQL databases, data lakes, and data integration techniques.
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data services, such as S3, Redshift, BigQuery, or similar.
Experience in implementing data governance practices and ensuring data quality and integrity.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication skills with the ability to effectively articulate complex technical concepts to non-technical stakeholders.
Familiarity with diabetes or other types of health data is a plus
Understanding and knowledge of diabetes treatments, therapy and diabetes data is a big plus.

Additional Information

Salary range

$101,028 - $216,027 annually. To learn more about Tidepool's compensation philosophy please see Tidepool's Employee Handbook.
Benefits
Flexible PTO
Paid parental leave
Medical, Dental, and Vision coverage
Health and Childcare FSA
Flexible work schedule
Wellness and Productivity stipend
Continuing Education Reimbursement
Other Information
While many of Tidepool’s team members have a personal connection to diabetes, this is not a requirement. We ask that you have empathy for chronic conditions and you are prepared to learn about the diabetes experience.
This is a remote position. You’ll be working from home and interacting with a team of colleagues that works around the world. Learn more about working at Tidepool, including our approach to inclusion and diversity in this blog post.
Tidepool is an Equal Opportunity Employer. The company supports diversity and inclusion in its core values and does not discriminate against qualified employees or applicants because of race, color, religion, gender identity, sex, sexual preference, sexual identity, pregnancy, national origin, ancestry, citizenship, age, marital status, physical disability, mental disability, medical condition, military status, or any other characteristic protected by U.S. federal or state law or local ordinance. When necessary, the company will reasonably accommodate employees and applicants with disabilities if the person is otherwise qualified to safely perform all of the essential functions of the position.",-1.0,Tidepool,"Palo Alto, CA",1 to 50 Employees,2013,Nonprofit Organization,Health Care Services & Hospitals,Healthcare,$5 to $25 million (USD)
797,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department.
The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
798,"Staff Engineer, Data Infrastructure",Employer Provided Salary:$180K - $240K,"The Mission:
This is a critical and exciting time at Enigma. We're transforming the small business financing ecosystem, and our product is gaining adoption even faster than we anticipated. This creates a new set of technical challenges for us as we continue to scale.
Over the past year we've made significant investments in our data infrastructure to allow engineers and data scientists to quickly deliver customer value by reliably testing and shipping changes to our data pipeline. The challenges ahead are to:
Reduce overall runtime of our data pipeline
Improve computational efficiency to reduce costs
The Role:
This is a role responsible for developing and implementing our strategy for driving data processing efficiency. Your impact will be measured by:
The increase in the team's capability to optimize data transformations (combination of methodology and tooling)
The specific improvements we achieve in processing time and computational cost
This role has an important hands-on component - you'll be showing the team what good looks like. However, your greatest impact will be in the technical strategy you'll formulate and the close coaching and mentorship you'll provide to team members.
We're looking for someone who:
Is motivated by leading engineers to increase the efficiency and speed of complicated data processing systems
Thrives as a coach and mentor and measures their impact by the increase in capabilities of the team
Operates with a bias for action and knows how to deliver value in the short, medium and long term
Adopts a principled approach to difficult data processing problems and demonstrates expertise and excellence in their engineering craft
Operates transparently, collaboratively and with low ego—loves learning from others and having their ideas questioned and challenged
What makes this job interesting?
Impact: Your decisions will determine the speed at which we can scale to take on new customers and impact the developer experience of dozens of teammates.
Technical Challenge: You will take on some of our thorniest technical problems and render them tractable.
Leadership: Success in this role will achieved through building the team's capabilities and influencing our engineering culture.
Our ideal candidate:
Knows Databricks and Spark inside and out
Brings a clear point of view on data processing optimization, data modeling, cluster management and Databricks performance techniques
Has a strong track record of technical leadership to grow and scale teams
About Us:
At Enigma, we're building the single, most reliable source of data on businesses to power the future of financial services. By engineering better data from hundreds of public and third-party sources, we aim to tell the complete story of every business, so that companies of every size can access the financial services they need to grow and thrive. Our core values – generosity, curiosity, ingenuity, & drive – guide everything we do, from how we make our most important product decisions to how we work with and support one another on a daily basis. We're a team of curious, driven individuals with diverse backgrounds and skills, but we're all passionate about engineering deeper understanding through data—together. If this resonates, we would love to hear from you!
We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.
Salary Range: $180,000-$240,000
A note on salary ranges: we pride ourselves on paying competitively for our size and industry. Salary is one piece of a total Enigma compensation package that includes additional benefits and opportunities. All of our compensation packages include equity because we believe 100% of Enigma employees should have the option to purchase ownership in the company and benefit from the value we're creating together.",-1.0,Enigma,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
799,Data Engineer,$88K - $119K (Glassdoor est.),"About the Job

Loopback is hiring an innovative and team-oriented Data Engineer/Operation who will be responsible for building and maintaining optimized and highly available data pipelines that facilitate deeper analysis and reporting across clinical sites in areas of Life Sciences industry analytics.

This engineer’s role will be to manage the data flow processes, analyze data, and lead partnerships with other Data and Analytics teams to identify and implement systems and process improvements. This engineer also designs, architects, implements, and supports key datasets.

Duties to Include

Manage ongoing shifts in data ingestion formats across Health Systems, Life Science, and Enterprise Partner types
Own code base, documentation, and roadmap for all data transformations via Data Lake and underlying Tables
Develop and sequence jobs and processes to transform data into data lake and data warehouse
Own integral Loopback data model, code base, and software to deliver against client/application SLA's
Design and develop highly scalable and reliable data engineering pipelines to process large volumes of data across diverse data sources and analytics use cases
Identify, design, and implement internal process improvements by automating manual processes and optimizing data delivery
Plan, coordinate and implement security measures to safeguard information in computer files against accidental or unauthorized damage, modification or disclosure.
Identify and implement ways to improve data reliability, efficiency, and quality
Develop and promote and implement best practices in data engineering
Business meetings to understand use cases and questions, capture agreement on business rules, understand analyst and stakeholder objectives, and support usage of data to solve business problems
Data profiling, data documentation, and measuring data quality with manual verification and
development of automated data quality tests

Requirements

You will thrive if you:
Exhibit a “self-starter” mindset in taking ownership over delegated responsibilities
Have excellent program/task organizational skills
Are detail and results oriented

You bring a toolkit of your past experiences of:
3-5 Years of experience as a Data Engineer/Operations
Implementing and designing data curation, and data analysis
Orchestrating data transformation through cloud native analytics platforms (Snowflake, Databricks) across cloud environments (Azure, AWS, GCP)
Building and modeling data in relational and non-relational data storage technologies including schema design, stored procedure development and performance and optimization techniques (e.g. SQL & NoSQL, C#, Python, etc.)
Effective collaboration, experienced in creating technical partnerships across teams
Learning and understanding the various technical domains across the enterprise and able to communicate complex technical and business concepts across the enterprise and various business stakeholders
Effectively delivering results in a fast-paced environment while managing multiple priorities
Documenting and testing your designed solutions, engaging with QA and DevOps teams
Contact

Founded in 2009, Loopback was rated as one of the best places to work in Dallas by the DBJ, Loopback Analytics is a leading provider of data-driven solutions for hospitals and health systems. The company’s comprehensive analytics platform drives growth for specialty and
ambulatory pharmacy programs while connecting pharmacy activities with clinical and economic outcomes. Loopback’s clients include leading Academic Medical Centers, health systems, and Life Sciences companies. For more information about our company and services please visit our website at www.loopbackanalytics.com.

This employer will not sponsor applicants for employment visa status (e.g., H1-B) for this position. All applicants must be currently authorized to work in the United States on a full-time basis.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.",4.4,"Loopback Analytics
4.4","Dallas, TX",51 to 200 Employees,2009,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
800,"Data Engineer, Election Platforms (all-levels)",-1,"Job Description
The Washington Post is hiring a data engineer for our Election Platforms team, to work on our data pipelines, tools, and infrastructure ahead of the 2024 presidential election.
The Election Platforms team focuses on leveraging data, partnering with stakeholders, and pioneering serviceable tools to facilitate The Post’s election coverage. This role is part of Elections Engineering, a cross-functional group of full-stack engineers and data scientists, with varied technical and academic backgrounds, responsible for delivering live election results across The Post’s platforms.
The role will be focused on contributing to our technical stack and developing new technology to facilitate robust and lively election coverage. In particular, we’re looking for candidates who would be comfortable working on deploying high-impact code in production and have an interest in data and newsroom tooling. We also work on other data engineering projects related to elections, such as voter registration data and campaign finance, and occasionally write
open
source
software
. Most of our work is in Python and TypeScript/React/Node.
We typically work at a higher level of abstraction than a single story, and aim to create general purpose tools that are reusable and easy to deploy. Our ideal candidates are creative problem-solvers, who are comfortable writing production level code to solve real-world newsroom problems. This position is remote-friendly for work in the US.
Motivation
You have a desire to contribute to The Post’s journalism through technology.
You are passionate about refining and improving team process, culture, documentation and communication while tackling complex data science and engineering challenges.
You want to work at a deeper level of abstraction than a single story or beat.
You are excited about learning new technologies and applying them to the practice of journalism.
Responsibilities
Work on the software components of our live election architecture including deployment, observability, participating in code reviews, and more.
Work closely with our engineers and data scientists to develop and maintain pipelines and administrative apps/dashboards for election data.
Work on tooling to make data preparation easier and more efficient for our live election model.
Work with newsroom stakeholders to assess needs, build useful and reusable tooling, and contribute to storytelling.
Foster effective team collaboration and communication by improving and documenting internal tools, processes and guidelines.
Assist in attracting and hiring team members who want to work at the intersection of journalism and technology by actively participating in recruiting – and contributing feedback on our overall hiring process.
Qualifications
A minimum of one year of experience building and maintaining backend and data engineering applications.
Proficiency across some of the languages and frameworks used in our engineering stack such as Python, TypeScript/React/Node, and AWS services.
A desire to work on cutting edge, cross-functional tools to help with features, data science, and machine learning/big data work in a team setting.
Experience with election data is not necessary, but is a plus.
Experience developing and deploying newsroom tools and applications is not necessary, but is a plus.
Levels
If you have less than three years, but at least one year, we’d love to evaluate you for a data engineer role.
If you have more than three years but less than about seven years, we’d love to evaluate you for a senior data engineer role.
If you have more than about seven years, we’d love to talk to you about a principal data engineer role.
Wherever you are in your life or career, The Washington Post offers comprehensive and inclusive benefits for every step of your journey:
Competitive medical, dental and vision coverage
Company-paid pension and 401(k) match
Three weeks of vacation and up to three weeks of paid sick leave
Nine paid holidays and two personal days
20 weeks paid parental leave for any new parent
Robust mental health resources
Backup care and caregiver concierge services
Gender affirming services
Pet insurance
Free Post digital subscription
Leadership and career development programs
Benefits may vary based on the job, full-time or part-time schedule, location, and collectively bargained status.
The Post strives to provide its readers with high-quality, trustworthy news and information while constantly innovating. That mission is best served by a diverse, multi-generational workforce with varied life experiences and perspectives. All cultures and backgrounds are welcomed.
The innovation doesn’t end in the Newsroom – dozens of teams power The Washington Post. We are now hiring the next innovator – how will you Impact Tomorrow?
#washpostlife",4.2,"The Washington Post
4.2","Washington, DC",1001 to 5000 Employees,1877,Company - Private,Publishing,Media & Communication,Unknown / Non-Applicable
801,Data Platform Engineer - Python,-1,"As the Data Platform Engineer, you will use your significant experience in Microsoft Azure to design and set up projects that combine information from various sources to enable analysis and decision-making as well as play a key role in implementing scalable and secure solutions that meet specific business requirements. Candidates must be able to work EST and travel to the DC area monthly to bimonthly.
Skills:
Develop Python code to ingest and transform Excel data, perform calculations, and check data quality
Troubleshoot and resolve issues related to ingesting and preparing data for visualization
Develop, test, and maintain production-level data pipelines running on a laptop and in the cloud
Support code refactoring to improve pipeline efficiency
Maintain data analytics (NLP, text classification, time series & forecast models)
Plan and deliver data warehouse and storage.
Design and run data services for individual projects.
Design, develop, adapt, and maintain data warehouse architecture and relational databases to support data mining.
Customize storage and extraction, meta-data, and information repositories.
Create and use effective metrics and monitoring processes.
Monitor key performance indicators to determine where current data operations can be improved.
Create the building blocks for transforming enterprise data solutions
Design and build modern data pipelines, data streams, and data service APIs
Create/maintain report forms and formats, information dashboards, data generators, scanned reports, and other information portals and resources
Excellent written and verbal communication skills in English.
Qualifications:
Minimum 5 years of work experience.
BS in Computer Science, Applied Mathematics, Statistics, or Machine Learning (or +3 years)
3 years of experience as an Azure or AWS Data Engineer in technology consulting
3 years of experience performing data engineering, warehousing, publishing and visualization throughout the full data lifecycle
1 year of experience defining ETA architecture and ETL process design
2 years of experience performing end-to-end implementation of data warehousing analytics solutions built on MS or Azure platforms
2 years of experience with Python, Databricks, Azure Synapse, SQL Server, Azure Data Lake, and/or Azure Data Factory (ADF).


About Harvard Partners, LLP, Trusted Advisors to IT:

Harvard Partners is a management consulting firm focused on helping companies more effectively leverage their IT investment. We engage with the C-Suite and Technology Team to help them better understand their IT infrastructure and process in order to align the technology strategy and organization to reach the firm’s strategic business goals.Some of our practices include:• Program/Project Management and ""PMO as a Service""• IT Assessments• Business Continuity/Disaster Recovery• Optimized Infrastructure• Concierge Managed Services• Data Center Strategy, Transformation, and Migration• Cloud Management Programs• Security Assessments and Remediation• Staffing, technical & tacticalWorking with the client’s staff, vendors, and consultants, we deliver supportive and collaborative engagements where direct dialog, simplified reporting, productive meetings, and clear responsibility and accountability encourage active participation resulting in consensus-based business outcomes.",2.0,"Harvard Partners, LLP, Trusted Advisors to IT
2.0",Remote,Unknown,-1,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
802,Data Governance Engineer,Employer Provided Salary:$100K - $125K,"Performance Health is seeking a Data Governance Engineer to join our team. In this role, you will be responsible for leveraging advanced data analytics techniques to optimize our manufacturing operations, streamline supply chain logistics, enhance overall efficiency, and support master data governance initiatives.

Essential Job Duties & Responsibilities
Develop and implement predictive and prescriptive analytics models to forecast demand, optimize inventory levels, and improve production scheduling in alignment with healthcare industry standards.
Analyze and interpret large datasets from various sources, including production systems, distribution centers, and market trends, to identify opportunities for process improvement, cost reduction, and master data cleansing.
Collaborate with manufacturing, procurement, and distribution teams to define key performance indicators (KPIs), establish data-driven goals, and measure progress towards operational excellence.
Design and execute A/B tests to evaluate the impact of process enhancements and initiatives, continuously refining strategies for improved outcomes.
Utilize machine learning techniques for anomaly detection, fault prediction, and quality control to ensure compliance with regulatory standards and product quality assurance.
Develop data visualizations, dashboards, and reports to effectively communicate insights and recommendations to stakeholders at all levels.
Stay current with advancements in data science methodologies, tools, and technologies, and proactively identify opportunities to apply them to manufacturing and distribution challenges.
Collaborate with Operational Excellence and Data Governance and IT teams to ensure data accessibility, integrity, and security, and assist in data integration efforts.
Performs other duties as assigned

Job Qualifications
Bachelor’s degree in Data Analytics or related field
3-5 years of experience applying data science techniques to manufacturing, supply chain, or distribution challenges, preferably within the healthcare industry.
Proficiency in programming languages such as Python or R for data analysis and statistical modeling
Strong expertise in data manipulation, feature engineering, and data preprocessing techniques.
Proficiency in SQL for data querying and manipulation.
Experience with data visualization tools (e.g., Tableau, Power BI) to create clear and impactful visualizations.
Excellent problem-solving skills and the ability to work effectively in cross-functional teams.
Ability to travel 10% of the time, including overnight travel

Benefits
Our benefits include healthcare; insurance benefits; retirement programs; paid time off plans; family and parenting leaves; wellness programs; discount purchase programs.
This is a full-time position with a base salary range of $100,000-$125,000 plus benefits.

To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. The requirements listed above are representative of the knowledge, skills, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Performance Health is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to sex, gender, gender identity, sexual orientation, race, color, religion, national origin, disability status, protected Veteran status, age, genetic information, and any other characteristic protected by law.",2.7,"Performance Health Supply,LLC
2.7","Warrenville, IL",Unknown,-1,Company - Private,Health Care Products Manufacturing,Manufacturing,Unknown / Non-Applicable
803,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
804,Data Engineer,Employer Provided Salary:$130K - $175K,"Rightworks is hiring a Data Engineer in Fort Worth for a Multi-Billion dollar private equity fund in Fort Worth. This position is fully in-office in Fort Worth (client will pay relocation if needed). Compensation is aggressive & flexible, typical 10% above your previous compensation level with tremendous growth opportunities. (Expected compensation range for this data engineer will fall somewhere between 130 & 180,000 per year depending on experience). 50 hours per week, paid lunch, casual office atmosphere.
-Looking for data engineer with multiple years of SQL Server maintenance experience.
-Optimally would like a data engineer with formal education in either MIS, Mathematics, Software Engineering, or Computer Science
-SQL maintenance, Data Extraction, Transforms, Macros
-Database maintenance
Job Type: Full-time
Pay: $130,000.00 - $175,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Tuition reimbursement
Vision insurance
Schedule:
10 hour shift
Ability to commute/relocate:
Fort Worth, TX 76102: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What compensation range are you looking for?
Are you willing to work in-person in Fort Worth, TX 50 hrs a week?
Experience:
Microsoft SQL Server: 1 year (Required)
Work Location: In person",4.8,"RightWorks Inc.
4.8","Fort Worth, TX",51 to 200 Employees,2006,Company - Private,HR Consulting,Human Resources & Staffing,$25 to $100 million (USD)
805,"Manager, Data Engineer & ETL Processing",$83K - $118K (Glassdoor est.),"Company Description

About Spark Foundry:

Spark Foundry is a global media agency that exists to bring HEAT – Higher Engagement, Affinity, and Transactions – to brands. By combining flawless media fundamentals with aggressive innovation, Spark inspires consumers to pay more attention, to care more about our clients’ brands, and to buy more products and services from them.
Balancing the nimble spirit of a startup with the powerhouse soul of Publicis Media, Spark Foundry delivers the best of both worlds to a client roster that spans some of the world’s best and most beloved brands and companies. We combine boutique-caliber insights and service with the buying clout and first-look access of a global leader, bringing the heat to challenger brands that want to act like giants, and to giant brands that want to act like challengers.
With a bottom-up culture that celebrates diversity and aims for all voices to be heard, Spark has become a magnet for the industry’s best talent, with one of the best retention rates in the industry. And by applying a whole-person approach to professional and personal development, Spark develops a workforce that is well prepared for today’s challenges, and also poised to create meaningful careers in the years to come.
Because we know that heat arises the intersection of complementary forces, our professionals come from myriad disciplines and backgrounds: data, analytics, and insights, content and creative production, communications and strategy, finance and marketing, and sociology, psychology, and other liberal arts disciplines.

Job Description

Overview:
The Manager, Data Engineering & ETL Development is a key driver to build data platform leveraging best in class ETL practice and is also a strategic thinker and a talented data visualization expert with emphasis on dashboard reporting. The primary responsibility will be developing business intelligence platform to support media and marketing data for our clients. This position will allow you to be a significant contributor as part of the team to support easy access of data and visualizations to fuel stronger insights and media optimizations.
This position requires strong technical and tactical skill sets with an eye for numbers, intellectual curiosity, proficiency at problem solving, and a critical understanding of online media. The candidate must have a proven track record in managing ETL’s, APIs and business intelligence platforms. Candidate should be a team player. A “roll up the sleeves” approach is mandatory and a “get it done” attitude is a must. Specific responsibilities include coordination between the research, analytics & media teams ensuring high quality data projects are effectively delivered.
Successful candidates will be multi-dimensional ‘rising-stars’ who are able to employ complex problem solving skills, and are able to communicate these succinctly to a broad client and media stakeholder audience.
Role Objectives:
Responsible for loading and validating data into the data warehouse from various source systems
Analyze, develop, fix, test, review and deploy functionality, and bug fixes in ETL data pipelines
Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required
Building Data mappings between Source to Target systems
Provides support for technical issues and ensuring system availability
Work with business customers to identify and develop additional data and reporting needs
Understand how business intelligence platform/data technologies work and offer the ability to explain technical concepts in ordinary terms (be technically savvy - understand the opportunities and limitations)
Establish and manage data integrations utilizing a taxonomy nomenclature to make recommendations for data visualization to showcase media performance through the use of Datorama or Tableau
Perform regular quality assurance/quality control checks on assigned client campaigns to ensure the data is processing accurately
Design and build backend data streams and processes to automate reporting capability with data visualization tools
Contribute to client status and reporting calls, including presentation of reporting as required
Develop subject matter expertise in ETL, API development, and Business Intelligence platforms
Clearly define project deliverables, timelines, and dependencies for junior team members, internal stakeholders and clients
Collaborate on an inclusive team, where members openly communicate and collectively problem-solve
Strong ability to evaluate new technologies and present findings to team
Contribute to knowledge sharing efforts and mentorships
Complete other duties as assigned.

Qualifications
Bachelor’s degree or combination of education, and equivalent work experience is preferred in the field of computer science, management information systems or Information Technology
3+ years’ related experience ETL development and business intelligence platform management
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
3+ plus years’ development experience with database engines including Presto, Mongo db and Hadoop.
3+ years’ experience in ETL development, Strong Database (Modeling, SQL), SQL Server, Hadoop, AWS Redshift, Qubole.
Experience managing team of 2 or more associates/analysts preferred
Strong database modeling and SQL skills. Ability to write complex SQL statement, Procedures and data automation programs
Knowledge of ETL tools like Airflow, AWS Glue, Alteryx, SSIS, Qubole/Spark is a must
Knowledge of Python or similar programming languages required
Proficiency with Datorama, Tableau, or other data visualization tools is preferred
Experience working with AWS or other cloud technologies
Advanced user Microsoft Office Suite and reporting tools
Demonstrated expertise in core MS Excel functions (vlookup, pivot tables, data visualization)
Experience in designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.
Strong analytical and problem-solving skills
Strong verbal/written communication and interpersonal skills is required
Self-starter with ability to thrive in a fast-paced environment and able to function independently while providing status updates to a team of analysts
Cooperative, flexible, conscientious, dependable, resourceful, self-motivated, and team-oriented
Problem solving, time management, and critical thinking skills with a professional and positive attitude
Ability to work independently and as part of an agile team, participating in daily stand-ups, sprint planning and sprint review

Character:
The following qualities help drive success as member of the Spark Data and Analytics team:
Entrepreneurial, engaging, resourceful, curious, and self-directed spirit
Willing and easily roll sleeves up or down; love the nitty-gritty and the strategy
Collaborative approach to building cohesive, strong teams
Loving and living the intersections between brands, people, media, communications
Relentlessly passionate and resolute
Planning and time management excellence.
Embrace challenges
Proactive, especially in pushing for new opportunities, approaches, and ideas.
Keenly focused on action and solutions; thrives with deep critical thinking and analysis.
Pioneering insight attitude and research in-the-know.
Resourcefulness, flexibility and adaptability, strong ability to pivot when the need arises.
Inspired to be part of the insight journey/revolution with a growing, dedicated team

Additional Information

All your information will be kept confidential according to EEO guidelines.
23-2795",3.9,"Spark Foundry
3.9","Dallas, TX",51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
806,Data Engineer,Employer Provided Salary:$83K - $104K,"General information
All posting locations: Remote, Illinois, United States of America
Job Function: 16 - Digital
Date Published: 13-Jun-2023
Ref #: R-71049
Base Salary Range: $83,400.00 - $104,200.00
Target Total Cash Range: $104,250.00 - $130,250.00
Target Total Cash: Target total cash represents this role's annualized cash earning potential at target (base salary + target bonus). Target total cash is contingent on targeted company performance achievement and individual attainment of performance goals. Therefore, target total cash is not guaranteed earnings.
Compensation Disclaimer: The compensation offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors.
Description & Requirements
3+ years of experience working in data engineering or architecture role.
Expertise in ELT and data analysis and experience with SQL and at least one programming language (Python/R preferred)
Experience developing and maintaining data warehouses in big data solutions e.g.- Snowflake
Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred)
Experience with cloud service providers including AWS- Azure- or Google.
Database development experience using Hadoop- SPARK or Big Query and experience with a variety of relational- NoSQL- and cloud database technologies.
Experience and/or knowledge of BI tools such as Alteryx- Tableau- Power BI- Looker.
Experience and/or knowledge of CI/CD (continuous integration and continuous deployment) practice using GitHub or Azure repos.
Conceptual knowledge of data and analytics- such as dimensional modeling- ELT- reporting tools- data governance- data warehousing- structured and unstructured data.
Familiarity with the Linux operating system
Familiarity with data engineering and workflow management frameworks such dbt.
Nice to have exposure to machine learning- data science- computer vision- artificial intelligence- statistics- and/or applied mathematics.
An agile learner who brings strong problem-solving skills- and enjoys working as part of a technical- cross functional team to solve complex data problems.
Bachelor’s degree required; Computer Science- MIS- or Engineering preferred or equivalent experience.
About Us
Kraft Heinz is a global food company with a delicious heritage. With iconic and emerging food and beverage brands around the world, we deliver the best taste, fun and quality to every meal table we touch. We’re on a mission to disrupt not only our own business, but the global food industry. A consumer obsession and unexpected partnerships fuel our progress as we drive innovation across every part of our company.
Around the world, our people are connected by a culture of ownership, agility and endless curiosity. We also believe in being good humans, who are working to improve our company, communities, and planet. We’re proud of where we’ve been – and even more thrilled about where we’re headed – as we nourish the world and lead the future of food.
Why Us
We grow our people to grow our business. We champion great people who bring ambition, curiosity, and high performance to the table as the guardians of our beloved and nostalgic brands. Good isn't good enough. We choose greatness every day by challenging the ordinary and making bold decisions. All while celebrating our wins - and our failures – as we work together to lead the future of food.
Challenging the status quo takes talent. We invest in your purpose and potential by developing skills and nurturing strengths that leave a legacy on our business and a lasting impact on your career. Because great people make great companies, and we’re growing something great here at Kraft Heinz.
Office Collaboration & Hybrid Work Environment
We believe our office environment fuels our collaboration, connection & community as an organization and allows our employees to grow toward greatness. We also believe providing a more flexible and agile model is essential in today’s workplace. A majority of our office-based employees will be able to work remotely for up to two days each week. Additionally, employees who are subject to this hybrid model will be eligible to work from anywhere for up to six weeks in a rolling 12-month period (in maximum two-week increments and according to benefits and tax guidelines). Some jobs may be required to be performed fully in office depending on the role’s responsibilities and requirements.
Kraft Heinz is an Equal Opportunity Employer that prohibits discrimination or harassment of any type. All qualified applicants are considered for employment without regard to race, color, national origin, age, sex, sexual orientation, gender, gender identity or expression, disability status, protected veteran status, or any other characteristic protected by law. Applicants who require an accommodation to participate in the job application or hiring process should contact NATAI@kraftheinz.com.",3.5,"Kraft Heinz Company
3.5",Illinois,10000+ Employees,2015,Company - Public,Food & Beverage Manufacturing,Manufacturing,$10+ billion (USD)
807,Quantitative Data Engineer - Sports Analytics,$94K - $125K (Glassdoor est.),"As a quantitative data engineer you will be working together with the quantitative research and trading teams, as well as with the trading strategy developers to:
Source and cleanse data, develop and maintain data pipelines as required by quant research and trading
Assist with researching new trading opportunities;
Develop, maintain and deploy interactive and packaged reports that will directly influence trading strategies
Performance-tune existing applications and processes, improve existing codebases and data flows to allow for efficient processing of large data sets

What we're looking for
PhD, Masters or bachelors’s degree in a technical discipline
Experience with the Python data science stack: NumPy, pandas etc.
Experience with numeric data storage methodologies, e.g. hdf5
Experience with processing of large and varied data sets
Ability to understand mathematical algorithms and develop high-performance implementations
Strong interpersonal and communication skills for interacting with traders, quantitative analysts, and other software developers
Experience with any of the common cloud environments (AWS, GCP, Azure) and with using distributed computing solutions in general.
Experience or interest in areas such as capital markets, probability, game theory and the application of IT solutions to these areas is a plus

SIG does not accept unsolicited resumes from recruiters or search firms. Any resume or referral submitted in the absence of a signed agreement will become the property of SIG and no fee will be paid.",4.3,"Susquehanna International Group
4.3","Bala Cynwyd, PA",1001 to 5000 Employees,1987,Company - Private,Stock Exchanges,Financial Services,Unknown / Non-Applicable
808,Senior Data Engineer,$130K - $170K (Glassdoor est.),"Responsibilities:
Who We Are:
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners - in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
Pandora
Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. Pandora provides consumers with a uniquely-personalized music and podcast listening experience with its proprietary Music Genome Project® and Podcast Genome Project® technology. Pandora is available through its mobile app, the web, and integrations with more than 2,000 connected products.
How you’ll make an impact:
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What you’ll do:
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What you’ll need:
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to pay attention to details and be organized.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $87,200 to $180,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.

Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.",3.6,"Sirius XM
3.6","Oakland, CA",1001 to 5000 Employees,1990,Company - Public,Broadcast Media,Media & Communication,$1 to $5 billion (USD)
809,Senior Data Engineer (Remote),Employer Provided Salary:$91K - $155K,"** This role is remote; if in area near an ICF office, hybrid may be recommended.**
Our Public Sector Group (PSG) is growing and we are looking for a talented, highly motivated Senior Data Engineer to join our Engineering and Emerging Technologies (EET) line of business for supporting a large government contract in the DC metro area.
Key Responsibilities:
Responsible for the data engineering on a large-scale technical modernization project in accordance with contract requirements and company policies, procedures and guidelines. Strong experience in I.T. modernization projects. Has a good mix of data requirements and data engineering, and knowledge of I.T. modernization efforts.
What you’ll be doing:
Support technical Data Warehouse/BI tasks for a major federal initiative, working as part of an extended system development team on project execution.
Develop extract, transform, and load (ETL) processing routines and data feeds, creating necessary data structures and data models to support data at all stages.
Perform extensive data profiling and analysis based on client data.
Work with UI and business analysis team members and the client to define BI and reporting requirements.
Design and implement custom data analytic and BI/reporting products, custom reports, and data visualization products.
What you must have:
Bachelor’s degree (e.g., Computer Science, Engineering or related discipline)
8+ years of experience in SQL and/or Python coding language.
8+ year of experience with procedural, functional, and object-oriented programming
3+ years of experience developing database ETL environments with business intelligence applications
3+ years of experience with AWS database, analytics, and compute services such as RDS/Aurora, AWS Glue, and Lambda
3+ years of experience with data warehouse design and development with Amazon Redshift
3+ years of experience working with BI tools such as Tableau, PowerBI or AWS Quicksight
3+ years of development experience in a DevSecOps environment, with programming languages such as Java, Node, or Python
Employment must be compliant with eligibility for Public Trust Clearance due to Government Contract.
Candidate must reside in the US and be a US Citizen
What we’d like you to know:
Understand ETL concepts of data flow, data enrichment, data consolidation, change data capture and transformation
Understand database concepts of referential integrity, indexes and keys and table metadata
Demonstrated experience showing strong critical thinking and problem-solving skills paired with a desire to take initiative
Knowledge of Big Data integration tools such as Hive, Airflow, Storm, Spark, AWS Kinesis, and Kafka
Experience building CI/CD pipelines with tools such as Jenkins and CodeBuild
Experience with containerization platforms including Docker
Experience with Agile development process
Technologies you’ll use in this role:
SQL, Node, Python, Tableau, Quicksight, Aurora, Lambda, Sequelize
Aurora, AWS Glue, S3, RedShift, AWS Kinesis
Git, Terraform, CodeBuild
Professional Skills:
Must have excellent written and oral communications skills.
Must be comfortable working in a fast-paced, matrixed team environment, with a client-centric culture, and an environment of high performers.
Must be able to multi-task and shift priorities as needed.
Why you’ll love working here:
Comprehensive health benefits
Generous vacation and retirement plans
Employee support program
Participation in charity initiatives
About ICF International:
ICF International (NASDAQ:ICFI) partners with government and commercial clients to deliver professional services and technology solutions in the energy and climate change; environment and infrastructure; health, human services, and social programs; and homeland security and defense markets. The firm combines passion for its work with industry expertise and innovative analytics to produce compelling results throughout the entire program life cycle, from research and analysis through implementation and improvement. Since 1969, ICF has been serving government at all levels, major corporations, and multilateral institutions. More than 4,000 employees serve these clients worldwide. ICF's Web site is
www.icf.com
ICF offers an excellent benefits package, an award-winning talent development project, and fosters a highly skilled, energized and empowered workforce.
ICF is an equal opportunity employer that values diversity at all levels. (EEO/AA – Minorities/Females/Veterans/Individuals with Disabilities)
For a listing of other career opportunities at ICF, please visit our Career Center at
www.icf.com/careers
Working at ICF
ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our
EEO & AA policy
.
Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email
icfcareercenter@icf.com
and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination:
Know Your Rights
and
Pay Transparency Statement.

Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
$90,940.00 - $154,598.00
Nationwide Remote Office (US99)",3.8,"ICF
3.8","Reston, VA",5001 to 10000 Employees,1969,Company - Public,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
810,Senior Data Engineer (Remote),Employer Provided Salary:$50.00 - $70.00 Per Hour,"Company Overview: IT Giants INC is a dynamic and innovative technology company at the forefront of delivering advanced data solutions across various industries. We are seeking an experienced Data Engineer with expertise in cloud platforms, programming languages, database technologies, and analytics tools. If you also have a strong background in the airline domain, this is an opportunity to join our dedicated team and shape the future of data-driven decision-making in the aviation sector.
Position Overview: As a Data Engineer at IT Giants INC, you will play a pivotal role in designing, building, and maintaining sophisticated data pipelines and analytical solutions for our airline industry clients. Leveraging your deep understanding of both data engineering and the airline domain, you will empower our clients to harness the power of their data to improve operations and customer experiences.
Key Responsibilities:
Data Pipeline Development: Architect, develop, and maintain end-to-end data pipelines, integrating data from diverse sources, including cloud platforms (GCP, Azure, AWS), APIs, and various databases (Teradata, Oracle, PostgreSQL, NoSQL).
Data Transformation and Modeling: Apply your expertise in SQL, Python, and Alteryx to transform raw data into meaningful insights, while ensuring data integrity and accuracy.
Cloud Expertise: Utilize your proficiency in GCP, Azure, and AWS to design and implement scalable and cost-efficient cloud-based solutions that meet the unique demands of the airline industry.
Database Management: Manage relational and NoSQL databases to store and retrieve data efficiently, ensuring optimal performance and security.
Analytics and Reporting: Collaborate with data scientists and analysts to develop interactive dashboards and reports using tools like Tableau, QuickSight, and CloudWatch, enabling stakeholders to make informed decisions.
Data Quality and Governance: Implement data quality checks, validation processes, and adherence to industry regulations to ensure data accuracy and compliance.
Version Control and Collaboration: Utilize GitLab and other version control tools to collaborate with cross-functional teams on code management and deployment.
Real-time Monitoring: Develop real-time data processing solutions using technologies like CloudWatch to enable timely insights into operational metrics.
Linux Environment: Leverage your proficiency in Linux to optimize system performance and troubleshoot issues as they arise.
Domain Knowledge: Apply your deep understanding of the airline industry to structure data effectively and drive actionable insights from the data.
Qualifications:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
years of proven experience in data engineering, with a focus on cloud platforms (GCP, Azure, AWS), Python, SQL, and relevant database technologies.
Experience with Alteryx, Tableau, and data visualization tools for creating impactful dashboards.
Proficiency in managing databases such as Teradata, Oracle, PostgreSQL, and NoSQL databases.
Strong command of Linux environments and version control using GitLab.
Familiarity with CloudWatch, QuickSight, and real-time data processing is advantageous.
Domain expertise in the airline industry, including a solid understanding of aviation data, standards, and operational challenges.
Excellent problem-solving skills and the ability to troubleshoot complex data issues.
Effective communication skills to collaborate with technical and non-technical teams.
Proven ability to work independently and in a team, delivering high-quality solutions.
Join Our Team: If you are a driven and knowledgeable Data Engineer with a passion for both technology and the airline industry, IT Giants INC offers an exciting opportunity to contribute to transformative data solutions. Join us in our mission to revolutionize data analytics within the aviation sector and drive meaningful impact.
Job Types: Full-time, Contract
Pay: $50.00 - $70.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Work Location: Remote",5.0,"IT Giants
5.0",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
811,BI Data Engineer (Remote),Employer Provided Salary:$75.00 - $85.00 Per Hour,"Stefanini Group is hiring!
Exciting opportunity awaits, let us help you get started!
Click Apply now or you may call: (248) 728-2611 / email: palak.agrawal@stefanini.com for faster processing!

Job Description:
We require a reliable, talented, highly motivated BI/Data Engineer with excellent SQL, PowerBI, Python skills, The firm is putting a huge focus on data and reporting, with an increased demand of data related tasks.
As a global leader in Asset management, you will be working as part of an innovative, fast paced technology driven culture who are always looking at ways to enhance how technology is used within their organization.
You will be a valued member of an effective data team working to deliver growth against ambitious targets.
You will play a key role in extracting useful information from data, providing important insights to global technology teams.
The role will focus on creating best practice, high quality data solutions in collaboration with numerous business stakeholders and technology partners.
We are looking for someone who enjoys problem solving, who has an inquisitive nature, a team player and can work in a collaborative environment with good communication skills.
Ability to help extract and analyze data is required for the job.



Requirements:
We are looking for a good mix of the following skills:
Excellent experience in the Microsoft BI stack, Excel.
Excellent experience in building data pipelines and integrations (ETL/ELT).
Excellent knowledge of data warehousing and analysis concepts, data normalization, dimensional modelling, and associated practices.
Knowledge and experience of data manipulation languages such as DAX or MDX, Visualization or dashboarding experience.
Python experience is a must. Azure cloud experience would be a bonus.
We believe in building a culture supported by strong values, trust and independence. Within the team, we are highly collaborative, supportive, open and have high standards in the work we produce.

Education:
Bachelor's degree or equivalent experience.

*Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives*

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",3.3,"Stefanini, Inc
3.3","New York, NY",10000+ Employees,1987,Company - Private,Enterprise Software & Network Solutions,Information Technology,$1 to $5 billion (USD)
812,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1.0,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
813,"Data Engineer (AWS, Python)",Employer Provided Salary:$50.00 - $52.00 Per Hour,"Title: Data Engineer (AWS, Python)
Location: Remote
Duration: 12+ Months
Job Description:
Typical task breakdown:-
Daily standup, collaborate with peers, work independently to complete sprint commitments, participate in agile ceremonies.
Interaction with team:-
Daily stand up meetings with team – working mainly with Helios Platform adoption team.
Work environment:
Education & Experience Required:
Bachelor's in computer science required plus 7+ Working experience .
Masters degree with 6+ years working expeirence acceptable as well.
Preferred: AWS cloud certifications
Technical Skills
(Required)
Exp with Object oriented programming - Python (4+ years)
AWS (3+ years overall exp) lambda and/or glue, EMR, S3
SQL (3+ years)
2+ years exp in test driven development, unit test, integration testing, regression testing
(Desired)
Event driven framework experience.
Kinesis or Kafka experience
Familiar with working in Agile methodology
Soft Skills
(Required)
Strong written and verbal communication, experience working as a member of agile engineering team, ability to commit to and deliver work within allocated sprint window.
Job Type: Contract
Pay: $50.00 - $52.00 per hour
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",5.0,"DSMH LLC
5.0",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
814,Data Engineer,Employer Provided Salary:$45.00 Per Hour,"Job Title: GCP Data Engineer
Duration: 12+ Months
Location: Hartford, CT (Onsite Role)
Tax term : W2 & 1099
Job Description:
As a GCP Data Engineer, you will play a pivotal role in designing, implementing, and optimizing data solutions on the Google Cloud Platform (GCP). Your expertise will be essential in leveraging various technologies to build robust data pipelines and contribute to the overall success of data-driven initiatives.
Roles and Responsibilities:
Design, develop, and maintain efficient data pipelines using Python or PySpark on the Google Cloud Platform.
Seamlessly integrate data solutions using GCP services like Google Cloud Storage, BigQuery, Dataflow, and Pub/Sub.
Utilize expertise in the Hadoop ecosystem, including HDFS and Hive, to craft seamless data workflows.
Transform raw data into insights by creating complex SQL queries for data transformation and analysis.
Implement data quality checks and validation procedures to ensure accuracy and completeness.
Optimize data pipelines and processes for maximum performance and efficiency.
Collaborate with cross-functional teams to understand data requirements and deliver tailored solutions.
Create comprehensive documentation for pipelines, workflows, and processes.
Stay updated with the latest data engineering and cloud technology trends.
Tackle complex challenges with a proactive problem-solving mindset.
Required Qualifications:
Expertise in Python/PySpark and hands-on experience with Google Cloud Platform (GCP) for data engineering tasks.
Strong understanding of the Hadoop ecosystem, including HDFS and Hive.
Proficiency in SQL for effective data manipulation and analysis.
Experience working with cloud platforms, preferably Google Cloud Platform (GCP).
Strong problem-solving skills and analytical thinking.
Effective communication and collaboration abilities.
Adaptability to evolving data engineering technologies and practices.
Job Type: Contract
Salary: From $45.00 per hour
Experience level:
9 years
Application Question(s):
What is your work authorization?
Experience:
Data engineer: 9 years (Required)
Google Cloud Platform: 5 years (Required)
Hadoop: 5 years (Required)
Python: 4 years (Preferred)
SQL: 4 years (Preferred)
Work Location: On the road",-1.0,Conglomerate IT,"Hartford, CT",-1,-1,-1,-1,-1,-1
815,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
816,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
817,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
818,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
819,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
820,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
821,Sr. Data Engineer,Employer Provided Salary:$105K - $110K,"Role : Data Engineer
Location : Remote
Note : We are looking Someone who can be technical and also be able to
structure and manage transformation projects as well.
JD :
* Data Engineer Skill: Spark, Python and Databricks
* 12+ years of experience in Data pipeline engineering for both batch
and streaming applications.
* Experience with data ingestion process, creating data pipelines
(should be able to move data between stages in data lake, apply
transformation rules) and performance tuning. Must be hands-on coding using
Python with Spark.
* Exposure working with complex data types like XML or JSON, CI/CD
pipeline and scheduling tools like Airflow.
* Proven experience in DWH (type-2 dimensions, aggregations, star
schema, etc.) design and implementation
* Proficient in writing complex SQL queries
* Effective communication and collaboration skills to work with
cross-functional teams and stakeholders.
* Oversee and monitor the work of offshore according to projects
timelines and, if necessary, resolve any issues as they arise
Job Type: Full-time
Salary: $105,000.00 - $110,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
technical and able to structure and manage projects: 5 years (Required)
Data pipeline engineering for both batch and streaming apps.: 10 years (Required)
data ingestion process, creating data pipelines: 5 years (Required)
Python and Spark: 4 years (Required)
Exposure of XML or JSON, CI/CD: 4 years (Required)
scheduling tools like Airflow: 5 years (Required)
writing complex SQL queries: 5 years (Required)
Oversee and monitor the work of offshore: 3 years (Required)
working on fulltime role: 2 years (Required)
Work Location: Remote",4.3,"keasis Inc
4.3",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
822,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
823,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
824,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
825,Azure Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Required Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 9 years (Required)
Python: 8 years (Required)
Data lake: 7 years (Required)
Data Bricks: 7 years (Required)
Work Location: Remote",-1.0,TekValue IT Solutions,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
826,Data Engineer,-1,"Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore.

Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office.
The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus.
Position Responsibilities
Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities
Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization
Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process
Qualifications & Desired Skills
Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance
Minimum 2 years' of development experience using either Python, C#/Java or C++
Minimum 2 years' years of database/SQL experience
Familiarity with object-oriented programming concepts
Must be comfortable with development across the application stack
Ability to complete complex projects independently
Detail-oriented with strong verbal and written communication skills
Ability to work effectively in a high-energy, time sensitive team environment
Interest or prior experience in the financial trading industry (particularly in fixed income) a plus
This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education.
Base Salary for this role is expected to be between:
$110,000—$150,000 USD",4.4,"Garda Capital Partners
4.4","New York, NY",51 to 200 Employees,-1,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
827,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
828,Data center infrastructure Engineer,Employer Provided Salary:$60.00 - $68.00 Per Hour,"Requirements and Responsibilities:
Must have Bachelor’s degree in IS or equivalent.
Must have experience with VMWARE, DATA CENTER OPERATIONS, IT INFRASTRUCTURE, and VIRTUALIZATION.
Experience working with Linux (CentOS/RHEL/Rocky) .
Experience working with VMWare 6.x / 7.x and storage area network (SAN).
Manage and support Microsoft domain / Active Directory Services, RBAC, Group Policy, Windows Servers, desktops and services automate tasks through scripting technologies Support execution of Information
Support and manage the VMWare virtualization and storage (SAN - Dell Extreme IO & Pure Storage) infrastructure to meet and exceed development operations growth expectations.
Job Type: Contract
Pay: $60.00 - $68.00 per hour
Benefits:
Vision insurance
Experience:
Data center operations: 4 years (Required)
IT infrastructure: 3 years (Required)
Work Location: In person",3.0,"BJ Services
3.0","Exton, PA",1001 to 5000 Employees,1872,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",$1 to $5 billion (USD)
829,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department.
The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
830,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1.0,Percept Health,Remote,-1,-1,-1,-1,-1,-1
831,Data Engineer (MongoDB),$80K - $114K (Glassdoor est.),"CrossCountry Mortgage (CCM) is one of the nation’s top 3 retail mortgage lenders with more than 7,000 employees operating nearly 600 branches across all 50 states. We have been recognized nine times on the Inc. 5000 list of America’s fastest-growing private businesses and have received countless awards for our standout culture. Through our dedication to getting it done, we make every mortgage feel like a win.

A culture where you can grow! CCM has created an exceptional culture driving employee engagement, exceeding employee expectations, and directly impacting company success. At our core, our entrepreneurial spirit empowers every employee to be who they are to help us move forward together. You’ll get unwavering support from all departments and total transparency from the top down.

CCM offers eligible employees a competitive compensation plan and a robust benefits package, including medical, dental, vision, as well as a 401K with an employer match. We also offer company-provided short-term disability, an employee assistance program, and a wellness program.
Position Overview:
We are looking for an experienced MongoDB developer to work with our development teams to integrate innovative features on our new products and applications, as well as maintain existing applications. The MongoDB developer will engineer scripts that automate daily tasks and ensure the security of data sets.
Job Responsibilities:
Upgrade databases through patches.
Understand and convert company needs into technical specifications and creating scalable solutions using MongoDB.
Optimize database schema designs, ensuring data integrity, and promoting best practices to enhance the overall performance and scalability of MongoDB solutions.
Assist developers in identifying and resolving performance issues by utilizing Mongo Profiler to profile and fine-tune database queries and operations.
Leverage performance optimization techniques by implementing proper indexes, including B-Tree, Geospatial, and Text indexes, to enhance query performance and overall system efficiency.
Implement the most effective backup and recovery methods to safeguard critical data and ensure business continuity in case of data loss or system failures.
Collaborate with cross-functional teams to understand business requirements.
Stay updated with the latest MongoDB advancements, features, and best practices.
Qualifications and Skills:
Bachelor’s/Master’s degree in software engineering or Information Technology.
3+ years of experience in executing data solutions using MongoDB.
Recommend and implement best practices for Rest API integration framework/model.
Analytical and problem-solving skills.
Configuring schema and MongoDB data modelling.
Experience with database security management.
Administration of MongoDB in Atlas.
Thorough understanding of MongoDB architecture.
Excellent communication skills.
This job description is intended to convey information essential to understanding the scope of the job and the general nature and level of work performed by job holders within this job. However, this job description is not intended to be an exhaustive list of qualifications, skills, efforts, duties, responsibilities or working conditions associated with the position.
CrossCountry Mortgage, LLC offers MORE than a job, we offer a career. Apply now to begin your path to success! https://crosscountrymortgage.com/about-us/careers/apply/
California residents: Please see CrossCountry’s privacy statement for information about how CrossCountry collects and uses personal information about California consumers.
CrossCountry Mortgage, LLC. (NMLS3029) is an Equal Opportunity Employer committed to workforce diversity. Qualified applicants will receive consideration without regard to race, religion, creed, color, orientation, gender, age, national origin, veteran status, disability status, marital status, sexual orientation, gender identity, or gender expression, or any other protected status in accordance with all applicable federal, state and local laws. The collective sum of the individual differences, life experiences, knowledge, inventiveness, innovation, self-expression, unique capabilities and talent that our employees invest in their work represents a significant part of not only our culture, but our reputation. The Company is committed to fostering, cultivating and preserving a culture that welcomes diversity and inclusion. Employment is contingent upon successful completion of a background investigation. CrossCountry Mortgage, LLC. is an FHA Approved Lending Institution and is not acting on behalf of or at the direction of HUD/FHA or the Federal government. To verify licensing, please visit www.NMLSConsumerAccess.org.",3.3,"CrossCountry Mortgage, LLC.
3.3","Charlotte, NC",1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
832,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
833,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
834,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
835,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1.0,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
836,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
837,Data Engineer,$76K - $101K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
The Data Governance/Data Quality Subject Matter Expert (SME) is a professional responsible for designing, implementing, and managing the client’s data governance strategy and ensuring high data quality standards across data-centric operations and decision-making.
The SME is in charge of developing and maintaining robust data pipelines, ensuring data observability, managing data within the Microsoft Azure environment, and implementing data governance principles in alignment with industry best practices and regulatory standards.
Primary Responsibilities and Accountabilities:
Data Governance & Quality: This includes developing and implementing data governance policies and procedures, monitoring data quality, managing data within the Microsoft Azure environment, and ensuring compliance with data privacy regulations.
Data Pipeline & Workflow Development: Designing and implementing efficient and secure data pipelines in Azure Data Factory and workflows, ensuring optimal performance and incorporating data quality principals.
Data Integration & Observability: Designing data integration strategies, integrating data from multiple sources, ensuring data quality and observability through tools like Azure Synapse.
Data Security: Defining and implementing data security policies and procedures, including access control and data encryption within the Azure environment.
Performance Tuning: Optimizing data access, indexing, and query performance within Azure Data Factory to ensure scalable and reliable data infrastructure.
Monitoring & Troubleshooting: Monitoring data systems, troubleshooting issues, identifying performance bottlenecks, and collaborating with team members to resolve problems.
Collaboration & Communication: Working closely with stakeholders to understand their data needs and develop solutions adhering to industry best practices and the needs of the business.
Automation & Tooling: Automating repetitive tasks and maintaining tools necessary for data processing and storage within the Azure environment.
Qualifications
A Bachelor's degree in a related field such as computer science, mathematics, engineering, or information systems.
3+ years of hands-on experience programming in SQL.
2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes within the Microsoft Azure environment.
Experience in Data Governance OR Data Quality Management is a must.
Skills
Competencies:
Expertise in Microsoft Azure data tools, with a focus on Azure Data Factory and Azure Synapse for data processing, storage, and analysis.
Strong SQL skills and knowledge of data warehousing concepts and ETL (Extract, Transform, Load) processes within the Azure environment.
Proficiency in programming languages such as Python, and experience with big data technologies such as Hadoop, Spark, and Kafka.
Ability to design, build, and maintain data pipelines, with a particular focus on Azure Data Factory.
Strong analytical and problem-solving skills, with the ability to identify and resolve data-related issues effectively.
Experience with data security and privacy regulations, and understanding of HIPAA compliance.
Strong experience with data governance, data quality, and data observability principles.
Excellent written and verbal communication skills, ability to explain complex technical concepts to non-technical stakeholders.
Excellent collaboration and teamwork skills, with the ability to work in a team involving client data engineers, data scientists, and data analysts.
Strong attention to detail and commitment to ensuring high data quality.
Continuous learning mindset and ability to stay up-to-date with new developments and technologies in data governance and data quality
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
838,GCP Data Engineer,Employer Provided Salary:$50.00 - $56.00 Per Hour,"JOB Description: -
ABOUT CONGLOMERATE
Conglomerate IT is a certified and a pioneer in providing premium end-to-end Global Workforce Solutions and IT Services to diverse clients across various domains. Visit us at ConglomerateIThttp://www.conglomerateit.comConglomerateIT
Conglomerate IT mission is to establish global cross culture human connections that further the careers of our employees and strengthen the businesses of our clients. We are driven to use the power of global network to connect business with the right people without bias. We provide Global Workforce Solutions with affability.
We offer a gamut of services across diverse domains, categories, skill sets with varying lengths of assignments /core capabilities included:
· Application Development Services
· Business Intelligence Services
· Enterprise Resource Planning
· Infrastructure Support Services
· Testing Services
· Program / Project Management
· Temporary & Permanent Staffing
Conglomerate IT is headquartered in Addison, Texas with branch offices across Canada, India, Sri Lanka, Argentina to further serve our clients.
JOB ROLE: GCP Data Engineer
Location: Job Type:Hartford, CT (Onsite Role)
Contract : 12+ Months
Work Authorization: w2 ,1099
Must have Skills: - Skills: Python/PySpark, GCP Cloud , HDFS, Hive, Big data
About Role: -
As a GCP Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and data architecture on the Google Cloud Platform. You will work closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and storage. Your expertise in GCP services and data engineering best practices will play a crucial role in maintaining a scalable, reliable, and high-performance data infrastructure.
Roles and Responsibilities:
· Design and implement data pipelines: Develop ETL (Extract, Transform, Load) processes to move data from various sources into GCP storage solutions such as BigQuery, Cloud Storage, and Datastore.
· Data modeling: Design and optimize data models to support data analytics, reporting, and machine learning initiatives. Ensure data accuracy, integrity, and appropriate access controls.
· Performance optimization: Fine-tune data pipelines and query performance to maximize efficiency and reduce latency, utilizing techniques like partitioning, indexing, and caching.
· GCP services utilization: Leverage GCP services like Dataflow, Dataproc, Pub/Sub, and Composer to create, schedule, and manage data processing and orchestration tasks.
· Data integration: Collaborate with cross-functional teams to integrate data from various sources, both internal and external, to enable comprehensive analysis and reporting.
· Data quality and governance: Implement data quality checks, monitoring, and validation processes to ensure data accuracy and consistency across the platform.
· Security and compliance: Implement security measures and access controls to protect sensitive data. Ensure compliance with industry standards and regulations.
· Documentation: Create and maintain technical documentation for data pipelines, workflows, and processes.
· Troubleshooting and support: Diagnose and resolve data-related issues, performance bottlenecks, and operational challenges in a timely manner.
Ideal Profile
· You have experience with Python Hive, Big data
· You have experience withYou GCP Cloud
· have strong experience with HDFS and other relevant tools/technologies.
· You have advanced python skills.
· You have PySparkknowledge.
· You have excellent communication and collaboration skills.
· You have attention to details.
· You are self-directed.
What's on Offer?
· Join a team of world class IT professionals
· Comprehensive salary package
· Great opportunity for growth and development
Job Type: Contract
Salary: $50.00 - $56.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Experience:
google cloud platform: 4 years (Required)
Work Location: In person",-1.0,CONGLOMERATE IT LLC,"Novi, MI",-1,-1,-1,-1,-1,-1
839,Snowflake Data Engineer,Employer Provided Salary:$70.00 - $80.00 Per Hour,"Job: Snowflake Data Engineer – Contract (6+ Months) – Stillwater, MN
REMOTE - HAVE TO WORK ONISTE WHEN EVER CLIENT CALL. Preferably local to MN NEEDED
EAD – GC – H4- EAD G C - CITIZEN
W2 Contract
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Expected hours: 40 per week
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 7 years (Required)
Snowflake API: 4 years (Required)
Snowflake administration: 6 years (Required)
Snowflake RBAC, RLS, CLS: 5 years (Required)
Willingness to travel:
50% (Required)
Work Location: Remote",-1.0,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
840,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
841,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1.0,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
842,2024 Data Engineer I,-1,"2024 Data Engineer I
316114

Textron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.

Responsibilities:
Analyze, manipulate, or process large sets of data using statistical software.
Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces
Identify business problems or management objectives that can be addressed through data analysis.
Document data standards, semantic layer mapping, data flows and conceptual flows.
Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.
Perform statistical analysis to provide recommendations on best courses of action to business leaders.
Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.
Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.
Collaborate with business users to ensure data is available for self-service analytics.
Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.


Qualifications:
Education: Bachelor’s degree in computer science, engineering, mathematics, or similar field required
Years of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience required
Software Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferred
Experience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)
EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

Recruiting Company Textron Specialized Vehicles
Primary Location US-Georgia-Augusta
Job Field Information Technology
Schedule Full-time
Job Level Individual Contributor
Shift First Shift
Job Posting 08/24/2023, 8:20:05 AM",3.8,"Textron Specialized Vehicles
3.8","Augusta, GA",10000+ Employees,1923,Company - Public,Aerospace & Defense,Aerospace & Defense,$10+ billion (USD)
843,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
844,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Job Role: Data Engineer with Snowflake
Location: REMOTE, Stillwater, MN.
HAVE TO WORK ONISTE WHEN EVER CLIENT CALL.
Duration: 6+ Months Contract
Visas: USC, GC, EAD-GC, H4-EAD
W2 Requirement
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Expected hours: 40 per week
Experience level:
7 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
Snowflake: 4 years (Required)
Work Location: Remote",-1.0,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
845,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
846,AWS Data Engineer,Employer Provided Salary:$60.00 - $75.00 Per Hour,"Title: AWS Data Engineer
Location: Columbus, OH (Day 1 onsite / Hybrid Model)
Duration: Contract
Job Description:
Data Engineer: Open positions – 2
Skills:
Must Have:
BS/BA degree or equivalent experience
General: Strong organizational, problem-solving, and critical thinking skills; Strong documentation skills
Coding: Proficiency in Java
• Cluster Computing frameworks: Proficiency in Spark and Spark SQL
• AWS Data Services: Proficiency in Lake formation, Glue ETL (or) EMR, S3, Glue Catalog, Athena, Kinesis (or) MSK, Airflow (or) Lambda + Step Functions + Event Bridge
Data De/Serialization: Expertise in at least 2 of the formats: Parquet, Iceberg, AVRO, JSON-LD
AWS Data Security: Good Understanding of security concepts such as: Lake formation, IAM, Service roles, Encryption, KMS, Secrets Manager
Good to Have:
• Linux Scripting, Jenkins
• DevOps: Git, CI/CD, JIRA, TDD
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Ability to commute/relocate:
Columbus, OH 43085: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Required)
Java: 4 years (Required)
Data warehouse, Glue and SparkSQL: 8 years (Required)
Work Location: In person",4.5,"Synkriom Technology Pvt. Ltd.
4.5","Columbus, OH",201 to 500 Employees,2015,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
847,Data Engineer I,Employer Provided Salary:$90K - $120K,"Interested in joining a growing company where you will work with talented colleagues, enhance a supportive and energetic culture, and be part of the climate solution? At Energy Solutions, we focus on the big impacts. And we believe that market-based programs can be a powerful force to deliver large-scale energy, carbon, and water-use savings. For 28 years, we've harnessed that power to offer proven, performance-based solutions for our utility, government, and institutional customers.
Energy Solutions (ES) is currently seeking a full-time Data Engineer I to join our Business Operations Team. The Data Engineer role is part of the Business Operations Team and will work on critical projects such as the Corporate Operational Data Warehouse (CODW) and the new ERP system. The CODW project is one of ES's strategic goals, to build and maintain a system that enables ES to centralize, integrate, and analyze internal data from various sources. The goal is to provide a single source of truth for ES internal data and enable data-driven decision-making across the organization. This role will potentially lead the design and implementation of the infrastructure for internal reporting needs, driving operational efficiencies and helping our company's transition to operating internationally with 1000+ employees.
Energy Solutions has a remote-friendly work environment for staff located throughout the United States. We also have offices in Oakland and Orange, California, Boston, Massachusetts, New York, and Chicago, IL, for those that wish to work from one of our offices.

Daily responsibilities include but are not limited to:
Collaborate with stakeholders to understand data requirements and develop data-driven solutions that support business goals
Analyze data sources and pipelines needed for the CODW project
Design and implement scalable data pipelines, ETL processes, and data integration solutions for the CODW project using Python, SQL, and AWS technologies
Develop and maintain a data warehouse and optimize data storage, processing, and retrieval
Build schema, populate the database, and automate reporting
Document and maintain technical specifications, ETL processes, data mappings, and dictionaries
Support the Data Science Center of Excellence (DSCOE) with other data engineering needs

Minimum Qualifications:
BS/BA in Computer Science, Physics/Engineering, Business or Mathematics
Experience in building ETL data pipelines, real-time pipelines are a plus
Strong programming skills in Python and SQL
Minimum 2 years of data warehouse development and strong fundamentals in dimensional data modeling
Minimum 2 years of experience creating SQL queries and ETL design, implementation, and maintenance
Minimum 2 years of experience developing data pipelines using Python
Minimum 2 years of experience with AWS, AWS Redshift, AWS Data Engineering and ML Ops tools and services (ie: S3, Redshift, RDS)
Proven ability to aggregate, normalize, munge, analyze, and summarize value from disparate datasets
Strong oral, written, and interpersonal communication skills
High degree of accuracy and attention to detail
Ability to establish priorities, work independently with minimum supervision
Preferred Qualifications:
Ability to manage and communicate data warehouse plans to a non-technical audience
Expertise in cloud services architecting and designing secure AWS environments
Experience using BI tools like Tableau and Looker
Experienced in analyzing and troubleshooting data quality issues
Self-driven, highly motivated, and able to learn quickly
Familiarity with large data sets, cloud-based development and deployment, open-source practices and frameworks, and intelligent applications is desirable
Proven success in migrating data from legacy systems

Salary DOE: $90k - $120k / Annually
Compensation is commensurate with experience and includes a generous retirement package. Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP).
To see this and our other open positions, please visit https://energy-solution.com/company/careers/.
We encourage candidates from diverse backgrounds to apply. For more information about Energy Solutions and our Diversity, Equity and Inclusion efforts, please visit us on our website at www.energy-solution.com.
Background Check Information
Information will be requested to perform the compulsory background check. A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment. Energy Solutions is an equal opportunity employer.
Reasonable Accommodations
Energy Solutions is committed to providing access and reasonable accommodation for individuals with disabilities. If you require an accommodation in completing this application, interviewing, and/or completing any pre-employment testing, or otherwise participating in the employee selection process, please email accommodation@energy-solution.com.
Privacy Notice for Job Applicants",3.9,"Energy Solutions
3.9",Remote,10000+ Employees,2008,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$25 to $100 million (USD)
848,Apple Music - Data Engineer,-1,"Summary
Posted: Aug 18, 2023
Weekly Hours: 40
Role Number:200496819
This team is more than a group of engineers — it’s a group of music lovers. That passion has made Apple Music the world’s most complete music experience, with over 60 million songs, thousands of playlists, and daily selections from music experts for 115 countries. The team’s data-driven engineers focus relentlessly on the customer experience by running worldwide experiments and analyzing usage and latency, while collaborating with Apple’s product groups. As a result, someone can use the Shazam app to identify an intriguing song in a café in the morning, add it to their playlist from Apple Watch, listen to it through their AirPods on their commute, and share it with their family on HomePod at dinnertime. And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love. Apple Music is a huge part of Apple’s business because it’s a big part of people’s lives.
Key Qualifications
Expertise in designing, implementing and supporting highly scalable data systems and services in Scala
Experience with distributed computing technologies such as Hadoop MapReduce, Spark/Spark-SQL, YARN/MR2
Expertise building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Solid understanding of data-modeling and data-architecture optimized for big data patterns, such as efficient storage and query on HDFS Experience with distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues
Experience with low-latency NoSQL datastores, and traditional relational databases is a plus
Description
We are seeking a hardworking, performance-savvy, engineer to build out the big data platform and services, which power many of these customer features — existing and new. As a core member of the Data Engineering team you will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability. This will include crafting systems to model, ingest, process and compute large-scale, mission-critical data across Apple Music. High-throughput and reliability are essential. The computed datasets are produced for internal reporting used by executives and they are also shared across many teams within the ASE organization, such as the Search team, Recommendation team, AB team, Marketing team. You will also enjoy the benefits of working in a fast growing business where you are encouraged to “Think Different” to solve very interesting technical challenges and where your efforts play a key role in the success of Apple’s business. This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users. Come join us to help deliver the next amazing Apple product!
Education & Experience
BS CS or equivalent work experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $131,500 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",4.2,"Apple
4.2","Seattle, WA",10000+ Employees,1976,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
849,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
850,Senior Data Engineer,Employer Provided Salary:$87K - $180K,"Oakland, California; Atlanta, Georgia; Irving, Texas
Regular Employee Full-Time
R-2023-08-81

Who We Are:
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners - in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
Pandora
Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. Pandora provides consumers with a uniquely-personalized music and podcast listening experience with its proprietary Music Genome Project® and Podcast Genome Project® technology. Pandora is available through its mobile app, the web, and integrations with more than 2,000 connected products.
How you’ll make an impact:
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What you’ll do:
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What you’ll need:
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to pay attention to details and be organized.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $87,200 to $180,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.

Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-08-81",3.6,"Adswizz
3.6","Oakland, CA",51 to 200 Employees,2007,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
851,Energy Data Engineer - Mid Level,$81K - $121K (Glassdoor est.),"About the Organization
Now is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.

The Data Engineer will support Redhorse in its execution of specific task areas related to the Federal Government Carbon Pollution Free Electricity (CFE) Technical and Strategy Support contract. The program will establish technical frameworks and analytical tools and methods to inform CFE procurement decisions for the entirety of the federal government. Work will be conducted primarily at Redhorse office locations (hybrid remote work) and on government client site as required. The ideal candidate will live in the DC/Virginia/Maryland area and willing to undergo a background check to obtain a Public Trust.
Primary Duties and Responsibilities for this position include
Review and analyze the US Federal Government Executive Agencies electric load data
Develop and test modeling techniques to understand an agency’s hourly load profile to inform the agency’s hourly CFE procurement requirement as an off taker from a new CFE technology source.
Support the configuration and ingestion of designated structured, unstructured, and semi-structured data repositories into capabilities that satisfy client requirements and support a data analytics pipeline to drive rapid delivery of functionality to the client.
Maintain all operational aspects of data transfers while accounting for the security posture of the underlying infrastructure and the systems and applications that are supported and monitoring the health of the environment through a variety of health tracking capabilities.
Automate configuration management, leverage tools, and stay current on data extract, transfer, and load (ETL) technologies and services.
Work under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.
Apply comprehension of data engineering-specific technologies and services, leverage expertise in databases and a variety of approaches to structuring and retrieving of data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically using vendor consoles.
Engage with multiple functional groups to comprehend client challenges, prototype new ideas and new technologies, help to create solutions to drive the next wave of innovation, and design, implement, schedule, test, and deploy full features and components of solutions.
Identify and implement scalable and efficient coding solutions.
Minimum Requirements for skills, Experience, and Credentials
Master’s Degree in Engineering, Computer Science, Mathematics or Statistics
Four (4) years of experience in enterprise data analysis with a preferred focus on utility data, to include advanced metering infrastructure (AMI) electricity data
Local to Washington D.C. metro area preferred, remote work is an option
Willingness to travel in support of on-site customer meetings
Willingness to obtain a Public Trust
Desired Basic Requirements for Skills, Experience, and Credentials include
Knowledge of U.S. Federal Government Energy Legislation and Executive Orders (especially EO 14057)
Experience in or knowledge of utility interval data and electricity pricing
Executive Order Requirement:
While currently on HOLD, Redhorse recognizes that we may need to comply with Executive Order 14042 which requires Federal Contractors to ensure all U.S. new hires be fully vaccinated for COVID-19. As required by the Executive Order, Redhorse will work in coordination with applicable contract agencies to consider requests for Reasonable Accommodations.
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.
EOE/M/F/Vet/Disabled",3.5,"Redhorse
3.5","Arlington, VA",51 to 200 Employees,2008,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
852,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
853,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Data Engineer
Candidate must have AWS (Redshift, Glue, S3), Spark, Phython, Scala and Should be expert in SQL
Basic qualifications
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
Design, implementation and management of data processing systems dealing with large data sets.
Proficiency in, at least, one modern programming language such as Python, Java or Scala
Hands-on experience and advanced knowledge of SQL.
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and cloud computing
Experience working with Open-Source Big Data tools (Hadoop, Spark, Hive, Presto and etc.)
Preferred qualifications
Experience in working and delivering end-to-end projects independently
Relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics
Experience working with AWS Big Data Technologies (AWS Glue, EMR, Athena, Ec2, Redshift and S3 etc.)
Experience working with Oracle, PostgreSQL, SQL Server or MySQL.
Proven track record of delivering a big data solution
Experience working with both Batch and Real Time data processing systems
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Expected hours: 40 per week
Compensation package:
1099 contract
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Austin, TX 78703: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 8 years (Required)
Python: 8 years (Required)
AWS: 5 years (Preferred)
Work Location: Hybrid remote in Austin, TX 78703",-1.0,PRISM IT LLC,"Austin, TX",-1,-1,-1,-1,-1,-1
854,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
855,Data Engineer,$79K - $115K (Glassdoor est.),"Job Title :- Data Engineer
Location:- San Antonio, TX
Required:- Active Top Secret Clearance
We are looking for a talented Data Engineer to support the acquisition of mission-critical and mission-support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is performed mostly on customer site in Columbia, MD.
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.
Minimum Qualifications
Security Clearance - Must have a current Secret level security clearance and therefore all candidates must be a U.S. Citizen with a willingness to go to TS/SCI and take the CI poly after starting the position.
5 years experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.
Experience with programming languages such as Python and Java.
Proficiency with acquisition and understanding of network data and the associated metadata.
Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
Experience with Kibana and Elasticsearch.
Familiarity with various log formats such as JSON, XML, and others.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must have a Security+ or similar certification or the ability to obtain it immediately.
Must be able to work on customer site in Columbia, MD at least 4 days a week. (Subject to change)
Desired Skills (Optional)
Experience with NOSQL databases such as Accumulo desired
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
Job Type: Full-time
Schedule:
8 hour shift
Security clearance:
Top Secret (Preferred)
Work Location: In person",3.9,"Helm360
3.9","San Antonio, TX",201 to 500 Employees,1997,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
856,Lead Azure Data Engineer,Employer Provided Salary:$135K - $140K,"Lead Data Engineer.
Location : New York, NY ( Hybrid)
Full Time
10+ years of experience in data engineering
5+ years of experience managing a team of data engineers
Strong understanding of SQL Server SSIS
Excellent communication and interpersonal skills
Ability to work independently and as part of a team
Ability to meet deadlines and work under pressure
Strong desire to be hands-on with the development and implementation of data pipelines and data warehouses
Experience working with stakeholders to develop product backlog grooming, sprint planning data engineering and
QA Testing
Migration of SQL Server to Azure is desired
Experience with Azure Data Factory, ADLS, and CI/CD highly desired
The Hands-On Data Engineering Lead is responsible for leading a team of data engineers in the design, development, and
implementation of data pipelines and data warehouses. The ideal candidate will have a strong understanding of data
engineering principles and practices, as well as experience with SQL Server SSIS. The Hands-On Data Engineering Lead will
also be responsible for managing the team's workload, mentoring junior engineers, and communicating with stakeholders.
Additionally, the Hands-On Data Engineering Lead will be expected to be hands-on with the development and
implementation of data pipelines and data warehouses, and to have experience working with stakeholders to develop
product backlog grooming, sprint planning data engineering and QA Testing.
Responsibilities:
Lead the design, development, and implementation of data pipelines and data warehouses
Develop and implement data pipelines and data warehouses using SQL Server SSIS
Troubleshoot data pipelines and data warehouses
Communicate with stakeholders to understand their needs and ensure that projects meet their requirements
Manage the team's workload and ensure that projects are completed on time and within budget
Mentor junior data engineers and help them develop their skills
Stay up-to-date on the latest data engineering technologies and best practices
Provide support to production teams and QA teams
Work with stakeholders to develop product backlog grooming, sprint planning data engineering and QA Testing
Working with other engineers to design and implement data pipelines and data warehouses using Azure Data
Factory, ADLS, and CI/CD
Experience in migrating SQL server to Azure
30% team management and 70% hands-on
Job Type: Full-time
Salary: $135,000.00 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 10 years (Required)
SQL: 10 years (Required)
Data warehouse: 10 years (Required)
SSIS: 10 years (Required)
Work Location: In person",-1.0,Rivago Infotech Inc,"New York, NY",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
857,Sr. Data Engineer,Employer Provided Salary:$84K - $191K,"Job Summary:
The primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver modules, stable application systems, and Data or Platform solutions. This includes developing, configuring, or modifying complex integrated business and/or enterprise infrastructure or application solutions within various computing environments. This role facilitates the implementation and maintenance of complex business and enterprise Data or Platform solutions to ensure successful deployment of released applications.
Minimum Qualifications
Bachelor's Degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)
5 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering
4 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)
Preferred Qualifications
Master's Degree in Computer Science, CIS, or related field
5 years of IT experience developing and implementing business systems within an organization
5 years of experience working with defect or incident tracking software
5 years of experience writing technical documentation in a software development environment
3 years of experience working with an IT Infrastructure Library (ITIL) framework
3 years of experience leading teams, with or without direct reports
5 years of experience working with source code control systems
Experience working with Continuous Integration/Continuous Deployment tools
5 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutions
Job Type: Full-time
Salary: $84,002.64 - $190,979.59 per year
Experience:
Data, BI or Platform Engineering, Data Warehousing/ETL: 3 years (Preferred)
developing and implementing business systems: 3 years (Preferred)
Data Engineer: 4 years (Preferred)
Work Location: In person",-1.0,Market Tree Research,"Carolina, WV",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
858,Data Engineer,$73K - $110K (Glassdoor est.),"Data Engineer

Posting Date: Aug 23, 2023
Location: Houston, TX, US
Lake Charles, LA, US
Company: Sasol
A job at Sasol is a career made by you, with purpose, development opportunities, benefits to support every stage of life and a working culture that embraces flexibility, diversity, and inclusion. Learn more.

Purpose of Job
The Data Engineer works as part of an agile team, interacts with data scientists/translators and industry experts to understand how data needs to be converted, loaded and presented. Collects, structures, analyses data and ensures quality. Responsible for the definition, conception, construction, and ongoing support of big data platforms and solutions.
Recruitment Description / Key Accountabilities
Integrate IT (Information Technology) and OT (Operational Technology) Systems.
Use programming languages and big data technologies following best practices and standards
Understands latest analytics tools and technologies (open source/ proprietary/ vendor supported) and determines fit with current architecture
Building and maintaining data pipelines that move data between various systems.
Ensuring data quality, integrity and security
Monitoring and optimizing data performance
Collaborate with technical teams to develop and maintain a flexible and sustainable workflow.
Collaborate with Product Owner, Data Scientists, Data Translators, and other Business Stakeholders to implement and deploy scalable solutions in an agile way of working; actively participate in scrum process and meetings with team where required
Assist with communication, improve transparency, radiate information, and address challenges effectively
Formal Education
University Bachelors Degree in Computer Science & Inf Systems or Engineering: Computer
Min Experience
Experience: 5+ relevant years
Long Description
Excellent communication and written skills.
Experience in building complex data pipelines using Cloud Services, REST APIs, SQL Databases. Python and SQL skills are are required. Experience in Azure ADF, Synapse or Databricks are beneficial.
Knowlege of Cloud Computing and Infrastructure, Data Models and Standards, Database Administration, Plant Information Management Systems, ERP Systems, Networking and Protocols, Security and Compliance, Change Managemnt, Problem Solving and Trouble Shooting are a plus.

Sasol (USA) Corporation is an Equal Opportunity Employer and gives consideration for employment to qualified applicants without regard to race, color, religion, creed, age, sex, sexual orientation, gender identity, pregnancy, national origin, disability or protected veteran status, as well as any other characteristic protected by applicable law, regulation or local ordinance. For more information about your rights under the law, see http://www.eeoc.gov/employers/upload/poster_screen_reader_optimized.pdf

Sasol treats work-authorized job applicants equally in recruiting and hiring without regard to their citizenship status or national origin unless required by law, regulation, executive order, government contract, or determination by the Attorney General. Please note Sasol will not sponsor, obtain, and/or petition for temporary visa status (for example, E, F-1, H-1, H-2, L, B, J, or TN) for candidates for this position and/or personnel hired for this position.


Nearest Major Market: Houston",3.9,"Sasol
3.9","Houston, TX",10000+ Employees,1950,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$10+ billion (USD)
859,Data Engineer (Informatica Power Center),Employer Provided Salary:$70.00 - $75.00 Per Hour,"This is a 6 months contract role
Skills: Informatica Power center, Data Warehouse Multi-Dimensional modelling, SQL and PL/SQL skills.
Should be strong in Informatica Power center Development and Data Warehouse Multi-Dimensional modelling along with good SQL and PL/SQL skills.
Job Summary
The Data Engineer is part of a Corporate Analytics team responsible for supporting data and analytics solutions for Client's Corporate Functions. This individual will collaborate with analytics team to design and implement Client corporate data strategy, ensuring reliable data infrastructure and creating data solutions for variety of business use cases. This individual will primarily support on enterprise financial data warehouse and multiple data marts working with various stakeholders and reporting applications team, successfully delivering the data requirement solutions.
What you need to have:
8+ years of experience with Oracle and SQL Server databases
• 8+ years of experience with Informatica Power Center.
8+ years software development experience in one or more of the following areas: ETL, BI and DW
• 8+ years of experience in DW Multi-Dimensional modelling.
Understanding of the Project Delivery Framework and SDLC
Understanding of Waterfall and Agile Project Management Methodology
Strong analytical, communication and interpersonal skills.
Excellent and proven collaboration skills
We will count on you to:
Designs, develops, automates, and supports complex applications to extract, transform, and load data.
Ensures data quality of the warehouse and data marts.
Plans and conducts ETL unit and development tests; monitors result and takes corrective action.
Work on basic and advanced transformations in informatica power center independently.
Performance Tuning of SQLs and handles huge volumes of data.
Ability to quickly diagnose the problem areas and come up with solutions and/or workarounds
Build, maintain, and enhance all objects packages/functions in PL/SQL to support application process.
Translates transformation and movement requirements into functional requirements and mapping designs.
Designs automation processes to control data access, transformation, and movement.
Ensures source data availability and update accessibility, data integrity, restart ability, and error handling.
Participates in the overall development process via architecture guidance, design and code reviews, and estimation and planning assistance
Performs other related duties as required Knowledge, Skills, And Abilities
Documents and troubleshoot problems and effectively communicate with business and technical team members at all levels
What makes you stand out?
Excellent communication and presentation skills
Domain knowledge in one of more of corporate functions such as HR, Finance, Real Estate is preferred
Excellent Problem-solving skills with innovative and proactive approach
Ability to recommend and implement best practices and processes
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Vision insurance
Experience level:
8 years
Application Question(s):
Please provide your LinkedIn profile link
Elaborate your experience in Datawarehouse Multi-Dimensional modelling.
Please describe your background in Informatica Power center development.
Please provide date of birth in MMDD format (Month and Date only)
What is your current location with the Zip code?
What is your desired hourly rate?
Have you applied or been interviewed for any role with Marsh McLennan in the past? If so, please provide details.
What is your work authorization status? US citizens, Green Card, Visa...
Elaborate on your SQL and PL/SQL's skills
Work Location: In person",2.9,"Sun Spread
2.9","New York, NY",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
860,Sr. Azure Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"LeadStack Inc. is an award-winning, one of the nation's fastest-growing, certified minority-owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Position Details:
Title: Azure Data Engineer
Location: Remote
Duration: 6+ Months Contract
Top 3 skills: SQL, Snowflake, and Data Warehousing/Design
Its a W2 Role. No C2C.
Client is seeking a talented and motivated Data Engineer to join our dynamic team. As a Data Engineer, you will play a critical role in developing and maintaining our data infrastructure, ensuring the smooth flow of data for our analytics and insights projects. The ideal candidate will have a strong background in SQL, data warehousing, ETL processes, and a preference for Snowflake experience. Additional experience with dbt or semantic layer tools will be a bonus, and proficiency in Python will also be advantageous.
Bachelor's degree in Computer Science, Engineering, or a related field.
Solid experience with SQL and data warehousing concepts.
Strong understanding of ETL processes and tools.
Preferred experience with Snowflake data warehouse platform.
Bonus for experience with dbt or other semantic layer tools.
Proficiency in Python is a plus.
Excellent problem-solving and analytical skills.
Strong attention to detail and ability to work in a fast-paced environment.
Effective communication skills and ability to collaborate with cross-functional teams.
Key Responsibilities:
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.
Optimize data ingestion and transformation processes to ensure efficient and scalable data flows.
Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.
Develop and maintain data models, schemas, and data dictionaries for efficient data storage and retrieval.
Perform data validation, quality checks, and troubleshooting to identify and resolve data-related issues.
Monitor and optimize database performance, including query tuning and index optimization.
Collaborate with data scientists and analysts to implement data models and support their data requirements.
Stay up-to-date with industry trends and best practices in data engineering, and proactively recommend and implement improvements to our data infrastructure
To know more about current opportunities at LeadStack, please visit us at https://leadstackinc.com/careers/
Should you have any questions, feel free to call me on 510-480-1006 or send an email on tarique.aziz@leadstackinc.com
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Data Warehousing/Design: 8 years (Required)
Snowflake: 5 years (Required)
Azure: 5 years (Required)
Work Location: Remote",4.3,"Leadstack Inc
4.3",Remote,51 to 200 Employees,2016,Company - Private,HR Consulting,Human Resources & Staffing,Unknown / Non-Applicable
861,Data Engineer (Informatica Power Center),Employer Provided Salary:$70.00 - $75.00 Per Hour,"Data Engineer (Informatica Power Center)
Job Summary
The Data Engineer is part of a Corporate Analytics team responsible for supporting data and analytics solutions for Client's Corporate Functions. This individual will collaborate with analytics team to design and implement Client corporate data strategy, ensuring reliable data infrastructure and creating data solutions for variety of business use cases. This individual will primarily support on enterprise financial data warehouse and multiple data marts working with various stakeholders and reporting applications team, successfully delivering the data requirement solutions.
Required experience
8+ years of experience with Oracle and SQL Server databases
• 8+ years of experience with Informatica Power Center.
8+ years software development experience in one or more of the following areas: ETL, BI and DW
• 8+ years of experience in DW Multi-Dimensional modelling.
Understanding of the Project Delivery Framework and SDLC
Understanding of Waterfall and Agile Project Management Methodology
Strong analytical, communication and interpersonal skills.
Excellent and proven collaboration skills
Responsibilities:
Designs, develops, automates, and supports complex applications to extract, transform, and load data.
Ensures data quality of the warehouse and data marts.
Plans and conducts ETL unit and development tests; monitors result and takes corrective action.
Work on basic and advanced transformations in informatica power center independently.
Performance Tuning of SQLs and handles huge volumes of data.
Ability to quickly diagnose the problem areas and come up with solutions and/or workarounds
Build, maintain, and enhance all objects packages/functions in PL/SQL to support application process.
Translates transformation and movement requirements into functional requirements and mapping designs.
Designs automation processes to control data access, transformation, and movement.
Ensures source data availability and update accessibility, data integrity, restart ability, and error handling.
Participates in the overall development process via architecture guidance, design and code reviews, and estimation and planning assistance
Performs other related duties as required Knowledge, Skills, And Abilities
Documents and troubleshoot problems and effectively communicate with business and technical team members at all levels
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Experience level:
10 years
7 years
8 years
9 years
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Required)
SQL: 5 years (Preferred)
Data warehouse: 8 years (Required)
Work Location: In person",-1.0,Innovato,"New York, NY",-1,-1,-1,-1,-1,-1
862,Senior Data Engineer,Employer Provided Salary:$115K - $140K,"ABOUT THE JOB
Data is one of the foundations of Everside Health and plays an integral role in delivering first-class healthcare services to our patient population. We utilize a wide variety of tools to produce valuable data and provide better patient care as a result.
As a Senior Data Engineer, you will be an established thought leader through close partnerships with expert resources to design, develop, and implement data assets for a wide range of new initiatives at Everside Health. The role involves heavy data exploration, proficiency with SQL, ETL, knowledge of service-based deployments and APIs, and the ability to discover and learn quickly through collaboration. There is a need to think analytically and outside of the box while questioning current processes and continuing to build your business acumen. There will be a combination of team collaboration and independent work efforts. This role involves interaction with the Analytics team as well as a wide range of business areas across Everside.
We seek candidates with a strong quantitative background and excellent analytical and problem-solving skills. This position combines business and technical skills involving interaction with business customers, Analytics partners, internal and external data suppliers, and information technology partners.
ESSENTIAL DUTIES & RESPONSIBILITIES
Deliver data warehouse and analytic solutions by ingesting, integrating, and curating data to deliver information to the business and stakeholders
Conduct ETL design, development, and maintenance including data extraction, manipulation, analysis, source-target mapping, change data capture, code performance
Ensure seamless integration of data across the enterprise and drive automation of common and repeated tasks
Conduct performance tuning and optimization of all processes executed across the data platform
Develop large scale data structures and pipelines to organize, collect, and standardize data that helps generate insights and addresses reporting needs
Collaborate closely with business users and stakeholders to define data for the design, development, and deployment of new solutions that support strategic business priorities
Provide coaching and training to junior and new team members on ETL architecture, standards, and documentation
QUALIFICATIONS
Bachelor’s degree in Computer Science or related field and 5+ years of Data Engineering work experience. Experience designing and implementing ETL pipelines, working with a variety of data warehousing models and design fundamentals, working with Matillion, Azure Data Factory, Data Bricks, or similar ETL tool. Experience working in Snowflake, Synapse, or similar MPP platform and experience in DataOps/DevOps and agile methodologies
DESIRED ATTRIBUTES
Experience with messaging streaming systems (e.g., Kafka, Azure Event Hubs)
Experience in hybrid data processing methods (batch and streaming)
Experience with AWS or Azure application deployment
Experience with API integration
Pay Range: $115,000 - $140,000/yr
The actual offer may vary dependent upon geographic location and the candidate’s years of experience and/or skill level.
Everside Benefits Summary
We believe in empowering teammates to do their best work and build better healthcare. Below are some of our benefit offerings. Eligibility is based on 24/hr week.
Health and Well-Being: Free Everside membership for in person and virtual care, employer paid life and disability insurance, and choice in medical/dental plans, vision, employer funded HSA, FSA, and voluntary illness, accident and hospitalization plans. Benefits are effective on the first of the month following date of hire.
Financial Support: Competitive compensation, 401k match, access to financial coaching through our Employee Assistance Program
Lifestyle: Paid time off for vacation, sick leave, and more, holiday schedule
Learn more at
https://www.eversidehealth.com/careers/",3.2,"Everside Health
3.2",Remote,1001 to 5000 Employees,2001,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
863,Senior Data Engineer,Employer Provided Salary:$166K - $232K,"Patreon is the best place for creators to build memberships by providing exclusive access to their work and a deeper connection with their communities. We're building a content and community platform where creators can engage directly with their fans and monetize their creativity, while maintaining full ownership over the work they make and the communities they create.
We're leaders in the membership space with 250,000+ active creators and over $3.5 billion paid directly to creators on our platform. Our team is building tools to optimize the creator-to-fan relationship, including native video, enhanced podcasting features, improved creation tools, and new community experiences. We're continuing to invest heavily in building the most talented team in the Creator Economy and are looking for a Data Engineer to support our mission.
This role is available to those wishing to work in our SF and NY offices on a hybrid work model or those wishing to be fully remote in the United States.
About the Role
Work on a tight-knit team of highly motivated and experienced data engineers with frequent collaboration with data scientists, product managers and product engineers.
Work on both ""data analytics"" and ""data infrastructure"" type projects in a fast-paced, high-growth startup environment.
Build core data sets and metrics to power analytics, reports and experimentation.
Write real-time and batch data pipelines to support a wide range of projects and features including our creator-facing analytics product, executive reporting, FP&A, marketing initiatives, model training, data science analytics, A/B testing, etc.
Help manage and build out our data platform and suite of data tools.
Be a driver of a data-centric culture at Patreon. Work autonomously on large green field initiatives and help define data best practices at the company.
About You
Expert in SQL, Spark and Python or Scala
Significant experience modeling data and developing core data sets and metrics to support analytics, reports and experimentation. Solid understanding of how to use data to inform the product roadmap.
Enjoy collaborating with Data Scientists, Product Managers and Product Engineers. Comfortable playing the role of a Project Manager in order to drive results.
Have previously built real-time and batch data pipelines using tooling such as Airflow, Spark, Kafka, S3, Fivetran, Census, etc. Experience working with event tracking frameworks, data observability frameworks and experimentation frameworks.
Experience managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc.
Highly motivated self-starter that is keen to make an impact and is unafraid of tackling large, complicated problems and putting in the work to ensure high craft deliverables.
About Patreon
Patreon powers creators to do what they love and get paid by the people who love what they do. Our team is passionate about making this mission and our core values come to life every day in our work. Through this work, our Patronauts:
Put Creators First | They're the reason we're here. When creators win, we win.****
Build with Craft | We sign our name to every deliverable, just like the creators we serve.
Make it Happen | We don't quit. We learn and deliver.
Win Together | We grow as individuals. We win as a team.
We hire talented and passionate people from different backgrounds because we want our company to represent the vast diversity of our creators. If you're excited about a role but your prior experience doesn't match with every bullet point outlined above, we strongly encourage you to apply anyway. If you're a creator at heart, are energized by our mission, and share our company values, we'd love to hear from you.
Patreon is proud to be an equal-opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.
Patreon offers a competitive benefits package including and not limited to salary, equity plans, healthcare, unlimited paid time off, company holidays and recharge days, commuter benefits, lifestyle stipends, learning and development stipends, patronage, parental leave, and 401k plan with matching.
The posted range represents the expected salary range for this job requisition and does not include any other potential components of the compensation package, benefits and perks previously outlined. Ultimately, in determining pay, we'll consider your experience, leveling, location and other job-related factors.
San Francisco Pay Range
$166,000—$231,500 USD",3.2,"Patreon
3.2","San Francisco, CA",51 to 200 Employees,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
864,Big Data Engineer,Employer Provided Salary:$48.00 - $52.00 Per Hour,"Greetings from KonnectingTree Inc.,
We are actively looking for a candidate with below mentioned experience,
Role : Bigdata Engineer
Location : Hybrid ( Bloomington, MN)
Experience : 3-5 years
Job Description:
* Good experience in build and deployment of Big data applications using Pyspark
* Good experience in Python.
* Experience in Hadoop file structure.
* Good Communication and team player
Job Type: Contract
Salary: $48.00 - $52.00 per hour
Experience level:
3 years
4 years
Schedule:
8 hour shift
Ability to commute/relocate:
Bloomington, MN 55425: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",-1.0,KONNECTINGTREE INC,"Bloomington, MN",-1,-1,-1,-1,-1,-1
865,Data Engineer,$77K - $107K (Glassdoor est.),"Position Summary:
Reporting to the Director of Data Science, the Data Engineer will play a crucial role in designing, developing, and maintaining data pipelines and workflows to facilitate data integration, storage, and analysis. With the support of the Data Science team, this position will utilize proven expertise in Microsoft Azure cloud computing to implement and automate processes, ensure the health and efficiency of the data ecosystem, and bring unmatched value to our +1.5 million members across the country and across the world.

Duties and Responsibilities:
Lead architectural efforts for implementing the Data Science team's models and solutions.
Design, implement, and manage data pipelines using Azure Data Factory to extract, transform, and load data from various sources efficiently and securely.
Develop and optimize storage solutions on Microsoft Azure, ensuring data availability, scalability, and reliability.
Automate data-related jobs, processes, and workflows to streamline data delivery
Collaborate closely with cross-functional teams, including data scientists, analysts, and developers, to understand data requirements and support their data-related needs.
Design and maintain data models and architecture, enabling efficient data consumption by business applications and analytical tools.
Ensure data quality, integrity, and security throughout the data lifecycle.
Troubleshoot and promptly resolve data-related issues, performance bottlenecks, and system failures.
Research new data sources, third-party platforms, and data engineering best practices.
Stay current with the latest data engineering and cloud computing trends, particularly in the Azure ecosystem.
Assist Data Scientists with general analysis and dashboards, as needed.
Qualifications:
3-5+ years of relevant Data Engineering, Software Engineering, and/or Analytics experience.
Proved experience with Azure cloud solutions, preferably through your work portfolio and Azure certifications.
Strong proficiency in Azure Data Factory, Azure Storage, and other relevant Azure services.
Extensive experience designing and implementing automated data pipelines, ETL processes, and data workflows.
Advanced skills in SQL and Python for data manipulation, querying, and scripting tasks.
Familiarity with data warehousing concepts, modeling, and integration techniques.
Expert in integrating cloud solutions with external API sources and knowledgeable in creating in-house API solutions that meet our needs.
Excellent problem-solving abilities and a track record of handling complex data engineering challenges.
Strong communication skills and the ability to collaborate effectively with multidisciplinary teams.
Demonstrated ability to prioritize tasks, meet deadlines, and adapt to evolving project requirements.
Azure certifications (e.g., Azure Data Engineer Associate) will be a plus.
Willingness to travel up to 10%.
Organizational Overview:
The National Association of REALTORS® (NAR) is a team of professionals dedicated to providing world-class service to over 1.5 million REALTORS® working in the United States and around the world. The real estate industry is fast-paced and fast-changing--each year, our members participate in the sale, lease, and management of billions of dollars in real estate. As in every industry, our members' value proposition is constantly being challenged by innovation.

It is our mission to empower REALTORS® as they preserve, protect and advance the right to real property for all. We cannot do that without the ideas, passion, and commitment from our talented employees. As our greatest assets, employees are offered their pick of competitive benefits/perks and flexible work options. NAR is certified as a Great Place to Work®. Our most successful employees are committed to our Core Values, which are:

Putting members first
Leading change
Advancing Diversity and Inclusion
Giving respect
Collaborating
Communicating",3.7,"National Association of REALTORS®
3.7","Washington, DC",201 to 500 Employees,1908,Nonprofit Organization,Real Estate,Real Estate,$100 to $500 million (USD)
866,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
867,AI/ML Data Automation Engineer,$88K - $125K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
868,Data Engineer,Employer Provided Salary:$150K - $300K,"What we're looking for
Stytch is the platform for user authentication. We build infrastructure that sits in the critical path of our customer's applications. As a data engineer, you'll work on designing and building event-driven architecture systems to drive analytics insights and observability tooling for our customers.

What Excites You
Championing data-driven insights - you see data analytics and observability as a product critical to success
Solving problems with pragmatic solutions — you know when to make trade-offs between completeness and utility and you know when to cut scope to ship something good enough quickly
Building products that make developers lives easier — as a data engineer for a developer infrastructure company, what you build will have an immediate impact on our customers
Shaping the culture and growing the team through recruiting, mentorship, and establishing best practices
Learning new skills and technologies in a fast paced environment

What Excites Us
Comfort working in a modern data stack using tools like Snowflake, Redshift, DBT, Fivetran, ElasticSearch, and Kinesis
Appreciation for schema design and architecture that balance flexibility and simplicity
Experience designing and building highly reliable back-end and ETL systems
3+ years as a data or backend engineer

What Success Looks Like
Technical — build new, highly reliable services that our customers can depend on
Ownership — advocate for projects and solutions that you believe in and ship them to production
Leadership — level up your teammates by providing mentorship and guidance

Our Tech Stack
Data moves through Snowflake, ElasticSearch, MySQL, and Kinesis
Go and Node for application services
We run on AWS with Kubernetes for containerization
gRPC and protobufs for internal service communication

Expected base salary $150,000-$300,000. The anticipated base salary range is not inclusive of full benefits including equity, health care insurance, time off, paid parental leave, etc. This base salary is accurate based on information at the time of posting. Actual compensation for hired candidates will be determined using a number of factors including experience, skills, and qualifications.
We're looking to hire a GREAT team and that means hiring people who are highly empathetic, ambitious, and excited about building the future of user authentication. You should feel empowered to apply for this role even if your experience doesn't exactly match up to our job description (our job descriptions are directional and not perfect recipes for exactly what we need). We are committed to building a diverse, inclusive, and equitable workspace where everyone (regardless of age, education, ethnicity, gender, sexual orientation, or any personal characteristics) feels like they belong. We look forward to hearing from you!

Learn more about our team and culture here!",5.0,"Stytch
5.0",California,1 to 50 Employees,2020,Company - Public,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
869,Data Movement Engineer C2H,Employer Provided Salary:$55.00 Per Hour,"Client required information:
***They need Someone who has strong Talend Experience minimum of 5 years***.
Talend API is Desirable, But any API is also fine like snowflake as a last option.
They are working on Data Base Movement from Talend to ETL/ELT.
Formally they need ETL/ELT Developers or Talend Developers with ETL/ELT data movement experience.
Implementation and Configuration Talend is Mandate.
They also need to take a an on call support after work hours for ongoing production Support team, this might happen on weekends too.
They are going to implement more API’s, so having extra API experience is plus.
They need to understand the baseline of Insurance( The Basic work or Knowledge on Insurance Modules).
They can Work Remote, But need only from EST Zones.
Manager is not going to accept any profiles without Talend Experience.
Skill Qualifications Required:
· Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
· Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
· Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
· Some experience with cloud-based database technologies required
· Working knowledge of data warehousing concepts, structures and ETL best practices
· Experience using query tools (e.g. AQT, MS Query)
· Ability to problem solve using analytical thinking skills
· Must work well independently - must be inquisitive and seek answers to complex questions without being prompted
· Strong organizational and time management skills
· Strong communication skills including verbal and written to communicate effectively with clients and management
· Strong project management skills to ensure that projects get done on time and within budget
· Effectively participates in teams and moves the team toward completion of goals
Job Types: Full-time, Contract
Salary: From $55.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Experience:
Talend: 5 years (Required)
ETL: 5 years (Required)
SQL query: 3 years (Required)
data warehousing: 3 years (Required)
business intelligence tools: 3 years (Required)
Work Location: Remote",-1.0,Technovant inc,"New Haven, CT",-1,-1,-1,-1,-1,-1
870,AI/ML Data Automation Engineer,$88K - $125K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
871,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
872,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
873,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
874,Cloud Data Engineer,Employer Provided Salary:$55.00 - $65.00 Per Hour,"Note: This is a W2 - contract to hire role
Azure Data Engineer
12+ months (contract to hire)
100% remote

Job Description:
As a Senior Engineer - DBA, the Cloud Data Engineer candidate will be responsible for using the candidate’s technical knowledge to solve business problems.
The client is looking for a talented individual who can serve as a subject matter expert in their area of focus and represent their department on complex assignments.
Candidates will be responsible for evaluating elements of technology effectiveness through requirements gathering, testing, research, and investigation and making recommendations for improvements that result in increased quality and effectiveness.
Candidates will be required to listen to and evaluate customer needs to determine and provide high-quality solutions that align with customer expectations.
In this role on the Cloud Data Engineering team, the candidate will be a part of a team responsible for migrating existing data platform solutions to the cloud.
Candidates will also collaborate with others to develop new data pipelines.
As a data engineer in the Cloud Data Engineering team, the candidate must work in a data-first mindset.
The candidate will use Microsoft Azure Cloud PaaS technologies to engineer client data solutions in a way that allows us to optimize client information, make better decisions, and meet client customer’s needs.
If the Candidate has a passion for engineering, working with massive amounts of data, and empowering smart business decisions, this is the role for you.
Equal Opportunity Employer/Disability/Veterans

Required Qualifications:
Must be presently authorized to work in the U.S. without a requirement for work authorization sponsorship by the company for this position now or
in the future
Must be committed to incorporating security into all decisions and daily job responsibilities
3 plus of related experience in Cosmos DB/Similar DB technology
Experience with configuration management and building automation capabilities such as Git/Jenkins/Bitbucket
Experience with Microsoft Azure Cloud workflows
Experience in T-SQL and scripting skills.
Knowledge of Microsoft cloud-managed DBs/Systems, e.g. Managed SQL Instance, Cosmos DB, Databricks Delta Lake
Experience with Big Data solutions such as Delta Lake by Databricks and SQL DBMSs
Independently analyze, solve, and correct issues in real-time, providing problem resolution end-to-end
Identify new opportunities and help refine automation of regular processes, track issues, and document changes
Solve/Assist in complex query tuning and schema refinement
Expert in troubleshooting performance issues
Experience rightsizing Database object workflow for cost management
Ability to multi-task and context-switch effectively between different activities and teams
Join the on-call rotation with other Engineers
Must be able to both collaborate in a team-oriented environment and work independently with direction
Must be able to work in a fast-paced environment with the ability to handle multiple tasks

Pay Range: $55/hr - $65/hr
The specific compensation for this position will be determined by a number of factors, including the scope, complexity and location of the role as well as the cost of labor in the market; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. Our full-time consultants have access to benefits including medical, dental, vision and 401K contributions as well as any other PTO, sick leave, and other benefits mandated by appliable state or localities where you reside or work.",3.7,"Pinnacle Technical Resources
3.7",Missouri,1 to 50 Employees,-1,Company - Public,-1,-1,$1 to $5 million (USD)
875,Senior Data Center Engineer,-1,"Marco is a one-stop shop for all things business tech. Our employees are “movers and shakers” and our company is always striving to do what’s right. Does this sound like a culture you want to be a part of? We’re hiring a new team member to help take Marco’s technology further – working full-time, Monday - Friday, 8am-5pm.
More about us. We do it all – from copy and print solutions to IT and managed services. We are an organization led by salespeople with 650+ engineers ready to fix any and all issues. We have offices in 12 states and service nationally.
Join our growing team. You won’t regret it.

The Senior Data Center Engineer is responsible for providing quality services and solutions to our internal clients while maintaining a high level of client satisfaction. This position works cooperatively with other teams in a coordinated effort to develop, design, implement and support Marco’s Data Centers, along with the IT environment that depends upon them. While not a leadership position, the Senior datacenter engineer displays leadership qualities and may be called upon to lead others in certain situations.

ESSENTIAL FUNCTIONS:
Lead the overall design and implementation of the technology stack within the Marco Data Centers
Maintain physical access to the Marco Data Centers and be a primary contact for essential services
Perform proactive maintenance and reactive support to the equipment within the Marco Data Centers
Create and maintain documentation for all equipment within the Marco Data Center. Review and assist others with their documentation, offering constructive feedback and pointing out deviations from our policies and standards where necessary.
Report on the health of the Marco Data Centers, perform capacity planning and proactively provide recommendations for future growth
Monitor and provide proactive recommendations for backup systems and disaster recovery processes
Review all systems and make technical/process recommendations for improving efficiency
Assist in defining standards for all systems and processes provided by the Data Center Services group
Administer and maintain a reliable, stable, secure and cost-effective hosting environment to deliver 24x7x365 operational service
Provide escalated remote technical support to Marco Data Center clients. Handle requests with due urgency, empathy, and a spirit of cooperation.
Document your work, including troubleshooting steps, time, and resolution in the service ticketing system.
Ensure your work and the work of others adheres to Marco internal policies and applicable regulatory requirements.
Create and maintain standard templates, documentation, images, policy, and automation that will aid in a consistent and rapid deployment of Marco Data Centers
Research, test and recommend new tools and products to enhance the offering and efficiency of Marco’s Data Centers
Document the procedures, infrastructure and environments and keep updated with changes to the environment
Implement and maintain Data Center system builds that are consistent, optimized for client experience, overall performance and supportability
Set deadlines for projects, then organize your time to meet these deadlines; be willing to put in extra time and effort to meet a time-bound goal when necessary
Mentor other staff while you assist with implementation or utilization within your area of technical specialty
Work multiple projects simultaneously and provide consistent, high-quality results
Assist with implementation and or migration of clients’ existing data and applications
Perform system and application testing to provide functional environment
Provide onsite and or remote technical support to Marco clients during onboarding process
Implement changes and work orders for existing Marco clients
Review customer hosted systems and make technical/process recommendations for improving efficiency
Stay up to date with relevant state-of-the-art technology, equipment, and/or systems
QUALIFICATIONS:
Bachelor’s Degree and 4+ years of experience or equivalent experience
Experience and current certifications or specializations aligning with the technologies used in Marco’s Data Center to include Cisco NX-OS, SAN-OS, IOS, UCS-B, ASAv, EMC VNX, Meraki MX, VMWare vSphere, Citrix XenApp, Zscaler and XenDesktop preferred.
Current high level industry recognized certifications including one or more of the following: CCNP, CCE, VCDX, EMCTA
Solid understanding of the administration of network operating systems and services found in a Data Center environment
Benefits:
We’re not just competitive when it comes to business tech – we’re also pretty proud of what we offer our employees. Our benefits include medical, dental, and vision insurance. We also have paid holidays and vacation, 401k with generous company match, flexible spending accounts, employee purchase program, employer-paid life insurance, voluntary-term life insurance, short and long-term disability, critical illness and accident benefits, and pet insurance. Yes, we care about your furry family too.

all benefits are dependent on employment status
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities
Applicant Labor Law Posters",4.0,"Marco Technologies LLC
4.0",Remote,1001 to 5000 Employees,1973,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
876,Data Engineer (multiple openings) - IHM,Employer Provided Salary:$88K - $157K,"Discover. A brighter future.
With us, you’ll do meaningful work from Day 1. Our collaborative culture is built on three core behaviors: We Play to Win, We Get Better Every Day & We Succeed Together. And we mean it — we want you to grow and make a difference at one of the world's leading digital banking and payments companies. We value what makes you unique so that you have an opportunity to shine.

Come build your future, while being the reason millions of people find a brighter financial future with Discover.

Job Description:
Employer: DFS Corporate Services LLC

Job Title: Data Engineer (multiple openings)

Job Location: Riverwoods, Illinois

Job Type: Full Time

Duties: Responsible for designing, developing, testing, and maintaining complex data solutions for the product. Mentor and influence peers to achieve commitments on data solutions on time and with quality. Telecommuting and/or working from home may be permissible pursuant to company policies.

Requirements: Employer will accept a Bachelor's degree in Computer Science, Computer Engineering, or a related field and 3 years of experience in Senior Data Engineer; ODI/DW/Batch Developer or related occupation.

Position required skills: Three (3) years of experience in the job offered or related occupation: working with Agile software development methodology and Agile management tools, including Jira; utilizing modern engineering practices, including the design and development of ETL data integration solutions in data warehouse environments; working with relational databases and Cloud-based technologies; utilizing design and architecture experience on large-scale ETL solutions; and working with Enterprise Data warehouse data models and dimensional modeling concepts, source to target mapping, and data integration architecture.

Position eligible for incentives under Employee Referral Program.

Rate of Pay: The base pay for this position generally ranges between $88,150.00 to $157,000.00. Additional incentives may be provided as part of a market competitive total compensation package. Factors, such as but not limited to, geographical location, relevant experience, education, and skill level may impact the pay for this position. We also offer a range of benefits and programs based on eligibility. Learn more at MyDiscoverBenefits.com .

QUALIFIED APPLICANTS: Please apply directly through our website by clicking on “Apply Now.” No calls. Equal Opportunity Employer/disability/vet.

What are you waiting for? Apply today!

All Discover employees place our customers at the very center of our work. To deliver on our promises to our customers, each of us contribute every day to a culture that values compliance and risk management.

Discover is committed to a diverse and inclusive workplace. Discover is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or other legally protected status. (Know Your Rights)",3.9,"Discover Financial Services
3.9","Riverwoods, IL",10000+ Employees,1985,Company - Public,Banking & Lending,Financial Services,$1 to $5 billion (USD)
877,Data Scientist / ML Engineer (REMOTE /Contract),-1,"Role: Data Scientist / Machine Learning Engineer
Location: REMOTE
Duration: 6+ months
Scheduling Interviews
Skills/Experience:
6+ years of experience working in Data Science and Machine Learning field.
Extensive experience in applying data preprocessing, statistical analysis, data analytics tools, predictive modelling, model deployment and evidence-based approaches to find lean, actionable solutions to various real-world enterprise business problems.
Experience in the application of Supervised and Unsupervised Learning algorithms, Naïve Bayes, Regression Analysis, Neural Networks/Deep Learning, Support Vector Machines (SVM), Random Forest, K-Means, Hierarchical, Spectral clustering, DBSCAN, Collaborative Filtering and other advanced machine learning techniques.
Used AWS Cloud Services Sagemaker, ECS, EKS, S3, Redshift, QuickSight
Experience with Google Cloud Platform (GCP) AutoML, Vertex AI, BigQuery, Colab, DataProc, Data Studio,
Exposure to Azure ML, PowerBI, Azure SQL and DataBricks
Design custom BI reporting dashboards or interactive data visualizations and widgets in R and Python using Shiny, Tableau, Ggplot2, Plotly, Matplotlib, and Seaborn.
Produce custom BI reporting dashboards in R and Python using Shiny, and Plotly for rapid dissemination of actionable, data driven insights.
Strong experience in Software Development Life Cycle (SDLC), MLOPS
Experience in working with relational databases (Teradata, Oracle) with advanced SQL skills.",3.5,"Glow Networks
3.5",Remote,51 to 200 Employees,2003,Subsidiary or Business Segment,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
878,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
879,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1.0,Radiant System,Remote,-1,-1,-1,-1,-1,-1
880,Senior Data Integration Engineer,-1,"If you are passionate about energy efficiency & carbon footprint reduction, growing your technical skills, working with clients, or love working in a cross functional team setup, this is the role for you!
The implementation engineering team helps our utility partners integrate with the Opower platform and plays an essential technical role in implementing Opower’s SaaS-based energy efficiency and customer engagement programs. You will work directly with our utility partners’ technical teams to integrate their data with our platform and to configure our applications to meet their program requirements. Additionally, you will work with a broad group of other teams at Opower, including project managers and product engineering teams.
Implementation engineers have a diverse set of technical skills, with a focus on delivering high-quality products customized for our utility partners' needs. A typical week might involve ingesting and analyzing utility data; running Unix command-line text manipulation tools; writing python scripts to automate work; adding documentation; working with a utility partner's technical team; and collaborating with R&D teams on future roadmap projects.
You will be an ideal candidate for this job if you have solid understanding of relational databases, are comfortable with advanced UNIX commands, have done some basic programming, and do not hesitate to ask questions. You will work with a variety of teams in different contexts, making the role a good opportunity if you are looking to develop new skills while helping us accomplish our mission of energy efficiency & carbon footprint reduction for our utility partners.
There is flexibility to work remotely full-time, otherwise we have teams located in Oracle’s San Francisco, CA and Arlington, VA offices. You will join a team of extremely helpful engineers with different cultural and professional backgrounds. Besides standard company benefits like 401k matches and unlimited PTO, you will have an excellent work-life balance, the ability to direct your career, and access to an abundance of educational material and professional trainings to help you grow

Responsibilities:
Work with utility project teams on data integrations and energy efficiency product implementations
Explain technical specifications of Opower data integration and products - including highlighting risks with customer experience when requirements are not met
Analyze, transform, and load utility provided data to meet Opower's data requirements for a successful delivery of downstream end-user communications and web experiences
Develop new or maintain tooling (in ruby or python) to make our data integration and product implementations more cost & time effective
Configure and customize Opower's energy efficiency SaaS platform to meet the specific needs of each client
Develop a deep understanding of our products with the ability to explain them to others with non-technical backgrounds
Improve our ability to customize and deliver energy efficiency products to our customers by optimizing delivery processes and writing useful documentation
About You:
Experience writing effective SQL or Hive queries to analyze large relational datasets
Have experience writing data transformations scripts using ETL tools
Experience performing advanced file searches and text manipulation using the Unix/Linux command-line
Comfortable working directly with client teams
Experience writing software tools using object-oriented programming
You understand code versioning concepts and have experience with tools like git
Can connect dots among difference pieces of information gained from multiple sources
Experience troubleshooting software applications that involve APIs, databases, and frontend
Can effectively prioritize multiple tasks at one time
Enjoy being part of a team, helping and learning from others.
Have 8+ years of professional experience. We are open to hiring at different levels too if there’s a better fit.",3.9,"Oracle
3.9",United States,10000+ Employees,1977,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
881,Data Engineer Azure databricks,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer - Databricks
For: CDW (Direct Client)
W2, Corp to Corp, W2 Contract Okay
Fully Remote
`
DESIRED SKILLS AND ABILITIES
Key Responsibilities
You will design, build and test cross cloud and on-premise data pipelines and leverage Azure Databricks for data processing.
Collaborate with cloud architects, tech leads to facilitate on-premise to Databricks migration
You should understand data pipelines and modern ways of automating data using cloud-based and on-premise technologies
Develop reference architectures and design pattern library for typical Cloud based solutions implementations
Advise on Cloud project set up, security and role based access implementation, and network optimizations
Qualifications:
2+ years with SQL and database table design – able to write structured and efficient queries on large data sets
1-2 years on Databricks on any cloud platform (AWS, Azure)
1-2 years working on AWS/Azure cloud
1-2 year’s design and/or implementation of enterprise applications
Experience in “migrating” on premise workloads to one or more industry leading public cloud(s)
Experience in AGILE development, SCRUM and Application Lifecycle Management (ALM) with scripting experience in Python and shell.
Hands-on experience in the full life-cycle of software development or methodology using Agile Scrum/Kanban etc.
Able to work with agile teams as they perform feature level design, development, testing, and performance analysis
Databricks certification is a plus.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Schedule:
8 hour shift
Work Location: Remote",-1.0,Data Ninjas Inc.,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
882,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
883,Data Engineer III,Employer Provided Salary:$117K - $194K,"As a member of the ShipBob Team, you will benefit from an environment where everything is achievable. We aim to be a place where you can:
Write Your Career Story. Because we are solving some of the most difficult problems in global commerce, you have the opportunity to write the story that will make your career.
Experience Global Impact and Global Connection. At ShipBob we benefit from diverse cultures and perspectives in service of the global community.
Grow With An Ownership Mindset. We believe that great innovation comes from great transparency. We are more resilient and more creative when we have an inclusive and transparent culture where everyone knows our strengths and opportunities.
Title: Data Engineer III
Location: Remote in these states: AZ, CA, CO, FL, GA, KS, KY, IA, ID, IL, IN, MA, ME, MI, MN, MO, NC, NH, NJ, NV, NY, OH, OR, PA, RI, SC, SD, TN, TX, VA, VT, WA, WI
Role Description:
As a Data Engineer III at ShipBob, you will be working within a team handling all facets of the database environment, from assisting engineering teams in table design, to building databases, to working with our data warehouse and ETL. We are looking for someone who enjoys working in a fast paced environment, who can think on his/her feet, and produce results.
What you'll do:
Focus on growing as an data engineer in an Azure SQL environment
Performing SQL programming and performance tuning
Capable of taking well-defined sub-tasks and completing these tasks.
Creating Power BI visualizations and paginated reports
Modeling and implementing databases and warehouses
Demonstrates they are a high energy individual.
Effective in communicating status to the team.
Exhibits ShipBobs's core values, focuses on understanding and living these values.
Accepts feedback graciously and learns from everything they do
Other duties/responsibilities as necessary.
What you'll bring to the table:
6-9 Years of Experience
Excellent problem solving skills
Excellent SQL skills
Excellent communication skills
Performance oriented mindset
Ability to work quickly and collaboratively in a fast-paced, entrepreneurial environment
Experience in the following:
SQL
Performance Tuning and Optimization
Data Modeling
Azure SQL Administration, including Security
Powershell
Azure Data Factory
Nice to have:
Power BI
Ability to own small well scoped projects and implement them
Experience in Azure Synapse and Cosmos
A passion for databases and an understanding that solutions you implement will affect our entire suite of applications
Experience with big data
Experience with Azure DevOps
Experience with Git
Experience with Agile
Experience with Azure DevOps
Experience with Azure Functions
Mongo Atlas experience
Spark
Classification: Exempt
Reports to: Manager, Database Administration
Perks & Benefits:
Medical, Dental, Vision & Basic Life Insurance
Paid Maternity/Parental Leave Program
Flexible Time Off Program
Paid Sick Leave and Paid Emergency Leave
Floating Holidays (2 days/year)
Wellness Days (1 day/quarter)
401K Match
Competitive Salary, Performance Bonus & Equity
Variety of voluntary benefits, such as, short term disability
Referral Bonus Program
Fun Culture >>> Check us out on Instagram (@lifeatshipbob)
ShipBob believes in transparency while providing a competitive total compensation package with a pay for performance approach. The expected base pay range for this position is $116,645 - $194,408.
We recognize that people come with a wealth of experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please still consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence; therefore, we encourage people from all backgrounds to apply to our positions.
About You:
At ShipBob, we're looking to bring on board people who embody our core values:
Be Mission-Driven. We want team members that are passionate about helping entrepreneurs improve their business, and bring that passion every day.
Be Humble. We have ambitious goals, and our team members understand that success or failure depends on us working together and leaving egos at the door.
Be Resilient. Logistics is a complicated business. So is software. We value team members that never give up and keep iterating until a problem is solved.
Be a Creative Problem Solver. As a startup, we value smart, innovative solutions to complex problems. We fall in love with the problem, not our ""favorite"" solution.
Be Safety Minded. It's not just talk; it's the way you work.
About Us:
ShipBob is a cloud-based logistics platform that partners with over 7,000+ e-commerce businesses to help make their entrepreneurial dreams a reality. We offer a full suite of fulfillment solutions for our merchants, including the ability to improve their transit times, shipping costs and deliver best in class experience to their customers. With an almost 100% accuracy rate in fulfilling orders and orders shipped on time, our merchants can count on us to deliver excellent service.
As one of the fastest growing tech companies in Chicago with over $300M+ raised from blue-chip investors like Menlo Ventures, Bain Capital Ventures, Hyde Park Venture Partners and SoftBank Vision Fund 2, our goal is to continue to be the #1 best fulfillment technology in the industry.
ShipBob provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.",4.4,"ShipBob, Inc.
4.4",Remote,1001 to 5000 Employees,2014,Company - Private,Shipping & Trucking,Transportation & Logistics,$100 to $500 million (USD)
884,Senior Data Analytics Engineer,$96K - $130K (Glassdoor est.),"At Wistia our goal is to help every business to thrive with video. That's a big task, and we're super excited to help the world get there faster through simple and delightful video-focused products.
Wistia's Business Intelligence team is charged with leveraging data to empower strategic, insights-driven decision making across the organization. The Analytics Engineer role is a hybrid role with responsibilities that typically fall to Data Analysts, Data Engineers and Data Scientists. We are a small team and are looking for someone who is comfortable wearing different ""data"" hats. A successful candidate will combine strong technical and communication skills to manage the full data lifecycle at Wistia by creating and maintaining ETL pipelines, automating critical workflows, analyzing data to surface insights and translating those insights into clear recommendations for stakeholders.
The other members of the Business Intelligence team are curious, attentive to detail, impact-focused and approach problem solving with an engineering mindset. The team is centralized (not embedded) with each team member working with data from across the business. We strive to adopt the tools and best practices of the ""modern data stack"". For us that looks like: Redshift, dbt, Fivetran, Mode Analytics, Census, Snowplow Analytics. The Redshift warehouse we maintain serves as the single source of truth for the business.
Key Responsibilities
Maintain our 3rd-Party and custom ETL/reverse-ETL jobs and create new ones as required
Develop new predictive models as required to meet business outcomes
Generate easily interpretable analyses of complex datasets using SQL or python
Create persuasive narratives with data visualizations that enable self-serve exploration
Assist in the design and analysis of A/B testing to optimize business outcomes
Define version-controlled, modular business logic and datasets to be reused across the team
Document your findings, insights and recommendations for stakeholders
Meet business stakeholders ""where they live"" by becoming an expert in the data models and usage of their preferred SaaS tools, eg, Salesforce, HubSpot, Google Analytics, Amplitude, Stripe
Collaborate with:
Team members to share learnings, brainstorm new approaches and define best-practices
Stakeholders to scope, shape and prioritize their data requests
Qualifications
This section should talk about the competencies needed for the role, including qualities. Be sure to seek the input of key stakeholders and make sure we distinguish between must-haves and nice-to-haves because we will be using this to build our job positing and interview plan.
Technical
Highly proficient in SQL and python (or comparable general purpose programming language)
Facile with statistics and hypothesis testing
Skilled at constructing meaningful data visualizations
Comfortable with the command line and bash scripting
Knowledgeable about database architecture, data warehousing principles, REST APIs and basic ML algorithms
Non-technical
Experienced at scoping broad, loosely-defined requests into well-structured plans
Adept at translating data into actionable insights for stakeholders
Strong in written communication
Able to work independently with minimal oversight
Familiar with SaaS metrics and business practices
Values fit (for Wistia at large and BI specifically)
collaborative
learning oriented
attention to detail
simplicity as the starting point starts simple and add complexity only as needed
Key Metrics
This is a great place for more detail on the metrics we expect this person to own, and any that we measure success by.
Product, marketing, sales, and customer success teams have the data and insights they need to make decisions
Product, marketing, sales, and customer success teams have confidence in the data they use
Product, marketing, sales, and customer success teams are able to monitor business performance and spot problem areas and new opportunities
Responses to data requests are timely
Success Looks Like
This section should outline what success looks like. Knowing this helps inform the level and type of candidates we need, interview questions, onboarding plan, and coaching our new hires to success. It also makes it easier to spot performance issues early and hold ourselves accountable in hiring (and learn how we can improve).
30 Days
Complete Wistia onboarding program
Ramp up and learn the tools of the role (Mode, dbt, github, Fivetran, Redshift, Google Analytics, Salesforce, Pendo)
Ramp up and learn Wistia's data architecture, starting with Wistia app database
Ramp up and learn data analysis best practices and procedures
Complete ~5 basic data requests
90 Days
Confident in use of Mode, dbt, and GitHub for data analysis process
Confident in data architecture and able to self-source most data tables
Regularly handling basic data requests
Completed 1 complex data investigation
1 Year
Regularly initiating new data investigations and unearthing new insights
Regularly making improvements to our approach to data presentation
Regularly making improvements to our data models and data cleanliness
Working at Wistia
We try to ensure Wistia is an inclusive and diverse place where everyone feels happy, fulfilled, respected, comfortable, and welcome. We're proud to be an equal-opportunity workplace. We care a lot, so our benefits are actually benefits, not just the fun stuff like swag and snacks in the office (though we also have lots of those too!).
We know the biggest investment we can make is in our employees, so we provide:
A competitive compensation package that includes internal equity stock options
401k with 3% company contribution, regardless of whether you contribute
Fully paid healthcare, dental, and vision insurance (family plans included)
Pre-Tax FSA and Dependent Care Account
Fitness reimbursement
Flexible working hours – work at the times when you operate best, or set aside time for child and/or elderly care responsibilities during traditional ""office hours""
16 weeks paid parental leave for all new and expecting parents
Unlimited PTO (pretty common for people to take 4+ weeks off throughout the year)
Remote-first culture (work from anywhere in the U.S.)
Annual professional development stipend (courses, conferences, and more)
New hire bonus to enhance your home office set up
Pet insurance discount
Location/Remote Opportunities
Wistia is a remote-first company. Employees can work from our beautiful office in Cambridge, MA, or anywhere in the continental US",4.8,"Wistia
4.8","Cambridge, MA",51 to 200 Employees,2006,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
885,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
886,GA/GCP Data Engineer,Employer Provided Salary:$65.00 - $85.00 Per Hour,"A Billion dollar women's clothier with over 1300 stores across North America is seeking a highly skilled Google Analytics / GCP Data Engineer to join our team for a 6 month plus project. As the Google Analytics / GCP Data Engineer, you will be responsible for building BQ tables in GCP, mapping GA data between GA4 and UA, owning, maintaining, and providing recommendations around the GA Schemas to leverage reporting, and utilizing DBT and Git for data transformation and version control. You will also implement ELT pipelines to extract, load, and transform data from various sources.
Responsibilities:
Build and maintain BigQuery tables in GCP
Map GA data between GA4 and UA
Own and maintain the GA Schemas to leverage reporting
Utilize DBT and Git for data transformation and version control
Implement ELT pipelines to extract, load and transform data from various sources
Provide recommendations for optimizing Big Query tables and data flows
Communicate effectively with stakeholders and team members
Requirements:
In-depth experience with Google Analytics (GA4 & UA)
Strong experience in Google Analytics Schema Mapping between Versions
Very strong GCP and BigQuery Skills
Experience using DBT and Git for data transformation and version control
Experience with implementing ELT pipelines to extract, load and transform data from various sources
Strong problem-solving and analytical skills
Excellent communication and collaboration skills
Job Type: Contract
Pay: $65.00 - $85.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Compensation package:
Hourly pay
Experience level:
7 years
Schedule:
Monday to Friday
Experience:
GCP: 5 years (Preferred)
Big Query: 5 years (Preferred)
Google Analytics: 5 years (Preferred)
Tableau: 4 years (Preferred)
Work Location: Remote",4.1,"V Soft Consulting
4.1",Remote,501 to 1000 Employees,1997,Company - Private,HR Consulting,Human Resources & Staffing,$100 to $500 million (USD)
887,Lead Azure Data Engineer,Employer Provided Salary:$135K - $140K,"Lead Data Engineer.
Location : New York, NY ( Hybrid)
Full Time
10+ years of experience in data engineering
5+ years of experience managing a team of data engineers
Strong understanding of SQL Server SSIS
Excellent communication and interpersonal skills
Ability to work independently and as part of a team
Ability to meet deadlines and work under pressure
Strong desire to be hands-on with the development and implementation of data pipelines and data warehouses
Experience working with stakeholders to develop product backlog grooming, sprint planning data engineering and
QA Testing
Migration of SQL Server to Azure is desired
Experience with Azure Data Factory, ADLS, and CI/CD highly desired
The Hands-On Data Engineering Lead is responsible for leading a team of data engineers in the design, development, and
implementation of data pipelines and data warehouses. The ideal candidate will have a strong understanding of data
engineering principles and practices, as well as experience with SQL Server SSIS. The Hands-On Data Engineering Lead will
also be responsible for managing the team's workload, mentoring junior engineers, and communicating with stakeholders.
Additionally, the Hands-On Data Engineering Lead will be expected to be hands-on with the development and
implementation of data pipelines and data warehouses, and to have experience working with stakeholders to develop
product backlog grooming, sprint planning data engineering and QA Testing.
Responsibilities:
Lead the design, development, and implementation of data pipelines and data warehouses
Develop and implement data pipelines and data warehouses using SQL Server SSIS
Troubleshoot data pipelines and data warehouses
Communicate with stakeholders to understand their needs and ensure that projects meet their requirements
Manage the team's workload and ensure that projects are completed on time and within budget
Mentor junior data engineers and help them develop their skills
Stay up-to-date on the latest data engineering technologies and best practices
Provide support to production teams and QA teams
Work with stakeholders to develop product backlog grooming, sprint planning data engineering and QA Testing
Working with other engineers to design and implement data pipelines and data warehouses using Azure Data
Factory, ADLS, and CI/CD
Experience in migrating SQL server to Azure
30% team management and 70% hands-on
Job Type: Full-time
Salary: $135,000.00 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 10 years (Required)
SQL: 10 years (Required)
Data warehouse: 10 years (Required)
SSIS: 10 years (Required)
Work Location: In person",-1.0,Rivago Infotech Inc,"New York, NY",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
888,Data Warehouse & Artificial Intelligence Engineer,$83K - $120K (Glassdoor est.),"Job Description:

Design, Implement and Maintain data warehouse.

Duties and Responsibilities:

Develop, test, and deploy extraction, transformation and loading (ETL) routines to load the data warehouse using SSIS, stored procedures, etc.
Develop analytics including reports, dashboards, and data visualizations
Troubleshoot BI-related problems and tune for performance
Implement and oversee a company-wide data governance program to support master data management and resolve data definition discrepancies among stakeholders
Plan the technical approach for implementing analytics strategy across multiple channels
Plan and implement activities to maintain data confidence
Provide technical direction and technical support for all member of the team
Architect and implement analytics solutions to meet data collection and reporting needs.
Work with team to recommend analytic software
Provide general troubleshooting assistance for analytic reports and break fixes
Wrangle data (a.k.a. data engineering) from a wide variety of complex data sets large volumes of data into normalized and enriched data sets.
Assemble and evaluate data such that new insights, solutions, and visualizations can be derived.
Perform data discoveries to understand data formats, source systems, file sizes, etc. and engage with internal and external business partners in this discovery process.
Deliver data acquisition, transformations, normalization, mapping, and loading of data into a variety of data models.
Collaborate extensively with Technology to design data ingestion, data models, and automated operational metrics for consistently high quality data
Supports the research, analysis and presentation of information by: producing reports, compiling and summarizing information, producing supporting documentation and exhibits, and verifying information received from external sources.
Assists other associates with preparation of reports and use of information systems, software and related sources of information. Trains other users on report preparation and database access.
Perform other duties as assigned.

Required Knowledge, skills and abilities:

Ability to manage change successfully in a fast-paced environment
A passion for learning and the ability to do so on the fly
Must be strategic, dynamic, and detail-oriented. The ideal candidate is an analytical thinker, a self-starter and a quick learner.
Ability to design and architect method of extracting data from its original source to another location (i.e. a data lake) to make that data consumable for a variety of downstream purposes such as applications, visualizations, analytics and other related data science activities.
Basic understanding of data science
Ability to transforming and map one ""raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.
Exceptional ability to clearly and effectively communicate with IT peers and users at all levels of technical and business acumen.
Strong time management skills and multi-tasking capabilities
Ability to understand business rules governing financial and budgeting process
Exceptional oral and written communication skills
Self-motivated, ability to set priorities and meet deadlines

Education and Experience:

Bachelor's degree in Computer Science, Business, Economics, Information Science, or Analytics
Minimum of 3 years of experience with data analytics across multiple channels
Experienced with the technologies required to implement Analytics across multiple channels, (for example, Data Layer, Custom JavaScript, JAVA, AJAX, HTML, Objective-C, Swift)
Knowledge of Tableau is a plus
Familiarity with Microsoft Power BI
Extensive experience with ETL and/or other Big Data processes.
Contract manufacturing experience is a plus.

Foxconn Assembly, LLC is an Equal Opportunity Employer (EOE). All qualified candidates will receive consideration without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or marital status in accordance with applicable federal, state and local laws.
Foxconn Assembly, LLC participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

NbXn0DyWY6",3.4,"Foxconn Corporation
3.4","Houston, TX",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$5 to $25 million (USD)
889,Senior Data Engineer (Remote),Employer Provided Salary:$91K - $155K,"** This role is remote; if in area near an ICF office, hybrid may be recommended.**
Our Public Sector Group (PSG) is growing and we are looking for a talented, highly motivated Senior Data Engineer to join our Engineering and Emerging Technologies (EET) line of business for supporting a large government contract in the DC metro area.
Key Responsibilities:
Responsible for the data engineering on a large-scale technical modernization project in accordance with contract requirements and company policies, procedures and guidelines. Strong experience in I.T. modernization projects. Has a good mix of data requirements and data engineering, and knowledge of I.T. modernization efforts.
What you’ll be doing:
Support technical Data Warehouse/BI tasks for a major federal initiative, working as part of an extended system development team on project execution.
Develop extract, transform, and load (ETL) processing routines and data feeds, creating necessary data structures and data models to support data at all stages.
Perform extensive data profiling and analysis based on client data.
Work with UI and business analysis team members and the client to define BI and reporting requirements.
Design and implement custom data analytic and BI/reporting products, custom reports, and data visualization products.
What you must have:
Bachelor’s degree (e.g., Computer Science, Engineering or related discipline)
8+ years of experience in SQL and/or Python coding language.
8+ year of experience with procedural, functional, and object-oriented programming
3+ years of experience developing database ETL environments with business intelligence applications
3+ years of experience with AWS database, analytics, and compute services such as RDS/Aurora, AWS Glue, and Lambda
3+ years of experience with data warehouse design and development with Amazon Redshift
3+ years of experience working with BI tools such as Tableau, PowerBI or AWS Quicksight
3+ years of development experience in a DevSecOps environment, with programming languages such as Java, Node, or Python
Employment must be compliant with eligibility for Public Trust Clearance due to Government Contract.
Candidate must reside in the US and be a US Citizen
What we’d like you to know:
Understand ETL concepts of data flow, data enrichment, data consolidation, change data capture and transformation
Understand database concepts of referential integrity, indexes and keys and table metadata
Demonstrated experience showing strong critical thinking and problem-solving skills paired with a desire to take initiative
Knowledge of Big Data integration tools such as Hive, Airflow, Storm, Spark, AWS Kinesis, and Kafka
Experience building CI/CD pipelines with tools such as Jenkins and CodeBuild
Experience with containerization platforms including Docker
Experience with Agile development process
Technologies you’ll use in this role:
SQL, Node, Python, Tableau, Quicksight, Aurora, Lambda, Sequelize
Aurora, AWS Glue, S3, RedShift, AWS Kinesis
Git, Terraform, CodeBuild
Professional Skills:
Must have excellent written and oral communications skills.
Must be comfortable working in a fast-paced, matrixed team environment, with a client-centric culture, and an environment of high performers.
Must be able to multi-task and shift priorities as needed.
Why you’ll love working here:
Comprehensive health benefits
Generous vacation and retirement plans
Employee support program
Participation in charity initiatives
About ICF International:
ICF International (NASDAQ:ICFI) partners with government and commercial clients to deliver professional services and technology solutions in the energy and climate change; environment and infrastructure; health, human services, and social programs; and homeland security and defense markets. The firm combines passion for its work with industry expertise and innovative analytics to produce compelling results throughout the entire program life cycle, from research and analysis through implementation and improvement. Since 1969, ICF has been serving government at all levels, major corporations, and multilateral institutions. More than 4,000 employees serve these clients worldwide. ICF's Web site is
www.icf.com
ICF offers an excellent benefits package, an award-winning talent development project, and fosters a highly skilled, energized and empowered workforce.
ICF is an equal opportunity employer that values diversity at all levels. (EEO/AA – Minorities/Females/Veterans/Individuals with Disabilities)
For a listing of other career opportunities at ICF, please visit our Career Center at
www.icf.com/careers
Working at ICF
ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our
EEO & AA policy
.
Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email
icfcareercenter@icf.com
and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination:
Know Your Rights
and
Pay Transparency Statement.

Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
$90,940.00 - $154,598.00
Nationwide Remote Office (US99)",3.8,"ICF
3.8","Reston, VA",5001 to 10000 Employees,1969,Company - Public,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
890,AWS Data Engineer,Employer Provided Salary:$60.00 - $75.00 Per Hour,"Title: AWS Data Engineer
Location: Columbus, OH (Day 1 onsite / Hybrid Model)
Duration: Contract
Job Description:
Data Engineer: Open positions – 2
Skills:
Must Have:
BS/BA degree or equivalent experience
General: Strong organizational, problem-solving, and critical thinking skills; Strong documentation skills
Coding: Proficiency in Java
• Cluster Computing frameworks: Proficiency in Spark and Spark SQL
• AWS Data Services: Proficiency in Lake formation, Glue ETL (or) EMR, S3, Glue Catalog, Athena, Kinesis (or) MSK, Airflow (or) Lambda + Step Functions + Event Bridge
Data De/Serialization: Expertise in at least 2 of the formats: Parquet, Iceberg, AVRO, JSON-LD
AWS Data Security: Good Understanding of security concepts such as: Lake formation, IAM, Service roles, Encryption, KMS, Secrets Manager
Good to Have:
• Linux Scripting, Jenkins
• DevOps: Git, CI/CD, JIRA, TDD
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Ability to commute/relocate:
Columbus, OH 43085: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Required)
Java: 4 years (Required)
Data warehouse, Glue and SparkSQL: 8 years (Required)
Work Location: In person",4.5,"Synkriom Technology Pvt. Ltd.
4.5","Columbus, OH",201 to 500 Employees,2015,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
891,Data Performance Engineer,Employer Provided Salary:$55.00 Per Hour,"Data Performance Engineer
Only candidate with years of experience: +8
Those authorized to work in the U.S. are encouraged to apply.
Remote
Interview Process: 3 Rounds of Zoom Video Interview
Candidate must have Data Performance Engineer experience.
Skill and Experience Required
· Experience in consuming large data volume
· Advance SQL skills
· Optimize strategy process for data handling for aggregation tables (Minimize processing time for large data volume)
· Experience in data loading with complex model for data and batch orchestration
· Throughput understanding, monitoring for efficient data loading
· Identifying bottlenecks and resource investigation
· Experience in periodic ETL Code maintenance and setting up SLA for cross functional team
· Implementing necessary techniques to improve ETL performance in Both DB and Application
· Improving query performance by turning ad re-evaluating data model
· Identifying dependency impact on both data and Application
Duties and responsibilities
· Performance Analysis and documenting factors (bottleneck, memory handling and inefficiency area which needs improvement)
· Identifying and documenting execution Benchmarks and Metrics
· Optimization on application and database
· Validation techniques for data and Orchestration
· Documentation for Optimization and Operations
· Implementation Alert mechanism in case of potential impact to downstream
· Collecting and documenting the Metrics for each ETL application
Job Type: Contract
Pay: $55.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Big data: 8 years (Required)
SQL: 8 years (Required)
ETL: 8 years (Required)
consuming large data volume: 8 years (Required)
Work Location: Remote",-1.0,Luttechub,Remote,Unknown,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
892,Cloud Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"We are seeking a highly experienced Principal Data Engineer with a deep understanding of PySpark using Databricks or AWS Glue or AWS EMR and cloud-based databases such as Snowflake. Proficiency in workflow management tools like Airflow is essential. Healthcare industry experience is a significant advantage. The ideal candidate will be responsible for designing, implementing, and maintaining data pipelines while ensuring the highest performance, security, and data quality.
Responsibilities:
Design, develop, and maintain scalable, reliable, and secure data pipelines to process large volumes of structured and unstructured healthcare data using PySpark and cloud-based databases.
Collaborate with data architects, data scientists, and analysts to understand data requirements and implement solutions that meet business and technical objectives.
Leverage AWS or Azure cloud services for data storage, processing, and analytics, optimizing cost and performance.
Utilize tools like Airflow for workflow management and Kubernetes for container orchestration to ensure seamless deployment, scaling, and management of data processing applications.
Develop and implement data ingestion, transformation, and validation processes to ensure data quality, consistency, and reliability across various healthcare datasets.
Monitor and troubleshoot data pipelines, proactively identifying and resolving issues to minimize downtime and ensure optimal performance.
Establish and enforce data engineering best practices, ensuring compliance with data privacy and security regulations specific to the healthcare industry.
Continuously evaluate and adopt new tools, technologies, and frameworks to improve the data infrastructure and drive innovation.
Mentor and guide junior data engineers, fostering a culture of collaboration, learning, and growth within the team.
Collaborate with cross-functional teams to align data engineering efforts with broader organizational goals and strategies.
Is familiar with SOC 2 compliance and its impact on company policies and processes.
Understands the importance of adhering to SOC 2 requirements and maintains an effort to do so.
Reviews and understands the Employee Handbook, and internal policies that define individual security responsibilities, and maintains segregation of duties in accordance with their role requirements.
Requirements:
Bachelor's or Master’s degree in Computer Science, Engineering, or a related field.
8+ years of experience in data engineering, with a strong background in Apache Spark and cloud-based databases such as Snowflake.
Strong Knowledge in Big Data Technologies, Databricks, AWS Services, and PySpark, thorough in one or more programming languages like Python.
Proven experience with AWS or Azure cloud services for data storage, processing, and analytics.
Expertise in workflow management tools like Airflow and container orchestration systems such as Kubernetes.
Strong knowledge of SQL and NoSQL databases, as well as data modeling and schema design principles.
Familiarity with healthcare data standards, terminologies, and regulations, such as HIPAA and GDPR, is highly desirable.
Excellent problem-solving, communication, and collaboration skills, with the ability to work effectively in cross-functional teams.
Demonstrated ability to manage multiple projects, prioritize tasks, and meet deadlines in a fast-paced environment.
A strong desire to learn, adapt, and contribute to a rapidly evolving data landscape.
Benefits:
401(k).
Dental Insurance.
Health insurance.
Vision insurance.
We are an equal-opportunity employer and value diversity, equality, inclusion, and respect for people.
The salary will be determined based on several factors including, but not limited to, location, relevant education, qualifications, experience, technical skills, and business needs.
Additional Responsibilities:
Participate in OrangePeople monthly team meetings, and participate in team-building efforts.
Contribute to OrangePeople technical discussions, peer reviews, etc.
Contribute content and collaborate via the OP-Wiki/Knowledge Base.
Provide status reports to OP Account Management as requested.
About us:
OrangePeople is an Enterprise Architecture and Project Management solutions company. Our most valuable asset is our people: dynamic, creative thinkers, who are passionate about doing quality work. As a member of the OrangePeople team, you will have access to industry-leading consulting practices, strategies & technologies, innovative training & education. An ideal Orange Person is a technology leader with a proven track record of technical achievements and a strong process/methodology orientation.
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Application Question(s):
Do you require sponsorship for this job?
Work Location: Remote",4.1,"OrangePeople
4.1",Remote,51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
893,Sr. Data Engineer (Remote),$78K - $109K (Glassdoor est.),"Chamberlain Group is a global leader in access solutions with top brands, such as LiftMaster and Chamberlain, found in millions of homes, businesses, and communities worldwide.
As a leader in the Smart Home industry, we boast one of the largest IoT install bases, with innovative products consisting of cameras, locks, card readers, garage door openers, gates and more, all powered by our myQ digital ecosystem.
This role is responsible for providing technical expertise and leadership to design and deliver end-to-end data engineering solutions to support advanced analytics capabilities and drive innovation and decision-making
across Chamberlain.
Essential Duties and Responsibilities
Build and maintain real-time and batch data pipelines across the advanced analytics platform.
Design, develop and orchestrate highly robust and scalable ETL pipelines.
Design and implement Dimensional and NoSQL data modelling as per the business requirements.
Develop highly optimal codebase and perform Spark optimizations for Big Data use cases.
Design, develop and deploy optimal monitoring and testing strategy for the data products.
Collaborate with stakeholders and advanced analytics business partners to understand business needs and translate requirements into scalable data engineering solutions.
Collaborate with data scientists to prepare data for model development and production.
Collaborate with data visualization and reporting application developers to ensure the sustainability of production applications and reports.
Collaborate with data architects on the enhancement of Chamberlain’s enterprise data architecture and platforms.
Provide leadership to third-party contractors.
Comply with health and safety guidelines and rules.
Protect CGI’s reputation by keeping information confidential.
Maintain professional and technical knowledge by attending educational workshops, professional publications, establishing personal networks, and participating in professional societies.
Minimum Qualifications
Education/Certifications:
Bachelor’s degree in computer science or related quantitative field of study
Experience:
4+ years of professional experience
Knowledge, Skills, and Abilities:
Natural sense of urgency, teamwork, and collaboration reflected in daily work ethic.
Proficient in Spark or Databricks, Cloud Data Engineering Services preferably Azure, Streaming frameworks like Event Hubs or Kafka.
Proficient in Microsoft Office.
Familiarity with modern Machine Learning Operationalization techniques.
Agile methodologies.
Familiarity with Data visualization tools, such as Qlik or Power BI.
Preferred Qualifications
Education/Certifications:
Master’s degree in computer science or related quantitative field of study
Experience:
4+ years of professional experience
2+ years of professional experience delivering engineering for advanced analytics or data science solutions
Knowledge, Skills, and Abilities:
Agile methodologies
Experience with IoT Data Architecture.
Machine Learning Operationalization (MLOps) proficiency.
REST API design and development.
Proficiency with streaming design patterns.

#LI-Hybrid
Chamberlain Group wants all of its employees to succeed and encourages people of all backgrounds to apply. We’re proud to be an Equal Opportunity Employer, and you’ll be considered for this role regardless of race, color, religion, sex, national origin, age, sexual orientation, ancestry; marital, disabled or veteran status. We’re committed to fostering an environment where people of all lived experiences feel welcome.

Persons with disabilities who anticipate needing accommodations for any part of the application process may contact, in confidence
Recruiting@Chamberlain.com
.

NOTE: Staffing agencies, headhunters, recruiters, and/or placement agencies, please do not contact our hiring managers directly.",3.7,"Chamberlain Group
3.7","Oak Brook, IL",1001 to 5000 Employees,1954,Company - Private,Consumer Product Manufacturing,Manufacturing,$500 million to $1 billion (USD)
894,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
895,Senior Software Engineer (Data),Employer Provided Salary:$160K - $200K,"About Juniper Square
Our mission is to unlock the full potential of private markets. Privately owned assets like commercial real estate, private equity, and venture capital make up half of our financial ecosystem yet remain inaccessible to most people. We are digitizing these markets, and as a result, bringing efficiency, transparency, and access to one of the most productive corners of our financial ecosystem. If you care about making the world a better place by making markets work better through technology – all while contributing as a member of a values-driven organization – we want to hear from you.
Juniper Square offers employees a variety of ways to work, ranging from a fully remote experience to working full-time in one of our physical offices. We invest heavily in digital-first operations, allowing our teams to collaborate effectively across most US states, 2 Canadian Provinces, and Mexico. We also have physical offices in San Francisco, CA and Austin, TX, for employees who prefer to work in an office some or all of the time.
GP Experience
Juniper Square serves two sides of the private capital markets, the investment managers (GPs) and the investors (LPs). The GP eXperience team (i.e., GPX) is responsible for Juniper Square’s product offering for General Partners (GPs). This is our core product that enables all other innovation at Juniper Square as we unlock and improve the world’s private capital markets. Our platform handles billions of dollars of transactions each month and we are actively expanding into additional private asset classes such as Venture Capital & Private Equity. Come help us innovate in fundraising, reporting, asset-ownership mapping, and more.
The Team
The Data Engineering team is responsible for Data pipelines that serve multiple types of customers including internal Juniper Square users for Business Intelligence and GPs for Analytics on their data. We support the ability for these customers to create and manage their custom dashboards. We also support the ability for other Product Engineering teams to add metrics to track product usage for the features they launch into production.
About your role
Juniper Square is growing rapidly, and our data needs are growing even faster, so we’re growing our Data Engineering Team. As a Senior Data Engineer your role will be pivotal to evolving our existing data and reporting experiences. You’ll build out pipelines to gather data from multiple sources and make it available for analysis. You will shape both internal and external analytics products to help guide business-critical decisions, enhance their workflows, and improve decision-making.
What you’ll do
Design and implement sophisticated data models in SQL.
Work closely with the other Software Engineers to ensure sound, scalable implementation.
Act as a technical expert on our team regarding all things data, especially as the data team grows and evolves.
Introduce new technologies to evolve and enhance our data pipeline capabilities.
Document data models, architectural decisions and data dictionaries to enable collaboration, maintainability and usability of our analytics platforms and code.
Assist with governance, guidance, code reviews, and access controls so that we maintain consistency, quality, and business confidentiality as we scale analytics access across the company and to customers.
Externally: learn our application data schema, and develop a fluency in how to transform it to enhance customer’s decision-making with data.
Internally: guide product and development teams, advising on instrumentation and laying development foundations for product usage reporting.
Fulfill projects with minimal guidance but with an appropriate sense of when and how to collaborate with others.
Build scalable, highly performant infrastructure for delivering clear business insights from a variety of raw data sources.
Qualifications
Bachelor's degree in Computer Science, or equivalent work experience
4+ years of experience building ETL (Extraction Transform Load) or ELT (Extraction Load Transform) pipelines from scratch
Strong command of relational databases (Postgresql preferred), data modeling and database design
Strong command of Python and experience building production web applications using Python
Experience with cloud based services (AWS RDS preferred)
Experience developing on (or administering) BI / data visualization platforms (ex. Looker, Tableau, PowerBI, Mode, Data Studio, Domo, QlikView etc.).
Basic understanding of data warehouses such as Amazon Redshift, Google BigQuery, Snowflake etc.
Demonstrated history of translating data into clear and actionable narratives and communicating opportunities and challenges relevant to stakeholders.
You must be flexible and adaptable—you will be operating in a fast-paced startup environment.
At Juniper Square, we believe building a diverse workforce and an inclusive culture makes us a better company. If you think this job sounds like a fit, we encourage you to apply even if you don’t meet all the qualifications.
Benefits
Compensation for this position includes a base salary, equity, and a variety of benefits. The U.S. base salary range for this role is $160,000 - $200,000. Actual base salaries will be based on candidate-specific factors, including experience, skillset, and location, and local minimum pay requirements as applicable. We are actively hiring for this role in Canada, and offer competitive local pay and benefits. Your recruiter can provide further details.
Competitive salary and meaningful equity
Health, dental, and vision care for you and your family
Unlimited vacation policy and paid holidays
Generous paid family leave, medical leave, and bereavement leave policies
401k retirement savings plan
Healthcare FSA and commuter benefits programs
Freedom to customize your work and technology setup as you see fit
Professional development stipend
Monthly work from home wellness stipend while we're all remote
Mental wellness coverage including live coaching and therapy sessions
Home office productivity allowance to help create an ideal work from home setup
#LI-AD1
#LI-Remote",4.0,"Juniper Square
4.0",Illinois,201 to 500 Employees,2014,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
896,Senior Software Engineer - Data Engineering,$120K - $153K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: The merchant Reporting and Reconciliation team is looking for a senior Back-end engineer responsible for providing technical leadership to the team and executing the unification projects end to end. Your primary focus will be developing massively scalable, distributed software systems that require high availability to our business
Job Description:
Merchant reporting is crucial for our Merchants so that they can close their account books timely and accurately with complete payment data in core markets. We are looking for people who have a passion for developing massively scalable, distributed software systems that require high availability to our business. As a member on the Merchant Reporting and Reconciliation team, you thrive in a fast-paced environment and enjoy driving innovation through rapid prototyping and iterative development. You will work directly with our Product Owners and Domain Technical Leads to create outstanding solutions and deliver incredible reporting products. You will be involved from ideation to rollout.
Your day-to-day:
Work with Product Managers and other business partners to identify opportunities for improvement
Analyze data based on product requirements
Create reports for internal teams and/or external clients
Use graphs, infographics, and other methods to visualize data
Structure large data sets to find usable information
Work with a team of analysts and other associates to process information
Create presentations and reports based on recommendations and findings
Define validation queries when needed and how to identify discrepancies in the data as they arise
Write queries for runbooks that automate the discrepancy identification process
Implement the reporting data model
Deliver within schedule in an Agile software development using test-driven development methodologies.
Participate in development life cycle activities like design, coding, testing, and production release.
Be proactive with identifying areas for improvement and innovation to improve development productivity
What do you need to bring:
BS in EE/CS or equivalent work experience and successful completion of major projects for which you can show code examples.
5+ years of hands-on data/software engineering experience
Experience working with coding languages—preferably SQL, Java, Spark-SQL, Pyspark, Python
Experience working with SQL and NoSQL DataBase
High proficiency in MS Excel, MS PowerPoint, GIT, Apache Airflow
Have a passion for quality and writing clean and solid code that scales and performs well.
Strong desire to learn, push the envelope and share knowledge with others.
Excellent analytical and time management skills
Teamwork skills with a problem-solving attitude
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
897,Data Movement Engineer C2H,Employer Provided Salary:$55.00 Per Hour,"Client required information:
***They need Someone who has strong Talend Experience minimum of 5 years***.
Talend API is Desirable, But any API is also fine like snowflake as a last option.
They are working on Data Base Movement from Talend to ETL/ELT.
Formally they need ETL/ELT Developers or Talend Developers with ETL/ELT data movement experience.
Implementation and Configuration Talend is Mandate.
They also need to take a an on call support after work hours for ongoing production Support team, this might happen on weekends too.
They are going to implement more API’s, so having extra API experience is plus.
They need to understand the baseline of Insurance( The Basic work or Knowledge on Insurance Modules).
They can Work Remote, But need only from EST Zones.
Manager is not going to accept any profiles without Talend Experience.
Skill Qualifications Required:
· Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
· Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
· Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
· Some experience with cloud-based database technologies required
· Working knowledge of data warehousing concepts, structures and ETL best practices
· Experience using query tools (e.g. AQT, MS Query)
· Ability to problem solve using analytical thinking skills
· Must work well independently - must be inquisitive and seek answers to complex questions without being prompted
· Strong organizational and time management skills
· Strong communication skills including verbal and written to communicate effectively with clients and management
· Strong project management skills to ensure that projects get done on time and within budget
· Effectively participates in teams and moves the team toward completion of goals
Job Types: Full-time, Contract
Salary: From $55.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Experience:
Talend: 5 years (Required)
ETL: 5 years (Required)
SQL query: 3 years (Required)
data warehousing: 3 years (Required)
business intelligence tools: 3 years (Required)
Work Location: Remote",-1.0,Technovant inc,"New Haven, CT",-1,-1,-1,-1,-1,-1
898,Data Engineer,Employer Provided Salary:$100K - $140K,"Oddball believes that the best products are built when companies understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space.
We are hiring a Data Engineer to work on a pivotal Federal program that is making a positive impact on millions of Americans' daily lives.
What you'll be doing:
Migrating custom-built Python ETL and ELT pipelines into a modern orchestration framework with Argo Workflows and dbt.
Improving observability, discoverability, governance, and implementing a common data integrity and data quality testing framework using tools such as dbt, OpenMetadata and Great Expectations.
Constructing reliable and performant high-volume ETL or ELT pipelines for sensitive healthcare data.
Contributing to and maintaining legacy ETL and ELT data pipelines.
Proactively monitoring data pipelines for potential problems and debugging issues if they arise.
Working with third party data owners to figure out ingestion strategies and cadence.
Helping to model data at various stages of refinement, curation, and enrichment to best suit different downstream targets and marts.
What you’ll bring:
5+ years of proven data and performance engineering experience.
Expert in SQL.
Experience with Python and other programming languages such as JavaScript, TypeScript, Rust, and Go.
Orchestration experience with tools like Argo Workflows, AirFlow, Dagster, Prefect, or Luigi.
Experience using databases like RedShift, Postgres, and Snowflake.
A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs.
Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures.
Experience with Docker, Kubernetes, and AWS (services like EKS, Kinesis, Athena, EKS, S3, RDS, Aurora, Lambda, ECS, EventBridge, SQS, etc)
Experience working in Agile environments.
Requirements:
Must be a US Citizen and able to work domestically
Must be able to attain low-level security clearance
Education:
Bachelor’s Degree
Benefits:
Fully remote
Tech & Education Stipend
Comprehensive Benefits Package
Company Match 401(k) plan
Flexible PTO, Paid Holidays
Oddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation by emailing hello@Oddball.io
Job Type: Full-time
Pay: $100,000.00 - $140,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Work Location: Remote",4.6,"Oddball
4.6",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
899,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
900,Senior Data Engineer (Remote),Employer Provided Salary:$50.00 - $70.00 Per Hour,"Company Overview: IT Giants INC is a dynamic and innovative technology company at the forefront of delivering advanced data solutions across various industries. We are seeking an experienced Data Engineer with expertise in cloud platforms, programming languages, database technologies, and analytics tools. If you also have a strong background in the airline domain, this is an opportunity to join our dedicated team and shape the future of data-driven decision-making in the aviation sector.
Position Overview: As a Data Engineer at IT Giants INC, you will play a pivotal role in designing, building, and maintaining sophisticated data pipelines and analytical solutions for our airline industry clients. Leveraging your deep understanding of both data engineering and the airline domain, you will empower our clients to harness the power of their data to improve operations and customer experiences.
Key Responsibilities:
Data Pipeline Development: Architect, develop, and maintain end-to-end data pipelines, integrating data from diverse sources, including cloud platforms (GCP, Azure, AWS), APIs, and various databases (Teradata, Oracle, PostgreSQL, NoSQL).
Data Transformation and Modeling: Apply your expertise in SQL, Python, and Alteryx to transform raw data into meaningful insights, while ensuring data integrity and accuracy.
Cloud Expertise: Utilize your proficiency in GCP, Azure, and AWS to design and implement scalable and cost-efficient cloud-based solutions that meet the unique demands of the airline industry.
Database Management: Manage relational and NoSQL databases to store and retrieve data efficiently, ensuring optimal performance and security.
Analytics and Reporting: Collaborate with data scientists and analysts to develop interactive dashboards and reports using tools like Tableau, QuickSight, and CloudWatch, enabling stakeholders to make informed decisions.
Data Quality and Governance: Implement data quality checks, validation processes, and adherence to industry regulations to ensure data accuracy and compliance.
Version Control and Collaboration: Utilize GitLab and other version control tools to collaborate with cross-functional teams on code management and deployment.
Real-time Monitoring: Develop real-time data processing solutions using technologies like CloudWatch to enable timely insights into operational metrics.
Linux Environment: Leverage your proficiency in Linux to optimize system performance and troubleshoot issues as they arise.
Domain Knowledge: Apply your deep understanding of the airline industry to structure data effectively and drive actionable insights from the data.
Qualifications:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
years of proven experience in data engineering, with a focus on cloud platforms (GCP, Azure, AWS), Python, SQL, and relevant database technologies.
Experience with Alteryx, Tableau, and data visualization tools for creating impactful dashboards.
Proficiency in managing databases such as Teradata, Oracle, PostgreSQL, and NoSQL databases.
Strong command of Linux environments and version control using GitLab.
Familiarity with CloudWatch, QuickSight, and real-time data processing is advantageous.
Domain expertise in the airline industry, including a solid understanding of aviation data, standards, and operational challenges.
Excellent problem-solving skills and the ability to troubleshoot complex data issues.
Effective communication skills to collaborate with technical and non-technical teams.
Proven ability to work independently and in a team, delivering high-quality solutions.
Join Our Team: If you are a driven and knowledgeable Data Engineer with a passion for both technology and the airline industry, IT Giants INC offers an exciting opportunity to contribute to transformative data solutions. Join us in our mission to revolutionize data analytics within the aviation sector and drive meaningful impact.
Job Types: Full-time, Contract
Pay: $50.00 - $70.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Work Location: Remote",5.0,"IT Giants
5.0",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
901,BI Data Engineer (Remote),Employer Provided Salary:$75.00 - $85.00 Per Hour,"Stefanini Group is hiring!
Exciting opportunity awaits, let us help you get started!
Click Apply now or you may call: (248) 728-2611 / email: palak.agrawal@stefanini.com for faster processing!

Job Description:
We require a reliable, talented, highly motivated BI/Data Engineer with excellent SQL, PowerBI, Python skills, The firm is putting a huge focus on data and reporting, with an increased demand of data related tasks.
As a global leader in Asset management, you will be working as part of an innovative, fast paced technology driven culture who are always looking at ways to enhance how technology is used within their organization.
You will be a valued member of an effective data team working to deliver growth against ambitious targets.
You will play a key role in extracting useful information from data, providing important insights to global technology teams.
The role will focus on creating best practice, high quality data solutions in collaboration with numerous business stakeholders and technology partners.
We are looking for someone who enjoys problem solving, who has an inquisitive nature, a team player and can work in a collaborative environment with good communication skills.
Ability to help extract and analyze data is required for the job.



Requirements:
We are looking for a good mix of the following skills:
Excellent experience in the Microsoft BI stack, Excel.
Excellent experience in building data pipelines and integrations (ETL/ELT).
Excellent knowledge of data warehousing and analysis concepts, data normalization, dimensional modelling, and associated practices.
Knowledge and experience of data manipulation languages such as DAX or MDX, Visualization or dashboarding experience.
Python experience is a must. Azure cloud experience would be a bonus.
We believe in building a culture supported by strong values, trust and independence. Within the team, we are highly collaborative, supportive, open and have high standards in the work we produce.

Education:
Bachelor's degree or equivalent experience.

*Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives*

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",3.3,"Stefanini, Inc
3.3","New York, NY",10000+ Employees,1987,Company - Private,Enterprise Software & Network Solutions,Information Technology,$1 to $5 billion (USD)
902,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1.0,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
903,"Data Engineer (AWS, Python)",Employer Provided Salary:$50.00 - $52.00 Per Hour,"Title: Data Engineer (AWS, Python)
Location: Remote
Duration: 12+ Months
Job Description:
Typical task breakdown:-
Daily standup, collaborate with peers, work independently to complete sprint commitments, participate in agile ceremonies.
Interaction with team:-
Daily stand up meetings with team – working mainly with Helios Platform adoption team.
Work environment:
Education & Experience Required:
Bachelor's in computer science required plus 7+ Working experience .
Masters degree with 6+ years working expeirence acceptable as well.
Preferred: AWS cloud certifications
Technical Skills
(Required)
Exp with Object oriented programming - Python (4+ years)
AWS (3+ years overall exp) lambda and/or glue, EMR, S3
SQL (3+ years)
2+ years exp in test driven development, unit test, integration testing, regression testing
(Desired)
Event driven framework experience.
Kinesis or Kafka experience
Familiar with working in Agile methodology
Soft Skills
(Required)
Strong written and verbal communication, experience working as a member of agile engineering team, ability to commit to and deliver work within allocated sprint window.
Job Type: Contract
Pay: $50.00 - $52.00 per hour
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",5.0,"DSMH LLC
5.0",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
904,Data Engineer,Employer Provided Salary:$45.00 Per Hour,"Job Title: GCP Data Engineer
Duration: 12+ Months
Location: Hartford, CT (Onsite Role)
Tax term : W2 & 1099
Job Description:
As a GCP Data Engineer, you will play a pivotal role in designing, implementing, and optimizing data solutions on the Google Cloud Platform (GCP). Your expertise will be essential in leveraging various technologies to build robust data pipelines and contribute to the overall success of data-driven initiatives.
Roles and Responsibilities:
Design, develop, and maintain efficient data pipelines using Python or PySpark on the Google Cloud Platform.
Seamlessly integrate data solutions using GCP services like Google Cloud Storage, BigQuery, Dataflow, and Pub/Sub.
Utilize expertise in the Hadoop ecosystem, including HDFS and Hive, to craft seamless data workflows.
Transform raw data into insights by creating complex SQL queries for data transformation and analysis.
Implement data quality checks and validation procedures to ensure accuracy and completeness.
Optimize data pipelines and processes for maximum performance and efficiency.
Collaborate with cross-functional teams to understand data requirements and deliver tailored solutions.
Create comprehensive documentation for pipelines, workflows, and processes.
Stay updated with the latest data engineering and cloud technology trends.
Tackle complex challenges with a proactive problem-solving mindset.
Required Qualifications:
Expertise in Python/PySpark and hands-on experience with Google Cloud Platform (GCP) for data engineering tasks.
Strong understanding of the Hadoop ecosystem, including HDFS and Hive.
Proficiency in SQL for effective data manipulation and analysis.
Experience working with cloud platforms, preferably Google Cloud Platform (GCP).
Strong problem-solving skills and analytical thinking.
Effective communication and collaboration abilities.
Adaptability to evolving data engineering technologies and practices.
Job Type: Contract
Salary: From $45.00 per hour
Experience level:
9 years
Application Question(s):
What is your work authorization?
Experience:
Data engineer: 9 years (Required)
Google Cloud Platform: 5 years (Required)
Hadoop: 5 years (Required)
Python: 4 years (Preferred)
SQL: 4 years (Preferred)
Work Location: On the road",-1.0,Conglomerate IT,"Hartford, CT",-1,-1,-1,-1,-1,-1
905,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
906,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
907,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
908,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
909,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
910,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
911,Sr. Data Engineer,Employer Provided Salary:$105K - $110K,"Role : Data Engineer
Location : Remote
Note : We are looking Someone who can be technical and also be able to
structure and manage transformation projects as well.
JD :
* Data Engineer Skill: Spark, Python and Databricks
* 12+ years of experience in Data pipeline engineering for both batch
and streaming applications.
* Experience with data ingestion process, creating data pipelines
(should be able to move data between stages in data lake, apply
transformation rules) and performance tuning. Must be hands-on coding using
Python with Spark.
* Exposure working with complex data types like XML or JSON, CI/CD
pipeline and scheduling tools like Airflow.
* Proven experience in DWH (type-2 dimensions, aggregations, star
schema, etc.) design and implementation
* Proficient in writing complex SQL queries
* Effective communication and collaboration skills to work with
cross-functional teams and stakeholders.
* Oversee and monitor the work of offshore according to projects
timelines and, if necessary, resolve any issues as they arise
Job Type: Full-time
Salary: $105,000.00 - $110,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
technical and able to structure and manage projects: 5 years (Required)
Data pipeline engineering for both batch and streaming apps.: 10 years (Required)
data ingestion process, creating data pipelines: 5 years (Required)
Python and Spark: 4 years (Required)
Exposure of XML or JSON, CI/CD: 4 years (Required)
scheduling tools like Airflow: 5 years (Required)
writing complex SQL queries: 5 years (Required)
Oversee and monitor the work of offshore: 3 years (Required)
working on fulltime role: 2 years (Required)
Work Location: Remote",4.3,"keasis Inc
4.3",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
912,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
913,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
914,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
915,Azure Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Required Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 9 years (Required)
Python: 8 years (Required)
Data lake: 7 years (Required)
Data Bricks: 7 years (Required)
Work Location: Remote",-1.0,TekValue IT Solutions,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
916,Data Engineer,-1,"Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore.

Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office.
The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus.
Position Responsibilities
Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities
Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization
Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process
Qualifications & Desired Skills
Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance
Minimum 2 years' of development experience using either Python, C#/Java or C++
Minimum 2 years' years of database/SQL experience
Familiarity with object-oriented programming concepts
Must be comfortable with development across the application stack
Ability to complete complex projects independently
Detail-oriented with strong verbal and written communication skills
Ability to work effectively in a high-energy, time sensitive team environment
Interest or prior experience in the financial trading industry (particularly in fixed income) a plus
This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education.
Base Salary for this role is expected to be between:
$110,000—$150,000 USD",4.4,"Garda Capital Partners
4.4","New York, NY",51 to 200 Employees,-1,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
917,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
918,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department.
The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
919,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1.0,Percept Health,Remote,-1,-1,-1,-1,-1,-1
920,Data Engineer (MongoDB),$80K - $114K (Glassdoor est.),"CrossCountry Mortgage (CCM) is one of the nation’s top 3 retail mortgage lenders with more than 7,000 employees operating nearly 600 branches across all 50 states. We have been recognized nine times on the Inc. 5000 list of America’s fastest-growing private businesses and have received countless awards for our standout culture. Through our dedication to getting it done, we make every mortgage feel like a win.

A culture where you can grow! CCM has created an exceptional culture driving employee engagement, exceeding employee expectations, and directly impacting company success. At our core, our entrepreneurial spirit empowers every employee to be who they are to help us move forward together. You’ll get unwavering support from all departments and total transparency from the top down.

CCM offers eligible employees a competitive compensation plan and a robust benefits package, including medical, dental, vision, as well as a 401K with an employer match. We also offer company-provided short-term disability, an employee assistance program, and a wellness program.
Position Overview:
We are looking for an experienced MongoDB developer to work with our development teams to integrate innovative features on our new products and applications, as well as maintain existing applications. The MongoDB developer will engineer scripts that automate daily tasks and ensure the security of data sets.
Job Responsibilities:
Upgrade databases through patches.
Understand and convert company needs into technical specifications and creating scalable solutions using MongoDB.
Optimize database schema designs, ensuring data integrity, and promoting best practices to enhance the overall performance and scalability of MongoDB solutions.
Assist developers in identifying and resolving performance issues by utilizing Mongo Profiler to profile and fine-tune database queries and operations.
Leverage performance optimization techniques by implementing proper indexes, including B-Tree, Geospatial, and Text indexes, to enhance query performance and overall system efficiency.
Implement the most effective backup and recovery methods to safeguard critical data and ensure business continuity in case of data loss or system failures.
Collaborate with cross-functional teams to understand business requirements.
Stay updated with the latest MongoDB advancements, features, and best practices.
Qualifications and Skills:
Bachelor’s/Master’s degree in software engineering or Information Technology.
3+ years of experience in executing data solutions using MongoDB.
Recommend and implement best practices for Rest API integration framework/model.
Analytical and problem-solving skills.
Configuring schema and MongoDB data modelling.
Experience with database security management.
Administration of MongoDB in Atlas.
Thorough understanding of MongoDB architecture.
Excellent communication skills.
This job description is intended to convey information essential to understanding the scope of the job and the general nature and level of work performed by job holders within this job. However, this job description is not intended to be an exhaustive list of qualifications, skills, efforts, duties, responsibilities or working conditions associated with the position.
CrossCountry Mortgage, LLC offers MORE than a job, we offer a career. Apply now to begin your path to success! https://crosscountrymortgage.com/about-us/careers/apply/
California residents: Please see CrossCountry’s privacy statement for information about how CrossCountry collects and uses personal information about California consumers.
CrossCountry Mortgage, LLC. (NMLS3029) is an Equal Opportunity Employer committed to workforce diversity. Qualified applicants will receive consideration without regard to race, religion, creed, color, orientation, gender, age, national origin, veteran status, disability status, marital status, sexual orientation, gender identity, or gender expression, or any other protected status in accordance with all applicable federal, state and local laws. The collective sum of the individual differences, life experiences, knowledge, inventiveness, innovation, self-expression, unique capabilities and talent that our employees invest in their work represents a significant part of not only our culture, but our reputation. The Company is committed to fostering, cultivating and preserving a culture that welcomes diversity and inclusion. Employment is contingent upon successful completion of a background investigation. CrossCountry Mortgage, LLC. is an FHA Approved Lending Institution and is not acting on behalf of or at the direction of HUD/FHA or the Federal government. To verify licensing, please visit www.NMLSConsumerAccess.org.",3.3,"CrossCountry Mortgage, LLC.
3.3","Charlotte, NC",1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
921,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
922,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
923,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
924,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1.0,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
925,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
926,Data Engineer,$76K - $101K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
The Data Governance/Data Quality Subject Matter Expert (SME) is a professional responsible for designing, implementing, and managing the client’s data governance strategy and ensuring high data quality standards across data-centric operations and decision-making.
The SME is in charge of developing and maintaining robust data pipelines, ensuring data observability, managing data within the Microsoft Azure environment, and implementing data governance principles in alignment with industry best practices and regulatory standards.
Primary Responsibilities and Accountabilities:
Data Governance & Quality: This includes developing and implementing data governance policies and procedures, monitoring data quality, managing data within the Microsoft Azure environment, and ensuring compliance with data privacy regulations.
Data Pipeline & Workflow Development: Designing and implementing efficient and secure data pipelines in Azure Data Factory and workflows, ensuring optimal performance and incorporating data quality principals.
Data Integration & Observability: Designing data integration strategies, integrating data from multiple sources, ensuring data quality and observability through tools like Azure Synapse.
Data Security: Defining and implementing data security policies and procedures, including access control and data encryption within the Azure environment.
Performance Tuning: Optimizing data access, indexing, and query performance within Azure Data Factory to ensure scalable and reliable data infrastructure.
Monitoring & Troubleshooting: Monitoring data systems, troubleshooting issues, identifying performance bottlenecks, and collaborating with team members to resolve problems.
Collaboration & Communication: Working closely with stakeholders to understand their data needs and develop solutions adhering to industry best practices and the needs of the business.
Automation & Tooling: Automating repetitive tasks and maintaining tools necessary for data processing and storage within the Azure environment.
Qualifications
A Bachelor's degree in a related field such as computer science, mathematics, engineering, or information systems.
3+ years of hands-on experience programming in SQL.
2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes within the Microsoft Azure environment.
Experience in Data Governance OR Data Quality Management is a must.
Skills
Competencies:
Expertise in Microsoft Azure data tools, with a focus on Azure Data Factory and Azure Synapse for data processing, storage, and analysis.
Strong SQL skills and knowledge of data warehousing concepts and ETL (Extract, Transform, Load) processes within the Azure environment.
Proficiency in programming languages such as Python, and experience with big data technologies such as Hadoop, Spark, and Kafka.
Ability to design, build, and maintain data pipelines, with a particular focus on Azure Data Factory.
Strong analytical and problem-solving skills, with the ability to identify and resolve data-related issues effectively.
Experience with data security and privacy regulations, and understanding of HIPAA compliance.
Strong experience with data governance, data quality, and data observability principles.
Excellent written and verbal communication skills, ability to explain complex technical concepts to non-technical stakeholders.
Excellent collaboration and teamwork skills, with the ability to work in a team involving client data engineers, data scientists, and data analysts.
Strong attention to detail and commitment to ensuring high data quality.
Continuous learning mindset and ability to stay up-to-date with new developments and technologies in data governance and data quality
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
927,GCP Data Engineer,Employer Provided Salary:$50.00 - $56.00 Per Hour,"JOB Description: -
ABOUT CONGLOMERATE
Conglomerate IT is a certified and a pioneer in providing premium end-to-end Global Workforce Solutions and IT Services to diverse clients across various domains. Visit us at ConglomerateIThttp://www.conglomerateit.comConglomerateIT
Conglomerate IT mission is to establish global cross culture human connections that further the careers of our employees and strengthen the businesses of our clients. We are driven to use the power of global network to connect business with the right people without bias. We provide Global Workforce Solutions with affability.
We offer a gamut of services across diverse domains, categories, skill sets with varying lengths of assignments /core capabilities included:
· Application Development Services
· Business Intelligence Services
· Enterprise Resource Planning
· Infrastructure Support Services
· Testing Services
· Program / Project Management
· Temporary & Permanent Staffing
Conglomerate IT is headquartered in Addison, Texas with branch offices across Canada, India, Sri Lanka, Argentina to further serve our clients.
JOB ROLE: GCP Data Engineer
Location: Job Type:Hartford, CT (Onsite Role)
Contract : 12+ Months
Work Authorization: w2 ,1099
Must have Skills: - Skills: Python/PySpark, GCP Cloud , HDFS, Hive, Big data
About Role: -
As a GCP Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and data architecture on the Google Cloud Platform. You will work closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and storage. Your expertise in GCP services and data engineering best practices will play a crucial role in maintaining a scalable, reliable, and high-performance data infrastructure.
Roles and Responsibilities:
· Design and implement data pipelines: Develop ETL (Extract, Transform, Load) processes to move data from various sources into GCP storage solutions such as BigQuery, Cloud Storage, and Datastore.
· Data modeling: Design and optimize data models to support data analytics, reporting, and machine learning initiatives. Ensure data accuracy, integrity, and appropriate access controls.
· Performance optimization: Fine-tune data pipelines and query performance to maximize efficiency and reduce latency, utilizing techniques like partitioning, indexing, and caching.
· GCP services utilization: Leverage GCP services like Dataflow, Dataproc, Pub/Sub, and Composer to create, schedule, and manage data processing and orchestration tasks.
· Data integration: Collaborate with cross-functional teams to integrate data from various sources, both internal and external, to enable comprehensive analysis and reporting.
· Data quality and governance: Implement data quality checks, monitoring, and validation processes to ensure data accuracy and consistency across the platform.
· Security and compliance: Implement security measures and access controls to protect sensitive data. Ensure compliance with industry standards and regulations.
· Documentation: Create and maintain technical documentation for data pipelines, workflows, and processes.
· Troubleshooting and support: Diagnose and resolve data-related issues, performance bottlenecks, and operational challenges in a timely manner.
Ideal Profile
· You have experience with Python Hive, Big data
· You have experience withYou GCP Cloud
· have strong experience with HDFS and other relevant tools/technologies.
· You have advanced python skills.
· You have PySparkknowledge.
· You have excellent communication and collaboration skills.
· You have attention to details.
· You are self-directed.
What's on Offer?
· Join a team of world class IT professionals
· Comprehensive salary package
· Great opportunity for growth and development
Job Type: Contract
Salary: $50.00 - $56.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Experience:
google cloud platform: 4 years (Required)
Work Location: In person",-1.0,CONGLOMERATE IT LLC,"Novi, MI",-1,-1,-1,-1,-1,-1
928,Snowflake Data Engineer,Employer Provided Salary:$70.00 - $80.00 Per Hour,"Job: Snowflake Data Engineer – Contract (6+ Months) – Stillwater, MN
REMOTE - HAVE TO WORK ONISTE WHEN EVER CLIENT CALL. Preferably local to MN NEEDED
EAD – GC – H4- EAD G C - CITIZEN
W2 Contract
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Expected hours: 40 per week
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 7 years (Required)
Snowflake API: 4 years (Required)
Snowflake administration: 6 years (Required)
Snowflake RBAC, RLS, CLS: 5 years (Required)
Willingness to travel:
50% (Required)
Work Location: Remote",-1.0,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
929,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1.0,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
930,Data Engineer I,Employer Provided Salary:$90K - $120K,"Interested in joining a growing company where you will work with talented colleagues, enhance a supportive and energetic culture, and be part of the climate solution? At Energy Solutions, we focus on the big impacts. And we believe that market-based programs can be a powerful force to deliver large-scale energy, carbon, and water-use savings. For 28 years, we've harnessed that power to offer proven, performance-based solutions for our utility, government, and institutional customers.
Energy Solutions (ES) is currently seeking a full-time Data Engineer I to join our Business Operations Team. The Data Engineer role is part of the Business Operations Team and will work on critical projects such as the Corporate Operational Data Warehouse (CODW) and the new ERP system. The CODW project is one of ES's strategic goals, to build and maintain a system that enables ES to centralize, integrate, and analyze internal data from various sources. The goal is to provide a single source of truth for ES internal data and enable data-driven decision-making across the organization. This role will potentially lead the design and implementation of the infrastructure for internal reporting needs, driving operational efficiencies and helping our company's transition to operating internationally with 1000+ employees.
Energy Solutions has a remote-friendly work environment for staff located throughout the United States. We also have offices in Oakland and Orange, California, Boston, Massachusetts, New York, and Chicago, IL, for those that wish to work from one of our offices.

Daily responsibilities include but are not limited to:
Collaborate with stakeholders to understand data requirements and develop data-driven solutions that support business goals
Analyze data sources and pipelines needed for the CODW project
Design and implement scalable data pipelines, ETL processes, and data integration solutions for the CODW project using Python, SQL, and AWS technologies
Develop and maintain a data warehouse and optimize data storage, processing, and retrieval
Build schema, populate the database, and automate reporting
Document and maintain technical specifications, ETL processes, data mappings, and dictionaries
Support the Data Science Center of Excellence (DSCOE) with other data engineering needs

Minimum Qualifications:
BS/BA in Computer Science, Physics/Engineering, Business or Mathematics
Experience in building ETL data pipelines, real-time pipelines are a plus
Strong programming skills in Python and SQL
Minimum 2 years of data warehouse development and strong fundamentals in dimensional data modeling
Minimum 2 years of experience creating SQL queries and ETL design, implementation, and maintenance
Minimum 2 years of experience developing data pipelines using Python
Minimum 2 years of experience with AWS, AWS Redshift, AWS Data Engineering and ML Ops tools and services (ie: S3, Redshift, RDS)
Proven ability to aggregate, normalize, munge, analyze, and summarize value from disparate datasets
Strong oral, written, and interpersonal communication skills
High degree of accuracy and attention to detail
Ability to establish priorities, work independently with minimum supervision
Preferred Qualifications:
Ability to manage and communicate data warehouse plans to a non-technical audience
Expertise in cloud services architecting and designing secure AWS environments
Experience using BI tools like Tableau and Looker
Experienced in analyzing and troubleshooting data quality issues
Self-driven, highly motivated, and able to learn quickly
Familiarity with large data sets, cloud-based development and deployment, open-source practices and frameworks, and intelligent applications is desirable
Proven success in migrating data from legacy systems

Salary DOE: $90k - $120k / Annually
Compensation is commensurate with experience and includes a generous retirement package. Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP).
To see this and our other open positions, please visit https://energy-solution.com/company/careers/.
We encourage candidates from diverse backgrounds to apply. For more information about Energy Solutions and our Diversity, Equity and Inclusion efforts, please visit us on our website at www.energy-solution.com.
Background Check Information
Information will be requested to perform the compulsory background check. A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment. Energy Solutions is an equal opportunity employer.
Reasonable Accommodations
Energy Solutions is committed to providing access and reasonable accommodation for individuals with disabilities. If you require an accommodation in completing this application, interviewing, and/or completing any pre-employment testing, or otherwise participating in the employee selection process, please email accommodation@energy-solution.com.
Privacy Notice for Job Applicants",3.9,"Energy Solutions
3.9",Remote,10000+ Employees,2008,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$25 to $100 million (USD)
931,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
932,AWS Data Engineer,Employer Provided Salary:$60.00 - $75.00 Per Hour,"Title: AWS Data Engineer
Location: Columbus, OH (Day 1 onsite / Hybrid Model)
Duration: Contract
Job Description:
Data Engineer: Open positions – 2
Skills:
Must Have:
BS/BA degree or equivalent experience
General: Strong organizational, problem-solving, and critical thinking skills; Strong documentation skills
Coding: Proficiency in Java
• Cluster Computing frameworks: Proficiency in Spark and Spark SQL
• AWS Data Services: Proficiency in Lake formation, Glue ETL (or) EMR, S3, Glue Catalog, Athena, Kinesis (or) MSK, Airflow (or) Lambda + Step Functions + Event Bridge
Data De/Serialization: Expertise in at least 2 of the formats: Parquet, Iceberg, AVRO, JSON-LD
AWS Data Security: Good Understanding of security concepts such as: Lake formation, IAM, Service roles, Encryption, KMS, Secrets Manager
Good to Have:
• Linux Scripting, Jenkins
• DevOps: Git, CI/CD, JIRA, TDD
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Ability to commute/relocate:
Columbus, OH 43085: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Required)
Java: 4 years (Required)
Data warehouse, Glue and SparkSQL: 8 years (Required)
Work Location: In person",4.5,"Synkriom Technology Pvt. Ltd.
4.5","Columbus, OH",201 to 500 Employees,2015,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
933,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
934,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
935,Apple Music - Data Engineer,-1,"Summary
Posted: Aug 18, 2023
Weekly Hours: 40
Role Number:200496819
This team is more than a group of engineers — it’s a group of music lovers. That passion has made Apple Music the world’s most complete music experience, with over 60 million songs, thousands of playlists, and daily selections from music experts for 115 countries. The team’s data-driven engineers focus relentlessly on the customer experience by running worldwide experiments and analyzing usage and latency, while collaborating with Apple’s product groups. As a result, someone can use the Shazam app to identify an intriguing song in a café in the morning, add it to their playlist from Apple Watch, listen to it through their AirPods on their commute, and share it with their family on HomePod at dinnertime. And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love. Apple Music is a huge part of Apple’s business because it’s a big part of people’s lives.
Key Qualifications
Expertise in designing, implementing and supporting highly scalable data systems and services in Scala
Experience with distributed computing technologies such as Hadoop MapReduce, Spark/Spark-SQL, YARN/MR2
Expertise building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Solid understanding of data-modeling and data-architecture optimized for big data patterns, such as efficient storage and query on HDFS Experience with distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues
Experience with low-latency NoSQL datastores, and traditional relational databases is a plus
Description
We are seeking a hardworking, performance-savvy, engineer to build out the big data platform and services, which power many of these customer features — existing and new. As a core member of the Data Engineering team you will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability. This will include crafting systems to model, ingest, process and compute large-scale, mission-critical data across Apple Music. High-throughput and reliability are essential. The computed datasets are produced for internal reporting used by executives and they are also shared across many teams within the ASE organization, such as the Search team, Recommendation team, AB team, Marketing team. You will also enjoy the benefits of working in a fast growing business where you are encouraged to “Think Different” to solve very interesting technical challenges and where your efforts play a key role in the success of Apple’s business. This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users. Come join us to help deliver the next amazing Apple product!
Education & Experience
BS CS or equivalent work experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $131,500 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",4.2,"Apple
4.2","Seattle, WA",10000+ Employees,1976,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
936,Big Data Engineer,Employer Provided Salary:$48.00 - $52.00 Per Hour,"Greetings from KonnectingTree Inc.,
We are actively looking for a candidate with below mentioned experience,
Role : Bigdata Engineer
Location : Hybrid ( Bloomington, MN)
Experience : 3-5 years
Job Description:
* Good experience in build and deployment of Big data applications using Pyspark
* Good experience in Python.
* Experience in Hadoop file structure.
* Good Communication and team player
Job Type: Contract
Salary: $48.00 - $52.00 per hour
Experience level:
3 years
4 years
Schedule:
8 hour shift
Ability to commute/relocate:
Bloomington, MN 55425: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",-1.0,KONNECTINGTREE INC,"Bloomington, MN",-1,-1,-1,-1,-1,-1
937,2024 Data Engineer I,-1,"2024 Data Engineer I
316114

Textron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.

Responsibilities:
Analyze, manipulate, or process large sets of data using statistical software.
Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces
Identify business problems or management objectives that can be addressed through data analysis.
Document data standards, semantic layer mapping, data flows and conceptual flows.
Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.
Perform statistical analysis to provide recommendations on best courses of action to business leaders.
Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.
Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.
Collaborate with business users to ensure data is available for self-service analytics.
Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.


Qualifications:
Education: Bachelor’s degree in computer science, engineering, mathematics, or similar field required
Years of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience required
Software Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferred
Experience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)
EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

Recruiting Company Textron Specialized Vehicles
Primary Location US-Georgia-Augusta
Job Field Information Technology
Schedule Full-time
Job Level Individual Contributor
Shift First Shift
Job Posting 08/24/2023, 8:20:05 AM",3.8,"Textron Specialized Vehicles
3.8","Augusta, GA",10000+ Employees,1923,Company - Public,Aerospace & Defense,Aerospace & Defense,$10+ billion (USD)
938,Sr. Azure Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"LeadStack Inc. is an award-winning, one of the nation's fastest-growing, certified minority-owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Position Details:
Title: Azure Data Engineer
Location: Remote
Duration: 6+ Months Contract
Top 3 skills: SQL, Snowflake, and Data Warehousing/Design
Its a W2 Role. No C2C.
Client is seeking a talented and motivated Data Engineer to join our dynamic team. As a Data Engineer, you will play a critical role in developing and maintaining our data infrastructure, ensuring the smooth flow of data for our analytics and insights projects. The ideal candidate will have a strong background in SQL, data warehousing, ETL processes, and a preference for Snowflake experience. Additional experience with dbt or semantic layer tools will be a bonus, and proficiency in Python will also be advantageous.
Bachelor's degree in Computer Science, Engineering, or a related field.
Solid experience with SQL and data warehousing concepts.
Strong understanding of ETL processes and tools.
Preferred experience with Snowflake data warehouse platform.
Bonus for experience with dbt or other semantic layer tools.
Proficiency in Python is a plus.
Excellent problem-solving and analytical skills.
Strong attention to detail and ability to work in a fast-paced environment.
Effective communication skills and ability to collaborate with cross-functional teams.
Key Responsibilities:
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.
Optimize data ingestion and transformation processes to ensure efficient and scalable data flows.
Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.
Develop and maintain data models, schemas, and data dictionaries for efficient data storage and retrieval.
Perform data validation, quality checks, and troubleshooting to identify and resolve data-related issues.
Monitor and optimize database performance, including query tuning and index optimization.
Collaborate with data scientists and analysts to implement data models and support their data requirements.
Stay up-to-date with industry trends and best practices in data engineering, and proactively recommend and implement improvements to our data infrastructure
To know more about current opportunities at LeadStack, please visit us at https://leadstackinc.com/careers/
Should you have any questions, feel free to call me on 510-480-1006 or send an email on tarique.aziz@leadstackinc.com
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Data Warehousing/Design: 8 years (Required)
Snowflake: 5 years (Required)
Azure: 5 years (Required)
Work Location: Remote",4.3,"Leadstack Inc
4.3",Remote,51 to 200 Employees,2016,Company - Private,HR Consulting,Human Resources & Staffing,Unknown / Non-Applicable
939,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
940,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
941,Energy Data Engineer - Mid Level,$81K - $121K (Glassdoor est.),"About the Organization
Now is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.

The Data Engineer will support Redhorse in its execution of specific task areas related to the Federal Government Carbon Pollution Free Electricity (CFE) Technical and Strategy Support contract. The program will establish technical frameworks and analytical tools and methods to inform CFE procurement decisions for the entirety of the federal government. Work will be conducted primarily at Redhorse office locations (hybrid remote work) and on government client site as required. The ideal candidate will live in the DC/Virginia/Maryland area and willing to undergo a background check to obtain a Public Trust.
Primary Duties and Responsibilities for this position include
Review and analyze the US Federal Government Executive Agencies electric load data
Develop and test modeling techniques to understand an agency’s hourly load profile to inform the agency’s hourly CFE procurement requirement as an off taker from a new CFE technology source.
Support the configuration and ingestion of designated structured, unstructured, and semi-structured data repositories into capabilities that satisfy client requirements and support a data analytics pipeline to drive rapid delivery of functionality to the client.
Maintain all operational aspects of data transfers while accounting for the security posture of the underlying infrastructure and the systems and applications that are supported and monitoring the health of the environment through a variety of health tracking capabilities.
Automate configuration management, leverage tools, and stay current on data extract, transfer, and load (ETL) technologies and services.
Work under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.
Apply comprehension of data engineering-specific technologies and services, leverage expertise in databases and a variety of approaches to structuring and retrieving of data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically using vendor consoles.
Engage with multiple functional groups to comprehend client challenges, prototype new ideas and new technologies, help to create solutions to drive the next wave of innovation, and design, implement, schedule, test, and deploy full features and components of solutions.
Identify and implement scalable and efficient coding solutions.
Minimum Requirements for skills, Experience, and Credentials
Master’s Degree in Engineering, Computer Science, Mathematics or Statistics
Four (4) years of experience in enterprise data analysis with a preferred focus on utility data, to include advanced metering infrastructure (AMI) electricity data
Local to Washington D.C. metro area preferred, remote work is an option
Willingness to travel in support of on-site customer meetings
Willingness to obtain a Public Trust
Desired Basic Requirements for Skills, Experience, and Credentials include
Knowledge of U.S. Federal Government Energy Legislation and Executive Orders (especially EO 14057)
Experience in or knowledge of utility interval data and electricity pricing
Executive Order Requirement:
While currently on HOLD, Redhorse recognizes that we may need to comply with Executive Order 14042 which requires Federal Contractors to ensure all U.S. new hires be fully vaccinated for COVID-19. As required by the Executive Order, Redhorse will work in coordination with applicable contract agencies to consider requests for Reasonable Accommodations.
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.
EOE/M/F/Vet/Disabled",3.5,"Redhorse
3.5","Arlington, VA",51 to 200 Employees,2008,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
942,Data Engineer II,$90K - $119K (Glassdoor est.),"Data Engineer II
Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What’s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.
Join us to do the best work of your career and make a profound social impact as a Data Engineer on our Growth Marketing team in Round Rock, Texas, Hopkinton, Massachusetts, or Remote United States
What you’ll achieve
As a Data Engineer II on a growing team, you will bring in your industry experience to develop, deploy, and maintain productionized data pipelines that support the training and productionization of machine-learning models.
You will:
Build, test, and implement data pipelines using tools like Apache Airflow, Spark, and Python for the training and inferencing of machine-learning models
Ensure data quality, monitor data pipelines, and troubleshoot production issues
Document metadata, data models, data pipelines and APIs to help data science teams understand and leverage data assets.
Work with data scientists and business analysts to understand data requirements
Take the first step toward your dream career
Every Dell Technologies team member brings something unique to the table. Here’s what we are looking for in this role:
Essential Requirements
U.S. citizen residing in the U.S.
Bachelor’s Degree in Computer Science, Engineering, IT, or equivalent professional experience
1 to 2 years experience developing production-quality Python and performant SQL, working with data warehouses, JSON, Apache Hive, Hadoop or Spark, or related technologies
1 to 2 years experience with orchestration tools, such as Apache AirFlow
Experience with monitoring tools and observability
Who we are

We believe that each of us has the power to make an impact. That’s why we put our team members at the center of everything we do. If you’re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we’re looking for you.

Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us.

Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. Read the full Equal Employment Opportunity Policy here.
Job ID:R227305

Dell’s Flexible & Hybrid Work Culture

At Dell Technologies, we believe our best work is done when flexibility is offered.

We know that freedom and flexibility are crucial to all our employees no matter where you are located and our flexible and hybrid work style allows team members to have the freedom to ideate, be innovative, and drive results their way. To learn more about our work culture, please visit our locations page.",4.2,"Dell Technologies
4.2","Round Rock, TX",10000+ Employees,1984,Company - Public,Information Technology Support Services,Information Technology,$10+ billion (USD)
943,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Data Engineer
Candidate must have AWS (Redshift, Glue, S3), Spark, Phython, Scala and Should be expert in SQL
Basic qualifications
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
Design, implementation and management of data processing systems dealing with large data sets.
Proficiency in, at least, one modern programming language such as Python, Java or Scala
Hands-on experience and advanced knowledge of SQL.
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and cloud computing
Experience working with Open-Source Big Data tools (Hadoop, Spark, Hive, Presto and etc.)
Preferred qualifications
Experience in working and delivering end-to-end projects independently
Relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics
Experience working with AWS Big Data Technologies (AWS Glue, EMR, Athena, Ec2, Redshift and S3 etc.)
Experience working with Oracle, PostgreSQL, SQL Server or MySQL.
Proven track record of delivering a big data solution
Experience working with both Batch and Real Time data processing systems
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Expected hours: 40 per week
Compensation package:
1099 contract
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Austin, TX 78703: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 8 years (Required)
Python: 8 years (Required)
AWS: 5 years (Preferred)
Work Location: Hybrid remote in Austin, TX 78703",-1.0,PRISM IT LLC,"Austin, TX",-1,-1,-1,-1,-1,-1
944,Data Engineer,$73K - $110K (Glassdoor est.),"Data Engineer

Posting Date: Aug 23, 2023
Location: Houston, TX, US
Lake Charles, LA, US
Company: Sasol
A job at Sasol is a career made by you, with purpose, development opportunities, benefits to support every stage of life and a working culture that embraces flexibility, diversity, and inclusion. Learn more.

Purpose of Job
The Data Engineer works as part of an agile team, interacts with data scientists/translators and industry experts to understand how data needs to be converted, loaded and presented. Collects, structures, analyses data and ensures quality. Responsible for the definition, conception, construction, and ongoing support of big data platforms and solutions.
Recruitment Description / Key Accountabilities
Integrate IT (Information Technology) and OT (Operational Technology) Systems.
Use programming languages and big data technologies following best practices and standards
Understands latest analytics tools and technologies (open source/ proprietary/ vendor supported) and determines fit with current architecture
Building and maintaining data pipelines that move data between various systems.
Ensuring data quality, integrity and security
Monitoring and optimizing data performance
Collaborate with technical teams to develop and maintain a flexible and sustainable workflow.
Collaborate with Product Owner, Data Scientists, Data Translators, and other Business Stakeholders to implement and deploy scalable solutions in an agile way of working; actively participate in scrum process and meetings with team where required
Assist with communication, improve transparency, radiate information, and address challenges effectively
Formal Education
University Bachelors Degree in Computer Science & Inf Systems or Engineering: Computer
Min Experience
Experience: 5+ relevant years
Long Description
Excellent communication and written skills.
Experience in building complex data pipelines using Cloud Services, REST APIs, SQL Databases. Python and SQL skills are are required. Experience in Azure ADF, Synapse or Databricks are beneficial.
Knowlege of Cloud Computing and Infrastructure, Data Models and Standards, Database Administration, Plant Information Management Systems, ERP Systems, Networking and Protocols, Security and Compliance, Change Managemnt, Problem Solving and Trouble Shooting are a plus.

Sasol (USA) Corporation is an Equal Opportunity Employer and gives consideration for employment to qualified applicants without regard to race, color, religion, creed, age, sex, sexual orientation, gender identity, pregnancy, national origin, disability or protected veteran status, as well as any other characteristic protected by applicable law, regulation or local ordinance. For more information about your rights under the law, see http://www.eeoc.gov/employers/upload/poster_screen_reader_optimized.pdf

Sasol treats work-authorized job applicants equally in recruiting and hiring without regard to their citizenship status or national origin unless required by law, regulation, executive order, government contract, or determination by the Attorney General. Please note Sasol will not sponsor, obtain, and/or petition for temporary visa status (for example, E, F-1, H-1, H-2, L, B, J, or TN) for candidates for this position and/or personnel hired for this position.


Nearest Major Market: Houston",3.9,"Sasol
3.9","Houston, TX",10000+ Employees,1950,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$10+ billion (USD)
945,Azure Data Engineer,Employer Provided Salary:$50.00 Per Hour,"7+ years of relevant experience
At least two years of experience building and leading highly complex, technical engineering teams.
Strong hands-on experience in Databricks
Experience managing distributed teams preferred.
Strong technical experience in large distributed systems, Data Warehousing, Data Lake at scale Project management skills: financial/budget management, scheduling and resource management experience with medium and large-scale projects
Comfortable working with ambiguity and multiple stakeholders.
Comfortable working cross functionality with product management and directly with customers; ability to deeply understand product and customer personas.
Architecture Design Experience for Cloud and Non-cloud platforms
Expertise on Azure Cloud platform
Knowledge on orchestrating workloads on cloud
Ability to set and lead the technical vision while balancing business drivers
Strong experience with PySpark, Python programming
Proficiency with APIs, containerization and orchestration is a plus.
Job Type: Contract
Salary: $50.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Work Location: Remote",3.8,"Visvak Solutions
3.8",Remote,51 to 200 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,Less than $1 million (USD)
946,Data Movement Engineer C2H,Employer Provided Salary:$55.00 Per Hour,"Client required information:
***They need Someone who has strong Talend Experience minimum of 5 years***.
Talend API is Desirable, But any API is also fine like snowflake as a last option.
They are working on Data Base Movement from Talend to ETL/ELT.
Formally they need ETL/ELT Developers or Talend Developers with ETL/ELT data movement experience.
Implementation and Configuration Talend is Mandate.
They also need to take a an on call support after work hours for ongoing production Support team, this might happen on weekends too.
They are going to implement more API’s, so having extra API experience is plus.
They need to understand the baseline of Insurance( The Basic work or Knowledge on Insurance Modules).
They can Work Remote, But need only from EST Zones.
Manager is not going to accept any profiles without Talend Experience.
Skill Qualifications Required:
· Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
· Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
· Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
· Some experience with cloud-based database technologies required
· Working knowledge of data warehousing concepts, structures and ETL best practices
· Experience using query tools (e.g. AQT, MS Query)
· Ability to problem solve using analytical thinking skills
· Must work well independently - must be inquisitive and seek answers to complex questions without being prompted
· Strong organizational and time management skills
· Strong communication skills including verbal and written to communicate effectively with clients and management
· Strong project management skills to ensure that projects get done on time and within budget
· Effectively participates in teams and moves the team toward completion of goals
Job Types: Full-time, Contract
Salary: From $55.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Experience:
Talend: 5 years (Required)
ETL: 5 years (Required)
SQL query: 3 years (Required)
data warehousing: 3 years (Required)
business intelligence tools: 3 years (Required)
Work Location: Remote",-1.0,Technovant inc,"New Haven, CT",-1,-1,-1,-1,-1,-1
947,Lead Azure Data Engineer,Employer Provided Salary:$135K - $140K,"Lead Data Engineer.
Location : New York, NY ( Hybrid)
Full Time
10+ years of experience in data engineering
5+ years of experience managing a team of data engineers
Strong understanding of SQL Server SSIS
Excellent communication and interpersonal skills
Ability to work independently and as part of a team
Ability to meet deadlines and work under pressure
Strong desire to be hands-on with the development and implementation of data pipelines and data warehouses
Experience working with stakeholders to develop product backlog grooming, sprint planning data engineering and
QA Testing
Migration of SQL Server to Azure is desired
Experience with Azure Data Factory, ADLS, and CI/CD highly desired
The Hands-On Data Engineering Lead is responsible for leading a team of data engineers in the design, development, and
implementation of data pipelines and data warehouses. The ideal candidate will have a strong understanding of data
engineering principles and practices, as well as experience with SQL Server SSIS. The Hands-On Data Engineering Lead will
also be responsible for managing the team's workload, mentoring junior engineers, and communicating with stakeholders.
Additionally, the Hands-On Data Engineering Lead will be expected to be hands-on with the development and
implementation of data pipelines and data warehouses, and to have experience working with stakeholders to develop
product backlog grooming, sprint planning data engineering and QA Testing.
Responsibilities:
Lead the design, development, and implementation of data pipelines and data warehouses
Develop and implement data pipelines and data warehouses using SQL Server SSIS
Troubleshoot data pipelines and data warehouses
Communicate with stakeholders to understand their needs and ensure that projects meet their requirements
Manage the team's workload and ensure that projects are completed on time and within budget
Mentor junior data engineers and help them develop their skills
Stay up-to-date on the latest data engineering technologies and best practices
Provide support to production teams and QA teams
Work with stakeholders to develop product backlog grooming, sprint planning data engineering and QA Testing
Working with other engineers to design and implement data pipelines and data warehouses using Azure Data
Factory, ADLS, and CI/CD
Experience in migrating SQL server to Azure
30% team management and 70% hands-on
Job Type: Full-time
Salary: $135,000.00 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 10 years (Required)
SQL: 10 years (Required)
Data warehouse: 10 years (Required)
SSIS: 10 years (Required)
Work Location: In person",-1.0,Rivago Infotech Inc,"New York, NY",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
948,Data Engineer (Onsite),Employer Provided Salary:$85K - $100K,"If you are seeking a career that has tremendous impact on manufacturing operations, then come be a part of Tokai Carbon GE's team to design, develop and implement our new manufacturing execution system. You'll play a critical role in our organization's success, providing easier access to data that engineers, analysts and decision-makers need to do their jobs. You will set the foundation of data for growth, relying on your programming and problem-solving skills to create scalable solutions. You will be part of the team building BI and Analytics tools that will take our operations to the next level. If you are excited about applying your knowledge and skills to meaningful challenges, apply today!
JOB SUMMARY
The Data Engineer will be responsible for designing, implementing, and supporting solutions used to collect, transform, store, and analyze data from a variety of company applications. They analyze and organize raw data, build data systems and pipelines, review and analyze data infrastructure, plan, and implement solutions to store and manage data. Also, they create reports, dashboards, and other analytics solutions. They will work closely with the lines of business and external partners, to ensure successful development and delivery of BI and data analytics solutions. This is a hands-on development and support position with ownership responsibility for data architecture and engineering for BI and analytics purposes.
ESSENTIAL FUNCTIONS
Work with business users and other technology teams to understand their data requirements, identify appropriate data sources, design, and develop data pipelines and design appropriate data structures to store the required data.
Design data pipelines using SQL, other programming languages and data integration tools.
Gather and combine data from a variety of sources to support actionable decision making. Prepare data for reporting, and predictive and prescriptive tools.
Address BI/EDW technical development working with operations, sales, supply chain, technical services, ERP applications, analytics, and finance & accounting teams.
Contribute to our suite of reports and data products while collaborating with business analysts, business process owners, and other team members, including UAT, and support for technical testing.
Work closely with senior team members defining data architecture, optimizing EDW performance, and updating EDW documentation, including outlining options to improve resilience and data quality monitoring.
Maintain the Enterprise Data Warehouse (EDW) and influence data quality and reliability.

COMPETENCIES
Data modeling, relational databases, data warehousing, business intelligence, stream processing.
SQL programming skills, including writing efficient and optimized SQL queries for data retrieval and manipulation.
ETL/ELT processes and data integration.
Strong analytical skills, development experience, and excellent verbal and written communications.
Manage all stages of the development lifecycle, including planning, requirements gathering, designing, developing, documenting, testing, training, deployment, and support.
Own individual and cross-functional team deliverables on small to large projects.
Project and change management methodologies including Agile/Scrum.
Willingness and availability to travel, and perform other duties as needed.
Must be a self-starter, manage workload autonomously, and balance competing priorities.
Ability and desire to continuously learn about recent technologies and Analytics trends.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Computer Engineering, Analytics, Information Systems, or equivalent experience.
1-3 years of experience working with SQL and relational databases (MS SQL Server preferred).
1-3 years of experience working with Microsoft Azure, particularly Azure Data Factory, Azure Synapse and Power Platform.
1-3 years of experience preferred in reporting, dashboards, and BI tools (Power BI preferred) for ERP and MES platforms.
1-3 years of experience preferred with Visual Studio, SSRS.
Experience with MS D365 and AVEVA or similar ERP and MES solutions preferred.
Proficient in coding and scripting in Python preferred.
Experience in Manufacturing, Supply Chain, Logistics, and Finance and Accounting solutions preferred.
How to stand out: Microsoft Azure Data Engineer Associate Certification
COMPENSATION AND BENEFITS
Available on your first day: Full medical, vision, dental benefits, short-term disability, and long-term disability
Paid time off
Monthly wellness reimbursement
Health Savings, Flexible Spending and Dependent Care accounts
401(k) retirement savings plan with employer match
Annual bonus",2.6,"Tokai Carbon GE LLC
2.6","Charlotte, NC",1001 to 5000 Employees,-1,Company - Public,Chemical Manufacturing,Manufacturing,$1 to $5 billion (USD)
949,"Data Engineer (Snowflake, Python, Power BI)",Employer Provided Salary:$40.00 - $45.00 Per Hour,"Title: Data Engineer (Snowflake, Python, Power BI)
Location: Morton, IL
Duration: 12+ Months
Job Description:
The main function of a data engineer is to ensure that the data assets of an organization are supported by an architecture that supports the organization in achieving its strategic goal. A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data.
Position’s Contributions to Work Group:
The PSLD Transformation Analytics group engages with various stakeholders across the organization to help solve their business problems. The individual will run the entire project end to end, so strong skills in gathering/understanding customer requirements, creating and maintaining optimal data pipeline architecture, choosing appropriate tools/techniques and delivering actionable insights are must. They will also learn all relevant BU specific data and often combine them with relevant enterprise data domains to bring insights that are not possible if BU data alone is analyzed.
Specific responsibilities are –
Extract large, complex data sets that meet business requirements. Work to build the on-prem /cloud infrastructure for optimal extraction, transformation, and loading of a wide variety of complex business data from on-prem/cloud databases.
Identify ways to improve data reliability, efficiency, and quality.
Work with internal and external stakeholders to assist with data-related technical issues and support data needs.
Own the design and development of ongoing business metrics/KPI, reports and dashboards to drive key business decisions.
Prepare data for predictive and prescriptive modeling.
Typical Day:
Work is typically directed by a direct supervisor, project or team lead. Decisions on routine, medium risk issues that may affect the project team, suppliers or internal customers may be made by this position. Challenges include meeting expectations in delivering results, learning to refine solutions to better fit complex situations, making timely decisions, and communicating effectively with all project stakeholders.
Education/Experience:
Associate's degree in computer programming or a relevant field required. Bachelor's degree preferred.
Critical Technical Skills:
Familiarity with database such as Snowflake, DB2, SQL Server, Oracle (2-3 of these are required)
Programming languages - SQL(required), Python(required) and SAS(preferred)
Experience working with large data sets, preferably in several GB or millions of transactions.
Visualization - PowerBI(required), Tableau(preferred)
Experience working with platform integration tool like Snaplogic is preferred
Experience working with AWS (required)
Soft Skills Required:
Communication, Team-work, Problem Solving, Customer Focus
Job Type: Contract
Pay: $40.00 - $45.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
Snowflake: 3 years (Required)
Power BI: 4 years (Required)
Python: 4 years (Required)
Work Location: In person",5.0,"DSMH LLC
5.0","Morton, IL",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
950,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
951,Junior Data Engineer,$83K - $119K (Glassdoor est.),"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a junior data engineer, you will be part of a team that is responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.
The position:
Develop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Support various components of the data pipelines, including ingestion, validation, cleansing and curation
Manage and ensure the success of ongoing data pipeline routines
Collaborate with our implementation team to assist with the identification and reconciliation of data anomalies
Create and maintain documentation on data pipelines
Engage with our software engineering team to ensure precise data points per application specifications
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering, or applicable experience
Expertise with SQL, database design and data manipulation methodologies
Expertise with ETL/ELT and the development of automated validation and data pipelines
Strong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW, master data management and other database design principles
Comfortable working with high volume data in a variety of formats
Excellent verbal and written communication
Self-motivated
Passionate at learning
Familiarity with healthcare data is a plus
Experience with CI/CD and version control tools is a plus
Experience working within hybrid cloud environment such as AWS is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation
Social outings",5.0,"Decision Point Healthcare
5.0","Boston, MA",1 to 50 Employees,2013,Company - Public,-1,-1,Less than $1 million (USD)
952,Senior Data Engineer,Employer Provided Salary:$87K - $180K,"Oakland, California; Atlanta, Georgia; Irving, Texas
Regular Employee Full-Time
R-2023-08-81

Who We Are:
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners - in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
Pandora
Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. Pandora provides consumers with a uniquely-personalized music and podcast listening experience with its proprietary Music Genome Project® and Podcast Genome Project® technology. Pandora is available through its mobile app, the web, and integrations with more than 2,000 connected products.
How you’ll make an impact:
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What you’ll do:
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What you’ll need:
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to pay attention to details and be organized.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $87,200 to $180,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.

Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-08-81",3.6,"Adswizz
3.6","Oakland, CA",51 to 200 Employees,2007,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
953,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1.0,Radiant System,Remote,-1,-1,-1,-1,-1,-1
954,Data Engineer,$79K - $115K (Glassdoor est.),"Job Title :- Data Engineer
Location:- San Antonio, TX
Required:- Active Top Secret Clearance
We are looking for a talented Data Engineer to support the acquisition of mission-critical and mission-support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is performed mostly on customer site in Columbia, MD.
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.
Minimum Qualifications
Security Clearance - Must have a current Secret level security clearance and therefore all candidates must be a U.S. Citizen with a willingness to go to TS/SCI and take the CI poly after starting the position.
5 years experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.
Experience with programming languages such as Python and Java.
Proficiency with acquisition and understanding of network data and the associated metadata.
Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
Experience with Kibana and Elasticsearch.
Familiarity with various log formats such as JSON, XML, and others.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must have a Security+ or similar certification or the ability to obtain it immediately.
Must be able to work on customer site in Columbia, MD at least 4 days a week. (Subject to change)
Desired Skills (Optional)
Experience with NOSQL databases such as Accumulo desired
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
Job Type: Full-time
Schedule:
8 hour shift
Security clearance:
Top Secret (Preferred)
Work Location: In person",3.9,"Helm360
3.9","San Antonio, TX",201 to 500 Employees,1997,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
955,Azure Data Engineer,Employer Provided Salary:$70.00 Per Hour,"Job Description
Seeking an experienced Azure Data Engineer to design and implement data solutions using Azure technologies. The ideal candidate will have a strong background in data modeling, ETL, and data warehousing, and ability in SQL, Python, and Azure data services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.
Essential Functions
Design and implement data pipelines using Azure Data Factory to move data from various sources to Azure Data Lake Storage and Azure SQL Database.
Create and keep data models in Azure Synapse Analytics for reporting and analytics with Power BI.
Work with data scientists and analysts to implement data solutions.
Collaborate with the DevOps team to automate the deployment of data pipelines using Azure DevOps.
Participate in data governance efforts to ensure data quality and compliance.
Qualifications
2-5+ years of experience as a data engineer or related role.
Strong background in data modeling, ETL, and data warehousing.
Proficient in SQL, Python, and Azure data services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.
Experience using version control software, preferably git.
Experience with Visual Studio, Azure DevOps, and Power BI is a plus.
Microsoft Certified: Azure Data Engineer Associate or higher is a plus.
Education: bachelor’s degree in computer science, data science, or a related field.
Strong diligence.
Excellent troubleshooting and communication skills
Able to work well in a team setting and mentor others.
Job Types: Full-time, Contract
Pay: Up to $70.00 per hour
Benefits:
Health insurance
Ability to commute/relocate:
Brentwood, TN: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
SQL: 2 years (Required)
Data warehouse: 2 years (Required)
Azure Data Engineering: 2 years (Required)
design and implement data solutions: 2 years (Required)
Azure technologies: 2 years (Required)
data modeling, ETL, and data warehousing: 2 years (Required)
SQL, Python, and Azure data services: 2 years (Required)
Azure Databricks, and Azure Synapse Analytics: 2 years (Required)
Azure Data Factory: 2 years (Required)
version control software: 2 years (Required)
Visual Studio, Azure DevOps, and Power BI: 2 years (Required)
License/Certification:
Microsoft Certified: Azure Data Engineer Associate or higher (Required)
Work Location: In person",-1.0,Vision,"Brentwood, TN",-1,-1,-1,-1,-1,-1
956,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
957,Sr. Data Engineer,Employer Provided Salary:$84K - $191K,"Job Summary:
The primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver modules, stable application systems, and Data or Platform solutions. This includes developing, configuring, or modifying complex integrated business and/or enterprise infrastructure or application solutions within various computing environments. This role facilitates the implementation and maintenance of complex business and enterprise Data or Platform solutions to ensure successful deployment of released applications.
Minimum Qualifications
Bachelor's Degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)
5 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering
4 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)
Preferred Qualifications
Master's Degree in Computer Science, CIS, or related field
5 years of IT experience developing and implementing business systems within an organization
5 years of experience working with defect or incident tracking software
5 years of experience writing technical documentation in a software development environment
3 years of experience working with an IT Infrastructure Library (ITIL) framework
3 years of experience leading teams, with or without direct reports
5 years of experience working with source code control systems
Experience working with Continuous Integration/Continuous Deployment tools
5 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutions
Job Type: Full-time
Salary: $84,002.64 - $190,979.59 per year
Experience:
Data, BI or Platform Engineering, Data Warehousing/ETL: 3 years (Preferred)
developing and implementing business systems: 3 years (Preferred)
Data Engineer: 4 years (Preferred)
Work Location: In person",-1.0,Market Tree Research,"Carolina, WV",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
958,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
959,Data center infrastructure Engineer,Employer Provided Salary:$60.00 - $68.00 Per Hour,"Requirements and Responsibilities:
Must have Bachelor’s degree in IS or equivalent.
Must have experience with VMWARE, DATA CENTER OPERATIONS, IT INFRASTRUCTURE, and VIRTUALIZATION.
Experience working with Linux (CentOS/RHEL/Rocky) .
Experience working with VMWare 6.x / 7.x and storage area network (SAN).
Manage and support Microsoft domain / Active Directory Services, RBAC, Group Policy, Windows Servers, desktops and services automate tasks through scripting technologies Support execution of Information
Support and manage the VMWare virtualization and storage (SAN - Dell Extreme IO & Pure Storage) infrastructure to meet and exceed development operations growth expectations.
Job Type: Contract
Pay: $60.00 - $68.00 per hour
Benefits:
Vision insurance
Experience:
Data center operations: 4 years (Required)
IT infrastructure: 3 years (Required)
Work Location: In person",3.0,"BJ Services
3.0","Exton, PA",1001 to 5000 Employees,1872,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",$1 to $5 billion (USD)
960,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1.0,Radiant System,Remote,-1,-1,-1,-1,-1,-1
961,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
962,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
963,Senior Data Analytics Engineer,$96K - $130K (Glassdoor est.),"At Wistia our goal is to help every business to thrive with video. That's a big task, and we're super excited to help the world get there faster through simple and delightful video-focused products.
Wistia's Business Intelligence team is charged with leveraging data to empower strategic, insights-driven decision making across the organization. The Analytics Engineer role is a hybrid role with responsibilities that typically fall to Data Analysts, Data Engineers and Data Scientists. We are a small team and are looking for someone who is comfortable wearing different ""data"" hats. A successful candidate will combine strong technical and communication skills to manage the full data lifecycle at Wistia by creating and maintaining ETL pipelines, automating critical workflows, analyzing data to surface insights and translating those insights into clear recommendations for stakeholders.
The other members of the Business Intelligence team are curious, attentive to detail, impact-focused and approach problem solving with an engineering mindset. The team is centralized (not embedded) with each team member working with data from across the business. We strive to adopt the tools and best practices of the ""modern data stack"". For us that looks like: Redshift, dbt, Fivetran, Mode Analytics, Census, Snowplow Analytics. The Redshift warehouse we maintain serves as the single source of truth for the business.
Key Responsibilities
Maintain our 3rd-Party and custom ETL/reverse-ETL jobs and create new ones as required
Develop new predictive models as required to meet business outcomes
Generate easily interpretable analyses of complex datasets using SQL or python
Create persuasive narratives with data visualizations that enable self-serve exploration
Assist in the design and analysis of A/B testing to optimize business outcomes
Define version-controlled, modular business logic and datasets to be reused across the team
Document your findings, insights and recommendations for stakeholders
Meet business stakeholders ""where they live"" by becoming an expert in the data models and usage of their preferred SaaS tools, eg, Salesforce, HubSpot, Google Analytics, Amplitude, Stripe
Collaborate with:
Team members to share learnings, brainstorm new approaches and define best-practices
Stakeholders to scope, shape and prioritize their data requests
Qualifications
This section should talk about the competencies needed for the role, including qualities. Be sure to seek the input of key stakeholders and make sure we distinguish between must-haves and nice-to-haves because we will be using this to build our job positing and interview plan.
Technical
Highly proficient in SQL and python (or comparable general purpose programming language)
Facile with statistics and hypothesis testing
Skilled at constructing meaningful data visualizations
Comfortable with the command line and bash scripting
Knowledgeable about database architecture, data warehousing principles, REST APIs and basic ML algorithms
Non-technical
Experienced at scoping broad, loosely-defined requests into well-structured plans
Adept at translating data into actionable insights for stakeholders
Strong in written communication
Able to work independently with minimal oversight
Familiar with SaaS metrics and business practices
Values fit (for Wistia at large and BI specifically)
collaborative
learning oriented
attention to detail
simplicity as the starting point starts simple and add complexity only as needed
Key Metrics
This is a great place for more detail on the metrics we expect this person to own, and any that we measure success by.
Product, marketing, sales, and customer success teams have the data and insights they need to make decisions
Product, marketing, sales, and customer success teams have confidence in the data they use
Product, marketing, sales, and customer success teams are able to monitor business performance and spot problem areas and new opportunities
Responses to data requests are timely
Success Looks Like
This section should outline what success looks like. Knowing this helps inform the level and type of candidates we need, interview questions, onboarding plan, and coaching our new hires to success. It also makes it easier to spot performance issues early and hold ourselves accountable in hiring (and learn how we can improve).
30 Days
Complete Wistia onboarding program
Ramp up and learn the tools of the role (Mode, dbt, github, Fivetran, Redshift, Google Analytics, Salesforce, Pendo)
Ramp up and learn Wistia's data architecture, starting with Wistia app database
Ramp up and learn data analysis best practices and procedures
Complete ~5 basic data requests
90 Days
Confident in use of Mode, dbt, and GitHub for data analysis process
Confident in data architecture and able to self-source most data tables
Regularly handling basic data requests
Completed 1 complex data investigation
1 Year
Regularly initiating new data investigations and unearthing new insights
Regularly making improvements to our approach to data presentation
Regularly making improvements to our data models and data cleanliness
Working at Wistia
We try to ensure Wistia is an inclusive and diverse place where everyone feels happy, fulfilled, respected, comfortable, and welcome. We're proud to be an equal-opportunity workplace. We care a lot, so our benefits are actually benefits, not just the fun stuff like swag and snacks in the office (though we also have lots of those too!).
We know the biggest investment we can make is in our employees, so we provide:
A competitive compensation package that includes internal equity stock options
401k with 3% company contribution, regardless of whether you contribute
Fully paid healthcare, dental, and vision insurance (family plans included)
Pre-Tax FSA and Dependent Care Account
Fitness reimbursement
Flexible working hours – work at the times when you operate best, or set aside time for child and/or elderly care responsibilities during traditional ""office hours""
16 weeks paid parental leave for all new and expecting parents
Unlimited PTO (pretty common for people to take 4+ weeks off throughout the year)
Remote-first culture (work from anywhere in the U.S.)
Annual professional development stipend (courses, conferences, and more)
New hire bonus to enhance your home office set up
Pet insurance discount
Location/Remote Opportunities
Wistia is a remote-first company. Employees can work from our beautiful office in Cambridge, MA, or anywhere in the continental US",4.8,"Wistia
4.8","Cambridge, MA",51 to 200 Employees,2006,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
964,Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Title: Data Engineer
Experience in Retail merchandising analytics is a must,
Duration: 12 months
Location: Remote
Interview Process:
1*_st_ round – Hirevue Video Call*
2*_nd_ round – Hiring Manager*
Skills Required:
Data engineering, analytics, and data modeling experience
Python, Spark, Databricks, Azure, Power BI
Experience in retail merchandising analytics is a must
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
retail merchandising analytics: 8 years (Required)
Python: 8 years (Required)
Power BI: 8 years (Required)
Work Location: Remote",-1.0,zettalogix.Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
965,Data Engineer,Employer Provided Salary:$150K - $300K,"What we're looking for
Stytch is the platform for user authentication. We build infrastructure that sits in the critical path of our customer's applications. As a data engineer, you'll work on designing and building event-driven architecture systems to drive analytics insights and observability tooling for our customers.

What Excites You
Championing data-driven insights - you see data analytics and observability as a product critical to success
Solving problems with pragmatic solutions — you know when to make trade-offs between completeness and utility and you know when to cut scope to ship something good enough quickly
Building products that make developers lives easier — as a data engineer for a developer infrastructure company, what you build will have an immediate impact on our customers
Shaping the culture and growing the team through recruiting, mentorship, and establishing best practices
Learning new skills and technologies in a fast paced environment

What Excites Us
Comfort working in a modern data stack using tools like Snowflake, Redshift, DBT, Fivetran, ElasticSearch, and Kinesis
Appreciation for schema design and architecture that balance flexibility and simplicity
Experience designing and building highly reliable back-end and ETL systems
3+ years as a data or backend engineer

What Success Looks Like
Technical — build new, highly reliable services that our customers can depend on
Ownership — advocate for projects and solutions that you believe in and ship them to production
Leadership — level up your teammates by providing mentorship and guidance

Our Tech Stack
Data moves through Snowflake, ElasticSearch, MySQL, and Kinesis
Go and Node for application services
We run on AWS with Kubernetes for containerization
gRPC and protobufs for internal service communication

Expected base salary $150,000-$300,000. The anticipated base salary range is not inclusive of full benefits including equity, health care insurance, time off, paid parental leave, etc. This base salary is accurate based on information at the time of posting. Actual compensation for hired candidates will be determined using a number of factors including experience, skills, and qualifications.
We're looking to hire a GREAT team and that means hiring people who are highly empathetic, ambitious, and excited about building the future of user authentication. You should feel empowered to apply for this role even if your experience doesn't exactly match up to our job description (our job descriptions are directional and not perfect recipes for exactly what we need). We are committed to building a diverse, inclusive, and equitable workspace where everyone (regardless of age, education, ethnicity, gender, sexual orientation, or any personal characteristics) feels like they belong. We look forward to hearing from you!

Learn more about our team and culture here!",5.0,"Stytch
5.0",California,1 to 50 Employees,2020,Company - Public,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
966,Data Performance Engineer,Employer Provided Salary:$55.00 Per Hour,"Data Performance Engineer
Only candidate with years of experience: +8
Those authorized to work in the U.S. are encouraged to apply.
Remote
Interview Process: 3 Rounds of Zoom Video Interview
Candidate must have Data Performance Engineer experience.
Skill and Experience Required
· Experience in consuming large data volume
· Advance SQL skills
· Optimize strategy process for data handling for aggregation tables (Minimize processing time for large data volume)
· Experience in data loading with complex model for data and batch orchestration
· Throughput understanding, monitoring for efficient data loading
· Identifying bottlenecks and resource investigation
· Experience in periodic ETL Code maintenance and setting up SLA for cross functional team
· Implementing necessary techniques to improve ETL performance in Both DB and Application
· Improving query performance by turning ad re-evaluating data model
· Identifying dependency impact on both data and Application
Duties and responsibilities
· Performance Analysis and documenting factors (bottleneck, memory handling and inefficiency area which needs improvement)
· Identifying and documenting execution Benchmarks and Metrics
· Optimization on application and database
· Validation techniques for data and Orchestration
· Documentation for Optimization and Operations
· Implementation Alert mechanism in case of potential impact to downstream
· Collecting and documenting the Metrics for each ETL application
Job Type: Contract
Pay: $55.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Big data: 8 years (Required)
SQL: 8 years (Required)
ETL: 8 years (Required)
consuming large data volume: 8 years (Required)
Work Location: Remote",-1.0,Luttechub,Remote,Unknown,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
967,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
968,Data Engineer III,Employer Provided Salary:$117K - $194K,"As a member of the ShipBob Team, you will benefit from an environment where everything is achievable. We aim to be a place where you can:
Write Your Career Story. Because we are solving some of the most difficult problems in global commerce, you have the opportunity to write the story that will make your career.
Experience Global Impact and Global Connection. At ShipBob we benefit from diverse cultures and perspectives in service of the global community.
Grow With An Ownership Mindset. We believe that great innovation comes from great transparency. We are more resilient and more creative when we have an inclusive and transparent culture where everyone knows our strengths and opportunities.
Title: Data Engineer III
Location: Remote in these states: AZ, CA, CO, FL, GA, KS, KY, IA, ID, IL, IN, MA, ME, MI, MN, MO, NC, NH, NJ, NV, NY, OH, OR, PA, RI, SC, SD, TN, TX, VA, VT, WA, WI
Role Description:
As a Data Engineer III at ShipBob, you will be working within a team handling all facets of the database environment, from assisting engineering teams in table design, to building databases, to working with our data warehouse and ETL. We are looking for someone who enjoys working in a fast paced environment, who can think on his/her feet, and produce results.
What you'll do:
Focus on growing as an data engineer in an Azure SQL environment
Performing SQL programming and performance tuning
Capable of taking well-defined sub-tasks and completing these tasks.
Creating Power BI visualizations and paginated reports
Modeling and implementing databases and warehouses
Demonstrates they are a high energy individual.
Effective in communicating status to the team.
Exhibits ShipBobs's core values, focuses on understanding and living these values.
Accepts feedback graciously and learns from everything they do
Other duties/responsibilities as necessary.
What you'll bring to the table:
6-9 Years of Experience
Excellent problem solving skills
Excellent SQL skills
Excellent communication skills
Performance oriented mindset
Ability to work quickly and collaboratively in a fast-paced, entrepreneurial environment
Experience in the following:
SQL
Performance Tuning and Optimization
Data Modeling
Azure SQL Administration, including Security
Powershell
Azure Data Factory
Nice to have:
Power BI
Ability to own small well scoped projects and implement them
Experience in Azure Synapse and Cosmos
A passion for databases and an understanding that solutions you implement will affect our entire suite of applications
Experience with big data
Experience with Azure DevOps
Experience with Git
Experience with Agile
Experience with Azure DevOps
Experience with Azure Functions
Mongo Atlas experience
Spark
Classification: Exempt
Reports to: Manager, Database Administration
Perks & Benefits:
Medical, Dental, Vision & Basic Life Insurance
Paid Maternity/Parental Leave Program
Flexible Time Off Program
Paid Sick Leave and Paid Emergency Leave
Floating Holidays (2 days/year)
Wellness Days (1 day/quarter)
401K Match
Competitive Salary, Performance Bonus & Equity
Variety of voluntary benefits, such as, short term disability
Referral Bonus Program
Fun Culture >>> Check us out on Instagram (@lifeatshipbob)
ShipBob believes in transparency while providing a competitive total compensation package with a pay for performance approach. The expected base pay range for this position is $116,645 - $194,408.
We recognize that people come with a wealth of experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please still consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence; therefore, we encourage people from all backgrounds to apply to our positions.
About You:
At ShipBob, we're looking to bring on board people who embody our core values:
Be Mission-Driven. We want team members that are passionate about helping entrepreneurs improve their business, and bring that passion every day.
Be Humble. We have ambitious goals, and our team members understand that success or failure depends on us working together and leaving egos at the door.
Be Resilient. Logistics is a complicated business. So is software. We value team members that never give up and keep iterating until a problem is solved.
Be a Creative Problem Solver. As a startup, we value smart, innovative solutions to complex problems. We fall in love with the problem, not our ""favorite"" solution.
Be Safety Minded. It's not just talk; it's the way you work.
About Us:
ShipBob is a cloud-based logistics platform that partners with over 7,000+ e-commerce businesses to help make their entrepreneurial dreams a reality. We offer a full suite of fulfillment solutions for our merchants, including the ability to improve their transit times, shipping costs and deliver best in class experience to their customers. With an almost 100% accuracy rate in fulfilling orders and orders shipped on time, our merchants can count on us to deliver excellent service.
As one of the fastest growing tech companies in Chicago with over $300M+ raised from blue-chip investors like Menlo Ventures, Bain Capital Ventures, Hyde Park Venture Partners and SoftBank Vision Fund 2, our goal is to continue to be the #1 best fulfillment technology in the industry.
ShipBob provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.",4.4,"ShipBob, Inc.
4.4",Remote,1001 to 5000 Employees,2014,Company - Private,Shipping & Trucking,Transportation & Logistics,$100 to $500 million (USD)
969,GA/GCP Data Engineer,Employer Provided Salary:$65.00 - $85.00 Per Hour,"A Billion dollar women's clothier with over 1300 stores across North America is seeking a highly skilled Google Analytics / GCP Data Engineer to join our team for a 6 month plus project. As the Google Analytics / GCP Data Engineer, you will be responsible for building BQ tables in GCP, mapping GA data between GA4 and UA, owning, maintaining, and providing recommendations around the GA Schemas to leverage reporting, and utilizing DBT and Git for data transformation and version control. You will also implement ELT pipelines to extract, load, and transform data from various sources.
Responsibilities:
Build and maintain BigQuery tables in GCP
Map GA data between GA4 and UA
Own and maintain the GA Schemas to leverage reporting
Utilize DBT and Git for data transformation and version control
Implement ELT pipelines to extract, load and transform data from various sources
Provide recommendations for optimizing Big Query tables and data flows
Communicate effectively with stakeholders and team members
Requirements:
In-depth experience with Google Analytics (GA4 & UA)
Strong experience in Google Analytics Schema Mapping between Versions
Very strong GCP and BigQuery Skills
Experience using DBT and Git for data transformation and version control
Experience with implementing ELT pipelines to extract, load and transform data from various sources
Strong problem-solving and analytical skills
Excellent communication and collaboration skills
Job Type: Contract
Pay: $65.00 - $85.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Compensation package:
Hourly pay
Experience level:
7 years
Schedule:
Monday to Friday
Experience:
GCP: 5 years (Preferred)
Big Query: 5 years (Preferred)
Google Analytics: 5 years (Preferred)
Tableau: 4 years (Preferred)
Work Location: Remote",4.1,"V Soft Consulting
4.1",Remote,501 to 1000 Employees,1997,Company - Private,HR Consulting,Human Resources & Staffing,$100 to $500 million (USD)
970,AWS Data Engineer,Employer Provided Salary:$60.00 - $75.00 Per Hour,"Title: AWS Data Engineer
Location: Columbus, OH (Day 1 onsite / Hybrid Model)
Duration: Contract
Job Description:
Data Engineer: Open positions – 2
Skills:
Must Have:
BS/BA degree or equivalent experience
General: Strong organizational, problem-solving, and critical thinking skills; Strong documentation skills
Coding: Proficiency in Java
• Cluster Computing frameworks: Proficiency in Spark and Spark SQL
• AWS Data Services: Proficiency in Lake formation, Glue ETL (or) EMR, S3, Glue Catalog, Athena, Kinesis (or) MSK, Airflow (or) Lambda + Step Functions + Event Bridge
Data De/Serialization: Expertise in at least 2 of the formats: Parquet, Iceberg, AVRO, JSON-LD
AWS Data Security: Good Understanding of security concepts such as: Lake formation, IAM, Service roles, Encryption, KMS, Secrets Manager
Good to Have:
• Linux Scripting, Jenkins
• DevOps: Git, CI/CD, JIRA, TDD
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Ability to commute/relocate:
Columbus, OH 43085: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Required)
Java: 4 years (Required)
Data warehouse, Glue and SparkSQL: 8 years (Required)
Work Location: In person",4.5,"Synkriom Technology Pvt. Ltd.
4.5","Columbus, OH",201 to 500 Employees,2015,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
971,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
972,Data Engineer,$77K - $107K (Glassdoor est.),"Position Summary:
Reporting to the Director of Data Science, the Data Engineer will play a crucial role in designing, developing, and maintaining data pipelines and workflows to facilitate data integration, storage, and analysis. With the support of the Data Science team, this position will utilize proven expertise in Microsoft Azure cloud computing to implement and automate processes, ensure the health and efficiency of the data ecosystem, and bring unmatched value to our +1.5 million members across the country and across the world.

Duties and Responsibilities:
Lead architectural efforts for implementing the Data Science team's models and solutions.
Design, implement, and manage data pipelines using Azure Data Factory to extract, transform, and load data from various sources efficiently and securely.
Develop and optimize storage solutions on Microsoft Azure, ensuring data availability, scalability, and reliability.
Automate data-related jobs, processes, and workflows to streamline data delivery
Collaborate closely with cross-functional teams, including data scientists, analysts, and developers, to understand data requirements and support their data-related needs.
Design and maintain data models and architecture, enabling efficient data consumption by business applications and analytical tools.
Ensure data quality, integrity, and security throughout the data lifecycle.
Troubleshoot and promptly resolve data-related issues, performance bottlenecks, and system failures.
Research new data sources, third-party platforms, and data engineering best practices.
Stay current with the latest data engineering and cloud computing trends, particularly in the Azure ecosystem.
Assist Data Scientists with general analysis and dashboards, as needed.
Qualifications:
3-5+ years of relevant Data Engineering, Software Engineering, and/or Analytics experience.
Proved experience with Azure cloud solutions, preferably through your work portfolio and Azure certifications.
Strong proficiency in Azure Data Factory, Azure Storage, and other relevant Azure services.
Extensive experience designing and implementing automated data pipelines, ETL processes, and data workflows.
Advanced skills in SQL and Python for data manipulation, querying, and scripting tasks.
Familiarity with data warehousing concepts, modeling, and integration techniques.
Expert in integrating cloud solutions with external API sources and knowledgeable in creating in-house API solutions that meet our needs.
Excellent problem-solving abilities and a track record of handling complex data engineering challenges.
Strong communication skills and the ability to collaborate effectively with multidisciplinary teams.
Demonstrated ability to prioritize tasks, meet deadlines, and adapt to evolving project requirements.
Azure certifications (e.g., Azure Data Engineer Associate) will be a plus.
Willingness to travel up to 10%.
Organizational Overview:
The National Association of REALTORS® (NAR) is a team of professionals dedicated to providing world-class service to over 1.5 million REALTORS® working in the United States and around the world. The real estate industry is fast-paced and fast-changing--each year, our members participate in the sale, lease, and management of billions of dollars in real estate. As in every industry, our members' value proposition is constantly being challenged by innovation.

It is our mission to empower REALTORS® as they preserve, protect and advance the right to real property for all. We cannot do that without the ideas, passion, and commitment from our talented employees. As our greatest assets, employees are offered their pick of competitive benefits/perks and flexible work options. NAR is certified as a Great Place to Work®. Our most successful employees are committed to our Core Values, which are:

Putting members first
Leading change
Advancing Diversity and Inclusion
Giving respect
Collaborating
Communicating",3.7,"National Association of REALTORS®
3.7","Washington, DC",201 to 500 Employees,1908,Nonprofit Organization,Real Estate,Real Estate,$100 to $500 million (USD)
973,Senior Data Center Engineer,-1,"Marco is a one-stop shop for all things business tech. Our employees are “movers and shakers” and our company is always striving to do what’s right. Does this sound like a culture you want to be a part of? We’re hiring a new team member to help take Marco’s technology further – working full-time, Monday - Friday, 8am-5pm.
More about us. We do it all – from copy and print solutions to IT and managed services. We are an organization led by salespeople with 650+ engineers ready to fix any and all issues. We have offices in 12 states and service nationally.
Join our growing team. You won’t regret it.

The Senior Data Center Engineer is responsible for providing quality services and solutions to our internal clients while maintaining a high level of client satisfaction. This position works cooperatively with other teams in a coordinated effort to develop, design, implement and support Marco’s Data Centers, along with the IT environment that depends upon them. While not a leadership position, the Senior datacenter engineer displays leadership qualities and may be called upon to lead others in certain situations.

ESSENTIAL FUNCTIONS:
Lead the overall design and implementation of the technology stack within the Marco Data Centers
Maintain physical access to the Marco Data Centers and be a primary contact for essential services
Perform proactive maintenance and reactive support to the equipment within the Marco Data Centers
Create and maintain documentation for all equipment within the Marco Data Center. Review and assist others with their documentation, offering constructive feedback and pointing out deviations from our policies and standards where necessary.
Report on the health of the Marco Data Centers, perform capacity planning and proactively provide recommendations for future growth
Monitor and provide proactive recommendations for backup systems and disaster recovery processes
Review all systems and make technical/process recommendations for improving efficiency
Assist in defining standards for all systems and processes provided by the Data Center Services group
Administer and maintain a reliable, stable, secure and cost-effective hosting environment to deliver 24x7x365 operational service
Provide escalated remote technical support to Marco Data Center clients. Handle requests with due urgency, empathy, and a spirit of cooperation.
Document your work, including troubleshooting steps, time, and resolution in the service ticketing system.
Ensure your work and the work of others adheres to Marco internal policies and applicable regulatory requirements.
Create and maintain standard templates, documentation, images, policy, and automation that will aid in a consistent and rapid deployment of Marco Data Centers
Research, test and recommend new tools and products to enhance the offering and efficiency of Marco’s Data Centers
Document the procedures, infrastructure and environments and keep updated with changes to the environment
Implement and maintain Data Center system builds that are consistent, optimized for client experience, overall performance and supportability
Set deadlines for projects, then organize your time to meet these deadlines; be willing to put in extra time and effort to meet a time-bound goal when necessary
Mentor other staff while you assist with implementation or utilization within your area of technical specialty
Work multiple projects simultaneously and provide consistent, high-quality results
Assist with implementation and or migration of clients’ existing data and applications
Perform system and application testing to provide functional environment
Provide onsite and or remote technical support to Marco clients during onboarding process
Implement changes and work orders for existing Marco clients
Review customer hosted systems and make technical/process recommendations for improving efficiency
Stay up to date with relevant state-of-the-art technology, equipment, and/or systems
QUALIFICATIONS:
Bachelor’s Degree and 4+ years of experience or equivalent experience
Experience and current certifications or specializations aligning with the technologies used in Marco’s Data Center to include Cisco NX-OS, SAN-OS, IOS, UCS-B, ASAv, EMC VNX, Meraki MX, VMWare vSphere, Citrix XenApp, Zscaler and XenDesktop preferred.
Current high level industry recognized certifications including one or more of the following: CCNP, CCE, VCDX, EMCTA
Solid understanding of the administration of network operating systems and services found in a Data Center environment
Benefits:
We’re not just competitive when it comes to business tech – we’re also pretty proud of what we offer our employees. Our benefits include medical, dental, and vision insurance. We also have paid holidays and vacation, 401k with generous company match, flexible spending accounts, employee purchase program, employer-paid life insurance, voluntary-term life insurance, short and long-term disability, critical illness and accident benefits, and pet insurance. Yes, we care about your furry family too.

all benefits are dependent on employment status
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities
Applicant Labor Law Posters",4.0,"Marco Technologies LLC
4.0",Remote,1001 to 5000 Employees,1973,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
974,Cloud Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"We are seeking a highly experienced Principal Data Engineer with a deep understanding of PySpark using Databricks or AWS Glue or AWS EMR and cloud-based databases such as Snowflake. Proficiency in workflow management tools like Airflow is essential. Healthcare industry experience is a significant advantage. The ideal candidate will be responsible for designing, implementing, and maintaining data pipelines while ensuring the highest performance, security, and data quality.
Responsibilities:
Design, develop, and maintain scalable, reliable, and secure data pipelines to process large volumes of structured and unstructured healthcare data using PySpark and cloud-based databases.
Collaborate with data architects, data scientists, and analysts to understand data requirements and implement solutions that meet business and technical objectives.
Leverage AWS or Azure cloud services for data storage, processing, and analytics, optimizing cost and performance.
Utilize tools like Airflow for workflow management and Kubernetes for container orchestration to ensure seamless deployment, scaling, and management of data processing applications.
Develop and implement data ingestion, transformation, and validation processes to ensure data quality, consistency, and reliability across various healthcare datasets.
Monitor and troubleshoot data pipelines, proactively identifying and resolving issues to minimize downtime and ensure optimal performance.
Establish and enforce data engineering best practices, ensuring compliance with data privacy and security regulations specific to the healthcare industry.
Continuously evaluate and adopt new tools, technologies, and frameworks to improve the data infrastructure and drive innovation.
Mentor and guide junior data engineers, fostering a culture of collaboration, learning, and growth within the team.
Collaborate with cross-functional teams to align data engineering efforts with broader organizational goals and strategies.
Is familiar with SOC 2 compliance and its impact on company policies and processes.
Understands the importance of adhering to SOC 2 requirements and maintains an effort to do so.
Reviews and understands the Employee Handbook, and internal policies that define individual security responsibilities, and maintains segregation of duties in accordance with their role requirements.
Requirements:
Bachelor's or Master’s degree in Computer Science, Engineering, or a related field.
8+ years of experience in data engineering, with a strong background in Apache Spark and cloud-based databases such as Snowflake.
Strong Knowledge in Big Data Technologies, Databricks, AWS Services, and PySpark, thorough in one or more programming languages like Python.
Proven experience with AWS or Azure cloud services for data storage, processing, and analytics.
Expertise in workflow management tools like Airflow and container orchestration systems such as Kubernetes.
Strong knowledge of SQL and NoSQL databases, as well as data modeling and schema design principles.
Familiarity with healthcare data standards, terminologies, and regulations, such as HIPAA and GDPR, is highly desirable.
Excellent problem-solving, communication, and collaboration skills, with the ability to work effectively in cross-functional teams.
Demonstrated ability to manage multiple projects, prioritize tasks, and meet deadlines in a fast-paced environment.
A strong desire to learn, adapt, and contribute to a rapidly evolving data landscape.
Benefits:
401(k).
Dental Insurance.
Health insurance.
Vision insurance.
We are an equal-opportunity employer and value diversity, equality, inclusion, and respect for people.
The salary will be determined based on several factors including, but not limited to, location, relevant education, qualifications, experience, technical skills, and business needs.
Additional Responsibilities:
Participate in OrangePeople monthly team meetings, and participate in team-building efforts.
Contribute to OrangePeople technical discussions, peer reviews, etc.
Contribute content and collaborate via the OP-Wiki/Knowledge Base.
Provide status reports to OP Account Management as requested.
About us:
OrangePeople is an Enterprise Architecture and Project Management solutions company. Our most valuable asset is our people: dynamic, creative thinkers, who are passionate about doing quality work. As a member of the OrangePeople team, you will have access to industry-leading consulting practices, strategies & technologies, innovative training & education. An ideal Orange Person is a technology leader with a proven track record of technical achievements and a strong process/methodology orientation.
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Application Question(s):
Do you require sponsorship for this job?
Work Location: Remote",4.1,"OrangePeople
4.1",Remote,51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
975,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
976,AI/ML Data Automation Engineer,$88K - $125K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
977,Data Engineer - Mid Level,-1,"Why USAA?
At USAA, we have an important mission: facilitating the financial security of millions of U.S. military members and their families. Not all of our employees served in our nation’s military, but we all share in the mission to give back to those who did. We’re working as one to build a great experience and make a real impact for our members.

We believe in our core values of honesty, integrity, loyalty and service. They’re what guides everything we do – from how we treat our members to how we treat each other. Come be a part of what makes us so special!
The Opportunity
We are seeking a dedicated Data Engineer – Mid Level for our Plano, TX Office. The candidate selected for this position is going to work with the Enterprise Data Engineering Services Certified Metrics team. They will work with data engineering technologies and support the team responsible for developing and maintaining USAA’s digital reporting and analytical environment in Snowflake. The resource should be experienced in building analytical applications in the cloud. SQL skills are a must and knowledge of Snowflake, dbt, and other cloud-based data movement tools would be highly beneficial.
This position is a hybrid work type and will be based in Plano, TX. Hybrid roles help employees gain the best of both worlds – collaborating in-person in the office and working from home when needed to achieve focused results.
What you'll do:
Independently conducts work on the full life cycle of data engineering to include analysis, solution design, data pipeline engineering, testing, deployment, scheduling, and production support.
Designs and implements complex technical solutions for data engineering and analytic systems.
Identifies and solves significant technical problems and architecture deficiencies to include design, security, and performance.
Collaborates on design reviews by providing feedback on trends and makes recommendations for solutions.
Breaks down business features and into technical stories and approaches. Influences stakeholders in technical adoption for best solutions.
Creates proof of concepts and prototypes that drive the vision and the outcome for complex initiatives to be delivered.
Collaborates with the team and other engineers on new technologies and alternatives to plan and execute complex assignments and tasks.
Helps on-board entry level engineers. May begin mentoring junior engineers.
Acquires data from multiple data sources and maintains resulting databases, data warehouses, and/or data lakes.
Assists to develop, maintain, and enforce the company’s data development tools and standards. Conducts code reviews on a regular basis to improve quality and ensure compliance.
Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled in accordance with risk and compliance policies and procedures.
What you have:
Bachelor’s degree; OR 4 years of related experience (in addition to the minimum years of experience required) may be substituted in lieu of degree.
4 years of data engineering, data analysis or software development experience implementing data solutions with at least 1 year of data engineering or data management experience.
Extensive knowledge and working experience in SQL and Relational Databases.
Strong analytical and problem-solving skills.
Basic understanding of cloud technologies and tools.
What sets you apart:
Experience with Snowflake
Experience with AWS
Visualization experience with Tableau or similar tool
Experience with DBT
Experience with Python
Experience providing technical leadership, guidance, or mentorship to other engineers
The above description reflects the details considered necessary to describe the principal functions of the job and should not be construed as a detailed description of all the work requirements that may be performed in the job.
What we offer:
Compensation: USAA has an effective process for assessing market data and establishing ranges to ensure we remain competitive. You are paid within the salary range based on your experience and market data of the position. The actual salary for this role may vary by location. The salary range for this position is: $86,520.00 – $165,340.00.
Employees may be eligible for pay incentives based on overall corporate and individual performance and at the discretion of the USAA Board of Directors.
Benefits: At USAA our employees enjoy best-in-class benefits to support their physical, financial, and emotional wellness. These benefits include comprehensive medical, dental and vision plans, 401(k), pension, life insurance, parental benefits, adoption assistance, paid time off program with paid holidays plus 16 paid volunteer hours, and various wellness programs. Additionally, our career path planning and continuing education assists employees with their professional goals.
For more details on our outstanding benefits, please visit our benefits page on USAAjobs.com.
Relocation assistance is not available for this position.
USAA is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",3.4,"USAA
3.4","Plano, TX",10000+ Employees,1922,Company - Private,Insurance Carriers,Insurance,$25 to $100 million (USD)
978,Data Engineer Azure databricks,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer - Databricks
For: CDW (Direct Client)
W2, Corp to Corp, W2 Contract Okay
Fully Remote
`
DESIRED SKILLS AND ABILITIES
Key Responsibilities
You will design, build and test cross cloud and on-premise data pipelines and leverage Azure Databricks for data processing.
Collaborate with cloud architects, tech leads to facilitate on-premise to Databricks migration
You should understand data pipelines and modern ways of automating data using cloud-based and on-premise technologies
Develop reference architectures and design pattern library for typical Cloud based solutions implementations
Advise on Cloud project set up, security and role based access implementation, and network optimizations
Qualifications:
2+ years with SQL and database table design – able to write structured and efficient queries on large data sets
1-2 years on Databricks on any cloud platform (AWS, Azure)
1-2 years working on AWS/Azure cloud
1-2 year’s design and/or implementation of enterprise applications
Experience in “migrating” on premise workloads to one or more industry leading public cloud(s)
Experience in AGILE development, SCRUM and Application Lifecycle Management (ALM) with scripting experience in Python and shell.
Hands-on experience in the full life-cycle of software development or methodology using Agile Scrum/Kanban etc.
Able to work with agile teams as they perform feature level design, development, testing, and performance analysis
Databricks certification is a plus.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Schedule:
8 hour shift
Work Location: Remote",-1.0,Data Ninjas Inc.,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
979,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
980,Data QA Engineer,-1,"Who We Are:
The Data Sherpas are a team of highly skilled and motivated engineers that help our clients at every phase of their cloud journey. If it touches the cloud, involves data, or lives as an application, we have either worked on it or have the skills and expertise to accomplish it.
What We Are Looking For:
We are seeking a dedicated and technically skilled Data QA Engineer. The Engineer will work closely with development leads and data engineers to define analysis strategies and create test plans that ensure the highest quality of code delivery. The candidate will build, manage, and maintain a robust testing environment.
What You'll Do:
Collaborate with development leads and data engineers to define analysis strategy and test plans to ensure quality delivery.
Define, build, automate, and execute testing for code in an agile practice in accordance with sprint planning.
Build, manage, and maintain unit, integration, and regression tests using DBT, Vertex, and Colab.
Troubleshoot issues found during the development and testing phases, proposing and implementing efficient solutions.
Design, monitor, and maintain QA reports, KPIs, and quality trends to evaluate the effectiveness of QA processes.
Communicate accurately the status and risks for ongoing work and timelines to stakeholders.
What You Have:
Expert-level proficiency in SQL and Python.
Hands-on experience with DBT, Vertex, and Colab.
Strong experience with agile development methodologies.
Knowledge of software QA methodologies, tools, and processes.
Familiarity with CI/CD tools and methods.
Bachelor's degree in Computer Science, Engineering, related field, or equivalent experience.

This is a contract to the end of the year with an expectation to extend into 2024
We cannot work with third-party agencies at this time. Resumes submitted via unapproved agencies will be automatically rejected.",-1.0,The Data Sherpas,Remote,-1,-1,-1,-1,-1,-1
981,GCP Data Engineer,$84K - $113K (Glassdoor est.),"Infosys is seeking an GCP Data Engineer with experience working in a Big Data ecosystem. The position will primarily be responsible interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle including Requirements Elicitation and Design. You will play an important role in creating the high-level design artifacts. You will also deliver high quality code deliverables for a module, lead validation for all types of testing and support activities related to implementation, transition and warranty. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued.

Required Qualifications:
Candidate must be located within commuting distance of Richardson, Tx or be willing to relocate to the area. This position may require travel in the US and Canada.
Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education.
US citizens and those authorized to work in the Canada are encouraged to apply. We are unable to sponsor at this time.
At least 4 years of experience with Information Technology
Minimum 2 years of hands on-experience working with technologies like – GCP with data engineering – data flow / air flow, pub sub/ kafta, data proc/Hadoop, Big Query.
At least 3 years of ETL development experience with strong SQL background
Consultant should have hands-on experience of building and operationalizing data processing systems
Prior experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka is required.
Candidate must have experience in working with data platforms (Data warehouse, Data Lake, ODS)
Minimum of 2 years of experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT, Control-M)

Preferred Qualifications:
GCP (google cloud platform) experience.
Data analysis / Data mapping skills
Experience in data manipulation JSON and XM.
CI / CD exposure
Ability to work in team in diverse/ multiple stakeholder environment
Ability to communicate complex technology solutions to diverse teams namely, technical, business and management teams

The job may entail extensive travel. The job may also entail sitting as well as working at a computer for extended periods of time. Candidates should be able to effectively communicate by telephone, email, and face to face.

About Us
Infosys is a global leader in next-generation digital services and consulting. We enable clients in more than 50 countries to navigate their digital transformation. With over four decades of experience in managing the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver unprecedented levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.

Infosys is an equal opportunity employer and all qualified applicants will receive consideration without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, spouse of protected veteran, or disability.",3.3,"Infosys
3.3","Richardson, TX",10000+ Employees,1981,Company - Public,Information Technology Support Services,Information Technology,$10+ billion (USD)
982,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
983,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
984,Lead Azure Data Engineer,Employer Provided Salary:$135K - $140K,"Lead Data Engineer.
Location : New York, NY ( Hybrid)
Full Time
10+ years of experience in data engineering
5+ years of experience managing a team of data engineers
Strong understanding of SQL Server SSIS
Excellent communication and interpersonal skills
Ability to work independently and as part of a team
Ability to meet deadlines and work under pressure
Strong desire to be hands-on with the development and implementation of data pipelines and data warehouses
Experience working with stakeholders to develop product backlog grooming, sprint planning data engineering and
QA Testing
Migration of SQL Server to Azure is desired
Experience with Azure Data Factory, ADLS, and CI/CD highly desired
The Hands-On Data Engineering Lead is responsible for leading a team of data engineers in the design, development, and
implementation of data pipelines and data warehouses. The ideal candidate will have a strong understanding of data
engineering principles and practices, as well as experience with SQL Server SSIS. The Hands-On Data Engineering Lead will
also be responsible for managing the team's workload, mentoring junior engineers, and communicating with stakeholders.
Additionally, the Hands-On Data Engineering Lead will be expected to be hands-on with the development and
implementation of data pipelines and data warehouses, and to have experience working with stakeholders to develop
product backlog grooming, sprint planning data engineering and QA Testing.
Responsibilities:
Lead the design, development, and implementation of data pipelines and data warehouses
Develop and implement data pipelines and data warehouses using SQL Server SSIS
Troubleshoot data pipelines and data warehouses
Communicate with stakeholders to understand their needs and ensure that projects meet their requirements
Manage the team's workload and ensure that projects are completed on time and within budget
Mentor junior data engineers and help them develop their skills
Stay up-to-date on the latest data engineering technologies and best practices
Provide support to production teams and QA teams
Work with stakeholders to develop product backlog grooming, sprint planning data engineering and QA Testing
Working with other engineers to design and implement data pipelines and data warehouses using Azure Data
Factory, ADLS, and CI/CD
Experience in migrating SQL server to Azure
30% team management and 70% hands-on
Job Type: Full-time
Salary: $135,000.00 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 10 years (Required)
SQL: 10 years (Required)
Data warehouse: 10 years (Required)
SSIS: 10 years (Required)
Work Location: In person",-1.0,Rivago Infotech Inc,"New York, NY",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
985,Cloud Data Engineer,Employer Provided Salary:$55.00 - $65.00 Per Hour,"Note: This is a W2 - contract to hire role
Azure Data Engineer
12+ months (contract to hire)
100% remote

Job Description:
As a Senior Engineer - DBA, the Cloud Data Engineer candidate will be responsible for using the candidate’s technical knowledge to solve business problems.
The client is looking for a talented individual who can serve as a subject matter expert in their area of focus and represent their department on complex assignments.
Candidates will be responsible for evaluating elements of technology effectiveness through requirements gathering, testing, research, and investigation and making recommendations for improvements that result in increased quality and effectiveness.
Candidates will be required to listen to and evaluate customer needs to determine and provide high-quality solutions that align with customer expectations.
In this role on the Cloud Data Engineering team, the candidate will be a part of a team responsible for migrating existing data platform solutions to the cloud.
Candidates will also collaborate with others to develop new data pipelines.
As a data engineer in the Cloud Data Engineering team, the candidate must work in a data-first mindset.
The candidate will use Microsoft Azure Cloud PaaS technologies to engineer client data solutions in a way that allows us to optimize client information, make better decisions, and meet client customer’s needs.
If the Candidate has a passion for engineering, working with massive amounts of data, and empowering smart business decisions, this is the role for you.
Equal Opportunity Employer/Disability/Veterans

Required Qualifications:
Must be presently authorized to work in the U.S. without a requirement for work authorization sponsorship by the company for this position now or
in the future
Must be committed to incorporating security into all decisions and daily job responsibilities
3 plus of related experience in Cosmos DB/Similar DB technology
Experience with configuration management and building automation capabilities such as Git/Jenkins/Bitbucket
Experience with Microsoft Azure Cloud workflows
Experience in T-SQL and scripting skills.
Knowledge of Microsoft cloud-managed DBs/Systems, e.g. Managed SQL Instance, Cosmos DB, Databricks Delta Lake
Experience with Big Data solutions such as Delta Lake by Databricks and SQL DBMSs
Independently analyze, solve, and correct issues in real-time, providing problem resolution end-to-end
Identify new opportunities and help refine automation of regular processes, track issues, and document changes
Solve/Assist in complex query tuning and schema refinement
Expert in troubleshooting performance issues
Experience rightsizing Database object workflow for cost management
Ability to multi-task and context-switch effectively between different activities and teams
Join the on-call rotation with other Engineers
Must be able to both collaborate in a team-oriented environment and work independently with direction
Must be able to work in a fast-paced environment with the ability to handle multiple tasks

Pay Range: $55/hr - $65/hr
The specific compensation for this position will be determined by a number of factors, including the scope, complexity and location of the role as well as the cost of labor in the market; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. Our full-time consultants have access to benefits including medical, dental, vision and 401K contributions as well as any other PTO, sick leave, and other benefits mandated by appliable state or localities where you reside or work.",3.7,"Pinnacle Technical Resources
3.7",Missouri,1 to 50 Employees,-1,Company - Public,-1,-1,$1 to $5 million (USD)
986,Data Engineer (multiple openings) - IHM,Employer Provided Salary:$88K - $157K,"Discover. A brighter future.
With us, you’ll do meaningful work from Day 1. Our collaborative culture is built on three core behaviors: We Play to Win, We Get Better Every Day & We Succeed Together. And we mean it — we want you to grow and make a difference at one of the world's leading digital banking and payments companies. We value what makes you unique so that you have an opportunity to shine.

Come build your future, while being the reason millions of people find a brighter financial future with Discover.

Job Description:
Employer: DFS Corporate Services LLC

Job Title: Data Engineer (multiple openings)

Job Location: Riverwoods, Illinois

Job Type: Full Time

Duties: Responsible for designing, developing, testing, and maintaining complex data solutions for the product. Mentor and influence peers to achieve commitments on data solutions on time and with quality. Telecommuting and/or working from home may be permissible pursuant to company policies.

Requirements: Employer will accept a Bachelor's degree in Computer Science, Computer Engineering, or a related field and 3 years of experience in Senior Data Engineer; ODI/DW/Batch Developer or related occupation.

Position required skills: Three (3) years of experience in the job offered or related occupation: working with Agile software development methodology and Agile management tools, including Jira; utilizing modern engineering practices, including the design and development of ETL data integration solutions in data warehouse environments; working with relational databases and Cloud-based technologies; utilizing design and architecture experience on large-scale ETL solutions; and working with Enterprise Data warehouse data models and dimensional modeling concepts, source to target mapping, and data integration architecture.

Position eligible for incentives under Employee Referral Program.

Rate of Pay: The base pay for this position generally ranges between $88,150.00 to $157,000.00. Additional incentives may be provided as part of a market competitive total compensation package. Factors, such as but not limited to, geographical location, relevant experience, education, and skill level may impact the pay for this position. We also offer a range of benefits and programs based on eligibility. Learn more at MyDiscoverBenefits.com .

QUALIFIED APPLICANTS: Please apply directly through our website by clicking on “Apply Now.” No calls. Equal Opportunity Employer/disability/vet.

What are you waiting for? Apply today!

All Discover employees place our customers at the very center of our work. To deliver on our promises to our customers, each of us contribute every day to a culture that values compliance and risk management.

Discover is committed to a diverse and inclusive workplace. Discover is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or other legally protected status. (Know Your Rights)",3.9,"Discover Financial Services
3.9","Riverwoods, IL",10000+ Employees,1985,Company - Public,Banking & Lending,Financial Services,$1 to $5 billion (USD)
987,Data Engineer,Employer Provided Salary:$100K - $160K,"Position Summary
The Effectual Data Engineer builds pipelines that are used to transport date from a data source to a data warehouse. These pipelines are crucial: they are what allow us to access and analyze an organizations data and use the insights to help them make decisions. Data pipelines transport and transform data according to established business rules or a line of exploratory analysis the business wants to undertake. As a Data Engineer, you will prepare and organize the data that organizations have built in their databases and other formats.
A Glimpse into the Daily Routine of a Data Engineer
As a Data Engineer, specializing in Confluent Kafka, you will take on big data challenges in an agile way. You will build data pipelines, utilizing Confluent Kafka, that enables our clients and their vision. You will also build data models to deliver insightful analytics while ensuring the highest standard in data integrity. You are encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.
Essential Duties and Responsibilities
Design, build and launch extremely efficient and reliable data pipelines, utilizing Confluent Kafka, to move data across several platforms including Data Lakes, Data Warehouses, and real-time systems.
Develop, construct, test and maintain data architectures from the data architect.
Analyze organic and raw data.
Build data systems and pipelines.
Build the infrastructure required for extraction, transformation, and loading of data from different data sources using SQL and AWS 'big data' technologies.
Write scripts for data architects, data scientists, and data quality engineers.
Data acquisition.
Identify ways to improve data reliability, efficiency, and quality.
Develop dataset processes.
Prepare data for prescriptive and predictive modeling.
Automate the data collection and analysis processes, data releasing and reporting tools.
Build algorithms and prototypes.
Develop analytical tools and programs.
Qualifications
MUST HAVE hands-on experience with Confluent Kafka including both administration and development.
Either Confluent Certified Administrator for Apache Kafka (CCAAK) or Confluent Certified Developer (CCDAK) for Apache Kafka certificates, however, both are preferred.
Bachelor's or master's degree in computer science, Engineering or a related field.
Experience working as a Data Engineer in a professional services or consulting environment.
Proficiency in programming languages such as Python, Java, or Scala, with expertise in data processing frameworks and libraries (e.g., Spark, Hadoop, SQL).
In-depth knowledge of database systems (relational and NoSQL), data modeling, and data warehousing concepts.
Strong knowledge of data architectures and data modeling and data infrastructure ecosystem.
Experience with cloud-based data platforms and services (e.g., AWS, Azure) including familiarity with relevant tools (e.g., S3, Redshift, BigQuery, etc.).
Proficiency in designing and implementing ETL processes and data integration workflows using tools like Apache Airflow, Informatica, or Talend.
Familiarity with data governance practices, data quality frameworks, and data security principles
Work minimal direction and turn a clients want and need into working stories, epics which can be performed upon during a sprint.
A firm understanding of the SDLC process.
An understanding of object-oriented programming.
The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex.
The ability to thrive in a dynamic environment. That means being flexible and willing to jump in and do whatever it takes to be successful.
Ability to travel, with strong preference to mid-west time zone or east coast.
Nice-to-Have Skills and Experience
Knowledge of batch and streaming data architectures.
Product mindset to understand business needs and come up with scalable engineering solutions.
AWS Certified Cloud Practitioner
AWS Certified Data Analytics Specialty
AWS Certified Machine Learning Specialty
AWS Certified Database Specialty
SnowPro Core Certification
Databricks Certified Data Engineer Associate
Company Offered Benefits
Full-time employees are eligible to participate in our employee benefit programs:
Medical, dental, and vision health insurances,
Short term disability, long term disability and life insurances,
401k with Company match
Paid time off (PTO) (120 hours PTO that accrue over one year)
Paid time off for major holidays (14 days per year)
These and any other employee benefit offerings are subject to management's discretion and may change at any time.
Physical Demands and Work Environment
The work is generally performed in an office environment. Physical demands include sitting, keyboarding, verbal communication, written communication. Employees are occasionally required to stand; walk; reach with hands and arms; climb or balance; and stoop, kneel, crouch, or crawl. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this position. Reasonable accommodation may be made to enable individuals with disabilities to perform the functions.
Salary Range for this position: $100,000-$160,000
""Salary ranges provided are for informational purposes only and may vary depending on factors such as experience, qualifications, and geographic location. The final salary offer will be determined based on the candidate's skills and alignment with the role requirements.""
This job description may not be inclusive of all assigned duties, responsibilities, or aspects of the job described, and may be amended anytime at the sole discretion of the Employer. Duties and responsibilities are subject to possible modification to reasonably accommodate individuals with disabilities. To perform this job successfully, the incumbents will possess the skills, aptitudes, and abilities to perform each duty proficiently. This document does not create an employment contract, implied or otherwise, other than an ""at will"" relationship. Effectual Inc. is an EEO employer and does not discriminate on the basis of any protected classification in its hiring, promoting, or any other job-related opportunity.",4.0,"Effectual
4.0",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
988,"Staff Engineer, Big Data",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

By this point in your career, it is not just about the tech you know or how well you can code. It is about what more you want to do with that knowledge. Were you given the tools to go beyond solving for X? Can you help your teammates proceed in the right direction? Can you tackle the challenges our clients face while always looking to take our solutions one step further to succeed at an even higher level? Yes? You may be ready to join us.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
989,Senior Data Engineer,$115K - $151K (Glassdoor est.),"Senior Data Engineer
Location: Various
Raleigh NC; Atlanta GA; Philadelphia PA; Charlottesville VA; others considered;
Hybrid, Remote, Travel required
Elder Research, Inc. (ERI) is a Data Science consulting firm specialized in providing analytic solutions to clients in Commercial and Government industries. The Commercial business unit is seeking to hire a Senior Data Engineer with strong engineering skills who will provide technical support across multiple project teams by leading, designing, and implementing the software and data architectures necessary to deliver analytics to our clients, as well as providing consulting and training support to client teams in the areas of architecture, data engineering, ML engineering and/or related areas. We are trusted advisors to our clients, building lasting relationships and partnering as preferred analytics providers. We use a variety of programming languages and tools to create analytic solutions, often fitting within our clients’ environment and needs.
We are looking to hire a Senior Data Engineer who can support robust and repeatable data manipulation, large-scale infrastructure for data ingestion, and data visualization for custom client applications. A Senior Data Engineer works collaboratively with Business Analysts, Data Scientists, Data Analysts, other Software/Data Engineers, and business stakeholders to create and deploy dynamic data applications that help our customers make meaningful business decisions.
Some of your responsibilities will include leading and implementing software engineering best practices, mentoring junior engineers, designing software architectures, wrangling and fusing large and disparate data sets, assisting in the deployment of models and algorithms, automating data pipelines in their entirety, and communicating model results through user-focused data visualizations. Candidates should have the ability and the willingness to tailor applications to a clients’ business goals using an iterative methodology. Candidates should also be able to consider both long-term stability and scalability while taking a user-focused approach to development and deployment. Excellent decision making skills are a must.
Desired Skills
Candidates should have a mix of the following experiences:
Set or contributed to data engineering and governance standards
Reviewed code/design/architecture and coached on adjustments (as needed)
Mentored junior engineers
Designed and implemented software architectures
Implemented a data pipeline that includes data transformations such as aggregations, joins, and cleaning.
Ingested data into SQL or NoSQL databases.
Interacted with SQL or NoSQL databases via Python, Java, or similar.
Provided access to transformed data for downstream applications or visualizations via REST APIs or similar.
Worked with visualization software such as Power BI, Tableau, or similar.
Skilled in both applying and teaching others to apply Git (or other distributed version control) for collaboration, record-keeping, and deployment scenarios
Skilled in both using and guiding others in using cloud platforms (AWS, Azure, Google Cloud) for building, deploying, evolving, and maintaining data pipelines
Skilled in both applying and guiding others in applying continuous integration and continuous deployment (CI/CD) techniques for surfacing situations where new work breaks existing functionality
Had exposure to modeling and/or data analysis.
Configured a technical service such as a database, version control system, or operating system.
Led and collaborated with others to accomplish a technical task.
Partnered on the development or deployment of a data strategy
Presented technical work to a non-technical audience via writing or presentation.
Desired Education and Experience
Education:
Master’s in a technical field and 4-5 years’ experience OR
Bachelor’s and 6+ years’ experience
Certifications:
AWS, Azure, or Google Cloud
Minimum Requirements
Bachelors/Master’s degree in Computer Science or related field requiring software development
4+ years’ of professional experience with Python, Java, or similar
Excellent written and verbal communication skills
Ability to parse project requirements, including asking questions to ensure full and correct understanding
Willingness and ability to learn new concepts and tools quickly
Willingness and ability to provide technical support across multiple projects
What You Would Do
Work on small teams in a highly collaborative environment.
Participate in both existing and new projects.
Contribute to consulting projects to solve interesting problems for multiple clients in various industries.
Communicate details of the technical architecture to fellow team members and clients in both technical and non-technical terms.
Work with Data Scientists to design and/or implement the technical architecture necessary to support analytics.
Work with Software/Data Engineers to improve the robustness and scalability of software products.
Participate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance.
Work on documentation of the technical architecture for internal and client use.
Design, create, and provision data stores.
Build trusted and lasting relationships with clients.
Provide value to our clients through analytics, software tools, and leadership.
Manage and monitor pipelines to extract, transform, and load data (ETL).
About Elder Research, Inc.
Headquartered in Charlottesville, VA with offices in Arlington, VA, Baltimore, MD, and Raleigh, NC, Elder Research, Inc. (ERI) is a fast-growing solutions and consulting firm specializing in predictive analytics. At ERI, you’ll be part of a fun, friendly community. In keeping with our entrepreneurial spirit, we want candidates that are self-motivated with an innate curiosity and strong team work ethic. We work hard to provide the best value to our clients and allow each person to contribute their ideas and put their skills to use immediately.
ERI believes in continuous learning; each week the entire company attends a “Tech Talk” and an office lunch. Individuals often share what they have learned at informal bag lunch talks or afternoon chats. ERI provides a supportive work environment with established parental, bereavement, and PTO policies. By prioritizing a healthy work-life balance - with reasonable hours, solid pay, low travel, and flexible time off - ERI enables and encourages its employees to serve others.
ERI provides analytic solutions to hundreds of companies across numerous industries. Our team enjoys great variety in the type of work we do and exposure to a wide range of techniques and tools. If you are passionate about integrating data, technology, and analytics in a team-based environment to solve problems, then ERI may be a good fit for you.",4.5,"Elder Research Inc
4.5","Charlottesville, VA",51 to 200 Employees,1995,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
