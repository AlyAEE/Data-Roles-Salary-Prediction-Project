,Job Title,Salary Estimate,Job Description,Rating,Company Name,Location,Size,Founded,Type of ownership,Industry,Sector,Revenue
0,Senior Data Engineer (Remote),Employer Provided Salary:$50.00 - $70.00 Per Hour,"Company Overview: IT Giants INC is a dynamic and innovative technology company at the forefront of delivering advanced data solutions across various industries. We are seeking an experienced Data Engineer with expertise in cloud platforms, programming languages, database technologies, and analytics tools. If you also have a strong background in the airline domain, this is an opportunity to join our dedicated team and shape the future of data-driven decision-making in the aviation sector.
Position Overview: As a Data Engineer at IT Giants INC, you will play a pivotal role in designing, building, and maintaining sophisticated data pipelines and analytical solutions for our airline industry clients. Leveraging your deep understanding of both data engineering and the airline domain, you will empower our clients to harness the power of their data to improve operations and customer experiences.
Key Responsibilities:
Data Pipeline Development: Architect, develop, and maintain end-to-end data pipelines, integrating data from diverse sources, including cloud platforms (GCP, Azure, AWS), APIs, and various databases (Teradata, Oracle, PostgreSQL, NoSQL).
Data Transformation and Modeling: Apply your expertise in SQL, Python, and Alteryx to transform raw data into meaningful insights, while ensuring data integrity and accuracy.
Cloud Expertise: Utilize your proficiency in GCP, Azure, and AWS to design and implement scalable and cost-efficient cloud-based solutions that meet the unique demands of the airline industry.
Database Management: Manage relational and NoSQL databases to store and retrieve data efficiently, ensuring optimal performance and security.
Analytics and Reporting: Collaborate with data scientists and analysts to develop interactive dashboards and reports using tools like Tableau, QuickSight, and CloudWatch, enabling stakeholders to make informed decisions.
Data Quality and Governance: Implement data quality checks, validation processes, and adherence to industry regulations to ensure data accuracy and compliance.
Version Control and Collaboration: Utilize GitLab and other version control tools to collaborate with cross-functional teams on code management and deployment.
Real-time Monitoring: Develop real-time data processing solutions using technologies like CloudWatch to enable timely insights into operational metrics.
Linux Environment: Leverage your proficiency in Linux to optimize system performance and troubleshoot issues as they arise.
Domain Knowledge: Apply your deep understanding of the airline industry to structure data effectively and drive actionable insights from the data.
Qualifications:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
years of proven experience in data engineering, with a focus on cloud platforms (GCP, Azure, AWS), Python, SQL, and relevant database technologies.
Experience with Alteryx, Tableau, and data visualization tools for creating impactful dashboards.
Proficiency in managing databases such as Teradata, Oracle, PostgreSQL, and NoSQL databases.
Strong command of Linux environments and version control using GitLab.
Familiarity with CloudWatch, QuickSight, and real-time data processing is advantageous.
Domain expertise in the airline industry, including a solid understanding of aviation data, standards, and operational challenges.
Excellent problem-solving skills and the ability to troubleshoot complex data issues.
Effective communication skills to collaborate with technical and non-technical teams.
Proven ability to work independently and in a team, delivering high-quality solutions.
Join Our Team: If you are a driven and knowledgeable Data Engineer with a passion for both technology and the airline industry, IT Giants INC offers an exciting opportunity to contribute to transformative data solutions. Join us in our mission to revolutionize data analytics within the aviation sector and drive meaningful impact.
Job Types: Full-time, Contract
Pay: $50.00 - $70.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Work Location: Remote",5.0,"IT Giants
5.0",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
1,"Data Engineer (AWS, Python)",Employer Provided Salary:$50.00 - $52.00 Per Hour,"Title: Data Engineer (AWS, Python)
Location: Remote
Duration: 12+ Months
Job Description:
Typical task breakdown:-
Daily standup, collaborate with peers, work independently to complete sprint commitments, participate in agile ceremonies.
Interaction with team:-
Daily stand up meetings with team – working mainly with Helios Platform adoption team.
Work environment:
Education & Experience Required:
Bachelor's in computer science required plus 7+ Working experience .
Masters degree with 6+ years working expeirence acceptable as well.
Preferred: AWS cloud certifications
Technical Skills
(Required)
Exp with Object oriented programming - Python (4+ years)
AWS (3+ years overall exp) lambda and/or glue, EMR, S3
SQL (3+ years)
2+ years exp in test driven development, unit test, integration testing, regression testing
(Desired)
Event driven framework experience.
Kinesis or Kafka experience
Familiar with working in Agile methodology
Soft Skills
(Required)
Strong written and verbal communication, experience working as a member of agile engineering team, ability to commit to and deliver work within allocated sprint window.
Job Type: Contract
Pay: $50.00 - $52.00 per hour
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",5.0,"DSMH LLC
5.0",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
2,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
3,Data Engineer,Employer Provided Salary:$45.00 Per Hour,"Job Title: GCP Data Engineer
Duration: 12+ Months
Location: Hartford, CT (Onsite Role)
Tax term : W2 & 1099
Job Description:
As a GCP Data Engineer, you will play a pivotal role in designing, implementing, and optimizing data solutions on the Google Cloud Platform (GCP). Your expertise will be essential in leveraging various technologies to build robust data pipelines and contribute to the overall success of data-driven initiatives.
Roles and Responsibilities:
Design, develop, and maintain efficient data pipelines using Python or PySpark on the Google Cloud Platform.
Seamlessly integrate data solutions using GCP services like Google Cloud Storage, BigQuery, Dataflow, and Pub/Sub.
Utilize expertise in the Hadoop ecosystem, including HDFS and Hive, to craft seamless data workflows.
Transform raw data into insights by creating complex SQL queries for data transformation and analysis.
Implement data quality checks and validation procedures to ensure accuracy and completeness.
Optimize data pipelines and processes for maximum performance and efficiency.
Collaborate with cross-functional teams to understand data requirements and deliver tailored solutions.
Create comprehensive documentation for pipelines, workflows, and processes.
Stay updated with the latest data engineering and cloud technology trends.
Tackle complex challenges with a proactive problem-solving mindset.
Required Qualifications:
Expertise in Python/PySpark and hands-on experience with Google Cloud Platform (GCP) for data engineering tasks.
Strong understanding of the Hadoop ecosystem, including HDFS and Hive.
Proficiency in SQL for effective data manipulation and analysis.
Experience working with cloud platforms, preferably Google Cloud Platform (GCP).
Strong problem-solving skills and analytical thinking.
Effective communication and collaboration abilities.
Adaptability to evolving data engineering technologies and practices.
Job Type: Contract
Salary: From $45.00 per hour
Experience level:
9 years
Application Question(s):
What is your work authorization?
Experience:
Data engineer: 9 years (Required)
Google Cloud Platform: 5 years (Required)
Hadoop: 5 years (Required)
Python: 4 years (Preferred)
SQL: 4 years (Preferred)
Work Location: On the road",-1,Conglomerate IT,"Hartford, CT",-1,-1,-1,-1,-1,-1
4,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
5,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
6,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
7,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
8,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
9,BI Data Engineer (Remote),Employer Provided Salary:$75.00 - $85.00 Per Hour,"Stefanini Group is hiring!
Exciting opportunity awaits, let us help you get started!
Click Apply now or you may call: (248) 728-2611 / email: palak.agrawal@stefanini.com for faster processing!

Job Description:
We require a reliable, talented, highly motivated BI/Data Engineer with excellent SQL, PowerBI, Python skills, The firm is putting a huge focus on data and reporting, with an increased demand of data related tasks.
As a global leader in Asset management, you will be working as part of an innovative, fast paced technology driven culture who are always looking at ways to enhance how technology is used within their organization.
You will be a valued member of an effective data team working to deliver growth against ambitious targets.
You will play a key role in extracting useful information from data, providing important insights to global technology teams.
The role will focus on creating best practice, high quality data solutions in collaboration with numerous business stakeholders and technology partners.
We are looking for someone who enjoys problem solving, who has an inquisitive nature, a team player and can work in a collaborative environment with good communication skills.
Ability to help extract and analyze data is required for the job.



Requirements:
We are looking for a good mix of the following skills:
Excellent experience in the Microsoft BI stack, Excel.
Excellent experience in building data pipelines and integrations (ETL/ELT).
Excellent knowledge of data warehousing and analysis concepts, data normalization, dimensional modelling, and associated practices.
Knowledge and experience of data manipulation languages such as DAX or MDX, Visualization or dashboarding experience.
Python experience is a must. Azure cloud experience would be a bonus.
We believe in building a culture supported by strong values, trust and independence. Within the team, we are highly collaborative, supportive, open and have high standards in the work we produce.

Education:
Bachelor's degree or equivalent experience.

*Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives*

About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like Americas, Europe, Africa and Asia, and more than 400 clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting, company with global presence. We are CMM Level 5 company.",3.3,"Stefanini, Inc
3.3","New York, NY",10000+ Employees,1987,Company - Private,Enterprise Software & Network Solutions,Information Technology,$1 to $5 billion (USD)
10,Sr. Data Engineer,Employer Provided Salary:$105K - $110K,"Role : Data Engineer
Location : Remote
Note : We are looking Someone who can be technical and also be able to
structure and manage transformation projects as well.
JD :
* Data Engineer Skill: Spark, Python and Databricks
* 12+ years of experience in Data pipeline engineering for both batch
and streaming applications.
* Experience with data ingestion process, creating data pipelines
(should be able to move data between stages in data lake, apply
transformation rules) and performance tuning. Must be hands-on coding using
Python with Spark.
* Exposure working with complex data types like XML or JSON, CI/CD
pipeline and scheduling tools like Airflow.
* Proven experience in DWH (type-2 dimensions, aggregations, star
schema, etc.) design and implementation
* Proficient in writing complex SQL queries
* Effective communication and collaboration skills to work with
cross-functional teams and stakeholders.
* Oversee and monitor the work of offshore according to projects
timelines and, if necessary, resolve any issues as they arise
Job Type: Full-time
Salary: $105,000.00 - $110,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Experience:
technical and able to structure and manage projects: 5 years (Required)
Data pipeline engineering for both batch and streaming apps.: 10 years (Required)
data ingestion process, creating data pipelines: 5 years (Required)
Python and Spark: 4 years (Required)
Exposure of XML or JSON, CI/CD: 4 years (Required)
scheduling tools like Airflow: 5 years (Required)
writing complex SQL queries: 5 years (Required)
Oversee and monitor the work of offshore: 3 years (Required)
working on fulltime role: 2 years (Required)
Work Location: Remote",4.3,"keasis Inc
4.3",Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
11,Azure Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Required Skills:
Strong Experience with Azure Cloud
Strong Experience With Databricks
Strong Experience With Python Programming
Experience with Spark, Hadoop, Hive, Pig etc. Proven expertise with extracting data from a wide variety of sources and transforming the data as needed.
Previous experience with statistical modeling and deep learning frameworks / libraries is a definite plus.
Strong Experience With Python Programming.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Azure: 9 years (Required)
Python: 8 years (Required)
Data lake: 7 years (Required)
Data Bricks: 7 years (Required)
Work Location: Remote",-1,TekValue IT Solutions,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
12,Data Logging Engineer,Employer Provided Salary:$50.00 Per Hour,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","Plano, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
13,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
14,Data Engineer,-1,"Job Details
Description
CivicPlus is a trusted technology company dedicated to empowering government staff and powering exceptional digital experiences for residents. With a comprehensive suite of solutions that combine to form THE Modern Civic Experience Platform, we strive to create one-stop, frictionless, interactions that delight residents and help staff collaborate and work efficiently. As a result, government administrations that choose CivicPlus realize greater trust and satisfaction among their community members.
Backed by over 25 years of experience and leveraging the insights of more than 950 team members, our solutions are chosen by over 12,500 global entities and are used daily by over 340 million people in the U.S. and Canada alone.
As a Data Engineer within our AI and ML Team, you will play a crucial role in developing and maintaining the infrastructure required for the successful implementation of artificial intelligence (AI) and machine learning (ML) projects. Your expertise in data engineering will be instrumental in ensuring the availability, reliability, and scalability of data pipelines, enabling efficient data processing, analysis, and model training.
We’re excited to hire a new Data Engineer who can:
Design, develop, and maintain data infrastructure and pipelines to support AI and ML initiatives.
Collaborate with software engineers to understand data requirements and implement efficient data processing workflows.
Build and optimize data integration and transformation processes for structured and unstructured data sources.
Create and manage data warehouses, data lakes, and other storage systems to support large-scale data processing and analysis.
Ensure data security and privacy compliance in alignment with organizational and regulatory requirements.
Develop and maintain documentation related to data infrastructure, pipelines, and processes.
Collaborate with cross-functional teams to identify and address data-related challenges and opportunities.
May participate in on-call rotation to gain better understanding of Product Suite
There is no perfect candidate, but we are looking for:
Bachelor's degree in computer science, engineering, or a related field.
Proven experience as a Data Engineer or similar role, working with large-scale data processing and analysis.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with big data technologies and frameworks
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data-related services.
Knowledge of data modeling and ETL principles.
Understanding of distributed computing architectures and parallel processing techniques.
Excellent problem-solving skills and ability to work effectively in a dynamic team environment.
Strong communication and interpersonal skills, with the ability to explain technical concepts to non-technical stakeholders.
Note: We know that excellent candidates can have all sorts of backgrounds and experiences, so please don’t hesitate to apply even if you don’t meet 100% of the listed requirements!
We offer competitive salary and benefits packages, including health insurance, retirement plans, flexible time off, and a commitment to Diversity, Equity, Inclusion, and Belonging. If you are passionate about technology and want to make a meaningful impact in your community, we’d love to hear from you.
We are an equal opportunity employer and value diversity at our company. We desire to have our employees reflect the diverse communities we serve, and we recognize that diverse and inclusive teams lead to more innovation and better financial returns. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
What is our hiring process?
Introductory Call with a member of our Talent Acquisition team.
First Interview with the up to 4 CivicPlus Team Members
Second Interview- Panel Interview with up to 4 CivicPlus Leaders
Interview Project Activity- This activity is designed to give us insight into your problem-solving approach and methods.
Offer
Please note that the specifics of this process may vary based on the position you're applying for.
Employment Practices
CivicPlus is proud to be an Equal Employment Opportunity employer. At CivicPlus, we celebrate and support diversity for the benefit of our employees, products, clients, and communities we serve.
Upon receiving an offer of employment, candidates must complete required pre-employment screenings, which include a drug test and background screen. Offer of employment is contingent upon this post-offer screening process. All testing will be conducted by a licensed independent administrator, which will follow testing standards and background screens in accordance with state law.
We are committed to providing equal employment opportunities to all qualified individuals and will make reasonable accommodations for individuals with disabilities during the interview process. If you require an accommodation, please let us know in advance so we can make appropriate arrangements. We welcome and encourage candidates of all abilities to apply for this position.",3.6,"CivicPlus
3.6",Remote,501 to 1000 Employees,1998,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
15,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
16,Healthcare Data Engineer for Databricks,Employer Provided Salary:$55.00 - $65.00 Per Hour,"As a healthcare data engineer, you will be a core member of the data engineering team to own, manage and monitor healthcare data extracts and transformations. You will work with a team of passionate, experienced, nimble, and goal-oriented team that is trying to solve complex problems for healthcare customers. You will embrace change, and rapidly build, test, and scale high-priority data engineering use cases that drive incremental business value for our customers and partners in the overall architecture.
****Please do not apply if you do not have healthcare data experiences****
Responsibilities
· Understand, document, and evolve the business, and technical requirements by working closely with the rest of the team, and customer team
· Develop and maintain clinical data extracts and SSIS packages using SQL (experience required) and other ETL tools to move the data into cloud instances
. Develop complex reporting and extract-based stored procedures and SQL queries
· Create and maintain data processing pipelines on AWS and Azure to enable downstream analytics and application consumption and develop applications on our own platform / third-party platforms using Python, Scala
. Develop and maintain data lake, data models, views, tables, and data marts to make healthcare data more liquid
· Develop and harmonize healthcare clinical and claims data models, data sets, and data warehouses.
. Develop and maintain Databricks lakehouse on Microsoft Azure cloud
· Consume different types of healthcare data payloads – HL7, clinical, and 835, 837 and adjudicated claims data, also in custom formats, including CSV formats
Requirements
· Bachelors or master’s degree in computer science or related field
· 4 to 5 years of experience as a data engineer/data analyst in the healthcare industry
· 3 to 5 years of working with Databricks on Microsoft Azure
· 4 to 5 years of experience designing, and developing high-performing normalized and flat data tables in SQL
· 3 to 5 years of healthcare experience and domain knowledge e.g., working with structured and sometimes unstructured data sets, including HL7, FHIR, and clinical data and/or claims (835, 837 and other adjudicated claims custom formats)
· 3 years of cloud data pipelines and analytics experience - data engineering using Spark with Python
· 3 to 5 years of experience in working with ETL, message queues, data processing, and data warehouses
· 3 to 5 years of experience in automating data pipelines (devops) using Terraform and other data tools
Preferred ­­­­­
· Mastery of data engineering - cleaning, transforming, and aggregating data for downstream analytics
· Experience with FHIR
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
SQL: 4 years (Required)
Healthcare data: 4 years (Required)
Databricks: 4 years (Required)
Python/Scala/Cloud Data Engineering: 5 years (Required)
Azure: 4 years (Required)
Work Location: Remote",-1,Percept Health,Remote,-1,-1,-1,-1,-1,-1
17,Data Engineer,-1,"Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore.

Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office.
The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus.
Position Responsibilities
Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities
Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization
Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process
Qualifications & Desired Skills
Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance
Minimum 2 years' of development experience using either Python, C#/Java or C++
Minimum 2 years' years of database/SQL experience
Familiarity with object-oriented programming concepts
Must be comfortable with development across the application stack
Ability to complete complex projects independently
Detail-oriented with strong verbal and written communication skills
Ability to work effectively in a high-energy, time sensitive team environment
Interest or prior experience in the financial trading industry (particularly in fixed income) a plus
This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education.
Base Salary for this role is expected to be between:
$110,000—$150,000 USD",4.4,"Garda Capital Partners
4.4","New York, NY",51 to 200 Employees,-1,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
18,Data Engineer,Employer Provided Salary:$58.00 - $60.00 Per Hour,"*Immediate Data Engineer - Databricks - 3 Positions Remote C2 or W2 No OPT *
Responsibilities :
●Designing and building the data Ingestion Pipeline
●Designing, building and maintain large, complex data processing pipelines using Databricks and Azure Data Factory in Azure to meet enterprise Data requirements
●Providing operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle
●Identifying, designing, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability
●Building the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
●Working with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.
●Working closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performance.
●Experience in building Meta data driven framework using Databricks
●Experience in Code review and writing Technical Design
●Implement DataOps and DevSecOps for data initiatives
●Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Qualifications :
●6+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
●4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,
●5+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.
●Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
Job Type: Contract
Pay: $58.00 - $60.00 per hour
Compensation package:
1099 contract
Experience level:
6 years
Schedule:
8 hour shift
Work Location: Remote",-1,Sankrish Consulting LLC,Remote,-1,-1,-1,-1,-1,-1
19,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
20,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
21,Data Engineer (MongoDB),$80K - $114K (Glassdoor est.),"CrossCountry Mortgage (CCM) is one of the nation’s top 3 retail mortgage lenders with more than 7,000 employees operating nearly 600 branches across all 50 states. We have been recognized nine times on the Inc. 5000 list of America’s fastest-growing private businesses and have received countless awards for our standout culture. Through our dedication to getting it done, we make every mortgage feel like a win.

A culture where you can grow! CCM has created an exceptional culture driving employee engagement, exceeding employee expectations, and directly impacting company success. At our core, our entrepreneurial spirit empowers every employee to be who they are to help us move forward together. You’ll get unwavering support from all departments and total transparency from the top down.

CCM offers eligible employees a competitive compensation plan and a robust benefits package, including medical, dental, vision, as well as a 401K with an employer match. We also offer company-provided short-term disability, an employee assistance program, and a wellness program.
Position Overview:
We are looking for an experienced MongoDB developer to work with our development teams to integrate innovative features on our new products and applications, as well as maintain existing applications. The MongoDB developer will engineer scripts that automate daily tasks and ensure the security of data sets.
Job Responsibilities:
Upgrade databases through patches.
Understand and convert company needs into technical specifications and creating scalable solutions using MongoDB.
Optimize database schema designs, ensuring data integrity, and promoting best practices to enhance the overall performance and scalability of MongoDB solutions.
Assist developers in identifying and resolving performance issues by utilizing Mongo Profiler to profile and fine-tune database queries and operations.
Leverage performance optimization techniques by implementing proper indexes, including B-Tree, Geospatial, and Text indexes, to enhance query performance and overall system efficiency.
Implement the most effective backup and recovery methods to safeguard critical data and ensure business continuity in case of data loss or system failures.
Collaborate with cross-functional teams to understand business requirements.
Stay updated with the latest MongoDB advancements, features, and best practices.
Qualifications and Skills:
Bachelor’s/Master’s degree in software engineering or Information Technology.
3+ years of experience in executing data solutions using MongoDB.
Recommend and implement best practices for Rest API integration framework/model.
Analytical and problem-solving skills.
Configuring schema and MongoDB data modelling.
Experience with database security management.
Administration of MongoDB in Atlas.
Thorough understanding of MongoDB architecture.
Excellent communication skills.
This job description is intended to convey information essential to understanding the scope of the job and the general nature and level of work performed by job holders within this job. However, this job description is not intended to be an exhaustive list of qualifications, skills, efforts, duties, responsibilities or working conditions associated with the position.
CrossCountry Mortgage, LLC offers MORE than a job, we offer a career. Apply now to begin your path to success! https://crosscountrymortgage.com/about-us/careers/apply/
California residents: Please see CrossCountry’s privacy statement for information about how CrossCountry collects and uses personal information about California consumers.
CrossCountry Mortgage, LLC. (NMLS3029) is an Equal Opportunity Employer committed to workforce diversity. Qualified applicants will receive consideration without regard to race, religion, creed, color, orientation, gender, age, national origin, veteran status, disability status, marital status, sexual orientation, gender identity, or gender expression, or any other protected status in accordance with all applicable federal, state and local laws. The collective sum of the individual differences, life experiences, knowledge, inventiveness, innovation, self-expression, unique capabilities and talent that our employees invest in their work represents a significant part of not only our culture, but our reputation. The Company is committed to fostering, cultivating and preserving a culture that welcomes diversity and inclusion. Employment is contingent upon successful completion of a background investigation. CrossCountry Mortgage, LLC. is an FHA Approved Lending Institution and is not acting on behalf of or at the direction of HUD/FHA or the Federal government. To verify licensing, please visit www.NMLSConsumerAccess.org.",3.3,"CrossCountry Mortgage, LLC.
3.3","Charlotte, NC",1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
22,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
23,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department.
The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
24,Lead Azure Data Engineer,Employer Provided Salary:$135K - $140K,"Lead Data Engineer.
Location : New York, NY ( Hybrid)
Full Time
10+ years of experience in data engineering
5+ years of experience managing a team of data engineers
Strong understanding of SQL Server SSIS
Excellent communication and interpersonal skills
Ability to work independently and as part of a team
Ability to meet deadlines and work under pressure
Strong desire to be hands-on with the development and implementation of data pipelines and data warehouses
Experience working with stakeholders to develop product backlog grooming, sprint planning data engineering and
QA Testing
Migration of SQL Server to Azure is desired
Experience with Azure Data Factory, ADLS, and CI/CD highly desired
The Hands-On Data Engineering Lead is responsible for leading a team of data engineers in the design, development, and
implementation of data pipelines and data warehouses. The ideal candidate will have a strong understanding of data
engineering principles and practices, as well as experience with SQL Server SSIS. The Hands-On Data Engineering Lead will
also be responsible for managing the team's workload, mentoring junior engineers, and communicating with stakeholders.
Additionally, the Hands-On Data Engineering Lead will be expected to be hands-on with the development and
implementation of data pipelines and data warehouses, and to have experience working with stakeholders to develop
product backlog grooming, sprint planning data engineering and QA Testing.
Responsibilities:
Lead the design, development, and implementation of data pipelines and data warehouses
Develop and implement data pipelines and data warehouses using SQL Server SSIS
Troubleshoot data pipelines and data warehouses
Communicate with stakeholders to understand their needs and ensure that projects meet their requirements
Manage the team's workload and ensure that projects are completed on time and within budget
Mentor junior data engineers and help them develop their skills
Stay up-to-date on the latest data engineering technologies and best practices
Provide support to production teams and QA teams
Work with stakeholders to develop product backlog grooming, sprint planning data engineering and QA Testing
Working with other engineers to design and implement data pipelines and data warehouses using Azure Data
Factory, ADLS, and CI/CD
Experience in migrating SQL server to Azure
30% team management and 70% hands-on
Job Type: Full-time
Salary: $135,000.00 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
New York, NY 10001: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 10 years (Required)
SQL: 10 years (Required)
Data warehouse: 10 years (Required)
SSIS: 10 years (Required)
Work Location: In person",-1,Rivago Infotech Inc,"New York, NY",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
25,GCP Data Engineer,Employer Provided Salary:$50.00 - $56.00 Per Hour,"JOB Description: -
ABOUT CONGLOMERATE
Conglomerate IT is a certified and a pioneer in providing premium end-to-end Global Workforce Solutions and IT Services to diverse clients across various domains. Visit us at ConglomerateIThttp://www.conglomerateit.comConglomerateIT
Conglomerate IT mission is to establish global cross culture human connections that further the careers of our employees and strengthen the businesses of our clients. We are driven to use the power of global network to connect business with the right people without bias. We provide Global Workforce Solutions with affability.
We offer a gamut of services across diverse domains, categories, skill sets with varying lengths of assignments /core capabilities included:
· Application Development Services
· Business Intelligence Services
· Enterprise Resource Planning
· Infrastructure Support Services
· Testing Services
· Program / Project Management
· Temporary & Permanent Staffing
Conglomerate IT is headquartered in Addison, Texas with branch offices across Canada, India, Sri Lanka, Argentina to further serve our clients.
JOB ROLE: GCP Data Engineer
Location: Job Type:Hartford, CT (Onsite Role)
Contract : 12+ Months
Work Authorization: w2 ,1099
Must have Skills: - Skills: Python/PySpark, GCP Cloud , HDFS, Hive, Big data
About Role: -
As a GCP Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and data architecture on the Google Cloud Platform. You will work closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and storage. Your expertise in GCP services and data engineering best practices will play a crucial role in maintaining a scalable, reliable, and high-performance data infrastructure.
Roles and Responsibilities:
· Design and implement data pipelines: Develop ETL (Extract, Transform, Load) processes to move data from various sources into GCP storage solutions such as BigQuery, Cloud Storage, and Datastore.
· Data modeling: Design and optimize data models to support data analytics, reporting, and machine learning initiatives. Ensure data accuracy, integrity, and appropriate access controls.
· Performance optimization: Fine-tune data pipelines and query performance to maximize efficiency and reduce latency, utilizing techniques like partitioning, indexing, and caching.
· GCP services utilization: Leverage GCP services like Dataflow, Dataproc, Pub/Sub, and Composer to create, schedule, and manage data processing and orchestration tasks.
· Data integration: Collaborate with cross-functional teams to integrate data from various sources, both internal and external, to enable comprehensive analysis and reporting.
· Data quality and governance: Implement data quality checks, monitoring, and validation processes to ensure data accuracy and consistency across the platform.
· Security and compliance: Implement security measures and access controls to protect sensitive data. Ensure compliance with industry standards and regulations.
· Documentation: Create and maintain technical documentation for data pipelines, workflows, and processes.
· Troubleshooting and support: Diagnose and resolve data-related issues, performance bottlenecks, and operational challenges in a timely manner.
Ideal Profile
· You have experience with Python Hive, Big data
· You have experience withYou GCP Cloud
· have strong experience with HDFS and other relevant tools/technologies.
· You have advanced python skills.
· You have PySparkknowledge.
· You have excellent communication and collaboration skills.
· You have attention to details.
· You are self-directed.
What's on Offer?
· Join a team of world class IT professionals
· Comprehensive salary package
· Great opportunity for growth and development
Job Type: Contract
Salary: $50.00 - $56.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Experience:
google cloud platform: 4 years (Required)
Work Location: In person",-1,CONGLOMERATE IT LLC,"Novi, MI",-1,-1,-1,-1,-1,-1
26,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
27,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
28,Data Engineer,Employer Provided Salary:$120K - $130K,"Role – Data Engineer- DataBricks -
Location – O Fallon, Missouri (Initially Remote)
Yrs. of experience – 8+
Full-Time
Job Description :
o 5+ years in a customer facing Technical architecture, consulting role with the expertise in The following technology.
o Developing modern Data warehouse solutions using Databricks and AWS/Azure stack.
o Drive technical discussion with client architect and team members.
o Knowledge in Databricks DELTA lake for the Analytical data lake use case
o Hand on experience in create MLOPS data pipeline creation.
o AIML Models develop, train and implement for the AIML use cases.
o Knowledge in Banking domain on card and payment areas.
o Any relation database – Data classification/ Data profiling for MLOPS use cases
o Good experience in offshore onsite coordination.
o Experience in translating a customer’s business needs.
Job Type: Full-time
Salary: $120,000.00 - $130,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
O'Fallon, MO 63366: Reliably commute or planning to relocate before starting work (Required)
Experience:
DataBricks: 1 year (Required)
MLOPS: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: In person",-1,Sbase Technologies,"O Fallon, MO",-1,-1,-1,-1,-1,-1
29,Snowflake Data Engineer,Employer Provided Salary:$70.00 - $80.00 Per Hour,"Job: Snowflake Data Engineer – Contract (6+ Months) – Stillwater, MN
REMOTE - HAVE TO WORK ONISTE WHEN EVER CLIENT CALL. Preferably local to MN NEEDED
EAD – GC – H4- EAD G C - CITIZEN
W2 Contract
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Expected hours: 40 per week
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 7 years (Required)
Snowflake API: 4 years (Required)
Snowflake administration: 6 years (Required)
Snowflake RBAC, RLS, CLS: 5 years (Required)
Willingness to travel:
50% (Required)
Work Location: Remote",-1,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
30,Data Engineer I,Employer Provided Salary:$90K - $120K,"Interested in joining a growing company where you will work with talented colleagues, enhance a supportive and energetic culture, and be part of the climate solution? At Energy Solutions, we focus on the big impacts. And we believe that market-based programs can be a powerful force to deliver large-scale energy, carbon, and water-use savings. For 28 years, we've harnessed that power to offer proven, performance-based solutions for our utility, government, and institutional customers.
Energy Solutions (ES) is currently seeking a full-time Data Engineer I to join our Business Operations Team. The Data Engineer role is part of the Business Operations Team and will work on critical projects such as the Corporate Operational Data Warehouse (CODW) and the new ERP system. The CODW project is one of ES's strategic goals, to build and maintain a system that enables ES to centralize, integrate, and analyze internal data from various sources. The goal is to provide a single source of truth for ES internal data and enable data-driven decision-making across the organization. This role will potentially lead the design and implementation of the infrastructure for internal reporting needs, driving operational efficiencies and helping our company's transition to operating internationally with 1000+ employees.
Energy Solutions has a remote-friendly work environment for staff located throughout the United States. We also have offices in Oakland and Orange, California, Boston, Massachusetts, New York, and Chicago, IL, for those that wish to work from one of our offices.

Daily responsibilities include but are not limited to:
Collaborate with stakeholders to understand data requirements and develop data-driven solutions that support business goals
Analyze data sources and pipelines needed for the CODW project
Design and implement scalable data pipelines, ETL processes, and data integration solutions for the CODW project using Python, SQL, and AWS technologies
Develop and maintain a data warehouse and optimize data storage, processing, and retrieval
Build schema, populate the database, and automate reporting
Document and maintain technical specifications, ETL processes, data mappings, and dictionaries
Support the Data Science Center of Excellence (DSCOE) with other data engineering needs

Minimum Qualifications:
BS/BA in Computer Science, Physics/Engineering, Business or Mathematics
Experience in building ETL data pipelines, real-time pipelines are a plus
Strong programming skills in Python and SQL
Minimum 2 years of data warehouse development and strong fundamentals in dimensional data modeling
Minimum 2 years of experience creating SQL queries and ETL design, implementation, and maintenance
Minimum 2 years of experience developing data pipelines using Python
Minimum 2 years of experience with AWS, AWS Redshift, AWS Data Engineering and ML Ops tools and services (ie: S3, Redshift, RDS)
Proven ability to aggregate, normalize, munge, analyze, and summarize value from disparate datasets
Strong oral, written, and interpersonal communication skills
High degree of accuracy and attention to detail
Ability to establish priorities, work independently with minimum supervision
Preferred Qualifications:
Ability to manage and communicate data warehouse plans to a non-technical audience
Expertise in cloud services architecting and designing secure AWS environments
Experience using BI tools like Tableau and Looker
Experienced in analyzing and troubleshooting data quality issues
Self-driven, highly motivated, and able to learn quickly
Familiarity with large data sets, cloud-based development and deployment, open-source practices and frameworks, and intelligent applications is desirable
Proven success in migrating data from legacy systems

Salary DOE: $90k - $120k / Annually
Compensation is commensurate with experience and includes a generous retirement package. Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP).
To see this and our other open positions, please visit https://energy-solution.com/company/careers/.
We encourage candidates from diverse backgrounds to apply. For more information about Energy Solutions and our Diversity, Equity and Inclusion efforts, please visit us on our website at www.energy-solution.com.
Background Check Information
Information will be requested to perform the compulsory background check. A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment. Energy Solutions is an equal opportunity employer.
Reasonable Accommodations
Energy Solutions is committed to providing access and reasonable accommodation for individuals with disabilities. If you require an accommodation in completing this application, interviewing, and/or completing any pre-employment testing, or otherwise participating in the employee selection process, please email accommodation@energy-solution.com.
Privacy Notice for Job Applicants",3.9,"Energy Solutions
3.9",Remote,10000+ Employees,2008,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$25 to $100 million (USD)
31,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
32,Energy Data Engineer - Mid Level,$81K - $121K (Glassdoor est.),"About the Organization
Now is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.

The Data Engineer will support Redhorse in its execution of specific task areas related to the Federal Government Carbon Pollution Free Electricity (CFE) Technical and Strategy Support contract. The program will establish technical frameworks and analytical tools and methods to inform CFE procurement decisions for the entirety of the federal government. Work will be conducted primarily at Redhorse office locations (hybrid remote work) and on government client site as required. The ideal candidate will live in the DC/Virginia/Maryland area and willing to undergo a background check to obtain a Public Trust.
Primary Duties and Responsibilities for this position include
Review and analyze the US Federal Government Executive Agencies electric load data
Develop and test modeling techniques to understand an agency’s hourly load profile to inform the agency’s hourly CFE procurement requirement as an off taker from a new CFE technology source.
Support the configuration and ingestion of designated structured, unstructured, and semi-structured data repositories into capabilities that satisfy client requirements and support a data analytics pipeline to drive rapid delivery of functionality to the client.
Maintain all operational aspects of data transfers while accounting for the security posture of the underlying infrastructure and the systems and applications that are supported and monitoring the health of the environment through a variety of health tracking capabilities.
Automate configuration management, leverage tools, and stay current on data extract, transfer, and load (ETL) technologies and services.
Work under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.
Apply comprehension of data engineering-specific technologies and services, leverage expertise in databases and a variety of approaches to structuring and retrieving of data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically using vendor consoles.
Engage with multiple functional groups to comprehend client challenges, prototype new ideas and new technologies, help to create solutions to drive the next wave of innovation, and design, implement, schedule, test, and deploy full features and components of solutions.
Identify and implement scalable and efficient coding solutions.
Minimum Requirements for skills, Experience, and Credentials
Master’s Degree in Engineering, Computer Science, Mathematics or Statistics
Four (4) years of experience in enterprise data analysis with a preferred focus on utility data, to include advanced metering infrastructure (AMI) electricity data
Local to Washington D.C. metro area preferred, remote work is an option
Willingness to travel in support of on-site customer meetings
Willingness to obtain a Public Trust
Desired Basic Requirements for Skills, Experience, and Credentials include
Knowledge of U.S. Federal Government Energy Legislation and Executive Orders (especially EO 14057)
Experience in or knowledge of utility interval data and electricity pricing
Executive Order Requirement:
While currently on HOLD, Redhorse recognizes that we may need to comply with Executive Order 14042 which requires Federal Contractors to ensure all U.S. new hires be fully vaccinated for COVID-19. As required by the Executive Order, Redhorse will work in coordination with applicable contract agencies to consider requests for Reasonable Accommodations.
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.
EOE/M/F/Vet/Disabled",3.5,"Redhorse
3.5","Arlington, VA",51 to 200 Employees,2008,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
33,Sr. Azure Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"LeadStack Inc. is an award-winning, one of the nation's fastest-growing, certified minority-owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Position Details:
Title: Azure Data Engineer
Location: Remote
Duration: 6+ Months Contract
Top 3 skills: SQL, Snowflake, and Data Warehousing/Design
Its a W2 Role. No C2C.
Client is seeking a talented and motivated Data Engineer to join our dynamic team. As a Data Engineer, you will play a critical role in developing and maintaining our data infrastructure, ensuring the smooth flow of data for our analytics and insights projects. The ideal candidate will have a strong background in SQL, data warehousing, ETL processes, and a preference for Snowflake experience. Additional experience with dbt or semantic layer tools will be a bonus, and proficiency in Python will also be advantageous.
Bachelor's degree in Computer Science, Engineering, or a related field.
Solid experience with SQL and data warehousing concepts.
Strong understanding of ETL processes and tools.
Preferred experience with Snowflake data warehouse platform.
Bonus for experience with dbt or other semantic layer tools.
Proficiency in Python is a plus.
Excellent problem-solving and analytical skills.
Strong attention to detail and ability to work in a fast-paced environment.
Effective communication skills and ability to collaborate with cross-functional teams.
Key Responsibilities:
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.
Optimize data ingestion and transformation processes to ensure efficient and scalable data flows.
Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.
Develop and maintain data models, schemas, and data dictionaries for efficient data storage and retrieval.
Perform data validation, quality checks, and troubleshooting to identify and resolve data-related issues.
Monitor and optimize database performance, including query tuning and index optimization.
Collaborate with data scientists and analysts to implement data models and support their data requirements.
Stay up-to-date with industry trends and best practices in data engineering, and proactively recommend and implement improvements to our data infrastructure
To know more about current opportunities at LeadStack, please visit us at https://leadstackinc.com/careers/
Should you have any questions, feel free to call me on 510-480-1006 or send an email on tarique.aziz@leadstackinc.com
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Data Warehousing/Design: 8 years (Required)
Snowflake: 5 years (Required)
Azure: 5 years (Required)
Work Location: Remote",4.3,"Leadstack Inc
4.3",Remote,51 to 200 Employees,2016,Company - Private,HR Consulting,Human Resources & Staffing,Unknown / Non-Applicable
34,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
35,Azure Data Engineer,Employer Provided Salary:$50.00 Per Hour,"7+ years of relevant experience
At least two years of experience building and leading highly complex, technical engineering teams.
Strong hands-on experience in Databricks
Experience managing distributed teams preferred.
Strong technical experience in large distributed systems, Data Warehousing, Data Lake at scale Project management skills: financial/budget management, scheduling and resource management experience with medium and large-scale projects
Comfortable working with ambiguity and multiple stakeholders.
Comfortable working cross functionality with product management and directly with customers; ability to deeply understand product and customer personas.
Architecture Design Experience for Cloud and Non-cloud platforms
Expertise on Azure Cloud platform
Knowledge on orchestrating workloads on cloud
Ability to set and lead the technical vision while balancing business drivers
Strong experience with PySpark, Python programming
Proficiency with APIs, containerization and orchestration is a plus.
Job Type: Contract
Salary: $50.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Work Location: Remote",3.8,"Visvak Solutions
3.8",Remote,51 to 200 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,Less than $1 million (USD)
36,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
37,Data Engineer,-1,"SalesIntel is the top revenue intelligence platform on the market. Our unique approach to data collection, enhancement, verification and growth distinguishes us from the market, solidifying our position as the best B2B data partner.

The Data Engineer will be responsible for building and extending our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys working with big data and building systems from the ground up.

You will collaborate with our software engineers our software engineers, database architects, data analysts and data scientists to ensure our data delivery architecture is consistent throughout the platform. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What You’ll Be Doing:
Design and build parts of our data pipeline architecture for extraction, transformation, and loading of data from a wide variety of data sources using the latest Big Data technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with machine learning, data, and analytics experts to drive innovation, accuracy and greater functionality in our data system.
Qualifications:
Bachelor's degree in Engineering, Computer Science, or relevant field.
5+ years of experience in a Data Engineer role.
3+ years experience with Apache Spark and solid understanding of the fundamentals.
Deep understanding of Big Data concepts and distributed systems.
Strong coding skills with Scala, Python, Java and other languages and the ability to quickly switch between them with ease.
Advanced working SQL knowledge and experience working with a variety of relational databases such as Postgres and MySQL.
Experience working with data stored in many formats including TSV, CSV, JSON, Parquet.
Cloud Experience with DataBricks or similar service: S3, Athena, EC2, EMR, RDS, Redshift.
Comfortable working in a linux shell environment and writing scripts as needed.
Comfortable working in an Agile environment
Machine Learning knowledge is a plus.
Must be capable of working independently and delivering stable, efficient and reliable software.
Excellent written and verbal communication skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

This is a remote position.",-1,"SalesIntel Research, Inc.",Remote,51 to 200 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
38,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
39,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
40,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
41,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
42,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
43,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
44,Azure Data Engineer,Employer Provided Salary:$70.00 Per Hour,"Job Description
Seeking an experienced Azure Data Engineer to design and implement data solutions using Azure technologies. The ideal candidate will have a strong background in data modeling, ETL, and data warehousing, and ability in SQL, Python, and Azure data services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.
Essential Functions
Design and implement data pipelines using Azure Data Factory to move data from various sources to Azure Data Lake Storage and Azure SQL Database.
Create and keep data models in Azure Synapse Analytics for reporting and analytics with Power BI.
Work with data scientists and analysts to implement data solutions.
Collaborate with the DevOps team to automate the deployment of data pipelines using Azure DevOps.
Participate in data governance efforts to ensure data quality and compliance.
Qualifications
2-5+ years of experience as a data engineer or related role.
Strong background in data modeling, ETL, and data warehousing.
Proficient in SQL, Python, and Azure data services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.
Experience using version control software, preferably git.
Experience with Visual Studio, Azure DevOps, and Power BI is a plus.
Microsoft Certified: Azure Data Engineer Associate or higher is a plus.
Education: bachelor’s degree in computer science, data science, or a related field.
Strong diligence.
Excellent troubleshooting and communication skills
Able to work well in a team setting and mentor others.
Job Types: Full-time, Contract
Pay: Up to $70.00 per hour
Benefits:
Health insurance
Ability to commute/relocate:
Brentwood, TN: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
SQL: 2 years (Required)
Data warehouse: 2 years (Required)
Azure Data Engineering: 2 years (Required)
design and implement data solutions: 2 years (Required)
Azure technologies: 2 years (Required)
data modeling, ETL, and data warehousing: 2 years (Required)
SQL, Python, and Azure data services: 2 years (Required)
Azure Databricks, and Azure Synapse Analytics: 2 years (Required)
Azure Data Factory: 2 years (Required)
version control software: 2 years (Required)
Visual Studio, Azure DevOps, and Power BI: 2 years (Required)
License/Certification:
Microsoft Certified: Azure Data Engineer Associate or higher (Required)
Work Location: In person",-1,Vision,"Brentwood, TN",-1,-1,-1,-1,-1,-1
45,Data Engineer,$79K - $116K (Glassdoor est.),"Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
Sr. Consultant /Data Engineer – Data Architecture & Engineering
Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
A Senior Consultant in the Data and Analytics Practice is someone that looks forward to being challenged and continuously grow their technical and team collaboration skills. You'll provide technical guidance, as needed, to other team members for quality assurance, mentoring and growth opportunities. If this is a path that excites you, let's chat!
Responsibilities
Technical delivery in both Agile and Waterfall methodologies
Implement solutions using the Microsoft Power BI tool suite and related technologies
Leverage advanced database systems including data warehouses, data marts, and models needed for business, financial and operational analysis and/or reporting
Develop high performing, reliable and scalable solutions
Ability to clearly communicate technical details to business and management personnel
Perform data analysis that will support and enhance Information Management systems
Author, oversee and gain approval of design documents for projects assigned
Develop reports, dashboards & scorecards, data exploration and visual analytics using existing and emerging tools and technologies.
Work independently or as part of a team to design and develop solutions
Assist business development team with pre-sales activities and RFPs
Provide meaningful feedback and coaching of other team members to successfully overcome technological challenges
Leading and demonstrating emerging technologies and concepts to teams
Requirements
Bachelor's Degree desired in Computer Science, Information Technology, or related field
2+ years of experience with data visualization
Expert knowledge of Data Management, Business Intelligence and Analytics concepts including:
Extract, Transform, Load (ETL)
Data Warehousing, Data Lakes, Data Marts, Data Stores, and Cubes
Data Quality and Profiling
Data Governance, Master Data Management, and Metadata Management
Data Visualization, Dashboard, Reporting, Self Service Business Intelligence (BI), Key Performance Indicators (KPI), and Scorecards
Predictive, Prescription, and Descriptive Analytics
Expert knowledge of Microsoft Business Intelligence stack including Power BI, Excel and SQL Server (SSAS, SSRS, SSIS)
Minimum of 3-5 years of experience (of which 2-3 years in consulting)
Experience with Azure Data Estate (Azure SQL DB, Cosmos DB, Azure SQL DW, Azure Data Lake) is a plus
Azure ML Studio/Services experience a plus
Certifications are a plus
Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and tuning is a plus
Knowledge of relational and dimensional database structures, theories, principles, and practice is a plus
Experience with Data Science tools, technologies and techniques (R, Python, algorithms) is a plus
Passionate about learning new technologies
Ability to learn new concepts and software quickly
Analytical approach to problem-solving; ability to use technology to solve business problems
Familiarity with database-centric applications
Ability to communicate effectively in both a technical and non-technical manner
Ability to work in a fast-paced environment
Ability to travel up to 30%
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways.
At this time, we cannot sponsor applicants for work visas. Candidates should be based in the United States.
About 3Cloud
3Cloud is a ""born in the cloud"" Gold-Certified Microsoft Azure technology consulting firm and Azure Expert Managed Services Provider that provides cloud strategy, design, implementation, acquisition and ongoing managed services to its clients across various industries. To meet the needs of its clients, 3Cloud offers comprehensive services across Cloud Infrastructure, DevOps & Automation, Cloud Application Development and Data, Analytics and AI. Founded by former Microsoft technology leaders, 3Cloud uniquely offers: (1) highly experienced and proven cloud architects and technologists, (2) strong business strategy, financial acumen and operational proficiency and (3) deep relationship and network into Microsoft engineering and field leadership.
3Cloud Total Rewards Highlights Include:
Flexible work location with a virtual first approach to work!
401(K) with match up to 50% of your 6% contributions of eligible pay
Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days
Three medical plan options to allow you the choice to elect what works best for you!
Option for vision and dental coverage
100% employer premium coverage for STD and LTD
Paid leave for birth parents and non-birth parents
Option for FSA, HSA, HRA and Dependent Care
$67.00 monthly tech and home office allowance
Utilization and/or discretionary bonus eligibility based on role
Robust Employee Assistance Program to help with everyday challenges
3Cloud offers competitive compensation. In addition to base pay employees are eligible to receive an annual discretionary/utilization bonus. If you are hired at 3Cloud your final base salary is based on factors such as skills, education, experience and/or geographic location. Please keep in mind that the range mentioned above includes the full base salary range for the role. It is not typical for offers to be made at or near the top of the range.
Base Salary Range
$90,000—$183,000 USD
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway.
At this time, we cannot sponsor applicants for work visas.",4.3,"3Cloud
4.3","Downers Grove, IL",501 to 1000 Employees,2016,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
46,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Job Role: Data Engineer with Snowflake
Location: REMOTE, Stillwater, MN.
HAVE TO WORK ONISTE WHEN EVER CLIENT CALL.
Duration: 6+ Months Contract
Visas: USC, GC, EAD-GC, H4-EAD
W2 Requirement
Primary Responsibility:
Enabling operational data use via our Snowflake Platform which includes creation and maintenance of Snowflake data shares for both internal and external use cases as well as administration of Snowflake and Snowflake permissions.
Secondary Responsibility:
Snowflake administration and permissions along with providing support to Enterprise Data Team in the form of creating data pipelines, curating, and modeling data, and helping team to deliver on analytics initiatives.
Required Skills:
Must have strong Data Engineering / Administration experience with the ability to support Snowflake API including creating and guiding business partners on best practices for calling APIs to read data.
Must have Snowflake administration experience including Snowflake API experience.
Knowledge of SQL with the ability to turn requirements into performant queries, create views, manage database objects (tables, views, schemas), and apply permissions.
Work with business to understand need and coordinate with Enterprise Data Team (EDT) Product Owner as needed to prioritize data sources for ingestion into Snowflake.
Manage permissions on data and sharing capabilities using Snowflake RBAC, RLS, CLS, and sharing (external & internal) via Snowflake data shares (e.g., reader account, separate account, exchange)
Coordination and potential ""modeling"" of data to enable operational needs.
Performance monitoring and support for operational ""data shares""
Desired Skills:
Any experience working with MuleSoft would be a plus.
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Expected hours: 40 per week
Experience level:
7 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
Snowflake: 4 years (Required)
Work Location: Remote",-1,Zettalogix Inc,Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
47,2024 Data Engineer I,-1,"2024 Data Engineer I
316114

Textron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.

Responsibilities:
Analyze, manipulate, or process large sets of data using statistical software.
Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces
Identify business problems or management objectives that can be addressed through data analysis.
Document data standards, semantic layer mapping, data flows and conceptual flows.
Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.
Perform statistical analysis to provide recommendations on best courses of action to business leaders.
Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.
Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.
Collaborate with business users to ensure data is available for self-service analytics.
Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.


Qualifications:
Education: Bachelor’s degree in computer science, engineering, mathematics, or similar field required
Years of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience required
Software Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferred
Experience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)
EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

Recruiting Company Textron Specialized Vehicles
Primary Location US-Georgia-Augusta
Job Field Information Technology
Schedule Full-time
Job Level Individual Contributor
Shift First Shift
Job Posting 08/24/2023, 8:20:05 AM",3.8,"Textron Specialized Vehicles
3.8","Augusta, GA",10000+ Employees,1923,Company - Public,Aerospace & Defense,Aerospace & Defense,$10+ billion (USD)
48,Data Scientist/Data Engineer - Economic Research,Employer Provided Salary:$121K - $151K,"Why Glassdoor?
Our mission is to help people everywhere find a job and company they love. In the process, we're transforming the workplace experience through the power of transparency and further cementing ourselves as the worldwide leader in employer branding and insights. By choosing a career at Glassdoor, you'll be directly contributing toward our vision for a world where transparency empowers the workforce and motivates companies to become better employers.
Please note: This role may be open to remote hiring. Our office locations are in San Francisco, CA; Chicago, IL; London, UK; and Dublin, Ireland.

About the Role
As a member of the Glassdoor Economic Research team, you will develop and own key public-facing data products, act as the go-to expert on Glassdoor data for our Economic Research team, and conduct original research in close collaboration with Glassdoor's Chief Economist and Lead Economist. This role will work deep in Glassdoor's proprietary data to share scalable insights on workplace communities, employee satisfaction, and the jobs market.
What You'll Do
Your day-to-day responsibilities will include analysis, development, and research using proprietary data to support Glassdoor Economic Research initiatives. Responsibilities and examples of projects may include:
Developing and maintaining a new public-facing data product using Glassdoor data to highlight a topical labor market or workplace trend. Building models in Python or R to support these data products.
Managing the data collection process for Glassdoor's annual Best Places to Work award.
Proposing and making improvements to the methodology and infrastructure for Glassdoor's data-driven awards.
Building dashboards, data sources, and tools that make data more accessible to support readers and/or internal teams.
Writing SQL queries to answer ad hoc media questions about Glassdoor's salaries or ratings data from top-tier media publications.
Working with product and engineering partners to understand what data is available, its definitions and limitations, and how to access it.
The role will primarily focus on developing, maintaining, visualizing and owning internal- & public-facing data products (approximately 70% of time) and 30% independent or collaborative research or support for data requests from internal stakeholders.
What You'll Bring
A commitment to add to our culture of DEI.
2-5 years of experience in a related field like data science, data engineering, decision science, or economics.
Excellent data fluency, including expertise with SQL.
Experience building ETL pipelines and/or deploying production-ready data products.
Strong experience with programming and data analysis using Python (preferred) and/or R. Writes clear, documented, and reproducible code.
Ability to work cross-functionally with product, engineering, and communications partners.
Nice to have: Demonstrated experience with statistics and applied econometrics.
Nice to have: Knowledge of data visualization tools like ggplot2, Tableau, R Shiny, or D3.js.
Compensation and Benefits
Base Salary Range: $120,500 - $150,600
You can learn more about our compensation philosophy here and see salary ranges for all Glassdoor jobs here.
Glassdoor base salaries are targeted to the market 75th percentile for technical roles and the 65th percentile for non-technical roles. In other words, 65-75% of comparable organizations in our industry will pay less.
Annual Bonus Target: 10%
**Bonuses are paid in 6-month intervals, aligning with bi-annual performance reviews
Generous Restricted Stock Units (RSU):
***Restricted Stock Units (RSU) are awarded at hire and may be refreshed annually. Additionally, as a pay-for-performance company, RSU grant awards are presented bi-annually to exceptional performers.
Health and Wellness: 100% employer-paid premiums for employee medical, dental, vision, life, short and long-term disability, select well-being programs, and 80% employer-paid premiums for all dependents.* Generous paid time off programs for birthing and non-birthing parents are provided, along with paid injury/illness leave and paid family emergency leave.
Coverage begins at the start of employment. After 48 months of continuous employment, 100% of all premiums for you and your dependents can be employer-paid!
Work/Life Balance: Open Paid Time Off policy, in addition to 15-20 paid company holidays/year
Investing in Your Future: 401(k) plan with a company match up to $5,000 per year, subsidized fertility and family planning services, and discounted legal assistance services.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands and our compensation philosophy are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Our Company Values and Commitments
Transparency: We are open and honest. We share information – the good and the bad – so we can continuously learn, collaborate and make the right decisions. Pay bands, our compensation philosophy, and employee feedback polls are shared publicly.
Innovation: We actively pursue new and different ways to further Glassdoor's mission. We forge our own path by challenging the status quo. The ultimate goal is not just to change how we operate at Glassdoor, but for every employer to follow our lead!
Good People: We work together with integrity, respect and compassion for one another. We have fun together! We are inclusive, fair and humble while remaining confident. We do the right thing, period.
Grit: We are resilient, inventive and fearless. We see challenges as opportunities. With passion and courage, we come together to get the job done.
Diversity, Equity, and Inclusion: We are dedicated to building a company that is more diverse and representative of society at large. Glassdoor externally publishes our Diversity & Inclusion report and information about our employee population to hold ourselves accountable to our dedication. We also provide programs and resources to build a greater sense of belonging for our employees.
Glassdoor is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Glassdoor is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",4.3,"Glassdoor
4.3","Chicago, IL",501 to 1000 Employees,2007,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
49,Data Engineer,$73K - $110K (Glassdoor est.),"Data Engineer

Posting Date: Aug 23, 2023
Location: Houston, TX, US
Lake Charles, LA, US
Company: Sasol
A job at Sasol is a career made by you, with purpose, development opportunities, benefits to support every stage of life and a working culture that embraces flexibility, diversity, and inclusion. Learn more.

Purpose of Job
The Data Engineer works as part of an agile team, interacts with data scientists/translators and industry experts to understand how data needs to be converted, loaded and presented. Collects, structures, analyses data and ensures quality. Responsible for the definition, conception, construction, and ongoing support of big data platforms and solutions.
Recruitment Description / Key Accountabilities
Integrate IT (Information Technology) and OT (Operational Technology) Systems.
Use programming languages and big data technologies following best practices and standards
Understands latest analytics tools and technologies (open source/ proprietary/ vendor supported) and determines fit with current architecture
Building and maintaining data pipelines that move data between various systems.
Ensuring data quality, integrity and security
Monitoring and optimizing data performance
Collaborate with technical teams to develop and maintain a flexible and sustainable workflow.
Collaborate with Product Owner, Data Scientists, Data Translators, and other Business Stakeholders to implement and deploy scalable solutions in an agile way of working; actively participate in scrum process and meetings with team where required
Assist with communication, improve transparency, radiate information, and address challenges effectively
Formal Education
University Bachelors Degree in Computer Science & Inf Systems or Engineering: Computer
Min Experience
Experience: 5+ relevant years
Long Description
Excellent communication and written skills.
Experience in building complex data pipelines using Cloud Services, REST APIs, SQL Databases. Python and SQL skills are are required. Experience in Azure ADF, Synapse or Databricks are beneficial.
Knowlege of Cloud Computing and Infrastructure, Data Models and Standards, Database Administration, Plant Information Management Systems, ERP Systems, Networking and Protocols, Security and Compliance, Change Managemnt, Problem Solving and Trouble Shooting are a plus.

Sasol (USA) Corporation is an Equal Opportunity Employer and gives consideration for employment to qualified applicants without regard to race, color, religion, creed, age, sex, sexual orientation, gender identity, pregnancy, national origin, disability or protected veteran status, as well as any other characteristic protected by applicable law, regulation or local ordinance. For more information about your rights under the law, see http://www.eeoc.gov/employers/upload/poster_screen_reader_optimized.pdf

Sasol treats work-authorized job applicants equally in recruiting and hiring without regard to their citizenship status or national origin unless required by law, regulation, executive order, government contract, or determination by the Attorney General. Please note Sasol will not sponsor, obtain, and/or petition for temporary visa status (for example, E, F-1, H-1, H-2, L, B, J, or TN) for candidates for this position and/or personnel hired for this position.


Nearest Major Market: Houston",3.9,"Sasol
3.9","Houston, TX",10000+ Employees,1950,Company - Public,Energy & Utilities,"Energy, Mining & Utilities",$10+ billion (USD)
50,Senior Data Center Engineer,-1,"Marco is a one-stop shop for all things business tech. Our employees are “movers and shakers” and our company is always striving to do what’s right. Does this sound like a culture you want to be a part of? We’re hiring a new team member to help take Marco’s technology further – working full-time, Monday - Friday, 8am-5pm.
More about us. We do it all – from copy and print solutions to IT and managed services. We are an organization led by salespeople with 650+ engineers ready to fix any and all issues. We have offices in 12 states and service nationally.
Join our growing team. You won’t regret it.

The Senior Data Center Engineer is responsible for providing quality services and solutions to our internal clients while maintaining a high level of client satisfaction. This position works cooperatively with other teams in a coordinated effort to develop, design, implement and support Marco’s Data Centers, along with the IT environment that depends upon them. While not a leadership position, the Senior datacenter engineer displays leadership qualities and may be called upon to lead others in certain situations.

ESSENTIAL FUNCTIONS:
Lead the overall design and implementation of the technology stack within the Marco Data Centers
Maintain physical access to the Marco Data Centers and be a primary contact for essential services
Perform proactive maintenance and reactive support to the equipment within the Marco Data Centers
Create and maintain documentation for all equipment within the Marco Data Center. Review and assist others with their documentation, offering constructive feedback and pointing out deviations from our policies and standards where necessary.
Report on the health of the Marco Data Centers, perform capacity planning and proactively provide recommendations for future growth
Monitor and provide proactive recommendations for backup systems and disaster recovery processes
Review all systems and make technical/process recommendations for improving efficiency
Assist in defining standards for all systems and processes provided by the Data Center Services group
Administer and maintain a reliable, stable, secure and cost-effective hosting environment to deliver 24x7x365 operational service
Provide escalated remote technical support to Marco Data Center clients. Handle requests with due urgency, empathy, and a spirit of cooperation.
Document your work, including troubleshooting steps, time, and resolution in the service ticketing system.
Ensure your work and the work of others adheres to Marco internal policies and applicable regulatory requirements.
Create and maintain standard templates, documentation, images, policy, and automation that will aid in a consistent and rapid deployment of Marco Data Centers
Research, test and recommend new tools and products to enhance the offering and efficiency of Marco’s Data Centers
Document the procedures, infrastructure and environments and keep updated with changes to the environment
Implement and maintain Data Center system builds that are consistent, optimized for client experience, overall performance and supportability
Set deadlines for projects, then organize your time to meet these deadlines; be willing to put in extra time and effort to meet a time-bound goal when necessary
Mentor other staff while you assist with implementation or utilization within your area of technical specialty
Work multiple projects simultaneously and provide consistent, high-quality results
Assist with implementation and or migration of clients’ existing data and applications
Perform system and application testing to provide functional environment
Provide onsite and or remote technical support to Marco clients during onboarding process
Implement changes and work orders for existing Marco clients
Review customer hosted systems and make technical/process recommendations for improving efficiency
Stay up to date with relevant state-of-the-art technology, equipment, and/or systems
QUALIFICATIONS:
Bachelor’s Degree and 4+ years of experience or equivalent experience
Experience and current certifications or specializations aligning with the technologies used in Marco’s Data Center to include Cisco NX-OS, SAN-OS, IOS, UCS-B, ASAv, EMC VNX, Meraki MX, VMWare vSphere, Citrix XenApp, Zscaler and XenDesktop preferred.
Current high level industry recognized certifications including one or more of the following: CCNP, CCE, VCDX, EMCTA
Solid understanding of the administration of network operating systems and services found in a Data Center environment
Benefits:
We’re not just competitive when it comes to business tech – we’re also pretty proud of what we offer our employees. Our benefits include medical, dental, and vision insurance. We also have paid holidays and vacation, 401k with generous company match, flexible spending accounts, employee purchase program, employer-paid life insurance, voluntary-term life insurance, short and long-term disability, critical illness and accident benefits, and pet insurance. Yes, we care about your furry family too.

all benefits are dependent on employment status
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities
Applicant Labor Law Posters",4.0,"Marco Technologies LLC
4.0",Remote,1001 to 5000 Employees,1973,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
51,Data Science Engineer,-1,"Operations Technology brings data accessibility to the organization, with the overarching goal that all business problems have data driven answers.
Within the Operations Technology group, the Data Science Engineer’s purpose is to design, develop, and maintain data infrastructure, pipelines, and workflows. They are responsible for developing and merging modeling to ensure it stays consistent with data flowing across the organization.
They work closely with platform data owners, data analysts and consumers, and other stakeholders to ensure the data is properly collected, stored, processed, and analyzed to drive informed business decisions.
This role will become more ML focused as the data warehouse matures, with the added implementation of ML and advanced analytics models
SKILLS AND ABILITIES:
Proficiency in Azure Data Warehousing utilizing Azure Synapse (Data Factory also acceptable)
Strong understanding of database technologies and SQL queries
Strong experience with ETL processes, data integration, and data modeling
Experience with RESTful API connections in the Azure environment
Experience with cloud-based data storage and computing services
Experience with Azure networking
Excellent problem-solving and analytical skills
Experience with data visualization tools such as Tableau or Power BI
Excellent communication and collaboration skills
Ability to work independently and prioritize tasks in a fast-paced & dynamic environment
What sets you apart:
Certified in SQL, SQL Analytics, R, Python, Power BI
Experience with ML in Azure: Spark MLib, SynapseML, R, Python Anaconda, or the like
Experience with data queries to cloud ERP (Netsuite), CRM (Hubspot), Cloud Storage (Sharepoint, MS Project Online), On Prem Storage (SQL)
TREW’s story:
Business gets done working together. Successful business happens when trusted partners work together, to win together. At TREW we know that our customers buy solutions and technology built by people. With over 400 team members, we work fearlessly every day to do the right thing, even when no one is watching. From seasoned professionals to undergraduate co-ops, our team members enjoy seeing the impact of their contributions every day.
Trew/Hilmot/TKO is an equal opportunity employer. Applicants will be considered for employment without regard to age, race, creed, color, national origin, ancestry, marital status, affectional or sexual orientation, gender identity or expression, disability, nationality, sex, or veteran status.",3.5,"TREW LLC
3.5",United States,51 to 200 Employees,-1,Company - Private,Taxi & Car Services,Transportation & Logistics,$25 to $100 million (USD)
52,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
53,Data Engineer Azure databricks,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer - Databricks
For: CDW (Direct Client)
W2, Corp to Corp, W2 Contract Okay
Fully Remote
`
DESIRED SKILLS AND ABILITIES
Key Responsibilities
You will design, build and test cross cloud and on-premise data pipelines and leverage Azure Databricks for data processing.
Collaborate with cloud architects, tech leads to facilitate on-premise to Databricks migration
You should understand data pipelines and modern ways of automating data using cloud-based and on-premise technologies
Develop reference architectures and design pattern library for typical Cloud based solutions implementations
Advise on Cloud project set up, security and role based access implementation, and network optimizations
Qualifications:
2+ years with SQL and database table design – able to write structured and efficient queries on large data sets
1-2 years on Databricks on any cloud platform (AWS, Azure)
1-2 years working on AWS/Azure cloud
1-2 year’s design and/or implementation of enterprise applications
Experience in “migrating” on premise workloads to one or more industry leading public cloud(s)
Experience in AGILE development, SCRUM and Application Lifecycle Management (ALM) with scripting experience in Python and shell.
Hands-on experience in the full life-cycle of software development or methodology using Agile Scrum/Kanban etc.
Able to work with agile teams as they perform feature level design, development, testing, and performance analysis
Databricks certification is a plus.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Schedule:
8 hour shift
Work Location: Remote",-1,Data Ninjas Inc.,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
54,Big Data Engineer,Employer Provided Salary:$48.00 - $52.00 Per Hour,"Greetings from KonnectingTree Inc.,
We are actively looking for a candidate with below mentioned experience,
Role : Bigdata Engineer
Location : Hybrid ( Bloomington, MN)
Experience : 3-5 years
Job Description:
* Good experience in build and deployment of Big data applications using Pyspark
* Good experience in Python.
* Experience in Hadoop file structure.
* Good Communication and team player
Job Type: Contract
Salary: $48.00 - $52.00 per hour
Experience level:
3 years
4 years
Schedule:
8 hour shift
Ability to commute/relocate:
Bloomington, MN 55425: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",-1,KONNECTINGTREE INC,"Bloomington, MN",-1,-1,-1,-1,-1,-1
55,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
56,Data Performance Engineer,Employer Provided Salary:$55.00 Per Hour,"Data Performance Engineer
Only candidate with years of experience: +8
Those authorized to work in the U.S. are encouraged to apply.
Remote
Interview Process: 3 Rounds of Zoom Video Interview
Candidate must have Data Performance Engineer experience.
Skill and Experience Required
· Experience in consuming large data volume
· Advance SQL skills
· Optimize strategy process for data handling for aggregation tables (Minimize processing time for large data volume)
· Experience in data loading with complex model for data and batch orchestration
· Throughput understanding, monitoring for efficient data loading
· Identifying bottlenecks and resource investigation
· Experience in periodic ETL Code maintenance and setting up SLA for cross functional team
· Implementing necessary techniques to improve ETL performance in Both DB and Application
· Improving query performance by turning ad re-evaluating data model
· Identifying dependency impact on both data and Application
Duties and responsibilities
· Performance Analysis and documenting factors (bottleneck, memory handling and inefficiency area which needs improvement)
· Identifying and documenting execution Benchmarks and Metrics
· Optimization on application and database
· Validation techniques for data and Orchestration
· Documentation for Optimization and Operations
· Implementation Alert mechanism in case of potential impact to downstream
· Collecting and documenting the Metrics for each ETL application
Job Type: Contract
Pay: $55.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Big data: 8 years (Required)
SQL: 8 years (Required)
ETL: 8 years (Required)
consuming large data volume: 8 years (Required)
Work Location: Remote",-1,Luttechub,Remote,Unknown,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
57,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Data Engineer
Candidate must have AWS (Redshift, Glue, S3), Spark, Phython, Scala and Should be expert in SQL
Basic qualifications
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
Design, implementation and management of data processing systems dealing with large data sets.
Proficiency in, at least, one modern programming language such as Python, Java or Scala
Hands-on experience and advanced knowledge of SQL.
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and cloud computing
Experience working with Open-Source Big Data tools (Hadoop, Spark, Hive, Presto and etc.)
Preferred qualifications
Experience in working and delivering end-to-end projects independently
Relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics
Experience working with AWS Big Data Technologies (AWS Glue, EMR, Athena, Ec2, Redshift and S3 etc.)
Experience working with Oracle, PostgreSQL, SQL Server or MySQL.
Proven track record of delivering a big data solution
Experience working with both Batch and Real Time data processing systems
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Expected hours: 40 per week
Compensation package:
1099 contract
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Austin, TX 78703: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 8 years (Required)
Python: 8 years (Required)
AWS: 5 years (Preferred)
Work Location: Hybrid remote in Austin, TX 78703",-1,PRISM IT LLC,"Austin, TX",-1,-1,-1,-1,-1,-1
58,Data Engineer (Contract),$77K - $111K (Glassdoor est.),"About RevOpsforce:
At RevOpsforce, our mission is to drive sustainable revenue growth for our clients by leveraging data-driven insights, technology, and process optimization. We strive to deliver measurable results that exceed expectations, while building strong partnerships with our clients and continuously improving our own capabilities. Our Expert Network is composed of the highest skilled and certified professionals in revenue operations. We leverage this network to solve our clients' most complex operational challenges.
Type: Contract
Job Description:
We are seeking a talented and experienced Data Engineer to join our growing team. As a Data Engineer at our company, you will be responsible for designing, building, and maintaining the data pipelines that power our data-driven applications. In this role, you will work closely with our data science and engineering teams to develop high-quality, efficient data pipelines that enable us to derive insights from vast amounts of data. You should be comfortable working with large volumes of structured and unstructured data, and have a strong understanding of SQL, Python, and other programming languages.
Responsibilities:
Design, build, and maintain data pipelines to support data-driven applications and analytics
Analyze data to identify trends and patterns
Collaborate with data scientists and engineers to develop data-driven solutions
Write and maintain documentation for data pipelines
Monitor and optimize data pipelines for performance and efficiency
Qualifications:
Bachelor's or Master's degree in a related field (e.g. Computer Science, Data Science, Engineering)
3+ years of experience in a data engineering role
Strong skills in SQL and at least one programming language (e.g. Python, Java, C++, etc.)
Experience with big data technologies (e.g. Hadoop, Spark, etc.) and cloud platforms (e.g. AWS, Azure, GCP)
Strong problem-solving and analytical skills
Excellent communication skills and ability to work in a team environment
Join our dynamic and forward-thinking team to make a significant impact on our clients' revenue growth journey!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Website is coming soon and will be located at www.revopsforce.com",-1,RevOpsforce,"Austin, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
59,Data Engineer II,$90K - $119K (Glassdoor est.),"Data Engineer II
Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What’s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.
Join us to do the best work of your career and make a profound social impact as a Data Engineer on our Growth Marketing team in Round Rock, Texas, Hopkinton, Massachusetts, or Remote United States
What you’ll achieve
As a Data Engineer II on a growing team, you will bring in your industry experience to develop, deploy, and maintain productionized data pipelines that support the training and productionization of machine-learning models.
You will:
Build, test, and implement data pipelines using tools like Apache Airflow, Spark, and Python for the training and inferencing of machine-learning models
Ensure data quality, monitor data pipelines, and troubleshoot production issues
Document metadata, data models, data pipelines and APIs to help data science teams understand and leverage data assets.
Work with data scientists and business analysts to understand data requirements
Take the first step toward your dream career
Every Dell Technologies team member brings something unique to the table. Here’s what we are looking for in this role:
Essential Requirements
U.S. citizen residing in the U.S.
Bachelor’s Degree in Computer Science, Engineering, IT, or equivalent professional experience
1 to 2 years experience developing production-quality Python and performant SQL, working with data warehouses, JSON, Apache Hive, Hadoop or Spark, or related technologies
1 to 2 years experience with orchestration tools, such as Apache AirFlow
Experience with monitoring tools and observability
Who we are

We believe that each of us has the power to make an impact. That’s why we put our team members at the center of everything we do. If you’re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we’re looking for you.

Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us.

Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. Read the full Equal Employment Opportunity Policy here.
Job ID:R227305

Dell’s Flexible & Hybrid Work Culture

At Dell Technologies, we believe our best work is done when flexibility is offered.

We know that freedom and flexibility are crucial to all our employees no matter where you are located and our flexible and hybrid work style allows team members to have the freedom to ideate, be innovative, and drive results their way. To learn more about our work culture, please visit our locations page.",4.2,"Dell Technologies
4.2","Round Rock, TX",10000+ Employees,1984,Company - Public,Information Technology Support Services,Information Technology,$10+ billion (USD)
60,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
61,2024 Data Engineer I,-1,"2024 Data Engineer I
316114

Textron Specialized Vehicles Inc. is a leading global manufacturer of golf cars, utility and personal transportation vehicles, snowmobiles, side-by-sides, all-terrain vehicles, professional turf-care equipment, and ground support equipment. Textron Specialized Vehicles markets products under several different brands. Its vehicles are found in environments ranging from golf courses to factories, airports to planned communities, and theme parks to hunting preserves.

Responsibilities:
Analyze, manipulate, or process large sets of data using statistical software.
Partner with functional and technology teams to develop and maintain mappings/rules between business applications & business data and interfaces
Identify business problems or management objectives that can be addressed through data analysis.
Document data standards, semantic layer mapping, data flows and conceptual flows.
Create graphs, charts, or other visualizations to convey the results of data analysis using specialized software.
Perform statistical analysis to provide recommendations on best courses of action to business leaders.
Design, implement, and maintain ELT pipelines that provide transformed data from source systems to analytics tools.
Collaborate with data professionals in maintaining and building data models for the enterprise data lakehouse.
Collaborate with business users to ensure data is available for self-service analytics.
Adhere to all IT change control and security processes when implementing BI solutions and clearly communicate project status, risks, and issues to key stakeholders.


Qualifications:
Education: Bachelor’s degree in computer science, engineering, mathematics, or similar field required
Years of Experience: P1- 0+ years of experience required, previous internship experience preferred // P2-2 or more years of related experience required
Software Knowledge: Proficiency in SQL; experience using Power BI or similar visualization tools; knowledge of dimensional modeling and star schema techniques preferred
Experience with the following resources: Azure Data Factory, Databricks Delta Lake, Azure Log Analytics, Qlik Replicate (formerly Attunity), and/or Azure Synapse)
EEO Statement
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.

Recruiting Company Textron Specialized Vehicles
Primary Location US-Georgia-Augusta
Job Field Information Technology
Schedule Full-time
Job Level Individual Contributor
Shift First Shift
Job Posting 08/24/2023, 8:20:05 AM",3.8,"Textron Specialized Vehicles
3.8","Augusta, GA",10000+ Employees,1923,Company - Public,Aerospace & Defense,Aerospace & Defense,$10+ billion (USD)
62,AI/ML Data Automation Engineer,$88K - $125K (Glassdoor est.),"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
63,Apple Music - Data Engineer,-1,"Summary
Posted: Aug 18, 2023
Weekly Hours: 40
Role Number:200496819
This team is more than a group of engineers — it’s a group of music lovers. That passion has made Apple Music the world’s most complete music experience, with over 60 million songs, thousands of playlists, and daily selections from music experts for 115 countries. The team’s data-driven engineers focus relentlessly on the customer experience by running worldwide experiments and analyzing usage and latency, while collaborating with Apple’s product groups. As a result, someone can use the Shazam app to identify an intriguing song in a café in the morning, add it to their playlist from Apple Watch, listen to it through their AirPods on their commute, and share it with their family on HomePod at dinnertime. And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love. Apple Music is a huge part of Apple’s business because it’s a big part of people’s lives.
Key Qualifications
Expertise in designing, implementing and supporting highly scalable data systems and services in Scala
Experience with distributed computing technologies such as Hadoop MapReduce, Spark/Spark-SQL, YARN/MR2
Expertise building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Solid understanding of data-modeling and data-architecture optimized for big data patterns, such as efficient storage and query on HDFS Experience with distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues
Experience with low-latency NoSQL datastores, and traditional relational databases is a plus
Description
We are seeking a hardworking, performance-savvy, engineer to build out the big data platform and services, which power many of these customer features — existing and new. As a core member of the Data Engineering team you will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability. This will include crafting systems to model, ingest, process and compute large-scale, mission-critical data across Apple Music. High-throughput and reliability are essential. The computed datasets are produced for internal reporting used by executives and they are also shared across many teams within the ASE organization, such as the Search team, Recommendation team, AB team, Marketing team. You will also enjoy the benefits of working in a fast growing business where you are encouraged to “Think Different” to solve very interesting technical challenges and where your efforts play a key role in the success of Apple’s business. This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users. Come join us to help deliver the next amazing Apple product!
Education & Experience
BS CS or equivalent work experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $131,500 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",4.2,"Apple
4.2","Seattle, WA",10000+ Employees,1976,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
64,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
65,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
66,Data Engineer (Fully Remote),$56K - $91K (Glassdoor est.),"Position Description: Who We Are
NIP Group www.nipgroup.com is a rapidly growing insurance service provider of specialty programs for commercial insurance brokers and carriers providing underwriting, distribution, product management, administration, and risk management services primarily by acting as a managing underwriter (MGA) and a Reciprocal Services Manager (RSM).
Our culture is one that empowers and encourages employees to be innovative, collaborative, and forward-thinking. If you are interested in being a part of a growing, entrepreneurial spirited organization, wed love to hear from you!

About the Position
Reporting into the Senior Actuary, you will own the continuous improvement culture for designing, building, and maintaining the infrastructure and systems required for collecting, storing, processing, and analyzing large volumes of data. As a Data Engineer, you will play a crucial role in ensuring that data is readily available, accessible, and usable by other data professionals, analysts, and stakeholders within an organization. You will be responsible for managing a data Lake / warehouse and implementing efficient data integration processes with internal and external systems.

What Youll Do
Design and maintain a scalable and secure Azure data warehouse.
Build data pipelines: Create and manage data integration processes, including data extraction, transformation, and loading (ETL) from various sources into the data warehouse.
Develop and implement data management strategies to ensure data quality, consistency, and accuracy.
Collaborate with cross-functional teams, including to identify data requirements and develop data models.
Develop, adhere, and enforce data governance policies and procedures to ensure compliance.
Monitor data quality and develop response mechanisms to address problems.
Collaborate with IT teams to ensure the availability, reliability, and performance of data systems and infrastructure.
Develop documentation related to data management processes, procedures, and data dictionaries.
Stay up to date with industry trends and advancements in data management technologies and techniques.
What Were Looking For
Education/Experience
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Proven experience working as Data Engineer or similar role, preferably in the insurance or financial industry.
Strong knowledge of data management principles, data quality assurance, and data governance practices.
Proficiency in data warehousing concepts, and ETL processes.
Experience with data integration tools and technologies (e.g., SQL, Python, ETL frameworks).
Solid understanding of regulatory requirements related to data privacy and security (e.g., GDPR, HIPAA).
Excellent problem-solving and analytical skills, with the ability to analyze complex data sets.
Strong communication and interpersonal skills to collaborate with cross-functional teams and communicate complex ideas effectively.
Technical Competencies
Proficiency in Microsoft services in Azure, Data Factory, Data Lake/warehouse storage.
Ability to design and implement data integration solutions using Azure Data Factory.
Expertise in designing and optimizing data warehousing solutions Azure SQL Data Warehouse.
Familiarity with Azure Data Lake Storage and data processing using Azure Data Lake Analytics.
Data Governance and Security: Understanding of data governance principles and compliance requirements within Azure, including access control and data privacy.
Monitoring and Troubleshooting: Skills in monitoring, optimizing, and troubleshooting data pipelines within Azure Data Factory.
Proficiency in scripting languages like PowerShell or Python for automation tasks in Azure.
Strong communication and collaboration skills to work effectively with cross-functional teams.
Proactive in staying updated with the latest Azure data services and advancements.
What You'll Receive
At NIP Group, we recognize there are many factors that contribute to your overall satisfaction both at work, and in your personal life. For that reason, we provide a perfect mix of compensation, benefits, company culture, and resources to ensure your everyday happiness. Below are some benefits that youll receive.
Competitive compensation to reward you for your hard work every day.
Progressive Paid-Time Off program for you to enjoy time out of the office, including time off for volunteering and life events.
Group Medical, Dental, Vision and Life insurance to encourage a healthy lifestyle.
Pretax Health and Dependent Care Spending Accounts to ease taxes on spending.
Discounts in retail and entertainment.",3.0,"NIP Group, Inc.
3.0","Woodbridge, NJ",51 to 200 Employees,-1,Company - Private,Insurance Agencies & Brokerages,Insurance,Unknown / Non-Applicable
67,Data Warehouse & Artificial Intelligence Engineer,$83K - $120K (Glassdoor est.),"Job Description:

Design, Implement and Maintain data warehouse.

Duties and Responsibilities:

Develop, test, and deploy extraction, transformation and loading (ETL) routines to load the data warehouse using SSIS, stored procedures, etc.
Develop analytics including reports, dashboards, and data visualizations
Troubleshoot BI-related problems and tune for performance
Implement and oversee a company-wide data governance program to support master data management and resolve data definition discrepancies among stakeholders
Plan the technical approach for implementing analytics strategy across multiple channels
Plan and implement activities to maintain data confidence
Provide technical direction and technical support for all member of the team
Architect and implement analytics solutions to meet data collection and reporting needs.
Work with team to recommend analytic software
Provide general troubleshooting assistance for analytic reports and break fixes
Wrangle data (a.k.a. data engineering) from a wide variety of complex data sets large volumes of data into normalized and enriched data sets.
Assemble and evaluate data such that new insights, solutions, and visualizations can be derived.
Perform data discoveries to understand data formats, source systems, file sizes, etc. and engage with internal and external business partners in this discovery process.
Deliver data acquisition, transformations, normalization, mapping, and loading of data into a variety of data models.
Collaborate extensively with Technology to design data ingestion, data models, and automated operational metrics for consistently high quality data
Supports the research, analysis and presentation of information by: producing reports, compiling and summarizing information, producing supporting documentation and exhibits, and verifying information received from external sources.
Assists other associates with preparation of reports and use of information systems, software and related sources of information. Trains other users on report preparation and database access.
Perform other duties as assigned.

Required Knowledge, skills and abilities:

Ability to manage change successfully in a fast-paced environment
A passion for learning and the ability to do so on the fly
Must be strategic, dynamic, and detail-oriented. The ideal candidate is an analytical thinker, a self-starter and a quick learner.
Ability to design and architect method of extracting data from its original source to another location (i.e. a data lake) to make that data consumable for a variety of downstream purposes such as applications, visualizations, analytics and other related data science activities.
Basic understanding of data science
Ability to transforming and map one ""raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.
Exceptional ability to clearly and effectively communicate with IT peers and users at all levels of technical and business acumen.
Strong time management skills and multi-tasking capabilities
Ability to understand business rules governing financial and budgeting process
Exceptional oral and written communication skills
Self-motivated, ability to set priorities and meet deadlines

Education and Experience:

Bachelor's degree in Computer Science, Business, Economics, Information Science, or Analytics
Minimum of 3 years of experience with data analytics across multiple channels
Experienced with the technologies required to implement Analytics across multiple channels, (for example, Data Layer, Custom JavaScript, JAVA, AJAX, HTML, Objective-C, Swift)
Knowledge of Tableau is a plus
Familiarity with Microsoft Power BI
Extensive experience with ETL and/or other Big Data processes.
Contract manufacturing experience is a plus.

Foxconn Assembly, LLC is an Equal Opportunity Employer (EOE). All qualified candidates will receive consideration without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or marital status in accordance with applicable federal, state and local laws.
Foxconn Assembly, LLC participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

NbXn0DyWY6",3.4,"Foxconn Corporation
3.4","Houston, TX",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$5 to $25 million (USD)
68,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
69,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
70,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
71,Senior Data Engineer,Employer Provided Salary:$87K - $180K,"Oakland, California; Atlanta, Georgia; Irving, Texas
Regular Employee Full-Time
R-2023-08-81

Who We Are:
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners - in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
Pandora
Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. Pandora provides consumers with a uniquely-personalized music and podcast listening experience with its proprietary Music Genome Project® and Podcast Genome Project® technology. Pandora is available through its mobile app, the web, and integrations with more than 2,000 connected products.
How you’ll make an impact:
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What you’ll do:
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What you’ll need:
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to pay attention to details and be organized.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $87,200 to $180,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.

Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-08-81",3.6,"Adswizz
3.6","Oakland, CA",51 to 200 Employees,2007,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
72,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
73,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
74,Lead Data Engineer - CCRD,Employer Provided Salary:$76K - $152K,"Make banking a Fifth Third better®

We connect great people to great opportunities. Are you ready to take the next step? Discover a career in banking at Fifth Third Bank.
GENERAL FUNCTION:
The data engineer designs and builds platforms, tools, and solutions that help the bank manage, secure, and generate value from its data. The person in this role creates scalable and reusable solutions for gathering, collecting, storing, processing, and serving data on both small and very large (i.e. Big Data) scales. These solutions can include on-premise and cloud-based data platforms, and solutions in any of the following domains ETL, business intelligence, analytics, persistence (relational, NoSQL, data lakes), search, messaging, data warehousing, stream processing, and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions, elevating concerns, and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues, and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite, achieves results by consistently identifying, assessing, managing, monitoring, and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design, Development, and Support of data solutions, APIs, tools, and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams, Enterprise architecture, infrastructure, information security, and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design, performance, integration, security, etc.
Conduct research and Development based on current trends and technologies related to the banking industry, data engineering and architecture, data security, and related topics.
Work with developers to Build CI/CD pipelines, Self-service Build tools, and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed, including legacy system replacement, Monitoring and analytics improvements, tool Development, and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE, SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management, business and IT sponsors, and technical resources in language that is appropriate for each group.
At least 6 years of related experience, including at least 4 years in a hands-on software development role.
Significant experience with at least one major RDBMS product.
Experience working with and supporting Unix/Linux and Windows systems.
Proficient in relational database modeling concepts and techniques.
Solid conceptual understanding of distributed computing principles.
Working knowledge of application and data security concepts, best practices, and common vulnerabilities.
Experience in one or more of the following disciplines preferred: big data technologies and distributions, metadata management products, commercial ETL tools, Bi and reporting tools, messaging systems, data warehousing, Java (language and run time environment), major version control systems, continuous integration/delivery tools, infrastructure automation and virtualization tools, major cloud, or rest API design and development.
Previous experience working with offshore teams desired.
Financial industry experience is a plus.
Lead Data Engineer - CCRD
Total Base Pay Range 76,000.00 - 152,400.00 USD Annual
LOCATION - Virtual, Florida 00000
Fifth Third Bank, National Association is proud to have an engaged and inclusive culture and to promote and ensure equal employment opportunity in all employment decisions regardless of race, color, gender, national origin, religion, age, disability, sexual orientation, gender identity, military status, veteran status or any other legally protected status.",3.7,"Fifth Third Bank
3.7",Remote,10000+ Employees,1989,Company - Public,Investment & Asset Management,Financial Services,$5 to $10 billion (USD)
75,Data Engineer (multiple openings) - IHM,Employer Provided Salary:$88K - $157K,"Discover. A brighter future.
With us, you’ll do meaningful work from Day 1. Our collaborative culture is built on three core behaviors: We Play to Win, We Get Better Every Day & We Succeed Together. And we mean it — we want you to grow and make a difference at one of the world's leading digital banking and payments companies. We value what makes you unique so that you have an opportunity to shine.

Come build your future, while being the reason millions of people find a brighter financial future with Discover.

Job Description:
Employer: DFS Corporate Services LLC

Job Title: Data Engineer (multiple openings)

Job Location: Riverwoods, Illinois

Job Type: Full Time

Duties: Responsible for designing, developing, testing, and maintaining complex data solutions for the product. Mentor and influence peers to achieve commitments on data solutions on time and with quality. Telecommuting and/or working from home may be permissible pursuant to company policies.

Requirements: Employer will accept a Bachelor's degree in Computer Science, Computer Engineering, or a related field and 3 years of experience in Senior Data Engineer; ODI/DW/Batch Developer or related occupation.

Position required skills: Three (3) years of experience in the job offered or related occupation: working with Agile software development methodology and Agile management tools, including Jira; utilizing modern engineering practices, including the design and development of ETL data integration solutions in data warehouse environments; working with relational databases and Cloud-based technologies; utilizing design and architecture experience on large-scale ETL solutions; and working with Enterprise Data warehouse data models and dimensional modeling concepts, source to target mapping, and data integration architecture.

Position eligible for incentives under Employee Referral Program.

Rate of Pay: The base pay for this position generally ranges between $88,150.00 to $157,000.00. Additional incentives may be provided as part of a market competitive total compensation package. Factors, such as but not limited to, geographical location, relevant experience, education, and skill level may impact the pay for this position. We also offer a range of benefits and programs based on eligibility. Learn more at MyDiscoverBenefits.com .

QUALIFIED APPLICANTS: Please apply directly through our website by clicking on “Apply Now.” No calls. Equal Opportunity Employer/disability/vet.

What are you waiting for? Apply today!

All Discover employees place our customers at the very center of our work. To deliver on our promises to our customers, each of us contribute every day to a culture that values compliance and risk management.

Discover is committed to a diverse and inclusive workplace. Discover is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or other legally protected status. (Know Your Rights)",3.9,"Discover Financial Services
3.9","Riverwoods, IL",10000+ Employees,1985,Company - Public,Banking & Lending,Financial Services,$1 to $5 billion (USD)
76,Data Engineer,Employer Provided Salary:$30.05 - $44.13 Per Hour,"AMAZING BENEFITS PACKAGE!
Medical, Dental, & Vision Insurance
Paid Time Off (PTO) – up to 6 weeks of PTO per year
Childcare Stipend – contribution of $5,000 per year to the employee’s dependent care spending account
Loan Reimbursement - reimbursement for student loan payments up to $5,250 per year
Flexible Work Arrangements - opportunity to work a modified schedule, work part-time, or work from home
Retirement Plans - organization matches the employee’s contribution, up to 6% of gross wages
above benefits dependent upon eligibility criteria
Click here for more detail about the benefits package!
Pay Range: $30.05 - $44.13 USD hourly.

Purpose of Job
Improves the overall health of the communities we serve by providing superior customer service and advanced technical support to ensure quality patient care and user satisfaction as follows:
Essential Duties and Responsibilities
Designs and implements databases, ETL routines (using SSIS), stored procedures, and OLAP solutions in addition to providing support and enhancements to existing data solutions.
Designs and builds normalized and denormalized database solutions using healthcare industry best practices for data warehousing for specific client requirements, using Microsoft SQL Server Database programming (stored procedures and database design).
Works on multiple concurrent deadline-driven projects while ensuring data quality and meeting service level agreements.
Builds and maintains cubes (internal and client-facing).
Optimizes database schemas, queries, cubes, and reports, implementing complex logic requirements.
Conducts tuning reviews / assessments (DB and SQL tuning), reporting, and query monitoring.
Determines, enforces, and documents database policies, procedures, and standards.
Performs other duties as assigned, including supporting the CHAS Health Mission and Core Values.
Qualifications
Education/Experience: Associate’s degree or commensurate experience in a technical field required. Prior experience in database development using Microsoft SQL server, SSIS and SSAS preferred; understanding performance, deployment, configuration, security, migration, and troubleshooting; as well as proficiency in building and optimizing SQL queries preferred. Previous experience building and supporting business intelligence/data warehousing solutions; and knowledge of healthcare data models preferred.
Skills: Excellent logical and problem-solving abilities, verbal and written communication skills required. Ability to work independently in a self-directed environment, contribute to a team, maintain a positive attitude, demonstrate very high attention to detail, and a commitment to quality while working in a high availability environment is required. Commitment to supporting a safe, respectful, equitable, and inclusive environment required. Valid drivers’ license and insurance required.
Physical Demands
Required to stand, sit, and be mobile up to two-thirds of the time. Required to read from text and computer screen over two-thirds of the time. Climbing or balancing, stooping, kneeling, or crouching occurs less than one-third of the time. Communicating occurs constantly throughout the day. Lifting occurs about half the time up to 10 lbs. and less than one-third of the day from 25-40 lbs. Rarely is there a need to lift more than 41 lbs.",3.8,"CHAS Health
3.8","Spokane, WA",1001 to 5000 Employees,1994,Nonprofit Organization,Health Care Services & Hospitals,Healthcare,$25 to $100 million (USD)
77,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
78,Data Engineer,-1,"Hummingbird is a remote-first, fully distributed team united by the shared mission of helping fight financial crime. Since our launch in 2017, we’ve helped major financial institutions and tech-savvy trailblazers alike (e.g. Stripe, Affirm, etc.) orchestrate their compliance programs through our thoughtfully designed, intuitive SaaS product. We believe finding and stopping financial crime is a problem rooted in code, language and design, so we built the product that the heroes doing this work deserve.
We are customer-obsessed, and we love building and shipping great products. We set a high bar, challenge our assumptions, seek diverse opinions, and support each other to do our best work.
We do our best to write inclusive, descriptive and accurate job descriptions, but we’re not always perfect. If you’re interested in the role, we’d love to hear from you even if you don’t feel like you meet everything we’re looking for. We’re always iterating and improving, and it’s possible that your experience is even more impactful than we could have imagined.

About the Role
We are looking for a driven data engineer to join our team and champion the use of data at Hummingbird. Data plays a crucial role in our mission to fight financial crime and you will help us find new and innovative ways to leverage it to provide powerful tools for our users, and allow us to better understand the usage of our product. Hummingbird is uniquely positioned at the intersection of financial technology, security, policy, and law enforcement and as such we have built up a one of a kind data set that we can now use to give our customers the edge in their efforts to stay ahead of criminals. As an employee at a small startup, you will have the opportunity to wear many hats, working from product discovery through implementation.
What you’re looking to do:
Level up our use of data to make better decisions, build more powerful features, and fight financial crime
Leverage Infrastructure as Code (IaC) to manage and deploy infrastructure that supports a variety of different projects, such as data replication and orchestration for machine learning workflows
Build new data pipelines for ingesting data into our data warehouse via both batch and streaming architectures
Work closely with data science to enable us to build products that benefit our customers while keeping compliance and security at the forefront
Achieve goals through a combination of independent building, educating your peers, and influencing others to contribute towards your vision
What we’re looking for:
A data engineer with a history of taking projects from the earliest stages through successful rollout to production
Someone who is excited by the prospect of pioneering data as a practice at a fast growing startup and who is unafraid to dig in to discover what is possible
A flexible self starter that will cut across organizational lines to understand the business and identify the most valuable work
An engineer who brings a pragmatic approach to problem solving, favoring simplicity and shortening delivery cycles
Experience building data pipelines for sensitive data, including best practices for de-identification and data security
Experience deploying infrastructure via terraform or a similar infrastructure as code tool
Expertise in SQL and one or more programming languages, especially python
What’s in it for you:
The chance to help build from the ground up. The hires we’re making now are foundational to our growth as a company, so you will have an opportunity to help shape the future of Hummingbird.
Competitive compensation including cash and equity.
Remote-first, fully distributed company with flexible working hours.
Awesome health, vision & dental benefits, and 401k.
Safe, respectful & comfortable work environment with colleagues and leadership who prioritize diversity, equity, inclusion, and belonging.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please don't hesitate to contact us to request accommodation.",-1,Hummingbird,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
79,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
80,Data Engineer - Data Transfer Project,$80K - $120K (Glassdoor est.),"Do you bring experience managing, modelling and extrapolating genomic or clinical data? Are you interested in supporting a major collaboration partnership?
As part of the SOPHiA GENETICS & MSK collaboration, we are looking for a new Data Engineer / Bioinformatics Technology Manager.
The collaboration agreement will blend the power of SOPHiA GENETICS’ large, global network and deep expertise in predictive algorithms with MSK’s clinical expertise in cancer genomics.
You will work on will work on and prioritize matters implementing, operationalizing, facilitating and managing secure and compliant access to data as part of the partnerships,
This is a remote role, in vicinity of Boston or New York.
Your Mission
Drive analysis of clinical and high-throughput biomedical data,
Query clinical DBs to extract cohort information by tumor type, treatment, and other meta data such as existence of genomic data or biological samples
Links disparate data such as genomic, imaging, or proteomics, to generate multi-modal data sets
Design and develop software, data ETL pipelines and scripts, leveraging databases and data lake technologies as part of bioinformatics and biomedical informatics data storage, visualization, processing, and analysis systems
Requirements
2-3 Years experience within Data Engineering
Direct exposure to clinical, genomic or biomedical metadata
Working knowledge of SQL
Previous experience with Data Extraction and Script creation
Benefits
The Process:

Apply now with your CV and any supporting information.
Suitably qualified candidates will be invited through an interview and screening process where you will speak with members of our Talent Acquisition Team, the hiring leader alongside key colleagues and stakeholders from across the business.
Shortlisting & Interviews from 4th September
Contract type: Permanent, Full-time",3.9,"SOPHiA GENETICS
3.9","New York, NY",201 to 500 Employees,2011,Company - Public,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
81,Senior Data Center Engineer,-1,"Marco is a one-stop shop for all things business tech. Our employees are “movers and shakers” and our company is always striving to do what’s right. Does this sound like a culture you want to be a part of? We’re hiring a new team member to help take Marco’s technology further – working full-time, Monday - Friday, 8am-5pm.
More about us. We do it all – from copy and print solutions to IT and managed services. We are an organization led by salespeople with 650+ engineers ready to fix any and all issues. We have offices in 12 states and service nationally.
Join our growing team. You won’t regret it.

The Senior Data Center Engineer is responsible for providing quality services and solutions to our internal clients while maintaining a high level of client satisfaction. This position works cooperatively with other teams in a coordinated effort to develop, design, implement and support Marco’s Data Centers, along with the IT environment that depends upon them. While not a leadership position, the Senior datacenter engineer displays leadership qualities and may be called upon to lead others in certain situations.

ESSENTIAL FUNCTIONS:
Lead the overall design and implementation of the technology stack within the Marco Data Centers
Maintain physical access to the Marco Data Centers and be a primary contact for essential services
Perform proactive maintenance and reactive support to the equipment within the Marco Data Centers
Create and maintain documentation for all equipment within the Marco Data Center. Review and assist others with their documentation, offering constructive feedback and pointing out deviations from our policies and standards where necessary.
Report on the health of the Marco Data Centers, perform capacity planning and proactively provide recommendations for future growth
Monitor and provide proactive recommendations for backup systems and disaster recovery processes
Review all systems and make technical/process recommendations for improving efficiency
Assist in defining standards for all systems and processes provided by the Data Center Services group
Administer and maintain a reliable, stable, secure and cost-effective hosting environment to deliver 24x7x365 operational service
Provide escalated remote technical support to Marco Data Center clients. Handle requests with due urgency, empathy, and a spirit of cooperation.
Document your work, including troubleshooting steps, time, and resolution in the service ticketing system.
Ensure your work and the work of others adheres to Marco internal policies and applicable regulatory requirements.
Create and maintain standard templates, documentation, images, policy, and automation that will aid in a consistent and rapid deployment of Marco Data Centers
Research, test and recommend new tools and products to enhance the offering and efficiency of Marco’s Data Centers
Document the procedures, infrastructure and environments and keep updated with changes to the environment
Implement and maintain Data Center system builds that are consistent, optimized for client experience, overall performance and supportability
Set deadlines for projects, then organize your time to meet these deadlines; be willing to put in extra time and effort to meet a time-bound goal when necessary
Mentor other staff while you assist with implementation or utilization within your area of technical specialty
Work multiple projects simultaneously and provide consistent, high-quality results
Assist with implementation and or migration of clients’ existing data and applications
Perform system and application testing to provide functional environment
Provide onsite and or remote technical support to Marco clients during onboarding process
Implement changes and work orders for existing Marco clients
Review customer hosted systems and make technical/process recommendations for improving efficiency
Stay up to date with relevant state-of-the-art technology, equipment, and/or systems
QUALIFICATIONS:
Bachelor’s Degree and 4+ years of experience or equivalent experience
Experience and current certifications or specializations aligning with the technologies used in Marco’s Data Center to include Cisco NX-OS, SAN-OS, IOS, UCS-B, ASAv, EMC VNX, Meraki MX, VMWare vSphere, Citrix XenApp, Zscaler and XenDesktop preferred.
Current high level industry recognized certifications including one or more of the following: CCNP, CCE, VCDX, EMCTA
Solid understanding of the administration of network operating systems and services found in a Data Center environment
Benefits:
We’re not just competitive when it comes to business tech – we’re also pretty proud of what we offer our employees. Our benefits include medical, dental, and vision insurance. We also have paid holidays and vacation, 401k with generous company match, flexible spending accounts, employee purchase program, employer-paid life insurance, voluntary-term life insurance, short and long-term disability, critical illness and accident benefits, and pet insurance. Yes, we care about your furry family too.

all benefits are dependent on employment status
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities
Applicant Labor Law Posters",4.0,"Marco Technologies LLC
4.0",Remote,1001 to 5000 Employees,1973,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
82,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
83,Data Engineer (Contract),$77K - $111K (Glassdoor est.),"About RevOpsforce:
At RevOpsforce, our mission is to drive sustainable revenue growth for our clients by leveraging data-driven insights, technology, and process optimization. We strive to deliver measurable results that exceed expectations, while building strong partnerships with our clients and continuously improving our own capabilities. Our Expert Network is composed of the highest skilled and certified professionals in revenue operations. We leverage this network to solve our clients' most complex operational challenges.
Type: Contract
Job Description:
We are seeking a talented and experienced Data Engineer to join our growing team. As a Data Engineer at our company, you will be responsible for designing, building, and maintaining the data pipelines that power our data-driven applications. In this role, you will work closely with our data science and engineering teams to develop high-quality, efficient data pipelines that enable us to derive insights from vast amounts of data. You should be comfortable working with large volumes of structured and unstructured data, and have a strong understanding of SQL, Python, and other programming languages.
Responsibilities:
Design, build, and maintain data pipelines to support data-driven applications and analytics
Analyze data to identify trends and patterns
Collaborate with data scientists and engineers to develop data-driven solutions
Write and maintain documentation for data pipelines
Monitor and optimize data pipelines for performance and efficiency
Qualifications:
Bachelor's or Master's degree in a related field (e.g. Computer Science, Data Science, Engineering)
3+ years of experience in a data engineering role
Strong skills in SQL and at least one programming language (e.g. Python, Java, C++, etc.)
Experience with big data technologies (e.g. Hadoop, Spark, etc.) and cloud platforms (e.g. AWS, Azure, GCP)
Strong problem-solving and analytical skills
Excellent communication skills and ability to work in a team environment
Join our dynamic and forward-thinking team to make a significant impact on our clients' revenue growth journey!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Website is coming soon and will be located at www.revopsforce.com",-1,RevOpsforce,"Austin, TX",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
84,Engineer II - Imaging Data and Solutions,$73K - $101K (Glassdoor est.),"About the Department
Novo Nordisk Data Management and Informatics within the Digital Science and Innovation Organization provides informatics solutions, data products and analysis support to the research organization in Novo Nordisk. Data Management and Informatics is establishing a data products organization across our research sites. Staff will be co-located to one of our global sites in Seattle, WA, Fremont, CA, Lexington, MA, Oxford, UK and Denmark.

The Position
The Imaging Data and Solutions Engineer has a subject matter knowledge on imaging systems and image processing. They will set directions and deliver well curated imaging data and software products via appropriately architected data pipelines to solve complex scientific problems. This role focuses on providing findable, accessible, interoperable, and reusable imaging data within our diabetes, obesity, chronic and rare disease therapeutic areas. They will provide support to data consumers including computational scientists, citizen data scientists, bots and laboratory scientists and contribute through application of expertise. They will use their knowledge of digital to speed the ability of scientists and data scientists to access and work with imaging data, and to use scientific applications
They will use their hands on expertise to provide solutions using preferred tooling and technologies, and will work with other data engineers, product owners and specialist across Data Management & Informatics and Global IT to address larger needs. This position will also enable the visualization needs for imaging data and be well versed in best practices required to streamline data handling, and represent the local needs at the sites in the context of the global data management and informatics organization.
Do you believe that the digitalization journey in Research and Early Development (R&ED) is crucial for the success of pharmaceutical companies in the future? Then apply to become part of the next wave of scientific discovery by joining Digital Science & Innovation (DSI).

Relationships
The Imaging Data and Solutions Engineer reports to the Director, Imaging Data and Solutions Engineering. Internal partners include therapeutic area scientists, computational biologists, data and software engineers, software developers, platform and compute engineers in Research and Development and Information Technology.
External relationships include relationships with commercial and academic collaboration partners.

Essential Functions
Strong grasp of the image processing specifically DICOM headers and image transforms
Gather/organize large, complex data sets and develop ETL/ELT systems to manage and utilize this data. This will involve profiling, cleansing, transforming and developing data structures, schema and dictionaries to create more efficient workflows
Build automated monitoring mechanisms to ensure compliance and integrity of the pipelines and database
Being a core contributor to agile product delivery teams with a focus on publishing data products into the research and enterprise data catalog, as well as platform engineering and delivering clinical data for research capability
Ensure scientists and data scientists are aware of available imaging data and can access, integrate and query it in a performant manner
Enable streamlined sharing of rich data with collaborators through Cloud, accelerated compute and AI/ML approaches
Assist with provisioning of compute and data pipelines to deliver performant imaging data products via the research and enterprise data ecosystem
Optimize workflows and exchange of research imaging data within and to the global organization
Ensure researchers are familiar with and can use preferred applications
Develop or acquire new systems and software in collaboration with internal and external partners
Conduct end-user training and technical support of informatics systems
Advocate for the use of data and data science including computational and machine learning approaches in research projects
Participate in sustaining a suite of tools and application such as Python, R, Jupyter Hub, Domino, DataLab, Omero.
This role is an individual contributor role that must leverage agile software development practices. They will have a proven track record of creating business results with impact at the VP level.

Physical Requirements
0-10% overnight travel required.

Qualifications
Bachelor’s degree preferred. Degree within insert subject mater expertise preferred
Bachelor’s degree with 3+ years’ and Master’s degree with 1+ years’ relevant experience can be considered
Relevant experience includes:
Experience with imaging data sets including the formulation of scientific hypotheses and analysis plans that require the integration of multi-modal data into integrated data products
Basic understanding of the ontologies, vocabularies and standards for data representation across the research and development value chain.
Proficiency in one or more programming languages such as Python, Matlab, C/C++ and Java.
Experience adapting, developing and adding functionality to programming tools such as ImageIO, python imaging library, scikit-image, Flask, FastAPI, Jinja preferred.
Experience on machine learning tools (Pytorch and Tensorflow) is desirable.
Experience open-source contributions and publications demonstrating value to life sciences and drug discovery projects preferred.
Proficiency in Amazon, Azure or Google Cloud is desirable.
2+ years imaging data and image processing expertise and experience designing, developing, implementing and maintaining data pipelines and using cloud research data platforms.
Experience with entire data and solution engineering lifecycle from design, deployment, implementing and maintenance of proprietary and commercial software
Preferred:
2+ years experience in life sciences, medical device or pharmaceutical industry
Broad expertise spanning data and digital, including the ability to perform hands on technical work
Strong analytical skills, the ability to plan and realize robust and scalable solutions with a structured approach and detail oriented
Ability to work independently or need occasional guidance from manager/senior colleagues
Automated testing skills preferred
Excellent communication skills at bench scientist/end-user level and with middle management
Excellent written and oral communication skills is required
We commit to an inclusive recruitment process and equality of opportunity for all our job applicants.

At Novo Nordisk we recognize that it is no longer good enough to aspire to be the best company in the world. We need to aspire to be the best company for the world and we know that this is only possible with talented employees with diverse perspectives, backgrounds and cultures. We are therefore committed to creating an inclusive culture that celebrates the diversity of our employees, the patients we serve and communities we operate in. Together, we’re life changing.

Novo Nordisk is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, ethnicity, color, religion, sex, gender identity, sexual orientation, national origin, disability, protected veteran status or any other characteristic protected by local, state or federal laws, rules or regulations.

If you are interested in applying to Novo Nordisk and need special assistance or an accommodation to apply, please call us at 1-855-411-5290. This contact is for accommodation requests only and cannot be used to inquire about the status of applications.",4.3,"Novo Nordisk
4.3","Lexington, MA",10000+ Employees,1923,Company - Public,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,$10+ billion (USD)
85,"Data Engineer, Product Analytics",Employer Provided Salary:$109K - $166K,"As a highly collaborative organization, our data engineers work cross-functionally with software engineering, data science, and product management to optimize growth, strategy, and experience for our 3 billion plus users, as well as our internal employee community. In this role, you will see a direct correlation between your work, company growth, and user satisfaction. Beyond this, you will work with some of the brightest minds in the industry, and you'll have a unique opportunity to solve some of the most interesting data challenges with efficiency and integrity, at a scale few companies can match.


Data Engineer, Product Analytics Responsibilities:
Manage and execute data warehouse plans for a product or a group of products to solve well-scoped problems
Identify the data needed for a business problem and implement logging required to ensure availability of data, while working with data infrastructure to triage issues and resolve
Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights in a meaningful way
Build data expertise and leverage data controls to ensure privacy, security, compliance, data quality, and operations for allocated areas of ownership
Design, build and launch new data models and visualizations in production, leveraging common development toolkits
Independently design, build and launch new data extraction, transformation and loading processes in production, mentoring others around efficient queries
Support existing processes running in production and implement optimized solutions with limited guidance
Define and manage SLA for data sets in allocated areas of ownership



Minimum Qualifications:
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.
2+ years of work experience in data engineering
Experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.)



Preferred Qualifications:
Experience with one or more of the following: data processing automation, data quality, data warehousing, data governance, business intelligence, data visualization, data privacy
Experience working with terabyte to petabyte scale data





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9",Remote,10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
86,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1,Radiant System,Remote,-1,-1,-1,-1,-1,-1
87,Senior Data Engineer,-1,"CoreTrust is the market leading commercial Group Purchasing Organization (GPO), leveraging the combined purchasing volume of its 3,000+ members to negotiate preferential pricing and terms across more than 80 indirect spend categories. Strategically aligned with private equity portfolios and large independent companies, we complement sourcing bandwidth and improve supply chain efforts with our industry-leading national contracts.
Recently acquired by Blackstone Private Equity, CoreTrust is growing rapidly and we’re looking for a passionate Senior Data Engineer.
Reporting to the Director of Data, you will be part of the data solutions team and be responsible for building our data platforms, enabling the use of advanced analytics to drive continued evolution and growth.
You will create and manage data pipelines to feed and curate our data lake solution and help develop our data roadmap. The ideal candidate has a great understanding of various data / tech solutions (e.g., data modeling tools, data pipeline, data catalogs, cloud databases) and a record of using them to bring tangible dollar impact. You should be excited to seek out and capitalize on a wide variety of opportunities to use data to create value across the organization.

Responsibilities
Lead data projects to build innovative and highly available solutions while ensuring adherence to budget, schedule, and scope of project
Mentor other members in the data solutions team
Develop and assist with oversight on the data tech infrastructure
Drive data & analytics solutions from conception to deployment/delivery with clear ROI impact
Develop and maintain relationships with all relevant business and tech stakeholders and functions
Provide input to proposals for assigned projects including project objectives, technologies, systems, information specifications, timelines, and staffing
Communicate timely status updates to affected internal or external customers and stakeholders
Collect, analyze, and summarize information and trends as needed to prepare project status reports
Assist in developing a culture of data-driven decision-making, including adoption of business intelligence, analysis, and advanced analytics globally
Perform other related duties as assigned

Qualifications
Bachelor’s degree in computer or information science or relevant experience
9+ years of relevant experience in a data-driven professional setting
Ability to assist with the vision of the team (e.g., mission, priorities, engagement model, tooling)
A record of accomplishment of successfully managing complex cross-functional projects under tight deadlines
Strong technical background – familiarity with Python, SQL, cloud technologies like Azure and AWS, statistics / machine learning, Snowflake, DBT, FiveTran, Data Visualization Software
Exceptional communication and presentation skills, particularly in the context of engaging senior management teams
A successful history of manipulating, processing, and extracting value from large, disconnected datasets
Build processes supporting data transformation, data structures, metadata, dependency, and workload management
Working knowledge of creating and leveraging API or Stream based data extraction processes such as Salesforce API
Strong command of databases and SQL
Proficiency with Python or R, especially for data manipulation and analysis, and ability to build, maintain and deploy sequences of automated processes with these tools
Ability to motivate groups of people to complete a project in a timely manner
Excellent analytical, logical thinking, and problem-solving skills
Thorough understanding of project management principles and planning
Thorough understanding of information technology procedures and practices
Proficient with, or able to quickly become proficient with, a range of general and specialized applications, software, and hardware used in the organization and the industry

Benefits
Competitive compensation package
Free individual employee medical coverage
Company subsidized dental and vision coverage
Dollar for dollar 401(k) match up to 6% of your salary with immediate vesting
Company-paid Short-Term and Long-Term Disability coverage
Employee Assistance Program to support your wellbeing and mental health
$1500 annual stipend for undergraduate/graduate college courses; $500 annual stipend for continuing education courses/certifications
Free snacks and beverages on-site
Brand new, state-of-the-art, tech-enabled work environment in downtown Nashville
Flexible/hybrid work culture",2.8,"CoreTrust
2.8",Remote,Unknown,-1,Subsidiary or Business Segment,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
88,AWS Data Engineer,Employer Provided Salary:$60.00 - $75.00 Per Hour,"Title: AWS Data Engineer
Location: Columbus, OH (Day 1 onsite / Hybrid Model)
Duration: Contract
Job Description:
Data Engineer: Open positions – 2
Skills:
Must Have:
BS/BA degree or equivalent experience
General: Strong organizational, problem-solving, and critical thinking skills; Strong documentation skills
Coding: Proficiency in Java
• Cluster Computing frameworks: Proficiency in Spark and Spark SQL
• AWS Data Services: Proficiency in Lake formation, Glue ETL (or) EMR, S3, Glue Catalog, Athena, Kinesis (or) MSK, Airflow (or) Lambda + Step Functions + Event Bridge
Data De/Serialization: Expertise in at least 2 of the formats: Parquet, Iceberg, AVRO, JSON-LD
AWS Data Security: Good Understanding of security concepts such as: Lake formation, IAM, Service roles, Encryption, KMS, Secrets Manager
Good to Have:
• Linux Scripting, Jenkins
• DevOps: Git, CI/CD, JIRA, TDD
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Ability to commute/relocate:
Columbus, OH 43085: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Required)
Java: 4 years (Required)
Data warehouse, Glue and SparkSQL: 8 years (Required)
Work Location: In person",4.5,"Synkriom Technology Pvt. Ltd.
4.5","Columbus, OH",201 to 500 Employees,2015,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
89,Senior Data Engineer,$115K - $151K (Glassdoor est.),"Senior Data Engineer
Location: Various
Raleigh NC; Atlanta GA; Philadelphia PA; Charlottesville VA; others considered;
Hybrid, Remote, Travel required
Elder Research, Inc. (ERI) is a Data Science consulting firm specialized in providing analytic solutions to clients in Commercial and Government industries. The Commercial business unit is seeking to hire a Senior Data Engineer with strong engineering skills who will provide technical support across multiple project teams by leading, designing, and implementing the software and data architectures necessary to deliver analytics to our clients, as well as providing consulting and training support to client teams in the areas of architecture, data engineering, ML engineering and/or related areas. We are trusted advisors to our clients, building lasting relationships and partnering as preferred analytics providers. We use a variety of programming languages and tools to create analytic solutions, often fitting within our clients’ environment and needs.
We are looking to hire a Senior Data Engineer who can support robust and repeatable data manipulation, large-scale infrastructure for data ingestion, and data visualization for custom client applications. A Senior Data Engineer works collaboratively with Business Analysts, Data Scientists, Data Analysts, other Software/Data Engineers, and business stakeholders to create and deploy dynamic data applications that help our customers make meaningful business decisions.
Some of your responsibilities will include leading and implementing software engineering best practices, mentoring junior engineers, designing software architectures, wrangling and fusing large and disparate data sets, assisting in the deployment of models and algorithms, automating data pipelines in their entirety, and communicating model results through user-focused data visualizations. Candidates should have the ability and the willingness to tailor applications to a clients’ business goals using an iterative methodology. Candidates should also be able to consider both long-term stability and scalability while taking a user-focused approach to development and deployment. Excellent decision making skills are a must.
Desired Skills
Candidates should have a mix of the following experiences:
Set or contributed to data engineering and governance standards
Reviewed code/design/architecture and coached on adjustments (as needed)
Mentored junior engineers
Designed and implemented software architectures
Implemented a data pipeline that includes data transformations such as aggregations, joins, and cleaning.
Ingested data into SQL or NoSQL databases.
Interacted with SQL or NoSQL databases via Python, Java, or similar.
Provided access to transformed data for downstream applications or visualizations via REST APIs or similar.
Worked with visualization software such as Power BI, Tableau, or similar.
Skilled in both applying and teaching others to apply Git (or other distributed version control) for collaboration, record-keeping, and deployment scenarios
Skilled in both using and guiding others in using cloud platforms (AWS, Azure, Google Cloud) for building, deploying, evolving, and maintaining data pipelines
Skilled in both applying and guiding others in applying continuous integration and continuous deployment (CI/CD) techniques for surfacing situations where new work breaks existing functionality
Had exposure to modeling and/or data analysis.
Configured a technical service such as a database, version control system, or operating system.
Led and collaborated with others to accomplish a technical task.
Partnered on the development or deployment of a data strategy
Presented technical work to a non-technical audience via writing or presentation.
Desired Education and Experience
Education:
Master’s in a technical field and 4-5 years’ experience OR
Bachelor’s and 6+ years’ experience
Certifications:
AWS, Azure, or Google Cloud
Minimum Requirements
Bachelors/Master’s degree in Computer Science or related field requiring software development
4+ years’ of professional experience with Python, Java, or similar
Excellent written and verbal communication skills
Ability to parse project requirements, including asking questions to ensure full and correct understanding
Willingness and ability to learn new concepts and tools quickly
Willingness and ability to provide technical support across multiple projects
What You Would Do
Work on small teams in a highly collaborative environment.
Participate in both existing and new projects.
Contribute to consulting projects to solve interesting problems for multiple clients in various industries.
Communicate details of the technical architecture to fellow team members and clients in both technical and non-technical terms.
Work with Data Scientists to design and/or implement the technical architecture necessary to support analytics.
Work with Software/Data Engineers to improve the robustness and scalability of software products.
Participate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance.
Work on documentation of the technical architecture for internal and client use.
Design, create, and provision data stores.
Build trusted and lasting relationships with clients.
Provide value to our clients through analytics, software tools, and leadership.
Manage and monitor pipelines to extract, transform, and load data (ETL).
About Elder Research, Inc.
Headquartered in Charlottesville, VA with offices in Arlington, VA, Baltimore, MD, and Raleigh, NC, Elder Research, Inc. (ERI) is a fast-growing solutions and consulting firm specializing in predictive analytics. At ERI, you’ll be part of a fun, friendly community. In keeping with our entrepreneurial spirit, we want candidates that are self-motivated with an innate curiosity and strong team work ethic. We work hard to provide the best value to our clients and allow each person to contribute their ideas and put their skills to use immediately.
ERI believes in continuous learning; each week the entire company attends a “Tech Talk” and an office lunch. Individuals often share what they have learned at informal bag lunch talks or afternoon chats. ERI provides a supportive work environment with established parental, bereavement, and PTO policies. By prioritizing a healthy work-life balance - with reasonable hours, solid pay, low travel, and flexible time off - ERI enables and encourages its employees to serve others.
ERI provides analytic solutions to hundreds of companies across numerous industries. Our team enjoys great variety in the type of work we do and exposure to a wide range of techniques and tools. If you are passionate about integrating data, technology, and analytics in a team-based environment to solve problems, then ERI may be a good fit for you.",4.5,"Elder Research Inc
4.5","Charlottesville, VA",51 to 200 Employees,1995,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
