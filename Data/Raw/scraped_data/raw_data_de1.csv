,Job Title,Salary Estimate,Job Description,Rating,Company Name,Location,Size,Founded,Type of ownership,Industry,Sector,Revenue
0,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
1,Sr. Data Engineer,Employer Provided Salary:$118K - $142K,"Data Engineer
Egen Solutions Inc offers competitive compensation, benefits, and an excellent culture of teamwork and collaboration. We are an Equal Opportunity Employer that does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.
Job Summary
We have some major internal projects underway at Egen and need a highly skilled Data Engineer to join our team on a full-time basis. Below is the JD, kindly go through it and let me know your interest and if you are interested revert back with your updated resume
Our Data Engineering teams build scalable data pipelines using Python, Spark, and cloud services (GCP and AWS). The pipelines we build typically integrate with technologies such as Kafka, Storm, and Elasticsearch. We are working on a continuous deployment pipeline that leverages rapid on-demand releases. Our developers work in an agile process to efficiently deliver high value applications and product packages.
As a Staff Data Engineer at Egen, you will leverage Spark and GCP (preferred) to architect and implement cloud-native data pipelines and infrastructure to enable analytics and machine learning on rich datasets.
Required Experience:
Built and run resilient data pipelines in production and have implemented ETL/ELT to load a multi-terabyte enterprise data warehouse.
Implemented analytics applications using multiple database technologies, such as relational, multidimensional (OLAP), key-value, document, or graph.
Defined data contracts, and wrote specifications including REST APIs.
Transformed data between data models and formats with the most modern PySpark practices. Have built cloud-native applications and supporting technologies / patterns / practices including: Cloud Services, Docker, CI/CD, DevOps, and microservices.
Planned and designed artifacts that describe software architectures involving multiple systems and technologies
You've worked in agile environments and are comfortable iterating quickly.
Nice to have's (but not required):
GCP expertise is preferred but will consider AWS
Experience moving trained machine learning models into production data pipelines.
Experience in biotech, genomics, clinical research or precision medicine.
Expert knowledge of relational database modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others.
This is full-time, On-site, and Remote. We are based out in Naperville, IL.
Looking forward to striking up a conversation with you on the opportunity!, Kindly visit our website egen.solutions
equirements
Bachelor’s degree in Computer Science, Engineering, or related field.
3+ years of experience with data engineering and/or data analysis.
Strong analytical skills and ability to think analytically.
Ability to work independently with minimal supervision.
Experience with SQL and/or NoSQL databases is a plus.
Experience with ETL processes is a plus.
Experience with cloud-based solutions such as Azure Data Factory is a plus.
Experience with Agile software development methods such as Scrum is a plus.
Ability to work in a fast-paced environment and adapt to changing priorities.
Ability to work in a team environment while maintaining strong individual initiative.
Job Type: Full-time
Pay: $117,577.88 - $141,599.17 per year
Benefits:
401(k)
Dental insurance
Health insurance
Experience level:
5 years
Application Question(s):
Are you a personal who hold any of these work authorizations currently(OPT EAD/GC/F1 OPT/H4 EAD.
Work Location: In person",-1,Egen Solutions Inc,"Naperville, IL",-1,-1,-1,-1,-1,-1
2,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
3,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
4,Data Devops Engineer,-1,"Overview
Health by Design (HBD), a well-respected company in San Antonio, Texas is known for its trusted onsite and near-site care clinic that focuses on the mind, body, and soul. In addition to providing a positive work experience for our employees, HBD provides employees with exclusive, convenient access to medical care focused on prevention and wellness. HBD was acquired by Medici Clinical Management, LLC in 2019. We are now actively seeking a Data Engineers to contribute to our growth. This is an exceptional opportunity to make meaningful impact on community health and well-being. Join us and help shape the future of healthcare.

About the role
Position Overview: We are seeking an experienced Data Engineer with 8 - 15 years of hands-on experience in working with SQL Server, SQL, ETL and schema design. As a Data DevOps Engineer, you will be responsible for managing and optimizing our data infrastructure, ensuring the efficient storage, retrieval, and processing of data in an early-stage fast paced environment. You will collaborate closely with cross-functional teams to design and implement scalable data solutions, enabling data-driven decision-making across the organization.


FULLY REMOTE POSITION MUST RESIDE IN TEXAS, LOUISIANA, OKLAHOMA, SOUTH CAROLINA, MARYLAND, TEXAS, NEW YORK, CALIFORNIA, TENNESSEE, UTAH, KENTUCKY OR FLORIDA
Key Responsibilities
Technical Leadership: Drive projects to modernize the data platform and migrate legacy systems. Data Infrastructure Management: Design, develop, and maintain the data infrastructure, including databases, data pipelines, and ETL processes, to support efficient data storage, retrieval, and processing including job creation, management, and monitoring.
Schema Design: Collaborate with data scientists, analysts, and other stakeholders to design database schemas that optimize data retrieval and storage while adhering to industry best practices.
Data Modeling: Create and maintain data models and data dictionaries to ensure consistency and standardization across data sources and systems. Performance Optimization: Monitor and fine-tune database performance by identifying and resolving bottlenecks, optimizing queries, and implementing indexing strategies.
ETL Development: Develop and maintain scalable Extract, Transform, Load (ETL) processes to ensure data quality, integrity, and timeliness.
Data Security: Implement and maintain data security measures, including access controls, data encryption, and compliance with relevant regulations (e.g., GDPR, HIPAA).
Data Quality and Governance: Establish data quality standards, perform data profiling, and implement data governance processes to ensure data accuracy, consistency, and completeness.
Collaboration and Communication: Collaborate with cross-functional teams, including data scientists, analysts, software engineers, and business stakeholders, to understand data requirements and translate them into technical solutions. Communicate effectively to present complex technical concepts to non-technical stakeholders.
Documentation: Document data infrastructure, processes, and technical specifications to ensure knowledge transfer and enable smooth maintenance and troubleshooting.
Continuous Learning: Stay up-to-date with emerging technologies, trends, and best practices in data engineering and apply them to improve existing systems and processes.
Required Skills and Qualifications
Bachelor's degree in Computer Science, Engineering, or a related field.
8-15 years of professional experience as a Data DevOps Engineer or in a similar role.
Strong proficiency in working with relational databases, particularly SQL Server, including database design, querying, optimization, and performance tuning.
Solid understanding of SQL and experience with advanced SQL concepts, such as subqueries, window functions, and stored procedures.
k,lopExperience with data modeling and schema design, including normalization, denormalization, and indexing strategies.
Strong knowledge of ETL processes and tools for data integration, transformation, and loading. Familiarity with data warehouse concepts and technologies (e.g., star schema, data lakes).
Understanding of data security principles and experience implementing data security measures.
Excellent problem-solving and analytical skills with the ability to work on complex data-related challenges.
Strong communication and collaboration skills with the ability to work effectively in a cross-functional team environment.
Attention to detail and a commitment to delivering high-quality work.
Self-motivated and able to work independently with minimal supervision.
Familiarity with other cloud-based database systems, such as SnowFlake, Google BigQuery, PostGres, MySQL is a plus.
Knowledge of cloud platforms and services, such as AWS, Azure, or GCP, is a plus.
Equal Employment
Medici is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state, and local law.
Check out our company websites
Visit Health by Design Website
Visit Medici Website",3.3,"Medici
3.3",Kentucky,1 to 50 Employees,2016,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
5,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
6,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
7,Senior Data Engineer,-1,"JOB DESCRIPTION:
Need to be expertise Coding on PySpark.
Supply Chain and Azure.
Experience in working with end-to-end technology implementation project, preferable those related to Data warehousing/Analytics/Reporting
Strong communication skills with the ability to collaborate effectively with business owners/users and technical teams.
Proven ability to gather product requirements from stakeholders and prioritize feature implementations based on business needs and product strategy.
Strong problem-solving skills and attention to detail.
Good to have experience with CPG/ Retail industry (preferable CPG companies such as Nestle, Unilever, P&G, Coca coal, PepsiCo etc)
Bachelor's/master's degree in business administration, computer science, engineering, or a related field.
Job Type: Contract
Experience:
PySpark: 5 years (Preferred)
SQL: 10 years (Preferred)
Data analytics: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Azure: 5 years (Preferred)
Data Bricks: 5 years (Preferred)
Data Lake: 5 years (Preferred)
Synapse: 5 years (Preferred)
Retail Domain: 8 years (Preferred)
CPG: 7 years (Preferred)
Work Location: Remote",4.0,"Quadrant Resource
4.0",Remote,1001 to 5000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
8,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
9,Data Engineer,Employer Provided Salary:$115K - $135K,"**This position is a majority Remote role with occasional in-office meetings on an as-needed basis. Because of this, interested applicants must live within a reasonable driving distance of Symmetry Lending's office in Anaheim, CA.**
Job Description
This role will play a pivotal role in assisting the IT team with the evolution of the company data architecture. In addition to supporting extensions to the data warehouse design, this role will design and develop ETL required to onboard new types of business data. Finally, this role will also provide support for Analytics assignments as needed.
Responsibilities
Display sense of ownership over assigned work, requiring minimal direction and driving to completion in a sometimes fuzzy and uncharted environment.
Designing and building new data pipelines that support business requirements.
Work with IT and other business partners to support the evolution of the enterprise data architecture.
Work with IT to build new ETLs to take data from various operational systems and extend existing data warehouse data model for analytics and reporting.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business constituents.
Support the development of the company BI infrastructure, including the construction of scalable analytic solutions, and on-premises reporting tools.
Utilize Power BI development to build scalable reporting models to serve BI reporting needs.
Demonstrate the ability to gather requirements, extract and manipulate data residing in multiple disparate databases, and articulate solutions to support the business.
Design, develop, and maintain performant data models in Power BI utilizing best practices.
Serve as a subject matter expert for all data warehouse and enterprise systems
Use new or existing technologies to produce analytics solutions (in the form of excel spreadsheets, dashboards, etc.).
Work with IT and Analytics team to architect and build data pipelines to optimize for performance, data quality, scalability, ease of future development, and cost.
Gather requirements, assess gaps and build roadmaps to help the analytics driven organization achieve its goals.
Develop data-related Proof of Concepts in order to demonstrate feasibility and value to Symmetry business constituents.
Qualifications / Requirements
Bachelor’s degree in Information Systems, Computer Science, Finance, or similar education from an accredited college
2-3 years Accounting and Finance background preferred
Strong skillset in Microsoft Excel (2-3 years, financial formulas & VBA is a plus) and familiarity with Microsoft Office Productivity Suite (Excel, Word, etc.)
5+years’ experience with advanced SQL concepts and writing SQL statements (SQL Server or similar).
Prefer 2 years’ experience with Microsoft BI Suite, PowerBI, Power Query, PowerPivot, Reporting Service (SSRS), SharePoint
Experience in mentoring other team members in development best practice, and methodologies.
You are passionate about data quality control and know how and where to anticipate potential errors.
Knowledge of the software development lifecycle, agile methodologies, and structured software development methodologies.
Experience performing analysis with large datasets in a cloud-based environment.
Ability to work effectively with stakeholders at all levels within the organization
Strong communication and time management skills and a self-motivated approach
Ability to work independently, detail-oriented, and execution focused
Highly collaborative and team oriented
Tenacious (doesn’t give up easily)
Genuine passion for clean and reliable data
At least 5 years of work experience
About Symmetry
Symmetry Lending, specializes in providing mortgage fulfillment services to include origination, servicing, and capital markets needs to various Lenders across the country with whom we partner. We have offices in Atlanta, GA, Eden Prairie, MN, Denver, CO, Orlando, FL, and Anaheim, CA, and we do business from coast to coast. We take great pride in building a diverse team of motivated professionals that contribute to an exciting work atmosphere. We provide a competitive benefits package including medical, dental, and vision plan options, paid time off, and more.
California Disclosure - Employee Notice at Collection
This disclosure is intended to comply with the California Consumer Privacy Act (CCPA), which gives California residents who are applicants, employees, or contractors of Symmetry Lending (“Symmetry”) the right to know what categories of personal information Symmetry collects about them and the purposes for which Symmetry uses that information. As used in this Privacy Notice, “Personal Information” means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a particular individual or household. Personal Information includes, but is not limited to, the categories of personal information identified below if such information identifies, relates to, describes, is reasonably capable of being associated with, or could be reasonably linked, directly or indirectly, with a particular individual or household.
The following is a list of the categories of Personal Information that we may collect about consumers:
Identifiers. This may include a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, Social Security number, driver's license number, passport number, or other similar identifiers.
Personal information described in the California Customer Records Statute (Cal. Civ. Code § 1798.80(e)). This may include a name, signature, Social Security number, physical characteristics or description, address, telephone number, passport number, driver's license or state identification card number, insurance policy number, education, employment, employment history, bank account number, or any other financial information, medical information, or health insurance information.
Characteristics of Protected Classification under California or Federal Law. This may include age, race, color, ancestry, national or ethnic origin, citizenship status, religion or belief, marital status, a childbirth or related medical condition, physical or mental disability, sex (including gender, gender identity, gender expression, pregnancy or childbirth, and related medical conditions), sexual orientation, veteran or military status.
Biometric information. This may include voice and video recordings.
Sensory data. This may include audio, electronic, visual, or similar information, including photos.
Professional or employment-related information. This may include current or past job history, compensation data, performance evaluations, or employee benefits.
Beneficiaries, dependents, and emergency contact information. This may include the name, gender, phone number, and relationship of beneficiaries, dependents, and emergency contacts.
We may use the categories of Personal Information for the following business or commercial purposes:
To perform background checks necessary to comply with licensing requirements, to perform reference checks, to verify eligibility to work in the United States, for contact purposes, to assess your qualification for employment, to conduct performance evaluations, for payrolls and budgeting purposes, for implementation of employee benefits, for internal organizational purposes to establish proper accommodations for sick time, PTO, leaves of absences, or emergency situations, and to conduct health screenings as allowed by OSHA and the CDC to protect the safety of our employee during pandemic situations.
For internal use, such as tracking access into buildings, timekeeping, activity logs, etc.
To comply with laws and regulations, including but not limited to applicable tax, health and safety, anti-discrimination, immigration, labor and employment, and social welfare laws.
For security or the prevention, detection, or investigation of fraud, suspected or actual illegal activity, violations of company policy or rules, or other misconduct.
To comply with civil, criminal, judicial, or regulatory inquiries, investigations, subpoenas, or summons.
To comply with all licensing requirements necessary for our business operations, including state exam audits.
To exercise or defend the legal rights of Symmetry and its employees, affiliates, customers, contractors, and agents.
To seek advice from lawyers, auditors, or other professional advisors.
If Symmetry uses Personal Information of an applicant, employee, or contractor for a purpose materially different than those disclosed in this notice, Symmetry will notify the employee and obtain explicit consent from the employee to use the Personal Information for this new purpose.
Job Type: Full-time
Pay: $115,000.00 - $135,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Paid time off
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
Accounting and Finance: 2 years (Preferred)
advanced SQL concepts and writing SQL statements: 5 years (Required)
Work Location: Hybrid remote in Anaheim, CA 92807",3.9,"Symmetry Lending
3.9","Anaheim, CA",51 to 200 Employees,-1,Company - Public,Banking & Lending,Financial Services,$5 to $25 million (USD)
10,ETL Data Pipeline Engineer,Employer Provided Salary:$60.00 Per Hour,"ETL Data Pipeline Engineer
We do not work with 3rd party employers. Visa Sponsorship NOT available.
We are seeking a ETL DATA Pipeline Engineer for a consulting engagement with a major entertainment and media company. This person will be hands-on-date engineering development across multiple projects.
Required Skills:
10+ years of experience as Data Engineer with Large Data Pipelines
Strong SQL skills
Distributed Systems (Spark, Hadoop)
Cloud experience
STRONG ETL Experience
Python/Bash
Agile/Scrum
----------------------------------------
ABOUT MOORECROFT
A quality oriented national consulting firm. For more than two decades, providing expert IT professionals to our clients around the nation. Our core values are professionalism, honesty and integrity. We are only interested in working with professionals that subscribe to the same values and are driven to truly serve clients.
Job Type: Contract
Pay: From $60.00 per hour
Experience level:
10 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Santa Monica, CA 90404: Reliably commute or planning to relocate before starting work (Required)
Work Location: Hybrid remote in Santa Monica, CA 90404",-1,Moorecroft Systems,"Santa Monica, CA",Unknown,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
11,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
12,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
13,Data Engineer,$100K - $144K (Glassdoor est.),"Position: Data Engineer
Duration: Contract
Location: Fremont, California
Skills:
Strong in SQL and Python
Data warehousing concepts , Data Modeling and RDBMS concepts
Good to have exposure to big data technologies – Apache Spark, Kafka, Airflow, NoSQL
Good to have exposure on Insurance & Finance business processes
Regards
Tejash Gupta
Recruiter
tejash@swifttechinc.com
Job Type: Contract
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Python: 7 years (Required)
SQL: 7 years (Required)
Data modeling: 6 years (Required)
RDBMS: 6 years (Required)
Work Location: In person",4.1,"Swift Technologies Inc
4.1","Fremont, CA",1 to 50 Employees,-1,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
14,Big Data Engineer,Employer Provided Salary:$50.00 - $60.00 Per Hour,"Job Description:
Experience working with Hadoop stack (Hive, HDFS, Spark).
Strong AWS/GCP/Azure experience.
Experience with Java or Scala or python.
Good knowledge of SQL.
Good Communication Skills.
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Phoenix, AZ 85027: Reliably commute or planning to relocate before starting work (Required)
Experience:
Hadoop: 4 years (Required)
SQL: 5 years (Required)
Scala: 2 years (Preferred)
Work Location: In person",-1,Reuben Cooley Inc.,"Phoenix, AZ",Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
15,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
16,Data Engineer I - People Analytics,$73K - $111K (Glassdoor est.),"POSITION SUMMARY Our People Analytics team is expanding! The Data Engineer I is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.

This position can work in a flexible, hybrid model with 3 days on-site in Appleton, WI and 2 days remote.

JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.

QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming. Preferred experience in Python applied to data engineering and transformation.
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated

#LI-Hybid

Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101 .",4.1,"US Venture
4.1","Appleton, WI",1001 to 5000 Employees,1951,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
17,Data Engineer,-1,"At MNTN, we've built a culture based on quality, trust, ambition, and accountability – but most importantly, we really enjoy working here. We pride ourselves on our self-service platform, originally coded by our President and CEO, and are constantly seeking to improve the user experience for our customers and scale for efficiency. Our startup spirit powers our growth mindset and supports our teammates as they build the future of ConnectedTV. We're looking for people who naturally want to do more, own more, and make an impact in their careers – and we're seeking someone to be part of our next stage of growth.
As a Senior Data Engineer on the Data team, you will help build the platform to generate, track, manage and triage key business and client success metrics. The goal is to have rapid insights across all available information to mitigate issues and identify opportunities for a smooth marketing experience.
You will:
Become the expert on the MNTN platforms, UI, data infrastructure, and data processes
Extract meaningful business metrics from raw data using SQL and other tools
Create and manage ETL/ELT workflows that transform our billions of raw data points daily into quickly accessible information across our databases and data warehouses
Organize data and metrics for measurable and trackable confidence in reporting and client performance to fulfill agreed-upon quality standards
Organize visualizations, reporting, and alerting necessary to rapidly illustrate performance, data quality, trends and opportunities
Investigate critical incidents and otherwise ensure that any issues reach resolution by the relevant parties
You have:
5+ years of experience related to data engineering, analysis and modeling complex data
Strong experience in SQL, data modeling, and manipulating and extracting large data sets.
Hands-on experience working with data warehouse technologies. Familiarity with building data pipelines and architectures and designing ETL flows.
Experience with programming languages such as Python, Java, or shell scripting. Familiarity with algorithms.
Familiarity with software processes and tools such as Git, CI/CD pipelines, Linux, and Airflow
Experience with working in a cloud computing environment such as AWS, Azure, or GCP
Familiarity in a business intelligence tool such as Domo, Looker or Tableau
Written and verbal communication skills to convey complex technical topics to non-technical audiences across the organization
MNTN Perks:
100% remote
Open-ended vacation policy with an annual vacation allowance
Three-day weekend every month of the year
Competitive compensation
100% healthcare coverage
401k plan
Flexible Spending Account (FSA) for dependent, medical, and dental care
Access to coaching, therapy, and professional development
About MNTN:
MNTN provides advertising software for brands to reach their audience across Connected TV, web, and mobile. MNTN Performance TV has redefined what it means to advertise on television, transforming Connected TV into a direct-response, performance marketing channel. Our web retargeting has been leveraged by thousands of top brands for over a decade, driving billions of dollars in revenue.
Our solutions give advertisers total transparency and complete control over their campaigns – all with the fastest go-live in the industry. As a result, thousands of top brands have partnered with MNTN, including, Petsmart, Build with Ferguson Master, Simplisafe, Yieldstreet and National University.
#Li-Remote",3.9,"MNTN
3.9",Remote,201 to 500 Employees,2009,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
18,Senior Data Engineer,-1,"Who We Are
Stuzo, with its Open Commerce® product suite and patent pending Wallet Steering® System, empowers Convenience & Fuel Retailers to gain more share of wallet and customer lifetime value than possible with any other solution provider. Stuzo’s unified Open Commerce product suite consists of: Activate for Intelligent 1:1 Loyalty, Transact for Contactless Commerce, and Experience for Cross-Channel Customer Experiences. Stuzo’s solutions are supported by a set of subscription-based program management services and are contractually backed by its 1.5X Performance Guarantee.
What You’ll Do:
We are seeking a Senior Data Engineer to join our team and help us build the next generation of data tools for our retail and commerce loyalty platform. The ideal candidate will have a passion for tackling complex challenges related to extracting insights from transactional data. In this role, you will work closely with our Application Development teams, Customer Support, Business SMEs, and stakeholders to design and implement data pipelines that transform our data into valuable insights. As we strive to scale and deploy new technologies that enhance the user experience and provide deeper insights, your expertise will be critical to our success.
Responsibilities + Activities:
Explores and documents new sources of data.
Implement schema parsers for standard event protocols.
Own Data Quality Management for specific data lake models.
Handle performance improvements and bottleneck removal.
Adds and maintains schemas for elements of the Open Commerce data lake.
Develops and tests data pipelines using PySpark.
Creates notebooks and configures jobs for running data pipelines on Databricks.
Collaborates with the team to ensure quality, consistency, and performance of all pipelines across the entire data platform.
Prepare and support internal documentation for the data lake.
Experience & Skills You’ll Need:
Excellent interpersonal, communication and organizational skills are essential.
Self motivated, ability to work as collaborative member of a team
5-8 years experience working with data in databases such as PostgreSQL, MySQL, etc.
Knowledge of ETL/ELT data pipelines and best practices
3-5 years experience coding and testing with Python
2-3 years experience working with Spark and Structured Streaming
2-3 years experience working with Spark orchestration platforms such as Databricks
Experience with project and bug-tracking systems like Jira
Experience with Scrum processes.
Reports To:
This position reports to the Principal Data Engineer.
Why You’ll Love Working at Stuzo
Stuzo has a unique way of working that is grounded in what we call Team Chemistry. Team Chemistry happens when people with complementary aptitudes and skills collaborate to leverage each other’s strengths while supporting each other in the areas that are harder for one team member than they are for another. Team Chemistry empowers us to combine our strengths together in a manner where we achieve multiples more together than we could individually.
We are looking to add amazing folks to our team who will bring diversity across many lines, including race, ethnicity, religion, sexual orientation, age, marital status, disability, gender identity, sex, and country of origin. And while we’re headquartered in Philadelphia, we have a “Remote First” approach to work and do not see being in the office or living in the Philadelphia area as a requirement. Several of Stuzo’s Senior Leaders (your future peers, we hope) are fully remote.",4.4,"Stuzo
4.4",Remote,51 to 200 Employees,-1,Company - Private,Enterprise Software & Network Solutions,Information Technology,Less than $1 million (USD)
19,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
20,Big Data Engineer,Employer Provided Salary:$60.00 Per Hour,"""Architect, build, and launch new data models that provide intuitive analytics to the team.
Lead large-scale data engineering, integration and warehousing projects, build
custom integrations between cloud-based systems using APIs and write complex and
efficient queries to transform raw data sources into easily accessible models by using
the Data integration tool with coding across several languages such as Java, Python,
and SQL.""
Banking/Finance domain experience, relational dimensional and/or unstructured data modeling experience
Big Data applications development experience, NoSql experience, Azure (preferred), Java, Kafka/Spark
Job Type: Contract
Salary: Up to $60.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Berkeley Heights, NJ 07922: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 6 years (Preferred)
NoSQL: 5 years (Preferred)
Java: 5 years (Preferred)
Work Location: In person",-1,Comprise IT Solutions,"Berkeley Heights, NJ",1 to 50 Employees,-1,Self-employed,-1,-1,Unknown / Non-Applicable
21,Data Center Engineer,$62K - $90K (Glassdoor est.),"Job Description
HCL America, Inc. is seeking a highly motivated and experienced Data Engineer to join our team. This position will be responsible for the development and maintenance of data systems for our Flint, MI client. The ideal candidate will have a strong background in data analysis, data management, and data visualization.
Install physical hardware and perform cabling in datacenter/Network closets whenever required.
· Install physical servers with OS
· ESXi configuration
· Install UPS and manage them.
· Understanding of networking, IP address, VLAN’s. configure the switches, access points.
· Maintain datacenters and assist all teams whenever needed.
· Replace bad hardware in the datacenter and coordinate with vendors.
· Follow data center standards.
· Maintain inventory of all the hardware in datacenter.
· Basic understanding of networking.
· Good to have service now ITIL experience.
Job Type: Full-time
Schedule:
Weekends as needed
Experience:
Computer networking: 1 year (Preferred)
LAN: 1 year (Preferred)
Security clearance:
Confidential (Preferred)
Work Location: In person",3.7,"HCL America, Inc.
3.7","Flint, MI",10000+ Employees,1991,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
22,Data Engineer ll,Employer Provided Salary:$130K - $160K,"The Company
Have you ever found yourself or a loved one waiting hours and hours in a hospital Emergency Room to get care? Or have you ever had a surgery scheduled for months in the future that needed to happen sooner? Unfortunately, our healthcare system is full of these types of operational problems. Our work saves lives and helps hospitals cut tens of millions of dollars in operational costs, while improving the quality of care they’re able to deliver.
Qventus is a real-time decision making platform for hospital operations. Our mission is to simplify how healthcare operates, so that hospitals and caregivers can focus on delivering the best possible care to patients. We use artificial intelligence and machine learning to create products that help nurses, doctors, and hospital staff anticipate issues and make operational decisions proactively.
Qventus works with leading public, academic and community hospitals across the United States. The company was recognized by the 2019 Black Book Awards in healthcare for patient flow and by CB Insights as a 2019 top 100 Most Promising Company in Artificial Intelligence. Recently, Qventus won the Robert Wood Johnson Foundation Emergency Response for the Healthcare System Innovation Challenge through its work helping health systems across the country plan for and operate in the COVID pandemic.
The role
Qventus is looking for a Data Platform Engineer II to help scale our solutions, focusing on our analytical and data science needs. The Data Platform team acts as stewards for Qventus’ data. We stream hospital EMR to our core warehouses in real time, build out curated data layers to power our Healthcare AI & Analytical insights and overall ensure Qventus data users have the tools they need to explore and power the Qventus product at scale and cost to improve the lives of patients and doctors across the country.
As a Data Platform Engineer II, you will build and own significant components of the Qventus solution pipelines. You will be comfortable designing, building, and leading cross-functional initiatives with analytical and data science partners - from schema design, to pipeline design, to scaling services to support company expansion within the healthcare space (and HIPAA restrictions). You will have a strong passion for well designed data models and be motivated and excited to have an impact on the team and in the company and to improve the quality of healthcare operations.
Key Responsibilities
Work closely with core data users to understand product needs and design, build, tune and improve our core data assets and the overall end-to-end workflow of data users at Qventus (incl. designing data structures, building and scheduling data transformation pipelines, improving transparency etc.).
Automate & manage the lifecycle of data sets (schema development, deprecation, and iteration).
Improve the data quality and transparency of the pipelines (defining data requirements, identifying and implementing data observability tooling - lineage, sources, transformations).
Work closely with core team members to develop, test, deploy, and operate high quality, scalable software and raise engineering standards.
Key Qualifications
Demonstrated experience in data modeling / schema design and transformation pipeline implementation in collaboration with data science and analytics partners
Experience developing & coordinating execution in a fast paced dynamic environment across multiple technologies
Strong cross-functional communication - ability to break down complex technical components for technical and non-technical partners alike
Interest in mentoring and supporting new developers particularly in data modeling and analytical collaboration
3+ years of professional experience working with modern programming languages such as Java, C/C++, Python with a dedication to high code quality.
Nice to Have Skills
Degree in Computer Science, Engineering, or related field, or equivalent training / experience
Competence participating in technical architecture discussions to help drive high quality technical development within your team
Practical hands on experience with:
building large-scale, high complexity metrics and monitoring
ELK, DBT, Snowflake, AWS, Terraform, Looker, Ansible experience
Experience building and maintaining robust and efficient backend data systems with functional proficiency with AWS cloud services & modern data warehouse services (Snowflake)
We consider several factors when determining compensation, including location, experience, and other job-related factors.
Salary Range: $130,000 to $160,000 annually + equity + benefits- Qventus expects to hire for this position near the middle of the range. Only in truly rare or exceptional circumstances where a candidate's experience, credentials, or expertise far exceed those required or expected will we consider and offer at the top of the salary range.
Qventus offers a competitive benefits package including medical, dental, vision, paid time off, company holidays, and a stock option plan.
Qventus is an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
Candidate information will be treated in accordance with our candidate privacy notice which can be found here: https://qventus.com/ccpa-privacy-notice/
This position does not provide visa sponsorship.
Employment is contingent upon the satisfactory completion of our pre-employment background investigation and drug test.
#LI-REMOTE",4.3,"Qventus
4.3",Remote,51 to 200 Employees,2012,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
23,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
24,Cloud Data Engineer,Employer Provided Salary:$69.61 - $80.00 Per Hour,"Top skills:
Must-Haves (Concepts & Tools):
AWS cloud—KMS, S3, Glue, Lambda etc.
Deployed data pipelines
Java, Python or PySpark hands on development experience
Nice-to-Haves (Concepts & Tools):
Prior ETL migration experience from on prem. To cloud
Exposure to even driven streaming
Job Types: Full-time, Contract, Temporary
Pay: $69.61 - $80.00 per hour
Experience level:
7 years
8 years
9 years
Work Location: Remote",-1,Cloudrex,Remote,-1,-1,-1,-1,-1,-1
25,Senior Data Engineer,$87K - $114K (Glassdoor est.),"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.

This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration.
How will you make an impact?
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
What are we looking for?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework
You might be a good fit if you like to:
Leverage technology to deliver data solutions to drive business outcomes
Drive for continuous improvement of processes and solutions
Bring energy and commitment to excellence to drive delivery of high-quality solutions
Share knowledge and mentor team members on technologies and solutions
What you can learn on the job:
Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers
Grow practical understanding of agile work practices through application of the scrum framework to deliver work products
Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science
What does success look like?
First 30 days:
Understand the existing analytics data platform and technologies
Understand team’s work management processes
First 6 months:
Develop data solutions for the business including shared components and specific data sets
Learn business processes and develop necessary contacts throughout the organization
First year:
Continue to develop data solutions for the business
Become expert on the team for other data engineers to go to for mentoring in completing their work
Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,"AE Business Solutions
4.2","Milwaukee, WI",51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
26,Big Data Engineer,$94K - $130K (Glassdoor est.),"Multiple Bigdata Job Positions across Dallas, TX and Tampa, FL
Position 1: Big Data Engineer
Location: Dallas TX
Experience Required - 6+ years
Job Description
Hadoop/HDFS.
Spark is a must. Scala preferred but Java is ok too.
Experience Spark core, Spark SQL, Spark Streaming ( these 3 are a must) and Spark ML is good to have.
Position 2: Sr. Big Data Engineer
Experience Required - 8-15 years
Location: Tampa, FL
Job Description
The client is looking for a senior engineer who can drive things rather than being managed by the client.
Hadoop/Hive and Kafka.
Spark streaming using Java is a must.
Position 3: Lead Big Data Engineer
Location: Dallas TX
Experience Required - 8-15 years
Job Description
Primary requirement - Spark, Scala developer with knowledge of Kafka.
Good to have exposure to other NoSQL databases like Casandra.
AWS experience will be a definite plus.",-1,Alt Shift USA,"Dallas, TX",1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
27,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
28,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
29,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
30,"Manager, Data Engineer & ETL Processing",$83K - $118K (Glassdoor est.),"Company Description

About Spark Foundry:

Spark Foundry is a global media agency that exists to bring HEAT – Higher Engagement, Affinity, and Transactions – to brands. By combining flawless media fundamentals with aggressive innovation, Spark inspires consumers to pay more attention, to care more about our clients’ brands, and to buy more products and services from them.
Balancing the nimble spirit of a startup with the powerhouse soul of Publicis Media, Spark Foundry delivers the best of both worlds to a client roster that spans some of the world’s best and most beloved brands and companies. We combine boutique-caliber insights and service with the buying clout and first-look access of a global leader, bringing the heat to challenger brands that want to act like giants, and to giant brands that want to act like challengers.
With a bottom-up culture that celebrates diversity and aims for all voices to be heard, Spark has become a magnet for the industry’s best talent, with one of the best retention rates in the industry. And by applying a whole-person approach to professional and personal development, Spark develops a workforce that is well prepared for today’s challenges, and also poised to create meaningful careers in the years to come.
Because we know that heat arises the intersection of complementary forces, our professionals come from myriad disciplines and backgrounds: data, analytics, and insights, content and creative production, communications and strategy, finance and marketing, and sociology, psychology, and other liberal arts disciplines.

Job Description

Overview:
The Manager, Data Engineering & ETL Development is a key driver to build data platform leveraging best in class ETL practice and is also a strategic thinker and a talented data visualization expert with emphasis on dashboard reporting. The primary responsibility will be developing business intelligence platform to support media and marketing data for our clients. This position will allow you to be a significant contributor as part of the team to support easy access of data and visualizations to fuel stronger insights and media optimizations.
This position requires strong technical and tactical skill sets with an eye for numbers, intellectual curiosity, proficiency at problem solving, and a critical understanding of online media. The candidate must have a proven track record in managing ETL’s, APIs and business intelligence platforms. Candidate should be a team player. A “roll up the sleeves” approach is mandatory and a “get it done” attitude is a must. Specific responsibilities include coordination between the research, analytics & media teams ensuring high quality data projects are effectively delivered.
Successful candidates will be multi-dimensional ‘rising-stars’ who are able to employ complex problem solving skills, and are able to communicate these succinctly to a broad client and media stakeholder audience.
Role Objectives:
Responsible for loading and validating data into the data warehouse from various source systems
Analyze, develop, fix, test, review and deploy functionality, and bug fixes in ETL data pipelines
Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required
Building Data mappings between Source to Target systems
Provides support for technical issues and ensuring system availability
Work with business customers to identify and develop additional data and reporting needs
Understand how business intelligence platform/data technologies work and offer the ability to explain technical concepts in ordinary terms (be technically savvy - understand the opportunities and limitations)
Establish and manage data integrations utilizing a taxonomy nomenclature to make recommendations for data visualization to showcase media performance through the use of Datorama or Tableau
Perform regular quality assurance/quality control checks on assigned client campaigns to ensure the data is processing accurately
Design and build backend data streams and processes to automate reporting capability with data visualization tools
Contribute to client status and reporting calls, including presentation of reporting as required
Develop subject matter expertise in ETL, API development, and Business Intelligence platforms
Clearly define project deliverables, timelines, and dependencies for junior team members, internal stakeholders and clients
Collaborate on an inclusive team, where members openly communicate and collectively problem-solve
Strong ability to evaluate new technologies and present findings to team
Contribute to knowledge sharing efforts and mentorships
Complete other duties as assigned.

Qualifications
Bachelor’s degree or combination of education, and equivalent work experience is preferred in the field of computer science, management information systems or Information Technology
3+ years’ related experience ETL development and business intelligence platform management
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
3+ plus years’ development experience with database engines including Presto, Mongo db and Hadoop.
3+ years’ experience in ETL development, Strong Database (Modeling, SQL), SQL Server, Hadoop, AWS Redshift, Qubole.
Experience managing team of 2 or more associates/analysts preferred
Strong database modeling and SQL skills. Ability to write complex SQL statement, Procedures and data automation programs
Knowledge of ETL tools like Airflow, AWS Glue, Alteryx, SSIS, Qubole/Spark is a must
Knowledge of Python or similar programming languages required
Proficiency with Datorama, Tableau, or other data visualization tools is preferred
Experience working with AWS or other cloud technologies
Advanced user Microsoft Office Suite and reporting tools
Demonstrated expertise in core MS Excel functions (vlookup, pivot tables, data visualization)
Experience in designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.
Strong analytical and problem-solving skills
Strong verbal/written communication and interpersonal skills is required
Self-starter with ability to thrive in a fast-paced environment and able to function independently while providing status updates to a team of analysts
Cooperative, flexible, conscientious, dependable, resourceful, self-motivated, and team-oriented
Problem solving, time management, and critical thinking skills with a professional and positive attitude
Ability to work independently and as part of an agile team, participating in daily stand-ups, sprint planning and sprint review

Character:
The following qualities help drive success as member of the Spark Data and Analytics team:
Entrepreneurial, engaging, resourceful, curious, and self-directed spirit
Willing and easily roll sleeves up or down; love the nitty-gritty and the strategy
Collaborative approach to building cohesive, strong teams
Loving and living the intersections between brands, people, media, communications
Relentlessly passionate and resolute
Planning and time management excellence.
Embrace challenges
Proactive, especially in pushing for new opportunities, approaches, and ideas.
Keenly focused on action and solutions; thrives with deep critical thinking and analysis.
Pioneering insight attitude and research in-the-know.
Resourcefulness, flexibility and adaptability, strong ability to pivot when the need arises.
Inspired to be part of the insight journey/revolution with a growing, dedicated team

Additional Information

All your information will be kept confidential according to EEO guidelines.
23-2795",3.9,"Spark Foundry
3.9","Dallas, TX",51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
31,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
32,Senior Data Engineer,Employer Provided Salary:$105K - $186K,"**(UPDATED 8/21/2023) We invite applicants to express their interest in this role, but please note that we currently don't have a specific timeline for filling this position. This posting is intended for talent pooling. While there isn't a definitive hiring period, potential candidates should be aware that the role might be filled at some point in the future. We encourage you to start the process and discover more about this opportunity, Slalom, and potentially a career in consulting through discussions with our talent acquisition team.
Who You’ll Work With
As a modern technology company, we've never met a technical challenge we didn't like. We enable our clients to learn from their data, create incredible digital experiences, and make the most of new technologies. We blend design, engineering, and analytics expertise to build the future. We surround our technologists with interesting challenges, innovative minds and emerging technologies.
A Senior Data Engineer at Slalom is a technical team member who designs and develops data platform solutions. It uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g., data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc.
What You’ll Do
Work as part of a team and with clients to develop cloud data and analytics solutions
Make recommendations and prove data architecture concepts to team members and clients
Development of cloud data platforms and business intelligence solutions
Data wrangling of heterogeneous data sources for exploring and discovering new insights
Work with highly collaborative agile teams on modern data architecture projects
Key success measures of a Senior Data Engineer include technical proficiency in data processing technologies, collaboration with other teams, communication, and willingness to learn new things
What You'll Bring
Experience in one or more cloud technologies (AWS, Azure, Google Cloud, Snowflake)
Experience in data modeling, design, and development using complex SQL (SQL Server, Oracle, etc.)
Experience in Apache Airflow, Talend, SSIS, Informatica, Alteryx, or other data processing tool
Experience in one or more programming languages (Python, C#, Scala, R, PowerShell, Java, etc.)
Experience in cloud engineering, serverless architecture, and/or data streaming services
About Us
Slalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six countries and 43 markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 13,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.
Compensation and Benefits
Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance.
Slalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $105,000 to $186,000. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.
EEO and Accommodations
Slalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team if you require accommodations during the interview process.
#LI-JS8
#LI-Hybrid",4.3,"Slalom Consulting
4.3","Los Angeles, CA",10000+ Employees,2001,Company - Private,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
33,Sr. Data Engineer( ETL testing experience),Employer Provided Salary:$50.00 - $60.00 Per Hour,"Key Skills to evaluate – Python (advanced level), Pyspark, data flow pipeline in AWS, distributed system, Snowflake, Redshift, ETL testing, QE knowledge
JD:
· Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Job Type: Full-time
Pay: $50.00 - $60.00 per hour
Experience level:
8 years
Experience:
python advanced: 10 years (Preferred)
pyspark: 10 years (Preferred)
aws: 10 years (Preferred)
snowflake: 10 years (Preferred)
Work Location: Remote",-1,Sana Pivot Inc,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
34,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
35,AWS Data Engineer II,$77K - $105K (Glassdoor est.),"Overview:
Shift4 (NYSE: FOUR) is boldly redefining commerce by simplifying complex payments ecosystems across the world. As the leader in commerce-enabling technology, Shift4 powers billions of transactions annually for hundreds of thousands of businesses in virtually every industry. For more information, visit www.shift4.com.

Shift4 Payments is seeking a Business Intelligence Engineer with an AWS Focus to join our growing internal Information Technology team. As a BI Engineer, you will help design and maintain the flow of information and analytics to the entire organization utilizing AWS capabilities to store, move and report on data. The position requires at least 3 years of prior experience as a Business Intelligence AWS engineer. We are looking for individuals that are extremely self-sufficient, available to work flexible hours and hold themselves to the highest standards of professionalism.Maintain and expand BI AWS architecture and content.
Responsibilities:
Support of BI AWS architecture and content up to and including off hours support when necessary.
Work on necessary bugs, enhancements and new system developments utilizing our Software Development Life Cycle (SDLC) standards.
Complete root cause analysis and provide recommendations to leadership to address opportunities.
Contribute to and enforce the use of the organization's Business Intelligence solution.
Develop software solutions that will provide solid foundations for the growth of the company.
Support key metrics and shared goals that determine success for a particular area.
Qualifications:
3-5+ years of professional experience in business intelligence AWS development or a related field.
Advanced experience working with AWS: S3, Glue, Lambda, Step Functions, Redshift, Athena.
Advanced experience working with various data sources such as AWS S3 and Redshift, Postgres, SQL Server, Amazon Athena, Excel, Flat Files, etc.
Advanced experience working with large data sets from sources such as AWS S3 and Redshift, Postgres, SQL Server, etc.
Strong PySpark, Python, CLI proficiency
Strong SQL proficiency (T-SQL, MySQL, and PostgreSQL).
Experience working with an Enterprise Data Warehouse and Dimensional Databases is a plus.
Advanced experience of the SDLC and how Development processes fit into KANBAN and Scrum Frameworks.
Experience with Jira and Confluence is a plus.
Ability to prioritize multiple tasks and easily adjust to changing priorities.
Ability to identify problems, initiate solutions, and effectively collaborate and communicate with team members.
Have excellent verbal and written communication skills.

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.",3.5,"Shift4 Payments
3.5","Allentown, PA",1001 to 5000 Employees,1999,Company - Public,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
36,Senior Data Engineer,Employer Provided Salary:$166K - $232K,"Patreon is the best place for creators to build memberships by providing exclusive access to their work and a deeper connection with their communities. We're building a content and community platform where creators can engage directly with their fans and monetize their creativity, while maintaining full ownership over the work they make and the communities they create.
We're leaders in the membership space with 250,000+ active creators and over $3.5 billion paid directly to creators on our platform. Our team is building tools to optimize the creator-to-fan relationship, including native video, enhanced podcasting features, improved creation tools, and new community experiences. We're continuing to invest heavily in building the most talented team in the Creator Economy and are looking for a Data Engineer to support our mission.
This role is available to those wishing to work in our SF and NY offices on a hybrid work model or those wishing to be fully remote in the United States.
About the Role
Work on a tight-knit team of highly motivated and experienced data engineers with frequent collaboration with data scientists, product managers and product engineers.
Work on both ""data analytics"" and ""data infrastructure"" type projects in a fast-paced, high-growth startup environment.
Build core data sets and metrics to power analytics, reports and experimentation.
Write real-time and batch data pipelines to support a wide range of projects and features including our creator-facing analytics product, executive reporting, FP&A, marketing initiatives, model training, data science analytics, A/B testing, etc.
Help manage and build out our data platform and suite of data tools.
Be a driver of a data-centric culture at Patreon. Work autonomously on large green field initiatives and help define data best practices at the company.
About You
Expert in SQL, Spark and Python or Scala
Significant experience modeling data and developing core data sets and metrics to support analytics, reports and experimentation. Solid understanding of how to use data to inform the product roadmap.
Enjoy collaborating with Data Scientists, Product Managers and Product Engineers. Comfortable playing the role of a Project Manager in order to drive results.
Have previously built real-time and batch data pipelines using tooling such as Airflow, Spark, Kafka, S3, Fivetran, Census, etc. Experience working with event tracking frameworks, data observability frameworks and experimentation frameworks.
Experience managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc.
Highly motivated self-starter that is keen to make an impact and is unafraid of tackling large, complicated problems and putting in the work to ensure high craft deliverables.
About Patreon
Patreon powers creators to do what they love and get paid by the people who love what they do. Our team is passionate about making this mission and our core values come to life every day in our work. Through this work, our Patronauts:
Put Creators First | They're the reason we're here. When creators win, we win.****
Build with Craft | We sign our name to every deliverable, just like the creators we serve.
Make it Happen | We don't quit. We learn and deliver.
Win Together | We grow as individuals. We win as a team.
We hire talented and passionate people from different backgrounds because we want our company to represent the vast diversity of our creators. If you're excited about a role but your prior experience doesn't match with every bullet point outlined above, we strongly encourage you to apply anyway. If you're a creator at heart, are energized by our mission, and share our company values, we'd love to hear from you.
Patreon is proud to be an equal-opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.
Patreon offers a competitive benefits package including and not limited to salary, equity plans, healthcare, unlimited paid time off, company holidays and recharge days, commuter benefits, lifestyle stipends, learning and development stipends, patronage, parental leave, and 401k plan with matching.
The posted range represents the expected salary range for this job requisition and does not include any other potential components of the compensation package, benefits and perks previously outlined. Ultimately, in determining pay, we'll consider your experience, leveling, location and other job-related factors.
San Francisco Pay Range
$166,000—$231,500 USD",3.2,"Patreon
3.2","San Francisco, CA",51 to 200 Employees,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
37,Data Engineer IV,Employer Provided Salary:$149K,"Who We Are
Invitation Homes is a fast-paced evolving publicly traded REIT that is pioneering a new industry with over 80,000 single family rental homes and a market capitalization of over $25 billion. We are a team of more than 1,400 associates who come from all walks of life. We call different communities “home,” but our shared values bind us together. Invitation Homes is a place where possibility lives.
THR Property Management LP / Data Engineer / Dallas, TX: Develop and maintain data pipelines into data warehouse and data lake, including design, and development. Develop and implement strategies to translate business requirements into feasible and acceptable data warehouse and data lake solutions. Architect and build new data models to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making. Design, build and launch data pipelines to move data to data lake and data warehouse build and maintain framework for auditing, error logging, and master data management for data pipelines. Build data expertise and own data quality for the data pipelines. Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes. Perform data analysis and assist in the resolution of data issues. Identify and resolve defects of complex scope using proper engineering tools and techniques. Provide support and maintain existing products and add new features. Mentor engineers by providing advice, coaching and educational opportunities. Telecommuting permitted from any location within the U.S. Salary: 148512/year.
The Senior Data Engineer is accountable for the maintenance, improvement, and movement of data in the Enterprise Data Warehouse, and other data sources used for enterprise reporting. A successful Senior Data Engineer delivers database design, implementation, and schematics that creates sustainable, competitive advantage for our company. Further, a Senior Data Engineer is responsible for the development and deployment of innovative analytics data platforms to support end-user enterprise reporting. Working with other data engineers, data analysts, and data scientists, a Senior Data Engineer must have a delivery first mentality and a firm grasp of Agile Development methodology. Lastly, the ideal candidate must demonstrate the ability to work as a team member and team leader.
What You’ll Do
Accountable for the overall performance and maintenance of our SQL Server environments:
Manage the EDW relational database for optimized performance
Design scalable ETL packages from the business source systems
Ensure all necessary testing and validation for all planned and unplanned releases
Mentor junior data engineers in best practices and documentation.
Act as our database evangelist in leading innovation activities through exploration, benchmarking and implementation of data technologies
Participate in solution-based activities; articulate your ideas through well thought out documentation and research best-in-class solutions
Lead and Develop and implement data architecture standards
Manage the necessary testing and validation to ensure proper compliance to data governance and quality
Prepare progress reports regarding our database environments’’ status and health
Manage troubleshooting data issues and present solutions to these issues.
Resolve tactical issues thoroughly while always looking for optimization and documenting lessons learned
Understand when tactical issues are really systematic issues requiring deeper level of analysis and strategic solutions
Cooperate with the support team to investigate and resolve data related defects
Proactively analyze and evaluate current state of data in order to identify and recommend improvements and optimization
Support our Agile Development methodology
Constantly prioritize user stories; balance feature development, defects, and tech debt according to business need
Be a champion of continuous improvement and delivery; define the minimum viable product, focus teams on key release milestones, and leverage fast follow-up sprints
Enable sprint progress; clarify story details, validate completed stories, and prepare stories for the next sprint
Who We’re Looking For
Bachelor’s degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field. An equivalent of the same in working experience is also accepted for the position.
A candidate for the position will have as at least 5 years of working experience as a database engineering or a database engineering administrator within a fast-paced a complex business setting.
Design, implement and maintain SQL Server databases
Design, implement and maintain ETL processes using SQL Server SSIS
Provide ongoing maintenance support through SQL query tuning and optimization
Strong working and conceptual knowledge of building and maintaining physical and logical data models
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience working in SaaS, IaaS, and PaaS
Excellent customer service and interpersonal skills; ability to relate to and get along with others
Professional verbal and written communication skills
Strong organizational and time-management skills
Ability to multi-task and maintain flexibility and creativity in a variety of situations
Ability to analyze and resolve problems
Ability to set and meet goals and consistently meet deadlines
Ability to maintain confidentiality
Why Invitation Homes
We stand for flexibility, opportunity, and a home that people can make their own. It’s as true for our associates as it is for our residents. Just like we help our residents live freer, we liberate our associates’ careers, too. Our associates know that at Invitation Homes goals matter, potential is unlocked, and careers thrive. Invitation Homes isn’t just a work place it is a possibility place. Invitation Homes offers the below to each new associate:
Competitive pay and an annual bonus program for all associates
Generous paid time off plans including vacation accrual, sick time, volunteer time, and standard and floating holidays
401k with matching company contributions
Awesome work environment with casual dress
Team events and gatherings
Employee resource groups: Together with Women, Asian Alliance, Black Collective, Juntos, Gen Next, and Open Invitation.
Invitation Homes truly is where possibility lives, pour a new foundation here!
Salary Range
$98,640.00 - $170,976.00
Compensation and Benefits
To attract and retain top talent, we're pleased to offer competitive compensation and benefits, including:
Annual bonus program
Health, dental, vision, and life insurance
Long-term and short-term disability insurance
Generous paid time off plans include vacation accrual, sick time, standard holidays and floating holidays
401(k) with company matching contributions
Awesome work environment with casual dress
Team events and gatherings (Pre- and Post-Covid)
Invitation Homes is an equal opportunity employer committed to fostering a diverse, inclusive and innovative environment with the best associates. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, Veteran status or any other factor protected by applicable federal, state or local law. If you have a disability or special need that requires accommodation, please contact us at
humanresources@invitationhomes.com
.
To all recruitment agencies: Invitation Homes does not accept agency resumes. Please do not forward resumes to Invitation Homes employees. Invitation Homes is not responsible for any fees related to unsolicited resumes.",3.9,"Invitation Homes
3.9","Dallas, TX",1001 to 5000 Employees,2012,Company - Public,Real Estate,Real Estate,$1 to $5 billion (USD)
38,Sr. Data Engineer,Employer Provided Salary:$84K - $131K,"Title: Sr Data Engineer
Location: Remote
Length: Contract-To-Hire
WHAT YOU'LL NEED:
Bachelor’s degree in computer science, Management information systems (MIS) or related degree / experience commensurate to a degree preferred
5+ years of hands-on experience implementing, maintaining, and supporting data management solutions including program/project delivery
7+ years of experience with SQL & T-SQL code development - experience with Snowflake preferred
5+ years of experience with Python – A MUST HAVE!
4+ years of experience with Databricks
4+ years of experience designing, building and deploying solutions with Azure Data Factory – A MUST HAVE !
4+ years of experience with logical modeling for visualization tools (Tableau, Power BI, Sigma)
4+ years of Experience with Data lake technologies including ADLS Gen. 2, AWS S3, AWS Glue preferred
Job Type: Full-time
Pay: $84,181.34 - $130,840.44 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Data Engineer: 10 years (Required)
Azure Data Lake: 6 years (Required)
Data bricks: 5 years (Preferred)
Python: 4 years (Required)
Work Location: Remote",3.9,"G Associates LLC
3.9",Remote,501 to 1000 Employees,2003,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
39,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
40,Senior Data Engineer,$115K - $155K (Glassdoor est.),"At Compass, our mission is to help everyone find their place in the world. Founded in 2012, we're revolutionizing the real estate industry with our end-to-end platform that empowers residential real estate agents to deliver exceptional service to seller and buyer clients.
The Data Platform team at Compass is responsible for building and operating a unified, secure and scalable data platform, making high-quality data available for business intelligence, financial reporting, and data science use cases.
As a Senior Engineer of the Data Platform team, you will play a crucial role in building and operating the data infrastructure that serves the entire company. Your responsibilities will include delivering high-quality results to ensure the security, compliance, optimization, and automation of the data infrastructure. Additionally, you will take ownership and actively contribute to the architectural innovations while maintaining a high standard for quality.
To thrive in this role, effective collaboration with engineers and stakeholders across the company is essential. A strong sense of ownership for the product you work on is important to you. You have a genuine passion for learning and find joy in sharing your knowledge with others. You try to understand others before seeking to be understood. Every day you wake up excited about the new things you'll learn and the people you can inspire.
Responsibilities:
Data architecture: Evaluate data platform architecture. Build and optimize data services to improve compliance, security, reliability, performance, and resource utilization.
Compliance: Build process and tools to ensure that the data platform is compliant with SOX (Sarbanes-Oxley Act), GLBA (Gramm-Leach-Bliley Act ), and CCPA (California Consumer Privacy Act).
Security: Identify infrastructure security-relevant actions, tests, and key performance indicators. Enhances the security posture of the Compass data platform, ensuring the protection of data, systems, and assets.
Infrastructure automation: Increases reliability, enhances employee productivity, reduces security attack surfaces, and eliminates human errors through standardization of process and Infrastructure as Code.
Data infrastructure monitoring. Monitor the platform health, reliability, cost, and usage.
Best practices and guardrails: Champion the best practices and guardrails for using the data platform. Identify cost-saving opportunities and optimize the data platform for all stakeholders.
Operational support: Support the platform users for troubleshooting, configuration, version updates, and feature adoptions.
Qualifications:
8 years of development experience with a focus on architecting cloud data platforms, setting up, managing and automating the infrastructure of cloud data platforms, including Airflow, Databricks, Spark, and Kafka
5 years of developer experience using Infrastructure as Code (IaC) technologies, such as AWS Cloud Formation and Terraform
3 years of experience with data compliance and governance. Demonstration of collaboration with the internal compliance team and external auditors in planning and execution of all phases of SOX compliance including risk assessment & scoping, documentation of process walkthroughs, identifying controls and key reports, testing, and reporting results. Implement customer data deletion requests that are required by CCPA. Standardize and automate the compliance process.
5 years of experience maintaining and improving OE of data platforms - service alerting, monitoring,and automation.
8 years of experience and proficient programming skills in Python or Java/C#. 3 years of programming experience in SQL.
B.S., M.S., or PhD. in Computer Science or equivalent
Desirable to Have:
Subject matter expert (SME) in architecting cloud data platforms using Databricks, Snowflake, AWS Lake Formation or other relevant technologies.
SME in big data technologies including Spark, Presto, Airflow, Kafka
Work in a startup-like environment building agile products and services
Create clear and concise documentation, including diagrams, service descriptions, decisions, and runbooks.
Deep knowledge of cloud platform such as AWS, GCP, Azure
Proven records of improving services in your domain throughout their lifecycle. Identify and remove bottlenecks to address inefficiencies in the developer experience.
Check out Compass's Engineering blog!
Perks that You Need to Know About:
Participation in our incentive programs (which may include where eligible cash, equity, or commissions). Plus paid vacation, holidays, sick time, parental leave, marriage leave, and recharge leave; medical, tele-health, dental and vision benefits; 401(k) plan; flexible spending accounts (FSAs); commuter program; life and disability insurance; Maven (a support system for new parents); Carrot (fertility benefits); UrbanSitter (caregiver referral network); Employee Assistance Program; and pet insurance.

Do your best work, be your authentic self.
At Compass, we believe that everyone deserves to find their place in the world — a place where they feel like they belong, where they can be their authentic selves, where they can thrive. Our collaborative, energetic culture is grounded in our Compass Entrepreneurship Principles and our commitment to diversity, equity, inclusion, growth and mobility. As an equal opportunity employer, we offer competitive compensation packages, robust benefits and professional growth opportunities aimed at helping to improve our employees' lives and careers.
Notice for California Applicants",3.6,"Compass
3.6","Austin, TX",1001 to 5000 Employees,2012,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
41,"Staff Engineer, Data Infrastructure",Employer Provided Salary:$180K - $240K,"The Mission:
This is a critical and exciting time at Enigma. We're transforming the small business financing ecosystem, and our product is gaining adoption even faster than we anticipated. This creates a new set of technical challenges for us as we continue to scale.
Over the past year we've made significant investments in our data infrastructure to allow engineers and data scientists to quickly deliver customer value by reliably testing and shipping changes to our data pipeline. The challenges ahead are to:
Reduce overall runtime of our data pipeline
Improve computational efficiency to reduce costs
The Role:
This is a role responsible for developing and implementing our strategy for driving data processing efficiency. Your impact will be measured by:
The increase in the team's capability to optimize data transformations (combination of methodology and tooling)
The specific improvements we achieve in processing time and computational cost
This role has an important hands-on component - you'll be showing the team what good looks like. However, your greatest impact will be in the technical strategy you'll formulate and the close coaching and mentorship you'll provide to team members.
We're looking for someone who:
Is motivated by leading engineers to increase the efficiency and speed of complicated data processing systems
Thrives as a coach and mentor and measures their impact by the increase in capabilities of the team
Operates with a bias for action and knows how to deliver value in the short, medium and long term
Adopts a principled approach to difficult data processing problems and demonstrates expertise and excellence in their engineering craft
Operates transparently, collaboratively and with low ego—loves learning from others and having their ideas questioned and challenged
What makes this job interesting?
Impact: Your decisions will determine the speed at which we can scale to take on new customers and impact the developer experience of dozens of teammates.
Technical Challenge: You will take on some of our thorniest technical problems and render them tractable.
Leadership: Success in this role will achieved through building the team's capabilities and influencing our engineering culture.
Our ideal candidate:
Knows Databricks and Spark inside and out
Brings a clear point of view on data processing optimization, data modeling, cluster management and Databricks performance techniques
Has a strong track record of technical leadership to grow and scale teams
About Us:
At Enigma, we're building the single, most reliable source of data on businesses to power the future of financial services. By engineering better data from hundreds of public and third-party sources, we aim to tell the complete story of every business, so that companies of every size can access the financial services they need to grow and thrive. Our core values – generosity, curiosity, ingenuity, & drive – guide everything we do, from how we make our most important product decisions to how we work with and support one another on a daily basis. We're a team of curious, driven individuals with diverse backgrounds and skills, but we're all passionate about engineering deeper understanding through data—together. If this resonates, we would love to hear from you!
We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.
Salary Range: $180,000-$240,000
A note on salary ranges: we pride ourselves on paying competitively for our size and industry. Salary is one piece of a total Enigma compensation package that includes additional benefits and opportunities. All of our compensation packages include equity because we believe 100% of Enigma employees should have the option to purchase ownership in the company and benefit from the value we're creating together.",-1,Enigma,Remote,Unknown,-1,Company - Public,-1,-1,Unknown / Non-Applicable
42,Senior Data Engineer,Employer Provided Salary:$190K,"Welcome to the MOMENTUM Family!
MOMENTUM is not just our company name; it is the highest value we deliver to our customers. We are a rapidly growing technology solutions company delivering innovative technology, engineering, and intelligence solutions across the DoD sector. The efforts of our high-capacity team ultimately strengthen our Nation and the warfighter.

Our team is dispersed throughout the US, which means we value the diversity and unique collaboration fostered throughout our team. We work incredibly hard for our customers and believe deeply in our core values. We're a high-energy, high-growth team and we love to win.

Data Engineer
The Data Engineer provides engineering support to the data science and software engineering team members. Includes augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments. Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, to include hardware and software support to existing servers.

In this role, you will:
Support ad-hoc data analysis requirements defined by the client's Leadership.
Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives.
Knowledge of business intelligence reporting tools and data visualization software including Tableau.
Will work on Data cleaning and transformation efforts in delivery of CDRLs.
Experienced in extracting and aggregating structured and unstructured data.
Experienced in data programming languages and tools such as Python and R.
Experience with SQL or similar database language.
Experience designing and implementing data models to enable, sustain and enhance the value of information they contain.
Strong analytical and critical thinking skills.
Ability to work collaboratively and effectively in a team environment.
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers
Make sure pedigree and provenance of the data is maintained such that the access to data is protected
Clean and preprocess data to enable analytic access
Collaborate with enterprise working groups to advance the state of data standards
Collaborate with the engineering team, data stewards, and mission partners to aid in processes getting actionable value out of the data holdings architects complex, repeatable ETL
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic and others
Supports Experience with Targeting using Sponsor Tools, Reverse Engineering
Develop API connectors to enable ingest of new data catalog entries from databases and files to ensure that data mappings will provide the best performance for expected user experience
Supports Deliverables and Reports

If you're suitable for this role, you have:
Top Secret SCI with FULL SCOPE POLY REQUIRED
9+ Years of verifiable experience


To learn more about us, check out our website at www.gomomentum.tech!

MOMENTUM is an EEO/M/F/Veteran/Disabled Employer:
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The qualifications listed above are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions.

Accommodations:
Consistent with the Americans with Disabilities Act (ADA) and Alabama civil rights law, it is the policy of Momentum to provide reasonable accommodation when requested by a qualified applicant or employee with a disability, unless such accommodation would cause an undue hardship. The policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If reasonable accommodation is needed, please include a request when applying.",3.6,"Momentum
3.6","Chantilly, VA",501 to 1000 Employees,1987,Company - Public,Advertising & Public Relations,Media & Communication,$100 to $500 million (USD)
43,Data Engineer,-1,"Description:
Who is Leafwell
Leafwell is a rapidly growing technology and data company that set out to increase access, education, and research into cannabis and to advance its application as medicine.
An exciting opportunity for a Data Engineer to join our growing team has arisen.
What to Expect as a Data Engineer at Leafwell
As a Data Engineer at Leafwell, you will be in charge of creating and orchestrating the Leafwell data pipeline, which means gathering data, creating & automating data transformations and producing actionable insights for Leafwell’s internal stakeholders. You will support critical testing and rollout of new data features. The Data Engineer will build and maintain systems that inform Leafwell’s business stakeholders about Key Performance Indicators (KPIs) and suggest data-driven strategies to optimize those metrics.
Essential Duties and Responsibilities
The Data Engineer will perform the following responsibilities:
Acquire, assemble, transform and analyze data
Create, manage and orchestrate the data pipeline and it’s infrastructure
Present findings, trends, and suggested optimizations
Identify new opportunities and threats to the company's business model
Update and revise reports, queries, and analytic procedures as necessary
Design and implement tracking so that optimization efforts can be measured
Identify inefficiencies in data processes and automate where appropriate
Write and update international SOPs and internal documentation
Support IT systems management with testing, validation, and user support
Proactively identify initiatives for data-related improvements
Why Leafwell
At Leafwell, we are passionate about our work and seek out employees who contribute the same level of dedication and enthusiasm. We are only as good as the people we hire, so we aim to be the best employer in order to attract the top talent in the industry.
Do we have your Interest?
Requirements:
Our Ideal Candidate
Our Ideal Candidate will possess the following:
Bachelor's degree in Computer Science, Mathematics, Economics, Information Systems, or another quantitative field
3-5 years of relevant professional experience in Data Engineering / Analytics
Advance working knowledge and experience in SQL and relational databases
Strong data management abilities, including data analysis, standardization, cleansing, querying, and consolidation of data
Data visualization experience (e.g Tableau, Looker, etc.)
Exercises self-reliance, initiative, and leadership while keeping stakeholders properly informed and involved
Reliably manage numerous duties during a workday, necessitating interactions with people located across the world
Technically competent, with the ability to quickly learn new processes and programs, and utilize various software applications
Excellent communication and interpersonal skills; ability to work with and appeal to a wide variety of personalities and professional tendencies
Bonus points if you have experience in the following:
Cannabis knowledge; industry experience is a plus
Used Data Orchestration tools like Dagster or Airflow
Used dbt or comparable data transformation software
Git
Python or R programming
Amazon Web Services
PostgreSQL and RedShift
CRM products
Data Visualization in Metabase
Effective project management skills and comfort utilizing a project management platform in collaboration with other team members
Data Science projects
Benefits Highlights
Our benefits include, but are not limited to:
Remote first - Most of our employees are 100% remote. Positions requiring travel enjoy a hybrid environment.
Rapid Growth - Our company is rapidly expanding, and our employees are advancing with us!
Training and Development - We foster a culture of learning. We encourage our employees to take part in educational, training, and development opportunities.
Leafwell is dedicated to bringing together people from diverse cultures and perspectives. We work hard to provide a welcoming atmosphere where everyone may flourish, have a sense of belonging, and collaborate effectively. As an equal opportunity employer, we do not tolerate any illegal discrimination against job applicants based on their race, color, religion, veteran status, sex, parental status, gender identity or expression, transgender status, sexual orientation, national origin, age, disability, or genetic information. We are committed to going above and above in creating diversity within our firm and follow the regulations upheld by the EEOC.",4.0,"LEAFWELL
4.0",Remote,51 to 200 Employees,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
44,Sr. Data Engineer,$80K - $112K (Glassdoor est.),"Object Technology Solutions, Inc (OTSI) has an immediate opening for Sr. Data Engineer
This is an on-site position in Dallas, TX OR Kansas City, MO (Long term)
Skills & abilities required:
Minimum 5+ years of experience as Data Engineer.
Prefer to have Databricks experience
Prefer to have understanding of embedding Python into Azure environment (containerization)
Expectation is that this position works with Data Scientist to rapidly deploy models into Azure DW and/or pipeline DW required data to Data Scientist requirements to build and test new models.
About us:
OTSI is a leading global technology company offering solutions, consulting, and managed services for businesses worldwide since 1999. OTSI serves clients from its 15 offices across 6 countries around the globe with a “Follow-the-Sun” model. Headquartered in Overland Park, Kansas, we have a strong presence in North America, Central America, and Asia-Pacific with a Global Delivery Center based in India. These strategic locations offer our customers the competitive advantages of onshore, nearshore, and offshore engagement and delivery options, with 24/7 support. OTSI works with 100+ enterprise customers, of which many are Fortune ranked, OTSI focuses on industry segments such as Banking, Financial Services & Insurance, Healthcare & Life Sciences, Energy & Utilities, Communications & Media Entertainment, Engineering & Telecom, Retail & Consumer Services, Hi-tech, Manufacturing, Engineering, transport logistics, Government, Defense & PSUs.
Our Center of Excellence:
Data & Analytics
Digital Transformation
QA & Automation
Enterprise Applications
Disruptive Technologies
Job Type: Contract
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: In person",4.6,"OTSi
4.6","Dallas, TX",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
45,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
46,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
47,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Data Engineer
Candidate must have AWS (Redshift, Glue, S3), Spark, Phython, Scala and Should be expert in SQL
Basic qualifications
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
Design, implementation and management of data processing systems dealing with large data sets.
Proficiency in, at least, one modern programming language such as Python, Java or Scala
Hands-on experience and advanced knowledge of SQL.
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and cloud computing
Experience working with Open-Source Big Data tools (Hadoop, Spark, Hive, Presto and etc.)
Preferred qualifications
Experience in working and delivering end-to-end projects independently
Relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics
Experience working with AWS Big Data Technologies (AWS Glue, EMR, Athena, Ec2, Redshift and S3 etc.)
Experience working with Oracle, PostgreSQL, SQL Server or MySQL.
Proven track record of delivering a big data solution
Experience working with both Batch and Real Time data processing systems
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Expected hours: 40 per week
Compensation package:
1099 contract
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Austin, TX 78703: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 8 years (Required)
Python: 8 years (Required)
AWS: 5 years (Preferred)
Work Location: Hybrid remote in Austin, TX 78703",-1,PRISM IT LLC,"Austin, TX",-1,-1,-1,-1,-1,-1
48,Data Engineer,Employer Provided Salary:$91K - $116K,"A reasonable salary expectation is between $91,000.00 and $116,022.40, based upon experience and internal equity.
This position is on a hybrid work schedule. (Mon & Fri - remote, Tues - Thurs onsite in Rancho Cucamonga, CA)
Position Summary/Position
The Data Engineer II assists in the implementation of methods to improve data reliability and quality. This role is responsible for combining raw information from different sources to create consistent and machine-readable formats. The Data Engineer II must also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling. The Data Engineer II will focus on data accessibility, which will enable the organization to utilize data for performance evaluation and optimization. The data Engineer II is also responsible for managing the entire back-end development life cycle for the company's enterprise data warehouse. In this role the incumbent will handle tasks associated with the implementation of ETL procedures, building warehouse databases, database performance management, and dimensional modeling and design of the table structures.
Major Functions (Duties and Responsibilities)
1. Design and develop data warehouse Extraction, Transformation and Loading (ETL) solutions using Microsoft SQL Server Integration Services (SSIS), Azure Data Factory, Synapse Analytics, Az Data Bricks, PySpark ETL.
2. Develop and implement data collection processes in conjunction with the data warehouse. Source data from legacy systems supporting a centralized data warehouse and reporting platform.
3. Develop technical solutions to meet the requirements for Data Warehouse, BI & Analytics
4. Work closely with the data engineering and BI & Analytics teams to design data and analytics solutions to increase the usability, completeness, and accuracy of enterprise data
5. Analyze user requirements and translate into database requirements and implement in database code
6. Create and maintain the optimal data pipeline architectures based on micro services based on platform and application requirements
7. Assemble large, complex data sets that meet functional / non-functional business requirements
8. Identify, design, and implement process improvements: automating manual pipeline processes, optimizing data ingestion and consumption, re-designing infrastructure for greater scalability, micro services, etc
9. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
10. Work closely with Data Warehouse Architect and Data Systems Architect to design data and analytics solutions to increase the usability, completeness, and accuracy of enterprise data
11. Create, maintain, and optimize SQL queries and routines
12. Analyze potential data quality issues to determine the root cause and create effective solutions.
13. Develop, adopt, and enforce Data Warehouse and ETL standards and architecture
14. Monitor and support ETL processes ensuring integrity and proper integration of all data sources
15. Create high throughput historical and incremental ETL jobs
16. Facilitate problem management, and communication among data architects, managers, informaticists and analysts
17. Provide detailed analysis of data issues; data mapping; and the process for automation and enhancement of data quality
18. Perform development activities such as source to target mapping validations, identify, document and execute unit test cases/scripts, peer and lead code reviews per code review checklist and document test and review results.
19. Collaborate and contribute to data integration strategies and visions
20. Provide ongoing proactive technical support for ETL and data warehouse system to ensure business continuity.
21. Work with Informaticists and Analysts to translate analytic requirements into technical solutions.
Experience Qualifications
Four (4) years of relevant work experience. Experience and knowledge in logical, rational, dimensional, and physical data modeling. Background in database systems along with a strong knowledge of SQL. Experience with Orchestration tools, Azure DevOps, and CI/CD. Intermediate experience with the following tools and technologies:
a. Azure Data Catalogue / Purview
b. Azure Cloud
c. Databricks
d. Power BI Dataflows
e. Power Query
f. Azure Cosmos
g. Azure Monitor
h. PowerShell
i. Python
Preferred Experience
Development experience using PySpark, Spark, Hadoop, Kubernetes, and RDMIS is highly desired.
Education Qualifications
Bachelor's degree from an accredited institution required.
Preferred Education
Master’s degree from an accredited institution preferred.
Professional Certification
Azure Data Engineering Certification is preferred.
Knowledge Requirement
Multi-server environment knowledge such as linked servers, data replication, backup/restore with MS SQL Server 2008+. Knowledge of applicable data privacy practices and laws.
Skills Requirement
Highly skilled in developing and optimizing T-SQL (DDL, DML, DCL) queries, stored procedures, functions, and views for various applications that involve numerous database tables and complex business logic. Good written and oral communication skills. Strong technical documentation skills. Good interpersonal skills.
Abilities Requirement
Highly self-motivated and directed. Keen attention to detail. Proven analytical and problem-solving abilities. Ability to effectively prioritize and execute tasks in a high-pressure environment.
Commitment to Team Culture
The IEHP Team environment requires a Team Member to participate in the IEHP Team Culture. A Team Member demonstrates support of the Culture by developing professional and effective working relationships that include elements of respect and cooperation with Team Members, Members and associates outside of our organization.
Working Conditions
Word processing and programming involving computer keyboard and screens.
Position is eligible for Hybrid work location upon completing the necessary steps and receiving HR approval. All IEHP positions approved for telecommute or hybrid work locations may periodically be required to report to IEHP’s main campus for mandatory in-person meetings or for other business needs as determined by IEHP leadership.
Inland Empire Health Plan (IEHP) is the largest not-for-profit Medi-Cal and Medicare health plan in the Inland Empire. We are also one of the largest employers in the region, designated as “Great Place to Work.” With a provider network of more than 5,000 and a team of more than 3,000 employees, IEHP provides quality, accessible healthcare services to more than 1.5 million members. And our Mission, Vision, and Values help guide us in the development of innovative programs and the creation of an award-winning workplace. As the healthcare landscape is transformed, we’re ready to make a difference today and in the years to come. Join our Team and make a difference with us! IEHP offers a competitive salary and stellar benefit package with a value estimated at 35% of the annual salary, including medical, dental, vision, team bonus, and state pension plan.
Job Types: Full-time, Permanent
Pay: $91,000.00 - $116,022.40 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Parental leave
Retirement plan
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Rancho Cucamonga, CA 91730: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
data engineering: 4 years (Required)
Work Location: Hybrid remote in Rancho Cucamonga, CA 91730",3.7,"Inland Empire Health Plan
3.7","Rancho Cucamonga, CA",1001 to 5000 Employees,1996,Company - Public,Health Care Services & Hospitals,Healthcare,$1 to $5 billion (USD)
49,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
50,Senior Data Engineer,-1,"CoreTrust is the market leading commercial Group Purchasing Organization (GPO), leveraging the combined purchasing volume of its 3,000+ members to negotiate preferential pricing and terms across more than 80 indirect spend categories. Strategically aligned with private equity portfolios and large independent companies, we complement sourcing bandwidth and improve supply chain efforts with our industry-leading national contracts.
Recently acquired by Blackstone Private Equity, CoreTrust is growing rapidly and we’re looking for a passionate Senior Data Engineer.
Reporting to the Director of Data, you will be part of the data solutions team and be responsible for building our data platforms, enabling the use of advanced analytics to drive continued evolution and growth.
You will create and manage data pipelines to feed and curate our data lake solution and help develop our data roadmap. The ideal candidate has a great understanding of various data / tech solutions (e.g., data modeling tools, data pipeline, data catalogs, cloud databases) and a record of using them to bring tangible dollar impact. You should be excited to seek out and capitalize on a wide variety of opportunities to use data to create value across the organization.

Responsibilities
Lead data projects to build innovative and highly available solutions while ensuring adherence to budget, schedule, and scope of project
Mentor other members in the data solutions team
Develop and assist with oversight on the data tech infrastructure
Drive data & analytics solutions from conception to deployment/delivery with clear ROI impact
Develop and maintain relationships with all relevant business and tech stakeholders and functions
Provide input to proposals for assigned projects including project objectives, technologies, systems, information specifications, timelines, and staffing
Communicate timely status updates to affected internal or external customers and stakeholders
Collect, analyze, and summarize information and trends as needed to prepare project status reports
Assist in developing a culture of data-driven decision-making, including adoption of business intelligence, analysis, and advanced analytics globally
Perform other related duties as assigned

Qualifications
Bachelor’s degree in computer or information science or relevant experience
9+ years of relevant experience in a data-driven professional setting
Ability to assist with the vision of the team (e.g., mission, priorities, engagement model, tooling)
A record of accomplishment of successfully managing complex cross-functional projects under tight deadlines
Strong technical background – familiarity with Python, SQL, cloud technologies like Azure and AWS, statistics / machine learning, Snowflake, DBT, FiveTran, Data Visualization Software
Exceptional communication and presentation skills, particularly in the context of engaging senior management teams
A successful history of manipulating, processing, and extracting value from large, disconnected datasets
Build processes supporting data transformation, data structures, metadata, dependency, and workload management
Working knowledge of creating and leveraging API or Stream based data extraction processes such as Salesforce API
Strong command of databases and SQL
Proficiency with Python or R, especially for data manipulation and analysis, and ability to build, maintain and deploy sequences of automated processes with these tools
Ability to motivate groups of people to complete a project in a timely manner
Excellent analytical, logical thinking, and problem-solving skills
Thorough understanding of project management principles and planning
Thorough understanding of information technology procedures and practices
Proficient with, or able to quickly become proficient with, a range of general and specialized applications, software, and hardware used in the organization and the industry

Benefits
Competitive compensation package
Free individual employee medical coverage
Company subsidized dental and vision coverage
Dollar for dollar 401(k) match up to 6% of your salary with immediate vesting
Company-paid Short-Term and Long-Term Disability coverage
Employee Assistance Program to support your wellbeing and mental health
$1500 annual stipend for undergraduate/graduate college courses; $500 annual stipend for continuing education courses/certifications
Free snacks and beverages on-site
Brand new, state-of-the-art, tech-enabled work environment in downtown Nashville
Flexible/hybrid work culture",2.8,"CoreTrust
2.8",Remote,Unknown,-1,Subsidiary or Business Segment,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
51,Data Science Engineer,-1,"Operations Technology brings data accessibility to the organization, with the overarching goal that all business problems have data driven answers.
Within the Operations Technology group, the Data Science Engineer’s purpose is to design, develop, and maintain data infrastructure, pipelines, and workflows. They are responsible for developing and merging modeling to ensure it stays consistent with data flowing across the organization.
They work closely with platform data owners, data analysts and consumers, and other stakeholders to ensure the data is properly collected, stored, processed, and analyzed to drive informed business decisions.
This role will become more ML focused as the data warehouse matures, with the added implementation of ML and advanced analytics models
SKILLS AND ABILITIES:
Proficiency in Azure Data Warehousing utilizing Azure Synapse (Data Factory also acceptable)
Strong understanding of database technologies and SQL queries
Strong experience with ETL processes, data integration, and data modeling
Experience with RESTful API connections in the Azure environment
Experience with cloud-based data storage and computing services
Experience with Azure networking
Excellent problem-solving and analytical skills
Experience with data visualization tools such as Tableau or Power BI
Excellent communication and collaboration skills
Ability to work independently and prioritize tasks in a fast-paced & dynamic environment
What sets you apart:
Certified in SQL, SQL Analytics, R, Python, Power BI
Experience with ML in Azure: Spark MLib, SynapseML, R, Python Anaconda, or the like
Experience with data queries to cloud ERP (Netsuite), CRM (Hubspot), Cloud Storage (Sharepoint, MS Project Online), On Prem Storage (SQL)
TREW’s story:
Business gets done working together. Successful business happens when trusted partners work together, to win together. At TREW we know that our customers buy solutions and technology built by people. With over 400 team members, we work fearlessly every day to do the right thing, even when no one is watching. From seasoned professionals to undergraduate co-ops, our team members enjoy seeing the impact of their contributions every day.
Trew/Hilmot/TKO is an equal opportunity employer. Applicants will be considered for employment without regard to age, race, creed, color, national origin, ancestry, marital status, affectional or sexual orientation, gender identity or expression, disability, nationality, sex, or veteran status.",3.5,"TREW LLC
3.5",Remote,51 to 200 Employees,-1,Company - Private,Taxi & Car Services,Transportation & Logistics,$25 to $100 million (USD)
52,Data Communications Engineer - III,Employer Provided Salary:$91K - $181K,"Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","San Francisco, CA",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
53,Junior Data Engineer,$83K - $119K (Glassdoor est.),"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a junior data engineer, you will be part of a team that is responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.
The position:
Develop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Support various components of the data pipelines, including ingestion, validation, cleansing and curation
Manage and ensure the success of ongoing data pipeline routines
Collaborate with our implementation team to assist with the identification and reconciliation of data anomalies
Create and maintain documentation on data pipelines
Engage with our software engineering team to ensure precise data points per application specifications
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering, or applicable experience
Expertise with SQL, database design and data manipulation methodologies
Expertise with ETL/ELT and the development of automated validation and data pipelines
Strong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW, master data management and other database design principles
Comfortable working with high volume data in a variety of formats
Excellent verbal and written communication
Self-motivated
Passionate at learning
Familiarity with healthcare data is a plus
Experience with CI/CD and version control tools is a plus
Experience working within hybrid cloud environment such as AWS is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation
Social outings",5.0,"Decision Point Healthcare
5.0","Boston, MA",1 to 50 Employees,2013,Company - Public,-1,-1,Less than $1 million (USD)
54,Senior Data Engineer,-1,"Are you smart, driven, curious, resourceful, and not afraid to fail? Then we want to meet you! Our team of bold, innovative, and creative teammates is what makes us a top startup to work for. FreightWaves delivers news and commentary as well as data and analytics which empower risk management and actionable market insights in the logistics and supply chain industry. If you are ready to join our team, it is time for YOU to apply!
FreightWaves is on the hunt for a curious, tenacious, and team-oriented Senior Data Engineer to join our fast paced engineering team. The ideal candidate is inquisitive, versatile, team oriented, thrives on change, and has a positive attitude. If you are ready to be challenged, learn new and exciting technologies, and have the unique opportunity to work with some of the most talented developers in the country, we want you to apply!
**This position is fully remote.**
**Must RESIDE in the United States and be eligible to work.**
What you will be doing:
Implementing ingestion pipelines, using Airflow as the orchestration platform, for consuming data from a wide variety of sources (API, SFTP, Cloud Storage Bucket, etc.).
Implementing transformation pipelines using software engineering best practices and tools (DBT)
Working closely with Software Engineering and DevOps to maintain reproducible infrastructure and data that serves both API-only customers and in-house SaaS products
Defining and implementing data ingestion/transformation quality control processes using established frameworks (Pytest, DBT)
Building pipelines that use multiple technologies and cloud environments (for example, an Airflow pipeline pulling a file from an S3 bucket and loading the data into BigQuery)
Create and ensure data automation stability with associated monitoring tools.
Review existing and proposed infrastructure for architectural enhancements that follow both software engineering and data analytics best practices.
Working closely with Data Science and facilitating advanced data analysis (like Machine Learning)
What you bring to the table:
Strong working knowledge of Apache Airflow
Experience supporting a SaaS or DaaS product, bonus points if you were creating new data products/features
Strong in Linux environments and experience in scripting languages
Python Expert
Strong understanding of software best practices and associated tools.
Experience in any major RDBMS (MySQL, Postgres, SQL Server, etc.).
Strong SQL Skills, bonus points for having used both T-SQL and Standard SQL
Experience with NoSQL (Elasticsearch, MongoDB, etc.)
Multi-cloud and/or hybrid-cloud experience
Strong interpersonal skills
Comfortable working directly with data providers, including non-technical individuals
Experience with the following (or transitioning from equivalent platform services):
Cloud Storage
Cloud Pubsub
BigQuery
Apache Airflow
dbt
DataFlow
Bonus knowledge/experience:
Experience implementing cloud architecture changes
Working knowledge of how to build and maintain APIs using Python/FastAPI
Transforming similar data from disparate sources to create canonical data structures
Surfacing data to BI platforms such as Looker Studio
Data Migration experience, especially from one cloud platform to another
Certification: Professional Google Cloud Certified Data Engineer
Our Benefits:
An excellent work environment, flat hierarchies, and short decision paths
A generous benefits package including 100% employer-paid health, dental, vision and Life insurance, STD, LTD
Stock options
Appealing 401k matching plan
Career Mentorship Opportunities
Personal Development Credit (Can be used toward Student loans or relevant PD Courses)
Annual life achievement bonus for having a baby, buying a house, or getting married (max one per year)
No set days off Vacation policy (our team takes time off as needed with supervisor approval)
Up to $50 for Gym or Virtual Gym membership.
Audible or Kindle Unlimited subscription
Discount on Ford vehicles
SieN3FASIc",3.8,"FreightWaves, Inc.
3.8",Remote,51 to 200 Employees,2017,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
55,Staff Data Engineer,Employer Provided Salary:$165K - $278K,"USA (Remote)
Affinity stitches together billions of data points from massive datasets to create a powerful, accurate representation of the world's professional relationship graph. Based on this data, we offer our users the insights and visibility they need to nurture and tap into their team's network of opportunities.
Reporting to the Director of Engineering, you'll support creating the magic that underlies Affinity's industry-leading relationship intelligence model as the key technical leader of Affinity’s Data Enrichment team.
In this role, you’ll leverage your past experiences and deep understanding of back-end technologies to help shape and execute Affinity's roadmap for dataflow and system architecture, champion engineering best practices, delivery velocity, and act as a technical mentor for other engineers on the team. You’ll play a significant role in defining the future of how businesses around the world use their relationships.
What you’ll be doing:
Drive complex technical, architecture, design, and product discussions
Lead data domain, technical and business discussions in relation to future architecture direction
Design, implement, and build data solutions that deliver data with measurable quality using Spark, Python, Databricks, and the AWS ecosystem (S3, Redshift, EMR, Athena, Glue)
Help define our data roadmap. You'll collaborate with our fast-growing team of data engineering, machine learning engineering, product, and business leaders to help to answer these questions and more
Mentoring, coaching, and inspiring the engineers on the team
Identify and fill gaps in the team, and create the processes necessary for the teams’ success

Qualifications
Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every qualification. At Affinity, we are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about this role, but your past experience doesn’t perfectly align with the qualifications above, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Required:
You have 10+ years of experience working in data engineering, with at least 3+ years of acting as a senior team lead or staff engineer, leading complex, sometimes ambiguous engineering projects across team boundaries
You have extensive hands-on experience in building scalable data platforms and reliable data pipelines using technologies such as Spark, Hadoop, DataBricks, AWS SQS, AWS Kinesis, and/or Kafka
You have experience working with large, multi-terabyte datasets and are comfortable with high-scale data ingestion, transformation, and distributed processing tools such as Apache Spark (Scala or Python)
Experience with AWS, DBX or related cloud technologies
You're comfortable with the building blocks of modern back-end systems, such as horizontally scalable data infrastructure, event-driven architecture, and beyond and can clearly articulate the pros/cons of different approaches, while also providing a recommended solution based on the current context
You have familiarity with databases and analytics technologies in the industry, including Data Warehousing, Data Lakes, ETL and Relational Databases
You have experience mentoring and helping the engineers around you grow
You have experience partnering with product and machine learning teams on large, strategic data projects and routine partner work
You take pride in delivering exceptionally high quality work in terms of data accuracy, performance, and reliability
You’re eager to contribute your ideas and experiences to help Affinity continuously improve as a product and company
Nice to have:
Experience leveraging machine learning to improve the quality of ingested data.
You have worked with multiple third party data vendors and have experience in conflict resolution approaches.

How we work:
Our culture is a key part of how we operate as well as our hiring process:
We iterate quickly. As such, you must be comfortable embracing ambiguity, be able to cut through it, and deliver incremental value to our customers each sprint
We are candid, transparent, and speak our minds while simultaneously caring personally with each person we interact with
We make data driven decisions and make the best decision for the moment based on the information available
Join us in enabling every professional on the planet to succeed by harnessing the power of their relationships.

What you'll enjoy at Affinity:
We live our values as playmakers, obsessed with learning, caring personally about our colleagues and clients, are radically open-minded, and take pride in everything we do.
We pay your medical, dental, and vision insurance with comprehensive PPO and HMO plans. And provide flexible personal & sick days. We want our team to be happy and healthy :)
We offer a 401k plan to help you plan for retirement.
We provide an annual budget for you to spend on education and offer a comprehensive L&D program – after all, one of our core values is that we're #obsessedwithlearning!
We support our employee's overall health and well-being and reimburse monthly for things such as; transportation, Home Internet, Meals, and Wellness memberships/equipment.
Virtual team building and socials. Keeping people connected is essential.
Please note that the role compensation details below reflect the base salary only and do not include any variable pay, equity, or benefits. This represents the salary range that Affinity believes, in good faith, at the time of this posting, that it will pay for the posted job.
A reasonable estimate of the current range is $165,000 to $278,000 USD. Within the range, individual pay is determined by factors such as job-related skills, experience, and relevant education or training.
About Affinity
We have raised over $120M and are backed by some of Silicon Valley’s best firms, with over 2,700 customers worldwide on our platform. We are proud to have a 4.5 Star Glassdoor rating and recently ranked; Inc.’s Best Workplaces of 2022 and Great Places to Work 2022. Passionate about helping dealmakers in the world’s biggest relationship-driven industries to find, manage, and close the most important deals; our Relationship Intelligence platform uses the data exhaust of trillions of interactions between Investment Bankers, Venture Capitalists, Consultants, and other strategic dealmakers with their networks to deliver automated relationship insights that drive over 450,000 deals every month.",4.1,"Affinity.co
4.1",Remote,201 to 500 Employees,2015,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
56,Data Engineer,Employer Provided Salary:$122K - $144K,"Join a leading fintech company that's democratizing finance for all.
Robinhood was founded on a simple idea: that our financial markets should be accessible to all. With customers at the heart of our decisions, Robinhood is lowering barriers and providing greater access to financial information. Together, we are building products and services that help create a financial system everyone can participate in.
As we continue to build...
We're seeking curious, growth minded thinkers to help shape our vision, structures and systems; playing a key-role as we launch into our ambitious future. If you're invigorated by our mission, values, and drive to change the world — we'd love to have you apply.
Robinhood has a primary in-office working environment; please be sure you have reviewed the preferred working location(s) for this role before applying.
About the team:
The preferred location for this position is in or around Robinhood's offices in Menlo Park, CA or New York, NY with in-office work capabilities, as may be required by management.
Robinhood is a metrics driven company and data is foundational to all key decisions from growth strategy to product optimization to our day-to-day operations. We are looking for a Senior Data Engineer to build and maintain foundational datasets that will allow us to reliably and efficiently power decision making at Robinhood. These datasets include application events, database snapshots, and the derived datasets that describe and track Robinhood's key metrics across all products. You'll partner closely with engineers, data scientists and business teams to power analytics, experimentation, and machine learning use cases. We are a fast-paced team in a fast growing company and this is a unique opportunity to help lay the foundation for reliable, impactful, data-driven decisions across the company for years to come.
What you'll do day-to-day:
Help define and build key datasets across all Robinhood product areas. Lead the evolution of these datasets as use cases grow.
Build scalable data pipelines using Python, Spark and Airflow to move data from different applications into our data lake.
Partner with upstream engineering teams to enhance data generation patterns.
Partner with data consumers across Robinhood to understand consumption patterns and design intuitive data models.
Ideate and contribute to shared data engineering tooling and standards.
Define and promote data engineering best practices across the company.
About you:
CS background or any other relevant fields of study.
Strong product mindset
4+ years of experience and a Bachelors degree or 3+ years and a Masters degree
Experience building high-quality data solutions
Strong analytical and problem solving skills.
Expertise building data pipelines using open source frameworks (Hadoop, Spark, etc)
Expertise in one or more programming languages (ideally Python).
Strong SQL (Presto, Spark SQL, etc) skills.
Familiarity with data visualization tools (Looker, Tableau, etc).
Great communication skills and ability to democratize data through actionable insights and solutions.
Bonus points:
Passion for working and learning in a fast-growing company.
The expected salary range for this role is based on the location where the work will be performed and is aligned to one of 3 compensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood's equity plan.

US Zone 1: $157000 - $185000
US Zone 2: $139000 - $163000
US Zone 3: $122000 - $144000
Base pay for the successful applicant will depend on a variety of job-related factors, which may include education, training, experience, location, business needs, or market demands. You can view comp zones for our US office locations in the table below. For other locations not listed, compensation can be discussed with your recruiter during the interview process.
Office locations (by comp zone)
US Zone 1: Menlo Park, NYC, Seattle, Washington DC
US Zone 2: Denver, Westlake (Dallas), Chicago
US Zone 3: Lake Mary
Click here to learn more about Robinhood's Benefits.
Robinhood promotes diversity and provides equal opportunity for all applicants and employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and skills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone. Additionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy rights. To review Robinhood's Privacy Policy please visit Robinhood - US Applicant Privacy Policy. If you are an applicant located in the UK or EEA, please visit the Robinhood - UK/EEA Applicant Privacy Policy.",3.4,"Robinhood
3.4","Menlo Park, CA",1001 to 5000 Employees,2013,Company - Public,Investment & Asset Management,Financial Services,Unknown / Non-Applicable
57,Sr. Data Engineer,Employer Provided Salary:$90K - $120K,"About SEAM Group:
SEAM Group is an innovative global leader in safety, reliability, and maintenance services. Our focus and dedication to creating a safer, more reliable world has earned us top recognition as award winners for both Inc 5000 and NorthCoast 99. By emphasizing values such as teamwork, trust, respect, and support, we have built an unparalleled people-first culture. Through leveraging innovative technologies and a commitment to excellence, we combine our employees’ unique talents and individualities to build the best solutions both within our organization and for our customers.
Position Summary:
At SEAM Group, we are on a mission to build a safer, more reliable world. Our software products coupled with comprehensive data analytics are key to our mission. We are looking to add a Senior Data Engineer to join our pursuit of excellence, providing ownership of SEAM Group’s data analytics and reporting. This includes the creation of detailed analytical models by providing expert opinion related to the integration and processing of data used to drive value and guide business decisions.
Essential Functions/ Responsibilities:
The demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions.
Lead technical design, implementation, and problem resolution
Partner cross-functionally to define and complete well-structured user stories
Maintain, evolve and adhere to a maturing data governance policy
Maintain and evolve a rapidly growing and robust data model containing vital customer information
Design and implement reliable ETL solutions from a variety of data sources (APIs, Proprietary DB, No-SQL )
Provide best in class business analytics in the form of web-based reports and dashboards
Define Data Engineering project feature delivery timelines and risks
Provide mitigation options to solve for blockers and technical risks as they arise
Identify technical debt, and communicate plans to manage it responsibly
Provide mentoring as needed to team members
Required Skills/Abilities:
The requirements listed below are representative of the knowledge, skills, and/or abilities required to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions.
Desire and passion to learn from like-minded, experienced, data-obsessed team members
A strong sense of ownership, pride and commitment to quality
Mastery of Microsoft Power BI
Mastery of Business Intelligence data visualization tools
Mastery of RDBMS (MSSQL) and/or NoSQL database technologies (MongoDB, Cosmos DB)
Experience with cloud-based and/or on premises data warehousing (Azure preferred)
Experience with Power BI Embedded Analytics
Experience with Agile software development principles
A track record of teamwork through high quality and timely Data Engineering projects
Benefits you will receive:
Opportunity to work on an exciting and highly visible project at SEAM Group
Participation in a fast-growing company
Opportunity to learn new technologies and expand your skillset
Flexible time off
Flexible work hours
Collaborative and embracing culture
Comprehensive healthcare plan that includes access to health, dental, vision, disability, and life insurance at group rates
8 Paid Holidays
Company 401k matching program
Educational Reimbursement up to $1,000/year
Education and Experience:
High school diploma or GED required; bachelor’s degree preferred
5+ years experience in Data Engineering required
SEAM Group is an equal-opportunity employer and is dedicated to diversity and inclusion in our workforce. All employment decisions are made based on qualifications, merit, and business need. EEO/M/F/D/V. SEAM Group is an e-verify employer. At this time, SEAM Group does not sponsor work visas.
Job Type: Full-time
Pay: $90,000.00 - $120,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible schedule
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
Do you now or will you ever require sponsorship to work in the United States?
Work Location: Remote",3.9,"SEAM Group
3.9",Remote,201 to 500 Employees,2018,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
58,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
59,Data Engineer,$78K - $112K (Glassdoor est.),"Translate business requirements and functional specifications into logical program designs and to deliver code modules, stable application systems, and software solutions.
Develop, configure, or modify complex integrated business and/or enterprise application solutions within various computing environments.
Implement and maintain complex business and enterprise software solutions to ensure successful deployment of released applications.
Translate complex cross-functional business requirements and functional specifications into logical program designs, code modules, stable application systems, and software solutions.
Partner with Product Team to understand business needs and functional specifications. .
Collaborate with cross-functional teams to ensure specifications are converted into flexible, scalable, and maintainable solution designs.
Evaluate project deliverables to ensure they meet specifications and architectural standards.
Coordinate, execute, and participate in component integration (CIT) scenarios, systems integration testing (SIT), and user acceptance testing (UAT) to identify application errors and to ensure quality software deployment.
Participate in all software development end-to-end product lifecycle phases by applying and sharing an in-depth understanding of complex company and industry methodologies, policies, standards, and controls.
Architect software design patterns and approaches, application level software architecture and make technical trade-off decisions at application level.
Automate and simplify team development, test, and operations processes.
Develop detailed architecture plans for large scale enterprise architecture projects and drives the plans to fruition.
Solve complex architecture/design and business problems; solutions are extensible; work to simplify, optimize, remove bottlenecks, etc.
Minimum Education Required:- All the responsibilities mentioned above are in line with the professional background and requires an absolute minimum of bachelor’s degree in computer science, computer information systems, information technology or a combination of education and experience equating to the U.S. equivalent of a bachelor's degree in one of the aforementioned subjects.",4.9,"Wise Equation Solutions
4.9","Mooresville, NC",51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
60,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
61,Senior Data Engineer,Employer Provided Salary:$105K - $186K,"**(UPDATED 8/21/2023) We invite applicants to express their interest in this role, but please note that we currently don't have a specific timeline for filling this position. This posting is intended for talent pooling. While there isn't a definitive hiring period, potential candidates should be aware that the role might be filled at some point in the future. We encourage you to start the process and discover more about this opportunity, Slalom, and potentially a career in consulting through discussions with our talent acquisition team.
Who You’ll Work With
As a modern technology company, we've never met a technical challenge we didn't like. We enable our clients to learn from their data, create incredible digital experiences, and make the most of new technologies. We blend design, engineering, and analytics expertise to build the future. We surround our technologists with interesting challenges, innovative minds and emerging technologies.
A Senior Data Engineer at Slalom is a technical team member who designs and develops data platform solutions. It uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g., data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc.
What You’ll Do
Work as part of a team and with clients to develop cloud data and analytics solutions
Make recommendations and prove data architecture concepts to team members and clients
Development of cloud data platforms and business intelligence solutions
Data wrangling of heterogeneous data sources for exploring and discovering new insights
Work with highly collaborative agile teams on modern data architecture projects
Key success measures of a Senior Data Engineer include technical proficiency in data processing technologies, collaboration with other teams, communication, and willingness to learn new things
What You'll Bring
Experience in one or more cloud technologies (AWS, Azure, Google Cloud, Snowflake)
Experience in data modeling, design, and development using complex SQL (SQL Server, Oracle, etc.)
Experience in Apache Airflow, Talend, SSIS, Informatica, Alteryx, or other data processing tool
Experience in one or more programming languages (Python, C#, Scala, R, PowerShell, Java, etc.)
Experience in cloud engineering, serverless architecture, and/or data streaming services
About Us
Slalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six countries and 43 markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 13,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.
Compensation and Benefits
Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance.
Slalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $105,000 to $186,000. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.
EEO and Accommodations
Slalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team if you require accommodations during the interview process.
#LI-JS8
#LI-Hybrid",4.3,"Slalom Consulting
4.3","Los Angeles, CA",10000+ Employees,2001,Company - Private,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
62,AWS Data Engineer II,$77K - $105K (Glassdoor est.),"Overview:
Shift4 (NYSE: FOUR) is boldly redefining commerce by simplifying complex payments ecosystems across the world. As the leader in commerce-enabling technology, Shift4 powers billions of transactions annually for hundreds of thousands of businesses in virtually every industry. For more information, visit www.shift4.com.

Shift4 Payments is seeking a Business Intelligence Engineer with an AWS Focus to join our growing internal Information Technology team. As a BI Engineer, you will help design and maintain the flow of information and analytics to the entire organization utilizing AWS capabilities to store, move and report on data. The position requires at least 3 years of prior experience as a Business Intelligence AWS engineer. We are looking for individuals that are extremely self-sufficient, available to work flexible hours and hold themselves to the highest standards of professionalism.Maintain and expand BI AWS architecture and content.
Responsibilities:
Support of BI AWS architecture and content up to and including off hours support when necessary.
Work on necessary bugs, enhancements and new system developments utilizing our Software Development Life Cycle (SDLC) standards.
Complete root cause analysis and provide recommendations to leadership to address opportunities.
Contribute to and enforce the use of the organization's Business Intelligence solution.
Develop software solutions that will provide solid foundations for the growth of the company.
Support key metrics and shared goals that determine success for a particular area.
Qualifications:
3-5+ years of professional experience in business intelligence AWS development or a related field.
Advanced experience working with AWS: S3, Glue, Lambda, Step Functions, Redshift, Athena.
Advanced experience working with various data sources such as AWS S3 and Redshift, Postgres, SQL Server, Amazon Athena, Excel, Flat Files, etc.
Advanced experience working with large data sets from sources such as AWS S3 and Redshift, Postgres, SQL Server, etc.
Strong PySpark, Python, CLI proficiency
Strong SQL proficiency (T-SQL, MySQL, and PostgreSQL).
Experience working with an Enterprise Data Warehouse and Dimensional Databases is a plus.
Advanced experience of the SDLC and how Development processes fit into KANBAN and Scrum Frameworks.
Experience with Jira and Confluence is a plus.
Ability to prioritize multiple tasks and easily adjust to changing priorities.
Ability to identify problems, initiate solutions, and effectively collaborate and communicate with team members.
Have excellent verbal and written communication skills.

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.",3.5,"Shift4 Payments
3.5","Allentown, PA",1001 to 5000 Employees,1999,Company - Public,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
63,Senior Data Engineer (remote),Employer Provided Salary:$111K - $121K,"As the Senior AWS Data Engineer, you will have the opportunity to make a significant impact on the success and growth of KeHE by engaging with internal business stakeholders of all levels. You will consult with them, provide support & guidance in defining and delivering their BI needs by turning data into actionable information.
Perform full lifecycle Business Intelligence (BI) development
Identify data sources, provide data flow diagrams and document source to target mapping and process.
Collaborate with others on requirements specifications and documentation
Design and develop SQL for large complex ETL jobs and conduct related database troubleshooting and maintenance
Publish and consume data to and from the enterprise data lake on AWS Perform code reviews, unit and integration testing
Maintain the data warehouse performance by optimizing batch processing through parallelization, performance tuning etc.
Design, develop, and maintain scalable reports, dashboards, and metrics
Maintain the design and development standards for Business Intelligence (BI) Keep current with Business Intelligence data trends and technological innovations
Execute on POC’s with new technologies, drive innovation, and new ideas
REQUIREMENTS--
4 Year College Degree from an accredited university in one of the following: Computer Science Information Systems, Operations Research, Mathematics, Statistics, or related technical field 3+ years of direct experience using databases, including PostgreSQL, MySQL, SQL Server or Redshift
3+ years hands-on experience developing with TSQL, pgSQL, SSIS, SSAS (Tabular model), PowerBI and C# or Python
MUST KNOW AWS, S3, Lambda, and Redshift
Strong understanding of data modeling (i.e. conceptual, logical and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Strong experience with business intelligence and data warehousing design principles and industry best practices including multi-dimensional modelling (star schemas, snowflakes, de-normalized models, handling slowly changing dimensions)
Experience developing and deploying SSRS reports, dashboards, and metrics Experience in creating Entity Relationship Diagram’s using tools like Toad Ability to work independently as well as with a team
Self-directed and able to learn and apply new technologies quickly Ability to prioritize and multi-task across multiple workstreams
Job Types: Full-time, Part-time
Pay: $110,701.80 - $121,000.00 per year
Benefits:
401(k)
Dental insurance
Employee discount
Flexible schedule
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
What are your compensation expectations???? (required)
What is the best email address to reach you?
Are you willing to take a skills/coding test before your interview? (required)
Will you now or at any time require sponsorship? (required)
Experience:
SQL: 5 years (Preferred)
Python: 5 years (Preferred)
AWS S3: 4 years (Preferred)
AWS Redshift: 4 years (Preferred)
AWS Lambda: 4 years (Preferred)
Work Location: Hybrid remote in Naperville, IL 60563",3.4,"KeHE Distributors, LLC
3.4","Naperville, IL",5001 to 10000 Employees,1954,Company - Private,Wholesale,Retail & Wholesale,Unknown / Non-Applicable
64,Data Engineer- Hadoop,Employer Provided Salary:$63.00 - $66.00 Per Hour,"Position Summary:
Synthetic data engineer is the experienced professional who can bring together skills and technologies to be able design and develop solutions to deliver data required for training models/test applications. Candidate will be working with a team of resources using Python, Sqoop, pyspark to create the data in a most efficient manner. Candidate should have prior experience & knowledge in highly scalable production grade applications with a strong engineering acumen. Should be able to work in a collaborative, fast paced environment. Candidate must have knowledge and/or experience working with Hadoop ecosystem.
Skills:
• Advanced knowledge in Python Programming
• Familiarity with Faker module
• pyspark, Sqoop, Hadoop, Hive, Shell script, Hadoop Shell commands
• Good experience dealing with bulk data extracts and bulk data loads
• Ability to translate requirements from stake holders into the technical requirements and the code
• Experience on statistical distribution, agent to model , deep learning is a plus
• Background of MF environment is a plus
Job Type: Contract
Pay: $63.00 - $66.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
Hadoop: 7 years (Required)
Python: 5 years (Required)
ETL: 3 years (Preferred)
Work Location: Hybrid remote in Charlotte, NC 28203",3.9,"Data Inc
3.9","Charlotte, NC",501 to 1000 Employees,1983,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
65,Data Science Engineer,$89K - $122K (Glassdoor est.),"Company Description

ClientSolv Technologies is an IT solution firm with over a decade of experience serving Fortune 1000 companies, public sector and small to medium sized companies. ClientSolv Technologies is a woman-owned and operated company that is certified as a WMBE, 8a firm by the Federal government's Small Business Administration.

Job Description

We are seeking a Data Science Engineer for a 12-month contract (with the option to extend further) in Gaithersburg, MD. This role will be onsite/ in the office Monday-Friday during normal business hours (there are no remote options) and will be working with the data science innovation team with:

Providing technical leadership and identifying solutions for complex problems.
Understand how to design and build a deployable application that uses any ML model that they design and recommend
Recommend, track, and report on the business goals and the resulting technical goals, projects, and technical tasks.
Perform proof of concepts, identify initial data wrangling, and then provide guidance for development of the productized version of the capabilities.
Analyze data and build predictive models based on stakeholder input.

Qualifications

Basic Requirements:
Bachelors degree in Computer Science, Data Science or related technical field (Masters in Data Science preferred)
10+ years of technical experience, preferably in computer networking and communications.
5+ years of experience performing data science
Preferred Qualifications:
Knowledge and experience in leading analysis efforts for large operational networks .
Advanced analytical and problem-solving abilities.
Ability to break down and understand complex business problems, define a solution and implement it using advanced quantitative methods.
Identify innovative solutions that apply data science, AI/ML technology to the problem so that a development team can then take that and build an application that leverages the DS approaches.
Must have strong DS design knowledge and experience in some ML/Ops/DevOps
Proficiency in programming for data analysis; ideally Python (or R) and SQL. Knowledge of Pandas is essential.
Knowledge of building machine learning models using Scikit-Learn, TensorFlow, Keras, or other open-source libraries.
Knowledge of data visualization tools and libraries in Python/R (Shiny preferably) Or Tableau.
Familiarity with software version control with git.
Strong work ethic and intellectual curiosity.
Knowledge of Cloud technologies such as AWS or Google
Knowledge of relational databases such as My SQL and BigQuery.
Team player with excellent communication and problem-solving skills. Solid oral and written communication skills, especially around analytical concepts and methods
Experience working across varying business and technical functional units

Additional Information

This 12-month contract (with the option to extend) will be located onsite/ in the office in Gaithersburg, MD during normal business hours, Monday- Friday (there are no remote options).",3.7,"ClientSolv Technologies
3.7","Gaithersburg, MD",Unknown,1994,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
66,"Principal Data Engineer, SFR RE",$103K - $154K (Glassdoor est.),"We are Evergreen Residential, a high growth early-stage institutional investment platform in the single-family residential sector. Our team is collaborative, open-minded and curious. Transparency is a core value, we speak our minds, are responsible for our actions and celebrate our wins. We are serious about the business without taking ourselves too seriously. We look for people who thrive in an entrepreneurial and fast paced environment. If you are self-motivated and mission driven with a 'can do' mindset and see solutions where others may see problems, come and grow with us!
We offer a flexible, empowering culture, competitive compensation and benefits, and potential for career growth through working closely with, and learning from, our experienced leadership team.
This is not a consulting position. Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time for this position.
This position will be (Full-time/Permanent employment) based in our downtown Dallas offices, and we will consider a hybrid work schedule.
Overview: As a technical/engineering expert, you also pride yourself on being able to quickly build strong business relationships both internally and externally e.g., with the leadership team, current and potential investors. With a passion for keeping current with advancements of the field, you deploy technology and data resources to provide innovative solutions to business needs. You will be responsible to plan, design, develop and manage the Company's data warehouse systems and infrastructure to ensure efficient data storage, security and access to accurate data that enables real-time insights from both internal and market data that will drive revenue growth and capital efficiency. This position is critical to the effectiveness of our operations, working in partnership with our analytics and reporting specialists to help us make the best investment decisions.
The Role: Priorities can often change in a fast-paced environment like ours. Initial focus is to
source, curate and store external purchased data and internal data within a Snowflake warehouse for use in our 3rd party property mgt system and BI reporting tool. Ensure a single source of truth.
The role includes, but is not limited to, the following responsibilities:
Design and implement data pipelines to extract, transform, and load data from various sources into a centralized data repository
Establish productive relationships and effective communications with Company leadership. Quickly understand business drivers and align on required outcomes
Collaborate with data analysts to ensure that data is readily available for analysis and modeling
Optimize database performance and troubleshoot issues as they arise
Implement data security and access controls to protect sensitive data
Highlight key trends derived from data analysis and be a resource for improving data proficiency throughout the organization
Invest time to stay current with emerging trends and technologies in data engineering
What You Will Bring to the Table:
Bachelor's Degree in a relevant field required, advanced qualifications beneficial
Min 10 years of experience in data engineering or a related field
Strong technical background in data science, business intelligence or data engineering, with hands-on experience in data modeling, database design, and ETL best practices.
Proficiency in one or more programming languages such as Python, Java, or Scala
Experience with data processing and storage technologies such as Hadoop, Spark, Kafka, Snowflake (required), and NoSQL databases
Demonstrated strategic impact working with executive teams, with experience in real estate investment and/or rental sector highly desirable (SFR or multifamily)
Experience building and maintaining a high performing and flexible data function and managing a Data Science or Data Engineering team.
Track record in deploying technology and data resources to provide innovative solutions to business needs
About Evergreen Residential
Founded in 2021, Evergreen Residential is a full-service SFR platform leveraging proven operational practices and the latest technological advances to optimize investor returns and achieve positive outcomes for our residents and the communities in which we operate. We offer a full suite of services, including Investment Management, Asset Origination, and Advisory Services. The firm is headquartered in Dallas with offices in New York City.
The leadership team has extensive experience dating back to the early institutionalization of SFR and unrivaled depth of experience in the complete asset life cycle. We are built to withstand changing market conditions, and our business produces resilient, predictable cash flows and margins. We are committed to charting new paths and using data to achieve best-in-class results. Our business is evergreen.
Beyond financial returns, the Company is committed to measurable impact objectives. We believe that inclusive and equitable management, environmentally sustainable long-term strategies, and resident-focused policies are good business - for our residents, our investors, and our team. We are committed to using environmentally sustainable practices and empowering our residents to improve their financial health.
Our cornerstone values - Accountability, Transparency and Partnership - are built on a foundation of Integrity and provide the roadmap for our daily actions, interactions and decisions.

Equal Opportunities and Other Employment Statements
We are deeply committed to building a workplace and community where inclusion is not only valued, but prioritized. We take pride in being an equal opportunity employer and seek to create a welcoming environment based on mutual respect, and to recruit, develop and retain the most talented people from a diverse candidate pool. All employment decisions shall be made without regard to race, color, religion, gender, gender identity or expression, family status, marital status, sexual orientation, national origin, genetics, neuro-diversity, disability, age, or veteran status, or any other basis as protected by federal, state, or local law.",-1,"Evergreen Residential Holdings, LLC","Dallas, TX",51 to 200 Employees,2021,Company - Private,Real Estate,Real Estate,Unknown / Non-Applicable
67,Data Engineer,$83K - $113K (Glassdoor est.),"ID: 6173 | 5-10 yrs | North Wales | careers
Job Description:
The Data Engineer is a critical role responsible for designing, developing, and maintaining data pipelines and infrastructure that enable efficient data processing, storage, and retrieval. This role involves working with large volumes of data from various sources, ensuring data quality, and creating scalable solutions to support data-driven decision-making within the organization. The Data Engineer collaborates with cross-functional teams to understand data needs and implement solutions that enable data analysis and reporting.
Responsibilities:
Data Pipeline Development:
Design and implement data pipelines to extract, transform, and load (ETL) data from various sources into data warehouses or data lakes.
Develop efficient and scalable ETL processes to ensure data quality, consistency, and accuracy.
Data Modeling and Architecture:
Design and implement data models that support the organization's analytical and reporting requirements.
Work with data architects to ensure proper data structure and architecture for optimal performance.
Data Integration:
Integrate data from different sources, including databases, APIs, and external systems.
Collaborate with application developers to embed data integration processes into applications.
Data Storage and Management:
Implement and manage data storage solutions, such as relational databases, NoSQL databases, and data lakes.
Optimize data storage for performance, scalability, and cost-effectiveness.
Data Transformation:
Develop data transformation processes to cleanse, enrich, and transform raw data into usable formats.
Utilize scripting and programming languages (e.g., Python, Java, Scala) to perform data transformations.
Data Quality and Governance:
Implement data quality checks and validation to ensure the accuracy and integrity of data.
Contribute to data governance initiatives by adhering to data security and privacy standards.
Performance Optimization:
Monitor and optimize data processing and query performance to ensure fast and efficient data retrieval.
Identify and address bottlenecks in data pipelines to improve overall system performance.
Collaboration and Communication:
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.
Communicate technical concepts and solutions to non-technical stakeholders effectively.
Version Control and Documentation:
Maintain version control for data pipelines and codebase.
Document data engineering processes, workflows, and data lineage.
Qualifications:
Bachelor's or Master's degree in Computer Science, Software Engineering, Information Technology, or a related field.
Strong proficiency in programming languages such as Python, Java, Scala, or similar.
Experience with data processing frameworks like Apache Spark, Apache Kafka, or similar.
Proficiency in SQL for querying and manipulation of data.
Knowledge of data modeling concepts and techniques.
Experience with ETL tools and data integration techniques.
Familiarity with data storage technologies such as relational databases (e.g., PostgreSQL, MySQL), NoSQL databases (e.g., MongoDB, Cassandra), and data lakes (e.g., AWS S3, Hadoop HDFS).
Understanding of cloud platforms (e.g., AWS, Azure, GCP) and related data services.
Strong problem-solving skills and attention to detail.
Ability to work in a collaborative team environment.
Excellent communication skills to work with technical and non-technical stakeholders.
The Data Engineer role involves designing, developing, and maintaining the data infrastructure necessary for effective data processing, integration, and storage, playing a key role in enabling data-driven decision-making within the organization.",3.7,"Jade Global
3.7","North Wales, PA",501 to 1000 Employees,2003,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
68,Senior Data Analysis Engineer,Employer Provided Salary:$140K - $224K,"The NVIDIA Datacenter organization is seeking an experienced technology professional for the position of Senior Data Analysis Engineer to support initiatives for the E2E Quality Platforms, Reporting, and Analytics Organization for our DGX™ and other Datacenter products.
As a Senior Data Analysis Engineer in our team, you will be an integral part of the team in Operations E2E Quality Platform. The team is responsible for turning data into information that leads to insights and actions to improve the business. You will be collaborating with a diverse and multi-functional team of developers/administrators and business users within Operations and other groups at NVIDIA.
What you’ll be doing:
Responsible for developing complex queries in SQL, SPL, stored procedures, or PowerBI from a very large data volume and multiple data sources.
Able to use and apply the right analytical techniques to identify hidden patterns and trends that can be leveraged to improve the business. And, lead data analysis to solve complex data issues, and support data research requests.
Setup and maintain B2B file transfer processes from our partners to Operations
Understand file archiving concept, and can build, update, and maintain scripts in C/C++ for file archiving processes. And, with strong programming skills to automate and streamline various data processes.
Responsible for creating reports and dashboards targeted at understanding data completeness, data accuracy, application performance and use case implementation.
Performing Data Analytics, Visualization, Dashboard Customization, and Alerts in various cloud data platforms such as ADX Azure, AWS Redshift, PowerBI or Splunk.
What we need to see:
Bachelor’s degree in Computer Science, or Software engineering (or equivalent experience), or related fields with extensive programming knowledges in Python, C/C++, and strong in writing complex DB query SQL.
8+ years of strong analytical skills able to extract actionable insights from raw data to help improve business.
Experience and knowledge of Core Splunk Enterprise, and extensively used Splunk SPL with certified power user, Reports, Alerts and Dashboards.
Experience with B2B file transfer, SFTP, file encryption and decryption, file compression, and file systems.
High energy and able to work in a fast-paced production environment with various constraints, problem-solving skills, interpersonal social skills, and the ability to multitask.
Technical expertise for data models, ETL, database design development, data mining and segmentation techniques.
Ways to stand out from the crowd:
Computer Science degree / Splunk Certification with 4+ years Splunk experience.
Background with Splunk backend data administration (data forwarding, indexing, summary indexing, field extraction, data models and acceleration).
Experience working with B2B file transfer operation, and GoAnywhere is preferred.
Self-starter, and solid ability to drive continuous improvement of systems and processes.
A demonstrable ability to work in a fast-paced environment where strong organizational skills are crucial.
NVIDIA is widely considered to be one of the technology world's most desirable employers. We have some of the most dedicated people in the world working for us. If you're passionate about innovation we want to hear from you. You can do your life’s work here! Join us!
The base salary range is $140,000 - $224,250. Your base salary will be determined based on your location, experience, and the pay of employees in similar positions.
You will also be eligible for equity and
benefits
.
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.",4.6,"NVIDIA
4.6","Santa Clara, CA",10000+ Employees,1993,Company - Public,Computer Hardware Development,Information Technology,$5 to $10 billion (USD)
69,"Data Engineer - Python, Spark?",$91K - $131K (Glassdoor est.),"FlexIT client is looking for an immediate Data Engineer - Python, Sparkfor a 12-month remote contract.
The client is looking for great Engineers with talent and persistence who can leverage their existing skills and learn new ones. You should have some of the specific technical skills were looking for and be expert enough in one or two to help ramp others quickly.
Job Duties:
We are building petabyte-class solutions that consume fast-moving streams from eCommerce, retail, and partner channels and power the critical decisions that drive our business. We are building the Cloud Platform for Data and Analytics on AWS that fuels in digital transformation.
Focus areas include:
Data Streaming / Enrichment / Business Rules / MDM
Data Lake / Warehousing
Data Governance / GDPR / SOX
Data Strategy / Unified Access / IAM / RBAC
Be a great teammate on an agile/SCRUM team that sets and meets aggressive goals.
Mentor new and less experienced developers to advance their proficiency.
Leverage expert development skills and solid design skills to deliver reliable, scalable, performant solutions with modern tooling, data structures and algorithms.
Work with Product Owners, Engineering Managers and Principal Engineers to deliver solutions that enable digital transformation",4.0,"FlexIT Inc
4.0","Beaverton, OR",1 to 50 Employees,-1,Company - Private,-1,-1,$5 to $25 million (USD)
70,AI Data Engineer,$96K - $126K (Glassdoor est.),"AI Data Engineer
Forge - Newton, MA
www.forgeco.com
About Us
Forge (forgoco.com) is a newly formed startup based in Boston, MA. We are a technology-enabled trades company with mobile, web, AI, and smart-glasses software applications that enable our professionals in the field and create amazing experiences for our customers. We are innovating rapidly within the professional home services industry – an industry that has barely changed in the last 100 years. Why? Because hiring tradespeople or contractors to do even simple tasks is a frustrating and time-consuming process for customers. There are many reasons for this, but one of the biggest is the shortage of skilled tradespeople in the U.S. — the result of long-term trends that have pushed entry-level workers away from the trades.
At Forge, we are focused on building the next generation of trades professionals and the software that will help make them successful. We believe more skilled workers, enabled by modern technology, will power a wholly new (and vastly improved) customer experience for all.
About Your Role
Forge is growing rapidly and we're looking for a data-focused AI Engineer to join our team! This person will support our newly-formed AI Technologies Team building out solutions that will power our Pros in the field. You will help determine our data collection & engineering best practices to support our AI applications. You will work closely with our product managers, test engineers, and other technical leaders as we move at a rapid pace.
Responsibilities:
Strategize an overall data collection process (including labeling, annotations, etc.) for our overall AI efforts;
Work with different cross-functional teams to devise and deploy data collection approaches, as well as to discover opportunities for enabling AI / Computer Vision;
Develop, construct, test, and maintain schema designs, protocols, and data architectures necessary for the creation of data, image and video libraries to support ML/AI projects;
Enable scalable, reliable, and secure data processing systems (e.g., data lakes and workflows) for both model training and production;
Participate in deploying and validating ML models and monitor their performance;
Collect, clean, preprocess, and analyze large datasets to develop valuable insights and identify potential AI use cases;
Contribute to the development of patents, copyrights, or other forms of intellectual property protection for AI innovations;
Grow with the team as we learn and apply new technologies that are evolving rapidly;
Actively engage with our scrum process;
Work with product managers and designers to flesh out technical specifications and requirements;
Document and communicate database design, data flow and data dependencies;
Work closely with other departments: Pro team, Operations, Customer Service, etc;
Stay up to date on new advancements in the field of data & AI, participating in ongoing education, workshops, and conferences to maintain expertise and incorporate new developments into ongoing projects.
Qualifications:
5+ years of experience building solutions in the data engineering field;
Experience working with a variety of standard database technologies (e.g. DynamoDB, mySQL, Mongo, Oracle, PostgreSQL), generally comfort working with end-to-end data pipelines and streaming data;
Experience building machine learning data stacks (e.g., databricks, airflow, dbt);
Great communication skills. You must be able to communicate technical information to other software engineers, our testing team, and our product managers;
Good team player; ability to work closely with an experienced software team and collaborate effectively;
Bachelor's or Master's degree in a Computer Science-related field is desirable, but not a hard requirement. Must show very deep math & data-management skills;
You should be a quick learner who is comfortable working in a highly agile startup environment where rapid change is a constant;
Ability to get work done and meet deadlines with minimal direct supervision;
Empathy and appreciation for the Trades.
Added Bonus:
Machine Learning/data engineering research or academic background;
Experience using Python to build software in the ML space;
Development background working with Node.js, preferably in a microservices environment;
Experience with reinforcement learning algorithms (e.g., Q-learning, Deep Q-Networks);
Familiarity with big data technologies (e.g., Hadoop, Spark);
Experience working with R, C++;
Agile development experience, or experience setting up Agile processes;
Working experience in the Trades.
At Forge, we value innovation, teamwork, and a commitment to excellence. We're dedicated to creating a supportive and collaborative environment where individuals can grow professionally and make a real impact. We offer competitive salaries, equity, benefits, flexible working arrangements, and a dynamic culture of intelligent, hard-working, and creative individuals.",3.6,"Forge
3.6","Newton, MA",51 to 200 Employees,2020,Company - Private,Construction,"Construction, Repair & Maintenance Services",Unknown / Non-Applicable
71,Data Communications Engineer - III,Employer Provided Salary:$91K - $181K,"Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","San Francisco, CA",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
72,Data Pipeline Engineer,$76K - $106K (Glassdoor est.),"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
The position:
Design and develop scalable data pipeline processes (including ingestion, cleansing, curation, unification, etc.)
Automate the processing of inbound client data feeds
Design and develop tools and processes to support automated data profiling and data quality methodologies
Work with our data science team to assist with the development of feature store data including data prep, enrichment, and feature engineering for AI/ML
Write and maintain documentation on data pipelines
Provide periodic support to our customer success team Skills & Experience
BS / MS in Computer Science, Engineering, or applicable experience
3+ Year using Python (Pandas/NumPy) in a production environment
3+ Year using PowerShell in a production environment
Expertise with ETL/ELT and the development of automated validation and data pipelines
Understand database design and data manipulation and transformation methodologies
Keen understanding of EDW, master data management and other database design principles
Experience designing solutions using a range of AWS Services
Experience with data engineering and workflow management frameworks such as Airflow and dbt
Comfortable working with high volume data in a variety of formats
Experience with CI/CD such as Jenkins
Experience with version control tools: Git preferred
Excellent verbal and written communication
Familiarity with healthcare data is a plus
Familiarity with ML pipelines, principles and libraries is a plus
Experience with REST API is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation",5.0,"Decision Point Healthcare
5.0","Boston, MA",1 to 50 Employees,2013,Company - Public,-1,-1,Less than $1 million (USD)
73,Senior Data Engineer,Employer Provided Salary:$166K - $232K,"Patreon is the best place for creators to build memberships by providing exclusive access to their work and a deeper connection with their communities. We're building a content and community platform where creators can engage directly with their fans and monetize their creativity, while maintaining full ownership over the work they make and the communities they create.
We're leaders in the membership space with 250,000+ active creators and over $3.5 billion paid directly to creators on our platform. Our team is building tools to optimize the creator-to-fan relationship, including native video, enhanced podcasting features, improved creation tools, and new community experiences. We're continuing to invest heavily in building the most talented team in the Creator Economy and are looking for a Data Engineer to support our mission.
This role is available to those wishing to work in our SF and NY offices on a hybrid work model or those wishing to be fully remote in the United States.
About the Role
Work on a tight-knit team of highly motivated and experienced data engineers with frequent collaboration with data scientists, product managers and product engineers.
Work on both ""data analytics"" and ""data infrastructure"" type projects in a fast-paced, high-growth startup environment.
Build core data sets and metrics to power analytics, reports and experimentation.
Write real-time and batch data pipelines to support a wide range of projects and features including our creator-facing analytics product, executive reporting, FP&A, marketing initiatives, model training, data science analytics, A/B testing, etc.
Help manage and build out our data platform and suite of data tools.
Be a driver of a data-centric culture at Patreon. Work autonomously on large green field initiatives and help define data best practices at the company.
About You
Expert in SQL, Spark and Python or Scala
Significant experience modeling data and developing core data sets and metrics to support analytics, reports and experimentation. Solid understanding of how to use data to inform the product roadmap.
Enjoy collaborating with Data Scientists, Product Managers and Product Engineers. Comfortable playing the role of a Project Manager in order to drive results.
Have previously built real-time and batch data pipelines using tooling such as Airflow, Spark, Kafka, S3, Fivetran, Census, etc. Experience working with event tracking frameworks, data observability frameworks and experimentation frameworks.
Experience managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc.
Highly motivated self-starter that is keen to make an impact and is unafraid of tackling large, complicated problems and putting in the work to ensure high craft deliverables.
About Patreon
Patreon powers creators to do what they love and get paid by the people who love what they do. Our team is passionate about making this mission and our core values come to life every day in our work. Through this work, our Patronauts:
Put Creators First | They're the reason we're here. When creators win, we win.****
Build with Craft | We sign our name to every deliverable, just like the creators we serve.
Make it Happen | We don't quit. We learn and deliver.
Win Together | We grow as individuals. We win as a team.
We hire talented and passionate people from different backgrounds because we want our company to represent the vast diversity of our creators. If you're excited about a role but your prior experience doesn't match with every bullet point outlined above, we strongly encourage you to apply anyway. If you're a creator at heart, are energized by our mission, and share our company values, we'd love to hear from you.
Patreon is proud to be an equal-opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.
Patreon offers a competitive benefits package including and not limited to salary, equity plans, healthcare, unlimited paid time off, company holidays and recharge days, commuter benefits, lifestyle stipends, learning and development stipends, patronage, parental leave, and 401k plan with matching.
The posted range represents the expected salary range for this job requisition and does not include any other potential components of the compensation package, benefits and perks previously outlined. Ultimately, in determining pay, we'll consider your experience, leveling, location and other job-related factors.
San Francisco Pay Range
$166,000—$231,500 USD",3.2,"Patreon
3.2","San Francisco, CA",51 to 200 Employees,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
74,AWS Data Engineer,$85K - $118K (Glassdoor est.),"About Infinitive:
Infinitive is a transformation and technology consultancy that enables global brands to deliver results through insights, innovation, and efficiency. We possess deep industry and technology expertise to drive and sustain adoption of new capabilities. We match our people and personalities to our clients' culture while bringing the right mix of talent and skills to enable high return on investment.
Infinitive has been named “Best Small Firms to Work For” by Consulting Magazine 6 times. Infinitive has also been named a Washington Post “Top Workplace,” Washington Business Journal “Best Places to Work,” and Virginia Business “Best Places to Work.”
About this Role:
Infinitive is looking for candidates who are accountable, passionate, assertive, proactive, open & honest, results oriented, and adaptable. The ideal candidate will drive client growth, foster deep relationships, drive account/client financial management, and lead end to end execution of projects/programs.
We are currently looking for a AWS Engineer who will:
Write effective, scalable code and be able to improve responsiveness and overall performance of the code
Support Data Warehouse development operations, Data Analytics and Visualizations development as well as developing reports and dashboards
Apply natural language processing, data mining techniques, statistical analysis, and machine learning integrated with our analytical and visualization tools.
Support Analytics and Business Intelligence development while providing development support to a team of data analysts.
Integrate user-facing elements into applications
Improve functionality of existing systems
Additionally, the ideal candidate has:
3+ years of experience developing with object-oriented programming concepts (Python, R)
3+ years of experience working with distributed scalable Big Data storage, including AWS EMR, Spark, Terraform, Scala.
3+ years of experience with data query and analysis using relational databases and query languages (SQL)
Implementation knowledge of, or desire to learn AWS / Azure data science capabilities.
Ability to test and debug programs/tools/applications
Fluent in Python libraries for Data Analysis (Pandas, Numpy, etc.)
Experience with Open Source technologies such as Apache Tomcat and Web Server, Elasticsearch, etc.
Ability to maintain source code and utilize configuration management systems such as CodeFoundry or GitLab
Technical degree in computer science or related field (preferred)
Experience working in an Agile environment
Background in management consulting (preferred)
Desire to become an AWS Certified architect / engineer
Strong analytical, conceptual, organizational, and problem-solving skills.
Have a Kick-Ass Attitude
Desire to Be Great and strive for continual growth
Required Qualifications:
Bachelor’s degree in business or related field of study.
Excellent verbal and written communication skills.
The ability and desire to function in a hybrid capacity, supporting both delivery and sales.
Experience managing multiple projects/programs in a fast-paced, dynamic, and sometimes ambiguous environment.
Desire and ability to provide mentorship and consistent, timely feedback to support the growth and success of others.
Desired Qualifications
Located in the DC Metro, Richmond, or New York City area with the ability to regularly be onsite in client offices in McLean, Richmond, or NYC.
Previous experience at a small-midsize consulting firm.
Applicants for employment in the U.S. must possess work authorization which does not require sponsorship by the employer for a visa.
Infinitive is an Equal Opportunity Employer.

q3YJaiLrEw",3.2,"Infinitive Inc
3.2","Ashburn, VA",51 to 200 Employees,2003,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
75,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
76,Senior Data Engineer,-1,"CoreTrust is the market leading commercial Group Purchasing Organization (GPO), leveraging the combined purchasing volume of its 3,000+ members to negotiate preferential pricing and terms across more than 80 indirect spend categories. Strategically aligned with private equity portfolios and large independent companies, we complement sourcing bandwidth and improve supply chain efforts with our industry-leading national contracts.
Recently acquired by Blackstone Private Equity, CoreTrust is growing rapidly and we’re looking for a passionate Senior Data Engineer.
Reporting to the Director of Data, you will be part of the data solutions team and be responsible for building our data platforms, enabling the use of advanced analytics to drive continued evolution and growth.
You will create and manage data pipelines to feed and curate our data lake solution and help develop our data roadmap. The ideal candidate has a great understanding of various data / tech solutions (e.g., data modeling tools, data pipeline, data catalogs, cloud databases) and a record of using them to bring tangible dollar impact. You should be excited to seek out and capitalize on a wide variety of opportunities to use data to create value across the organization.

Responsibilities
Lead data projects to build innovative and highly available solutions while ensuring adherence to budget, schedule, and scope of project
Mentor other members in the data solutions team
Develop and assist with oversight on the data tech infrastructure
Drive data & analytics solutions from conception to deployment/delivery with clear ROI impact
Develop and maintain relationships with all relevant business and tech stakeholders and functions
Provide input to proposals for assigned projects including project objectives, technologies, systems, information specifications, timelines, and staffing
Communicate timely status updates to affected internal or external customers and stakeholders
Collect, analyze, and summarize information and trends as needed to prepare project status reports
Assist in developing a culture of data-driven decision-making, including adoption of business intelligence, analysis, and advanced analytics globally
Perform other related duties as assigned

Qualifications
Bachelor’s degree in computer or information science or relevant experience
9+ years of relevant experience in a data-driven professional setting
Ability to assist with the vision of the team (e.g., mission, priorities, engagement model, tooling)
A record of accomplishment of successfully managing complex cross-functional projects under tight deadlines
Strong technical background – familiarity with Python, SQL, cloud technologies like Azure and AWS, statistics / machine learning, Snowflake, DBT, FiveTran, Data Visualization Software
Exceptional communication and presentation skills, particularly in the context of engaging senior management teams
A successful history of manipulating, processing, and extracting value from large, disconnected datasets
Build processes supporting data transformation, data structures, metadata, dependency, and workload management
Working knowledge of creating and leveraging API or Stream based data extraction processes such as Salesforce API
Strong command of databases and SQL
Proficiency with Python or R, especially for data manipulation and analysis, and ability to build, maintain and deploy sequences of automated processes with these tools
Ability to motivate groups of people to complete a project in a timely manner
Excellent analytical, logical thinking, and problem-solving skills
Thorough understanding of project management principles and planning
Thorough understanding of information technology procedures and practices
Proficient with, or able to quickly become proficient with, a range of general and specialized applications, software, and hardware used in the organization and the industry

Benefits
Competitive compensation package
Free individual employee medical coverage
Company subsidized dental and vision coverage
Dollar for dollar 401(k) match up to 6% of your salary with immediate vesting
Company-paid Short-Term and Long-Term Disability coverage
Employee Assistance Program to support your wellbeing and mental health
$1500 annual stipend for undergraduate/graduate college courses; $500 annual stipend for continuing education courses/certifications
Free snacks and beverages on-site
Brand new, state-of-the-art, tech-enabled work environment in downtown Nashville
Flexible/hybrid work culture",2.8,"CoreTrust
2.8",Remote,Unknown,-1,Subsidiary or Business Segment,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
77,Data Visualization Engineer,$77K - $101K (Glassdoor est.),"Passionate about precision medicine and advancing the healthcare industry?
Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.
With the advent of genomic sequencing, we can finally decode and process our genetic makeup. We now have more data than ever before but providers don't have the infrastructure or expertise to make sense of this data. We're on a mission to connect an entire ecosystem to redefine how genomic data is used in clinical settings.
What You'll Do:
Own the design, development and administration of advanced analytics dashboards and reporting and business intelligence tools. Beginning with requirements gathering across cross-functional teams, to the delivery of analytic solutions into production.
Collaborate with stakeholders, providing guidance, data and analytical support for developing new datasets, analyses and tools in support of strategic initiatives.
Perform deep-dive root cause and problem-solving analyses for and with laboratory, business operations and engineering teams.
Design and build data views to support advanced analytical reporting, self-service data analysis and exploration, utilizing multiple data and visualization technologies.
Work closely with technical and product teams to identify and align changes to internal systems in support of future data analysis needs and requirements.
Develop and facilitate data governance initiatives to assure data integrity across distributed systems.
Solve new and interesting problems.
Qualifications:
Bachelor's degree in Information Systems, Business Administration, Finance, Economics, Mathematics, Operations Research, Marketing or similar
3+ years of experience developing data sets for analysis use-cases
3+ years of experience with direct stakeholder management and ownership of full lifecycle of analytics/data projects and presenting analysis results to stakeholders
3+ years report-development and data visualization experience with BI tools (Looker, Tableau, etc.)
Ability to deliver both high-level explanations of complex problems while also able to deep dive into complex problems
Advanced SQL skills for querying and analysis of large, complex datasets
Must work well in a team environment and be able to partner with both business and technical team members and translate technical language to the non-technical need
Creative problem solver, ability to handle multiple projects, and strong work ethic needed
Excellent written and verbal communication skills
Highly organized and systematic, superb attention to detail
Able to complete tasks with a high degree of accuracy and an eye towards process improvement
Persistent and resilient person who thrives in a fast-paced, highly collaborative environment
Bonus points for:
Advanced analytics experience - classification/predictive modeling, optimization models, forecasting
Experience with Python for data engineering or advanced data analysis projects/use-cases
Healthcare domain knowledge and experience
Expert knowledge of database/data modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others
Experience building cloud-native applications and supporting technologies / patterns / practices including: GCP, AWS, Docker, CI/CD, DevOps, and microservices

#LI-EH1
#LI-Hybrid
We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.1,"Tempus
3.1","Chicago, IL",501 to 1000 Employees,2015,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
78,"Data Quality Engineer, Sr.",$79K - $112K (Glassdoor est.),"At Oshkosh, we build, serve and protect people and communities around the world by designing and manufacturing some of the toughest specialty trucks and access equipment. We employ over 15,000 team members all united by a common purpose. Our engineering and product innovation help keep soldiers and firefighters safe, is critical in building and keeping communities clean and helps people do their jobs every day.
THE ROLE:
Oshkosh Advanced Analytics & Artificial Intelligence Practice is Seeking a Data Quality Engineer to guide a team in developing, deploying, and enforcing policies and procedures that promote efficient, accurate data, and effective information management to ensure data is used and maintained properly across an organization.
YOUR IMPACT:
Design, develop, implement, and sustain data governance policies and procedures as part of a broader enterprise data management program including, but not limited to:
Data quality framework, metrics, and adherence to data standards
Data cataloging and data flow/lineage visualization
Identify and collaborate with data stewards
Create and maintain metadata
Collaborate with infrastructure team on data architecture and data lifecycle management
Communicate across varying data roles to solve cross-domain data governance issues.
Ability to plan, execute and manage data quality initiatives
Use appropriate data governance, data management, or metadata management tools to organize and streamline data governance processes, procedures, and workflows.
Design and execute materials related to adaption and education of DG domain and tools
Analyze current data architectures and operating environments and make recommendations for future functionality and enhancements.
Support executive level briefings on data solution architecture, design and technology alternatives.
YOUR EXPERIENCE AND EXPERTISE:
Bachelor’s degree in computer science, data science or a related field with five (5) or more years of working as a data engineer, ETL developer and/or data warehouse DBA.
Strong oral and written communications skills, including an ability to successfully communicate technical and non-technical concepts, are required
Hands-on experience with industry leading data governance, data cataloging, data management, metadata management software/tools/platforms ( Informatica, Collibra)
Strong data governance background with deep knowledge of metadata management, data quality management, data integration or data management.
Proficiency with data management or metadata management technology products
Working knowledge of databases and database structures to enable articulating data concepts between business-focused, functional, and technical data users.
4+ years of experience in data analysis, metadata management, data quality or a related field.
Experience with Azure cloud architecture, Databricks, Synapse, Power BI or similar technologies
Experience with developing highly responsive data structures, metadata capture strategies, ontologies, and data dictionaries
Ability to work effectively in teams of technical and non-technical individuals
Skill and comfort working in a rapidly changing environment with dynamic objectives and iteration with users.
Demonstrated ability to continuously learn, work independently, and make decisions with minimal supervision.
Proven track-record of strong customer communications including feedback gathering, execution updates, and troubleshooting.
STANDOUT QUALIFICATIONS:
Relevant technology and/or data management certifications
Oshkosh is committed to working with and offering reasonable accommodation to job applicants with disabilities. If you need assistance or an accommodation due to disability for any part of the employment process, please contact us at: 920-502-3009 or corporatetalentacquisition@oshkoshcorp.com.
Oshkosh Corporation is an Equal Opportunity and Affirmative Action Employer. This company will provide equal opportunity to all individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status. Information collected regarding categories as provided by law will in no way affect the decision regarding an employment application.
Oshkosh Corporation will not discharge or in any manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with Oshkosh Corporation's legal duty to furnish information.
Certain positions with Oshkosh Corporation require access to controlled goods and technologies subject to the International Traffic in Arms Regulations or the Export Administration Regulations. Applicants for these positions may need to be ""U.S. Persons,"" as defined in these regulations. Generally, a ""U.S. Person"" is a U.S. citizen, lawful permanent resident, or an individual who has been admitted as a refugee or granted asylum.",3.9,"Oshkosh Corporation
3.9","Austin, TX",10000+ Employees,1917,Company - Public,Transportation Equipment Manufacturing,Manufacturing,$5 to $10 billion (USD)
79,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
80,Data Engineer,$78K - $112K (Glassdoor est.),"Translate business requirements and functional specifications into logical program designs and to deliver code modules, stable application systems, and software solutions.
Develop, configure, or modify complex integrated business and/or enterprise application solutions within various computing environments.
Implement and maintain complex business and enterprise software solutions to ensure successful deployment of released applications.
Translate complex cross-functional business requirements and functional specifications into logical program designs, code modules, stable application systems, and software solutions.
Partner with Product Team to understand business needs and functional specifications. .
Collaborate with cross-functional teams to ensure specifications are converted into flexible, scalable, and maintainable solution designs.
Evaluate project deliverables to ensure they meet specifications and architectural standards.
Coordinate, execute, and participate in component integration (CIT) scenarios, systems integration testing (SIT), and user acceptance testing (UAT) to identify application errors and to ensure quality software deployment.
Participate in all software development end-to-end product lifecycle phases by applying and sharing an in-depth understanding of complex company and industry methodologies, policies, standards, and controls.
Architect software design patterns and approaches, application level software architecture and make technical trade-off decisions at application level.
Automate and simplify team development, test, and operations processes.
Develop detailed architecture plans for large scale enterprise architecture projects and drives the plans to fruition.
Solve complex architecture/design and business problems; solutions are extensible; work to simplify, optimize, remove bottlenecks, etc.
Minimum Education Required:- All the responsibilities mentioned above are in line with the professional background and requires an absolute minimum of bachelor’s degree in computer science, computer information systems, information technology or a combination of education and experience equating to the U.S. equivalent of a bachelor's degree in one of the aforementioned subjects.",4.9,"Wise Equation Solutions
4.9","Mooresville, NC",51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
81,Data Engineer,Employer Provided Salary:$100K - $145K,"Garner's mission is to transform the healthcare economy, delivering high quality and affordable care for all. By helping employers restructure their healthcare benefit to provide clear incentives and data-driven insights, we direct employees to higher quality and lower cost healthcare providers. The result is that patients get better health outcomes while doctors are rewarded for practicing well, not performing more procedures. We are backed by top-tier venture capital firms, are growing rapidly and looking to expand our team.
We are looking for a Data Engineer to support our technical teams by ensuring ease of access to data within our organization. The ideal candidate for this role will have strong technical skills, including Python, SQL, and AWS as well as a desire to be a hands-on contributor to building out a data platform from the ground up.
Main Responsibilities:
Build the data pipelines that power our business
Collaborate across disciplines to high-quality datasets
Protect our users' privacy and security through best practices
Support data pipelines in production
Our Tools:
Python, AWS, Snowflake, dbt, Terraform, Postgres
The ideal candidate has:
2+ years of experience building data pipelines in a fast-paced environment
Strong Python knowledge
Experience with Big Data technologies such as Snowflake, RedShift, BigQuery, or DataBricks
Ability to think in principles and frameworks to understand and decompose abstract problems
An aptitude to learn new technologies and tools quickly
Why You Should Join Our Team:
You are mission-driven and want to work at a company that can change the healthcare system
You want to be on a small, fast-paced team that nimbly moves to meet new challenges
You love ideating on new features and working with data to find new insights
You're excited about researching and working with the latest tools and technologies
The salary range for this position is $100,000 - $145,000. Compensation for this role will depend on a variety of factors including qualifications, skills and applicable laws. In addition to base compensation this role is eligible to participate in our equity incentive and competitive benefits plans.

Garner Health is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics.
Garner Health is committed to providing accommodations for qualified individuals with disabilities in our recruiting process. If you need assistance or an accommodation due to a disability, you may contact us at talent@getgarner.com.",3.5,"Garner Health
3.5","Dallas, TX",51 to 200 Employees,2019,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
82,Senior Data Engineer,-1,"Senior Data Engineer
Hybrid Position (3 days per week average in Downtown Austin, TX office)
Note: This is a full-time, in-house position. We do not offer C2C or C2H employment and are not able to sponsor visas for this position.
Acrisure Innovation is a fast-paced, AI-driven team building innovative software to disrupt the $6T+ insurance industry. Our mission is to help the world share its risk more intelligently to power a more vibrant economy. To do this, we are transforming insurance distribution and underwriting into a science.
At the core of our operating model is our technology: we're building the premier AI Factory in the world for risk and applying it at the center of Acrisure, a privately held company recognized as one of the world's top 10 insurance brokerages and the fastest growing insurance brokerage globally. By using the latest technology and advances in AI to push the boundaries of understanding risk, we are systematically converting data into predictions, insights, and choices, and we believe we can remove the constraints associated with scale, scope, and learning that have existed in the insurance industry for centuries.
We are a small team of extremely high-caliber engineers, technologists, and successful startup founders, with diverse backgrounds across industries and technologies. Our engineers have worked at large companies such as Google and Amazon, hedge funds such as Two Sigma and Jump Trading, and a variety of smaller startups that quickly grew such as Indeed, Bazaarvoice, RetailMeNot, and Vrbo.
The Role
The Business Intelligence team's mission is to unify data across the enterprise to optimize business decisions made at the strategic, tactical, and operational levels of the organization. We accomplish this by providing an enterprise data warehouse, data lake, reporting platform, and business processes that provide quality data, in a timely fashion, from any channel of the company and present them in such a manner as to maximize the value of that data for both internal and external customers.
The Senior Data Engineer is responsible for designing and developing moderate to complex ETL processes required to populate a data lake and structured data warehouse which supply data for the machine learning, AI & BI teams. Responsibility includes working with a team of contracted developers as well as coaching and mentoring junior and mid-level developers. Ensuring high quality and best practices are maintained through the development cycle is key to this position.
You will interact with some of the top technologists on the planet. Our technology runs on Google Cloud and is configured with Kubernetes, leveraging various services in that environment. Our data storage layer includes BigQuery, BigTable, and Postgres. We code primarily in Kotlin, Python, Java, and JavaScript and make use of many frameworks, including Dataflow, Cloud AI Platform, KubeFlow, Spring, and React.
Here are some of the ways in which you'll achieve impact
Leverage established guidelines and custom designs to create complex ETL processes to meet the needs of the business
Develop from strategic and non-strategic data sources including data preparation/ETL and modeling for data visualizations in a self-service platform
Contribute to the definition and development of the overall reporting roadmap
Translate reporting requirements into reporting models, visualizations and reports by having a strong understanding of the enterprise architecture
Standardize reporting that helps generate efficiencies, optimization, and end user standards
Integrate dashboards and reports from a variety of sources, ensuring that they adhere to data quality, usability, and business rule standards
Independently determine methods and procedures for new or existing requirements and functionality
Work closely with analysts and data engineers to identify opportunities and assess improvements of our products and services
Contribute to workshops with the business user community to further their knowledge and use of the data ecosystem
Produce and maintain accurate project documentation
Collaborate with various data providers to resolve dashboard, reporting and data related issues
Perform Data Services reporting benchmarking, enhancements, optimizations, and platform analytics
Participate in the research, development, and adoption of trends in reporting and analytics
Mentor BI Developers and BI Analysts
Other projects as assigned in order to support necessary business goals across teams
You may be fit for this role if you have
Minimum 10 years required, particularly in an Azure environment with Azure Data Bricks, Azure Data Factory, Azure Data Lake
Minimum 10 years designing data warehouses, data modeling, and end-to-end ETL processes in a MS-SQL environment
Minimum 4 years developing machine learning models with Azure ML, ML Flow, BQML
Expert working knowledge of SQL, Python and Spark (and ideally PySpark) with a demonstrated ability to create ad-hoc SQL queries to analyze data, create prototypes, etc required.
Successfully delivered 4+ end to end projects – from Inception to Execution - in Data Engineering / Data Science / Data Integration as a Tech Senior/Principal
Ability to Analyze, summarize, and characterize large or small data sets with varying degrees of fidelity or quality, and identify and explain any insights or patterns within them.
Experience with multi-source data warehouses
Strong skills in in data analytics and reporting, particularly with Power BI
Experience with other cloud environments (GCS, AWS) a definite plus
Strong experience creating reports, dashboards, and/or summarizing large amounts of data into actionable intelligence to drive business decisions required
Strong understanding of core principles of data science and machine learning; experience developing solutions using related tools and libraries
Hands on experience building logical data models and physical data models and using tools like ER/Studio/Idera
Write SQL fluently, recognize and correct inefficient or error-prone SQL, and perform test-driven validation of SQL queries and their results
Proficient in writing Spark sql using complex syntax and logic like analytic functions etc.
Well versed in Data Lake & Delta Lake Concepts
Well versed in Databricks usage in dealing with Delta tables (external \ managed)
Well versed with Key Vault \ create & maintenance and usage of secrets in both Databricks & ADF
Should be knowledgeable in Stored procedures \ functions and be able to use them by ADF & Databricks as this is a widely used Practice internally
Familiar with DevOps process for Azure artifacts and database artifacts
Well versed with ADF concepts like chaining pipelines, passing parameters, using APIs for ADF & Databricks to perform various activities.
Experience creating and sharing standards, best practices, documentation, and reference examples for data warehouse, integration/ETL systems, and end user reporting
Apply disciplined approach to testing software and data, identifying data anomalies, and correcting both data errors and their root causes
Academics: Undergraduate degree preferred or equivalent experience along with a demonstrated desire for continuing education and improvement
Location: Austin, TX hybrid
We are interested in every qualified candidate who is eligible to work in the United States. We are not able to sponsor visas for this position.
#LI-Hybrid",-1,Acrisure Innovation,"Austin, TX",-1,-1,-1,-1,-1,-1
83,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
84,Data engineer,$97K - $136K (Glassdoor est.),"Qualification:
Master's Degree in Statistical Analytics, Data Science, or Bachelor's Degree in computer science engineering will be considered with at least three - five years of applicable work experience
Preferred Proficiency:

Web development experience (AngularJS, D3).
Experience in a statistical programming language like R or Python; applied machine learning techniques including dimensionality reduction strategies, supervised/unsupervised classification and natural language processing frameworks.
Experience in at least one data visualization tools (e.g. Tableau, QlikView) and data warehousing tools (e.g. Informatica) is preferred
Huge Advantage:
Building and scaling Machine Learning frameworks
Hadoop (Hive, Spark, UDF's)
Definite Plus:

Web development experience (AngularJS, D3).
Experience:

5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.
5+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets.
2+ years of experience in scripting languages like Python etc.",4.7,"Stacklogy
4.7","Fremont, CA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
85,Senior Data Engineer,Employer Provided Salary:$105K - $160K,"Senior Data Engineer, San Diego, CA based OR willing to relocate to San Diego in 60 Days.
Company
PointPredictive is a fast-growing technology start-up that leverages a patented combination of artificial and natural intelligence [Ai+Ni] to provide risk assessments in the auto lending, mortgage, and retail space. The platform has been proven to reduce lender loan losses by 40-60% with review rates of 5-10% of their applications, resulting in higher productivity of lender risk management departments, significantly lower losses to their bottom lines, and improved customer experience. The company was founded in 2013 by a seasoned team of technology entrepreneurs with over 20 years of experience in the startup space (including several acquisitions) and has financial backing from top tier investors.
Role:
The company is looking for an outstanding Senior Data Engineer to focus on its Data Asset, scale the Database and Data Asset for high performance and reliability. You will also serve as an engineering resource for data related questions, issues, and bugs. Core skills include Python, Snowflake, database systems and SQL, and Amazon Web Services (AWS).
Responsibilities:
· Developing new extract-transform-load (ETL) processes and pipelines. Must be able to manage large volumes of data flowing in from a variety of formats and into a variety of location.
· Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Automation experience highly desired.
Write complex SQL; be fluent in relational based systems; have strong analytical and problem-solving skills.
Ability to represent complex algorithms in software; bring strong understanding of database technologies, management systems, data structures, and algorithms; a deep understanding in database architecture testing methodology.
Develop and execute test plan, debugging, and testing scripts and tools.
Building real-time streaming data pipelines; building and deploying data pipelines, data streams, and extract-transform-load (ETL) processes.
Manage Data Governance and Data Cleansing, as well as supporting production issues and customer requests.
Provide engineering support to customer issues and bugs. Research and implement fixes.
About you:
You have 5+ years of relevant software development industry experience building and operating scalable, fault-tolerant, distributed systems.
Database and software development experience with Python, SQL, Redshift, and experience with Amazon Web Services, Snowflake along with pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming, etc.
Experience with container services.
Fluid with Amazon Web Services.
Experience with concurrency, multithreading, and the deployment of distributed system architectures.
Experience leading and shipping large scope technical projects in collaboration with multiple experienced engineers.
You have excellent communication skills and the ability to work well within a team and across engineering teams.
You are a strong problem solver and have solid production debugging skills.
You Thrive in a fast-paced environment and see yourself as a partner with the business with the shared goal of moving the business forward.
You have a high level of responsibility, ownership, and accountability.
Job Type: Full-time
Competitive pay, bonus, equity, and benefits:
Benefits:
401(k)
Health insurance
Dental insurance
Flexible spending account
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Education:
Bachelor's or Master’s (Preferred)
Work Location: San Diego (Del Mar)
Job Type: Full-time
Pay: $105,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Stock options
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Are you willing to be in the office 5 days a week to influence culture and capabilities ?
Are you willing to move to San Diego within 60 days if not within the commuting region ?
Work Location: Hybrid remote in San Diego, CA 92130",-1,"Point Predictive, Inc.","San Diego, CA",-1,-1,-1,-1,-1,-1
86,Data Engineer,Employer Provided Salary:$150K,"Data Engineer
We are looking for a Data Engineer that will architect and implement systems handling the ingestion, cleaning, and structuring of data. They will be a member of the Data Science team and will have direct interaction with the people using the data ingested. The hire will also be responsible for managing our group’s core infrastructure including our region redundant pipeline orchestration servers (Airflow) and webserver stack (NGINX + Gunicorn + Django). The ideal candidate is an experienced data pipeline builder and core infrastructure guru.
Responsibilities:
Create and maintain an optimal data pipelines in Python + SQL
Build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python and SQL (Snowflake & SQL Server) in both on premise and cloud environments
Manage and improve the user facing server infrastructure (web + api) including authorization and load balancing
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that that provide actionable insights on both the user interactions and infrastructure loads
Technical Qualifications:
Python expert, notably savvy with data science stack (Pandas, NumPy, SciPy)
Relational SQL databases, Microsoft SQL Server and Postgres preferred
Snowflake cloud database experience
Pipeline and workflow management tools: Airflow, Luigi, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Athena, SQS, etc.
2+ professional years’ experience with Masters or 3+ professional years with Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Highly Valued:
Data Science/Analysis background
Computer science understanding, data structures, processes, threading, memory usage
Unix/Linux command-line experience
Broad understanding of equities, derivatives, futures, FX, or other financial-services instruments
Other Qualifications:
Excellent listening, and communication (both oral and written) skills
Self-starter and critical thinker, takes ownership of own projects and makes improvement suggestions for the entire infrastructure.
Proactive, assertive and attentive to details.
Can work independently and in a collaborative environment.
Can handle several projects with different priorities at the same time in a fast-paced environment.
Excellent self-management and problem-solving skills.
Results-oriented, can deliver quality code quickly
Millennium pays a total compensation package which includes a base salary, discretionary performance bonus, and a comprehensive benefits package. The estimated base salary range for this position is $150,000, which is specific to New York and may change in the future. When finalizing an offer, we take into consideration an individual’s experience level and the qualifications they bring to the role to formulate a competitive total compensation package.",3.8,"Millennium Management LLC
3.8","New York, NY",1001 to 5000 Employees,1989,Company - Private,Investment & Asset Management,Financial Services,Unknown / Non-Applicable
87,Data Engineer,-1,"Who We Are
Land Intelligence is a software technology company serving the commercial real estate industry. We focus on Land Development. We have been recognized as an industry technology leader in providing solutions on a national scale. Our team are visionaries that see a better, faster, and more valuable way to research, finance, and trade land.

Our Culture
We are entrepreneurs first. Which means we manage the people, processes and product. We create new ways of doing things to drive value. We are builders and growth minded. Our leadership team has been recognized as a Best Places to work in the industry nationally. Our team drives for personal and professional development, as personal growth is instrumental to our success. Your learning will be supported by specialized in-house training programs and mentoring by the industry’s leading experts, many of whom are our investors and strategic partners.
Job Overview
Land Intelligence is seeking a savvy Data Engineer to join our growing team and help us continue to enhance our SaaS platform, LandSUITE®. The hire will be responsible for expanding and optimizing our data and data pipeline architecture to support product development and internal tools. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of re-designing our company's data architecture to support our next generation of products and data initiatives.
Responsibilities
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies
Build analytics tools that use the data pipeline to provide actionable insights into user behavior and market trends
Work with stakeholders including the executive and product development teams to assist with data-related technical issues and support their data infrastructure needs
Keep our data separated and secure across AWS regions
Create data tools for analytics and team members that assist them in building and optimizing LandSUITE® into an innovative industry leader
Qualifications
We are looking for a candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large, disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
Experience with the following software/tools:
Experience with relational SQL and NoSQL databases, including Postgres, MySQL, and Cassandra
Experience with data pipeline and workflow management tools
Experience with stream-processing systems
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java
We are a startup, but this isn't our first time doing this. As a result, you can get the thrill of working at a startup, with the resources of a publicly traded company. We offer a best-in-class benefits package, as we are a Professional Employment Organization (PEO) with our partner Insperity that includes medical, vision, dental and life insurance. Our 401 (k) program offers an employer match, along with a 401(k)-profit sharing and performance-based bonuses. • Generous paid time off

Land Intelligence is an EOE/Affirmative Action Employer M/F/D/V. If you are interested in applying for employment and need special assistance to apply for a posted position, please send an e-mail to careers@landintelligence.net.",1.8,"Land Intelligence Inc
1.8",Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
88,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
89,Sr. Data Engineer I,$82K - $109K (Glassdoor est.),"Pax8 is the leading value-added cloud-based SaaS distributor, simplifying the cloud journey for our partners by integrating technology, business intelligence and proactive service to deliver an unparalleled experience. Serving thousands of partners through the indirect sales channel, our mission is to be the world's favorite place to buy cloud products. We are a fast-growing, dynamic and high-energy startup organization, allowing you to make a meaningful impact on the business. Culture is important to us, and at Pax8, it's business, and it IS personal. We are passionate, creative and humorously offbeat. We work hard, keep it fun, and expect the best.
We Elev8 each other. We Advoc8 for our partners. We Innov8 continuously. We Celebr8 life.
No matter who you are, Pax8 is a place you can call home. We know there's no such thing as a ""perfect"" candidate, so we don't look for the right ""fit"" – instead, we look for the add. We encourage you to apply for a role at Pax8 even if you don't meet 100% of the bullet points. We believe in cultivating an environment with a diversity of perspectives, in hopes that we can all thrive in an inclusive environment.
We are only as great as our people. And we have great people all over the world. No matter where you live and work, you're a part of the Pax8 team. This means embracing hybrid- and remote-work whenever possible.
Position Summary:
The Sr. Data Engineer I designs, develops, tests, deploys, maintains, and improves systems that collect, transform, store, and manage data for end users. They deliver individual projects based upon deadlines and required deliverables. The Sr. Engineer builds complex features independently and collaborates with other teams to conduct design and code reviews. They develop and/or provide technical leadership in the development of data systems involving the application of new technologies with significant technical risk. The Sr. Engineer prepares detailed plans, which may span over a year for complex projects. They determine test philosophy, goals, and objectives, and participate in the formation of project goals, scope, and schedule.
Essential Responsibilities:
Builds pipelines to ingest new data sources
Transforms data to support varied use cases
Includes testing in all aspects of the development process
Mentors junior and mid-level Engineers
Optimizes existing data pipelines and improves existing code quality
Makes updates and improvements to deployment processes
Participates in project planning and architecture discussions
Analyzes potential problems and finds solutions to pressing data issues
Participates in on-call rotation
Ideal Skills, Experience, and Competencies:
At least Four (4) years of relevant data engineering experience
Advanced experience with Python
Expert experience with SQL
Intermediate experience with a JVM language
Exposure to other software development languages
Advanced experience with Apache Spark or other distributed processing engines
Advanced experience with Apache Kafka or other stream processing frameworks
Intermediate experience with Terraform, Docker, Kubernetes, or other similar infrastructure tooling
Advanced experience with cloud data tools such as S3, Glue, and Athena
Intermediate experience with building CI/CD pipelines
Effective problem solving and troubleshooting abilities
Ability to consistently achieve results, even under tough circumstances
Effective technical leadership abilities
Excellent verbal and written communication skills
Experience with innovative application design and implementation
Ability to make sense of complex, high quantity, and sometimes contradictory information to effectively solve problems
Required Education & Certifications:
B.A./B.S. in related field or equivalent work experience
M.S./M.A. in related field or equivalent work experience
Compensation:
Qualified candidates can expect a salary beginning at $140,000 or more depending on experience
#LI-Remote #LI-JF1 #Dice-J #BI-Remote

Note: Compensation is benchmarked on local Denver Metro area market rates. Qualified candidates in other locations can expect a salary package that may be adjusted based off applicable cost of wages in their respective location.
At Pax8 we believe that your Total Rewards should include a benefits package that shows how much we value our greatest assets. All FTE Pax8 people enjoy the following benefits:
Non-Commissioned Bonus Plans or Variable Commission
401(k) plan with employer match
Medical, Dental & Vision Insurance
Employee Assistance Program
Employer Paid Short & Long Term Disability, Life and AD&D Insurance
Flexible, Open Vacation
Paid Sick Time Off
Extended Leave for Life events
RTD Eco Pass (For local Colorado Employees)
Career Development Programs
Stock Option Eligibility
Employee-led Resource Groups
Pax8 is an EEOC Employer.",4.1,"Pax8
4.1","Greenwood Village, CO",501 to 1000 Employees,2012,Company - Private,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
90,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
91,Junior Data Engineer,$83K - $119K (Glassdoor est.),"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a junior data engineer, you will be part of a team that is responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.
The position:
Develop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Support various components of the data pipelines, including ingestion, validation, cleansing and curation
Manage and ensure the success of ongoing data pipeline routines
Collaborate with our implementation team to assist with the identification and reconciliation of data anomalies
Create and maintain documentation on data pipelines
Engage with our software engineering team to ensure precise data points per application specifications
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering, or applicable experience
Expertise with SQL, database design and data manipulation methodologies
Expertise with ETL/ELT and the development of automated validation and data pipelines
Strong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW, master data management and other database design principles
Comfortable working with high volume data in a variety of formats
Excellent verbal and written communication
Self-motivated
Passionate at learning
Familiarity with healthcare data is a plus
Experience with CI/CD and version control tools is a plus
Experience working within hybrid cloud environment such as AWS is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation
Social outings",5.0,"Decision Point Healthcare
5.0","Boston, MA",1 to 50 Employees,2013,Company - Public,-1,-1,Less than $1 million (USD)
92,Data Engineer,$74K - $107K (Glassdoor est.),"Analyze Business Requirement Documents and Implement Technical Solutions for privacy related applications.
Develop ETL process for supporting Data Extraction, transformations and loading.
Perform data conversions and aggregations using different transformations such as Merge, Merge join, Union condition split, sort, order by. Derived columns convert and cast transformations and row count and lookup and fuzzy lookup transformations.
Develop UNIX scripts to load the data from Source server to Teradata and validate the files between different servers.
Develop new process to implement state level privacy regulations based on each state law in Big Data Platform.
Create Temperory/Fact tables, loading with data and writing Teradata and Spark SQL queries.
Optimize/tune ETL objects, indexing and partitioning for better performance and efficiency.
Validate the performance metrics and work on performance tuning for SQL, HQL and Spark SQL queries.
Perform testing and Provide test support for various level of testing phases like Unit, User Acceptance, Regression, Parallel and System testing.
Promote the components to production environment through CI/CD process by using Git hub .
Script task and execute SQL tasks to execute SQL code. Work on containers for loop and for each loop container to run a group of tasks into a single container and repeating tasks.
Create the data flow to extract data from sources to OLEDB Source, Excel, XML, flat files sources and destination is SQL data warehouse.
Minimum Education Required:- All the responsibilities mentioned above are in line with the professional background and requires an absolute minimum of a Bachelor’s degree in computer science, computer information systems, technology management, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.",4.9,"Wise Equation Solutions
4.9","La Vista, NE",51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
93,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
94,Data Visualization Engineer,$77K - $101K (Glassdoor est.),"Passionate about precision medicine and advancing the healthcare industry?
Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.
With the advent of genomic sequencing, we can finally decode and process our genetic makeup. We now have more data than ever before but providers don't have the infrastructure or expertise to make sense of this data. We're on a mission to connect an entire ecosystem to redefine how genomic data is used in clinical settings.
What You'll Do:
Own the design, development and administration of advanced analytics dashboards and reporting and business intelligence tools. Beginning with requirements gathering across cross-functional teams, to the delivery of analytic solutions into production.
Collaborate with stakeholders, providing guidance, data and analytical support for developing new datasets, analyses and tools in support of strategic initiatives.
Perform deep-dive root cause and problem-solving analyses for and with laboratory, business operations and engineering teams.
Design and build data views to support advanced analytical reporting, self-service data analysis and exploration, utilizing multiple data and visualization technologies.
Work closely with technical and product teams to identify and align changes to internal systems in support of future data analysis needs and requirements.
Develop and facilitate data governance initiatives to assure data integrity across distributed systems.
Solve new and interesting problems.
Qualifications:
Bachelor's degree in Information Systems, Business Administration, Finance, Economics, Mathematics, Operations Research, Marketing or similar
3+ years of experience developing data sets for analysis use-cases
3+ years of experience with direct stakeholder management and ownership of full lifecycle of analytics/data projects and presenting analysis results to stakeholders
3+ years report-development and data visualization experience with BI tools (Looker, Tableau, etc.)
Ability to deliver both high-level explanations of complex problems while also able to deep dive into complex problems
Advanced SQL skills for querying and analysis of large, complex datasets
Must work well in a team environment and be able to partner with both business and technical team members and translate technical language to the non-technical need
Creative problem solver, ability to handle multiple projects, and strong work ethic needed
Excellent written and verbal communication skills
Highly organized and systematic, superb attention to detail
Able to complete tasks with a high degree of accuracy and an eye towards process improvement
Persistent and resilient person who thrives in a fast-paced, highly collaborative environment
Bonus points for:
Advanced analytics experience - classification/predictive modeling, optimization models, forecasting
Experience with Python for data engineering or advanced data analysis projects/use-cases
Healthcare domain knowledge and experience
Expert knowledge of database/data modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others
Experience building cloud-native applications and supporting technologies / patterns / practices including: GCP, AWS, Docker, CI/CD, DevOps, and microservices

#LI-EH1
#LI-Hybrid
We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.1,"Tempus
3.1","Chicago, IL",501 to 1000 Employees,2015,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
95,Data Engineer,$67K - $96K (Glassdoor est.),"Massanutten Resort
Job Summary
The Data Engineer will play a crucial role in designing, developing, and maintaining the data infrastructure and systems that enable efficient and reliable data processing, storage, and analysis. The primary responsibility is to transform raw data into usable formats for various business applications and support data-informed decision-making processes. This role includes working in various Microsoft SQL Environments as well as supporting a Snowflake Data Warehouse. The Data Engineer is expected to have a broad understanding of data concepts and be avid about continuing education to keep up with the ever-changing data landscape.
Required
Degree in computer science or related file and 3+ years of experience with Python, SQL, and data visualization/exploration tools (in lieu of a degree, at least 5 years of experience in the above is equivalent)
Preferred
A degree in computer science or a related field
Certifications in database management and/or related technical certifications
Familiarity with advanced data concepts (i.e., AI and Machine Learning) and business intelligence/visualization tools
Typical Schedule
Days:
Monday-Friday, weekends and holidays as needed
Position will be required to be part of rotating on-call schedule
Hours:
8am-5pm, additional hours as needed
Core Responsibilities
Develop Data Pipeline: Design and implement scalable and efficient data pipelines, integrating diverse data sources and ensuring smooth data flow from ingestion to storage and processing
Transform and Model Data: Develop and maintain data transformation processes, such as ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform), to convert raw data into structured formats suitable for analysis and reporting. Create and manage data models to facilitate data integration and provide a consistent view of information across systems.
Implement and oversee Data Warehousing and Storage: Build and maintain data warehouses or data lakes, optimizing storage and retrieval processes to ensure high availability, reliability, and performance. Implement appropriate data governance and security measures to protect sensitive data.
Ensure Data Quality and Governance: Collaborate with data analysts and business stakeholders to understand data requirements and define data quality standards. Implement data validation, data profiling, and data cleansing techniques to ensure data accuracy, consistency, and integrity.
Optimize Performance: Identify and address performance bottlenecks in data processing and storage systems, leveraging optimization techniques and technologies to improve overall data engineering efficiency and reduce latency
Assist with Data Problem Solving: Support team with data related escalation issues, including help desk support tickets that are generated from our business users
Manage Data Integration and API Development: Establish connections and integrations between internal and external data sources, including APIs, databases, and third-party platforms. Develop and maintain data APIs to enable seamless data access and consumption by other applications and services.
Collaborate and Document: Work closely with cross-functional teams, including analysts, software engineers, and business stakeholders, to understand data requirements and ensure data solutions align with business objectives. Document data engineering processes, including data pipelines, transformations, and system architectures, to facilitate knowledge sharing and maintain a robust knowledge base.
Stay up-to-date with the latest advancements in data engineering, data management, and big data technologies. Evaluate and recommend new tools, frameworks, and methodologies to improve data engineering practices and enhance overall data infrastructure
Other duties as assigned by management
For more information, contact Ana at 540-289-4939.",3.6,"Great Eastern Resort Management, Inc.
3.6","McGaheysville, VA",1001 to 5000 Employees,-1,Self-employed,-1,-1,Unknown / Non-Applicable
96,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
97,Data Engineer,Employer Provided Salary:$100K - $145K,"Garner's mission is to transform the healthcare economy, delivering high quality and affordable care for all. By helping employers restructure their healthcare benefit to provide clear incentives and data-driven insights, we direct employees to higher quality and lower cost healthcare providers. The result is that patients get better health outcomes while doctors are rewarded for practicing well, not performing more procedures. We are backed by top-tier venture capital firms, are growing rapidly and looking to expand our team.
We are looking for a Data Engineer to support our technical teams by ensuring ease of access to data within our organization. The ideal candidate for this role will have strong technical skills, including Python, SQL, and AWS as well as a desire to be a hands-on contributor to building out a data platform from the ground up.
Main Responsibilities:
Build the data pipelines that power our business
Collaborate across disciplines to high-quality datasets
Protect our users' privacy and security through best practices
Support data pipelines in production
Our Tools:
Python, AWS, Snowflake, dbt, Terraform, Postgres
The ideal candidate has:
2+ years of experience building data pipelines in a fast-paced environment
Strong Python knowledge
Experience with Big Data technologies such as Snowflake, RedShift, BigQuery, or DataBricks
Ability to think in principles and frameworks to understand and decompose abstract problems
An aptitude to learn new technologies and tools quickly
Why You Should Join Our Team:
You are mission-driven and want to work at a company that can change the healthcare system
You want to be on a small, fast-paced team that nimbly moves to meet new challenges
You love ideating on new features and working with data to find new insights
You're excited about researching and working with the latest tools and technologies
The salary range for this position is $100,000 - $145,000. Compensation for this role will depend on a variety of factors including qualifications, skills and applicable laws. In addition to base compensation this role is eligible to participate in our equity incentive and competitive benefits plans.

Garner Health is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics.
Garner Health is committed to providing accommodations for qualified individuals with disabilities in our recruiting process. If you need assistance or an accommodation due to a disability, you may contact us at talent@getgarner.com.",3.5,"Garner Health
3.5","Dallas, TX",51 to 200 Employees,2019,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
98,AWS Data Engineer Snowflake,-1,"Company Description

Orchestrated by adept technical architects with over fifty years of applied expertise, KYNITE is an advanced technology company specializing in the disciplines of: Blockchain, Cloud Services, Big Data & Analytics, Artificial Intelligence, Enterprise, Staff Augmentation and Managed Services
We are BigData Experts
We are Cloud Experts
We are Enterprise Architects
We are Artificial Intelligence Innovators
We are Technological Evangelists
We are Doers
We are Kynite

Additional Information

All your
This job is only for individuals residing in US
US Citizens, Green Card holders, EAD's can apply
W2, C2c,
Information will be kept confidential according to EEO guidelines.",-1,Kynite,"Myrtle Point, OR",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
99,Data Insights Engineer,Employer Provided Salary:$85K - $95K,"Who We Are
We're purpose-driven. With every ride, we aim to redefine health and happiness. It's all about being more than a workout: SoulCycle is a mind-body-soul experience, built on community, love, respect, acceptance, and a lot of fun. It comes to life through the ride, the relationships, and the unparalleled hospitality. And all of that comes from our people. Join us—we'd love to have you.
Our Mission
To foster an open, diverse, & inclusive community—while embracing each unique individual exactly as they are. We empower each other by listening with an open mind, finding ways to learn and grow together, and always nurturing a sanctuary of trust. To make a real, lasting impact, we'll work nonstop to embrace and create change. Because nobody is equal until everyone is equal.
Job Description
The Data Insights Engineer will play a pivotal role in driving data-driven decisions at SoulCycle. You'll be responsible for building and maintaining the data infrastructure that supports all business functions, from marketing and operations to finance and customer experience, in addition to providing analysis to each of these teams. By leveraging your technical expertise and analytical skills, you will empower stakeholders to derive insights from data, enabling them to make strategic and informed decisions that positively impact the business.
Roles and Responsibilities
Insights and Recommendations: collaborate with cross-functional teams to understand business requirements, provide analytical support, and identify opportunities for data-driven improvements
Visualizations and Dashboarding: design and develop ad-hoc and recurring Looker reports; create and monitor business metrics; identify patterns, trends, and opportunities for performance improvement
Data Modeling: build, optimize, and document LookML data models that support quick and efficient analysis
Prediction: build predictive models that forecast business outcomes, customer behavior, and other relevant metrics
Qualifications
1-3 years of professional experience transforming and analyzing data across platforms such as Looker, Tableau, Mode, Jupyter Notebooks, Excel, and GCP/AWS. Looker/LookML experience is a plus.
Expert in SQL (able to write structured and efficient queries on large data sets) and familiarity with Python
Ability to identify patterns and trends in data and solve problems
Excellent communication skills to work with stakeholders to translate business needs and ideas into analyses and recommendations
Top-notch organizational skills and ability to manage projects in a fast-paced environment
Creative problem solving skills to find solutions to vague questions
Experience with Python data analysis and visualization packages is a plus (pandas, tensorflow, matplotlib, etc.)
Pay Range: $85,000 - $95,000 per year. This role is on-site 4 days a week.",3.9,"SoulCycle HQ
3.9","New York, NY",1001 to 5000 Employees,2006,Company - Private,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
100,"Data Engineer, Election Platforms (all-levels)",-1,"Job Description
The Washington Post is hiring a data engineer for our Election Platforms team, to work on our data pipelines, tools, and infrastructure ahead of the 2024 presidential election.
The Election Platforms team focuses on leveraging data, partnering with stakeholders, and pioneering serviceable tools to facilitate The Post’s election coverage. This role is part of Elections Engineering, a cross-functional group of full-stack engineers and data scientists, with varied technical and academic backgrounds, responsible for delivering live election results across The Post’s platforms.
The role will be focused on contributing to our technical stack and developing new technology to facilitate robust and lively election coverage. In particular, we’re looking for candidates who would be comfortable working on deploying high-impact code in production and have an interest in data and newsroom tooling. We also work on other data engineering projects related to elections, such as voter registration data and campaign finance, and occasionally write
open
source
software
. Most of our work is in Python and TypeScript/React/Node.
We typically work at a higher level of abstraction than a single story, and aim to create general purpose tools that are reusable and easy to deploy. Our ideal candidates are creative problem-solvers, who are comfortable writing production level code to solve real-world newsroom problems. This position is remote-friendly for work in the US.
Motivation
You have a desire to contribute to The Post’s journalism through technology.
You are passionate about refining and improving team process, culture, documentation and communication while tackling complex data science and engineering challenges.
You want to work at a deeper level of abstraction than a single story or beat.
You are excited about learning new technologies and applying them to the practice of journalism.
Responsibilities
Work on the software components of our live election architecture including deployment, observability, participating in code reviews, and more.
Work closely with our engineers and data scientists to develop and maintain pipelines and administrative apps/dashboards for election data.
Work on tooling to make data preparation easier and more efficient for our live election model.
Work with newsroom stakeholders to assess needs, build useful and reusable tooling, and contribute to storytelling.
Foster effective team collaboration and communication by improving and documenting internal tools, processes and guidelines.
Assist in attracting and hiring team members who want to work at the intersection of journalism and technology by actively participating in recruiting – and contributing feedback on our overall hiring process.
Qualifications
A minimum of one year of experience building and maintaining backend and data engineering applications.
Proficiency across some of the languages and frameworks used in our engineering stack such as Python, TypeScript/React/Node, and AWS services.
A desire to work on cutting edge, cross-functional tools to help with features, data science, and machine learning/big data work in a team setting.
Experience with election data is not necessary, but is a plus.
Experience developing and deploying newsroom tools and applications is not necessary, but is a plus.
Levels
If you have less than three years, but at least one year, we’d love to evaluate you for a data engineer role.
If you have more than three years but less than about seven years, we’d love to evaluate you for a senior data engineer role.
If you have more than about seven years, we’d love to talk to you about a principal data engineer role.
Wherever you are in your life or career, The Washington Post offers comprehensive and inclusive benefits for every step of your journey:
Competitive medical, dental and vision coverage
Company-paid pension and 401(k) match
Three weeks of vacation and up to three weeks of paid sick leave
Nine paid holidays and two personal days
20 weeks paid parental leave for any new parent
Robust mental health resources
Backup care and caregiver concierge services
Gender affirming services
Pet insurance
Free Post digital subscription
Leadership and career development programs
Benefits may vary based on the job, full-time or part-time schedule, location, and collectively bargained status.
The Post strives to provide its readers with high-quality, trustworthy news and information while constantly innovating. That mission is best served by a diverse, multi-generational workforce with varied life experiences and perspectives. All cultures and backgrounds are welcomed.
The innovation doesn’t end in the Newsroom – dozens of teams power The Washington Post. We are now hiring the next innovator – how will you Impact Tomorrow?
#washpostlife",4.2,"The Washington Post
4.2","Washington, DC",1001 to 5000 Employees,1877,Company - Private,Publishing,Media & Communication,Unknown / Non-Applicable
101,Data Engineer,-1,"Who We Are
Land Intelligence is a software technology company serving the commercial real estate industry. We focus on Land Development. We have been recognized as an industry technology leader in providing solutions on a national scale. Our team are visionaries that see a better, faster, and more valuable way to research, finance, and trade land.

Our Culture
We are entrepreneurs first. Which means we manage the people, processes and product. We create new ways of doing things to drive value. We are builders and growth minded. Our leadership team has been recognized as a Best Places to work in the industry nationally. Our team drives for personal and professional development, as personal growth is instrumental to our success. Your learning will be supported by specialized in-house training programs and mentoring by the industry’s leading experts, many of whom are our investors and strategic partners.
Job Overview
Land Intelligence is seeking a savvy Data Engineer to join our growing team and help us continue to enhance our SaaS platform, LandSUITE®. The hire will be responsible for expanding and optimizing our data and data pipeline architecture to support product development and internal tools. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of re-designing our company's data architecture to support our next generation of products and data initiatives.
Responsibilities
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies
Build analytics tools that use the data pipeline to provide actionable insights into user behavior and market trends
Work with stakeholders including the executive and product development teams to assist with data-related technical issues and support their data infrastructure needs
Keep our data separated and secure across AWS regions
Create data tools for analytics and team members that assist them in building and optimizing LandSUITE® into an innovative industry leader
Qualifications
We are looking for a candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large, disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
Experience with the following software/tools:
Experience with relational SQL and NoSQL databases, including Postgres, MySQL, and Cassandra
Experience with data pipeline and workflow management tools
Experience with stream-processing systems
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java
We are a startup, but this isn't our first time doing this. As a result, you can get the thrill of working at a startup, with the resources of a publicly traded company. We offer a best-in-class benefits package, as we are a Professional Employment Organization (PEO) with our partner Insperity that includes medical, vision, dental and life insurance. Our 401 (k) program offers an employer match, along with a 401(k)-profit sharing and performance-based bonuses. • Generous paid time off

Land Intelligence is an EOE/Affirmative Action Employer M/F/D/V. If you are interested in applying for employment and need special assistance to apply for a posted position, please send an e-mail to careers@landintelligence.net.",1.8,"Land Intelligence Inc
1.8",Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
102,Data Engineer,Employer Provided Salary:$150K,"Data Engineer
We are looking for a Data Engineer that will architect and implement systems handling the ingestion, cleaning, and structuring of data. They will be a member of the Data Science team and will have direct interaction with the people using the data ingested. The hire will also be responsible for managing our group’s core infrastructure including our region redundant pipeline orchestration servers (Airflow) and webserver stack (NGINX + Gunicorn + Django). The ideal candidate is an experienced data pipeline builder and core infrastructure guru.
Responsibilities:
Create and maintain an optimal data pipelines in Python + SQL
Build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python and SQL (Snowflake & SQL Server) in both on premise and cloud environments
Manage and improve the user facing server infrastructure (web + api) including authorization and load balancing
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that that provide actionable insights on both the user interactions and infrastructure loads
Technical Qualifications:
Python expert, notably savvy with data science stack (Pandas, NumPy, SciPy)
Relational SQL databases, Microsoft SQL Server and Postgres preferred
Snowflake cloud database experience
Pipeline and workflow management tools: Airflow, Luigi, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Athena, SQS, etc.
2+ professional years’ experience with Masters or 3+ professional years with Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Highly Valued:
Data Science/Analysis background
Computer science understanding, data structures, processes, threading, memory usage
Unix/Linux command-line experience
Broad understanding of equities, derivatives, futures, FX, or other financial-services instruments
Other Qualifications:
Excellent listening, and communication (both oral and written) skills
Self-starter and critical thinker, takes ownership of own projects and makes improvement suggestions for the entire infrastructure.
Proactive, assertive and attentive to details.
Can work independently and in a collaborative environment.
Can handle several projects with different priorities at the same time in a fast-paced environment.
Excellent self-management and problem-solving skills.
Results-oriented, can deliver quality code quickly
Millennium pays a total compensation package which includes a base salary, discretionary performance bonus, and a comprehensive benefits package. The estimated base salary range for this position is $150,000, which is specific to New York and may change in the future. When finalizing an offer, we take into consideration an individual’s experience level and the qualifications they bring to the role to formulate a competitive total compensation package.",3.8,"Millennium Management LLC
3.8","New York, NY",1001 to 5000 Employees,1989,Company - Private,Investment & Asset Management,Financial Services,Unknown / Non-Applicable
103,Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"Job title: Data Engineer ( USC GC )
Location: Dallas, TX ( Hybrid )
Client: Southwest Airlines
Looking for strong experience with Abinitio and AWS.
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Work Location: On the road",-1,Kommforcesolutions,"Dallas, TX",-1,-1,-1,-1,-1,-1
104,Data Engineer- Hadoop,Employer Provided Salary:$63.00 - $66.00 Per Hour,"Position Summary:
Synthetic data engineer is the experienced professional who can bring together skills and technologies to be able design and develop solutions to deliver data required for training models/test applications. Candidate will be working with a team of resources using Python, Sqoop, pyspark to create the data in a most efficient manner. Candidate should have prior experience & knowledge in highly scalable production grade applications with a strong engineering acumen. Should be able to work in a collaborative, fast paced environment. Candidate must have knowledge and/or experience working with Hadoop ecosystem.
Skills:
• Advanced knowledge in Python Programming
• Familiarity with Faker module
• pyspark, Sqoop, Hadoop, Hive, Shell script, Hadoop Shell commands
• Good experience dealing with bulk data extracts and bulk data loads
• Ability to translate requirements from stake holders into the technical requirements and the code
• Experience on statistical distribution, agent to model , deep learning is a plus
• Background of MF environment is a plus
Job Type: Contract
Pay: $63.00 - $66.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
Hadoop: 7 years (Required)
Python: 5 years (Required)
ETL: 3 years (Preferred)
Work Location: Hybrid remote in Charlotte, NC 28203",3.9,"Data Inc
3.9","Charlotte, NC",501 to 1000 Employees,1983,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
105,Data Science Engineer,$90K - $131K (Glassdoor est.),"We are looking for strong experience in Python, AWS, Machine Learning/Data Science, CI/CD integration and the ability work with cross functional team. The work will also involve building and incorporate automated unit & integration tests into the Data science platform",4.0,"FlexIT Inc
4.0","Beaverton, OR",1 to 50 Employees,-1,Company - Private,-1,-1,$5 to $25 million (USD)
106,Data Engineer,$66K - $92K (Glassdoor est.),"(This is a remote position open to candidates residing near Austin, TX, Raleigh, NC, or Nashville, TN. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that challenge state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

Double Line understands the importance of creating a safe and comfortable work environment and encourages individualism and authenticity in every member of our team. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment.

Double Line does not currently offer relocation assistance.

o7APWT65D0",4.1,"Double Line, Inc.
4.1","Nashville, TN",51 to 200 Employees,2009,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
107,Data Engineer,Employer Provided Salary:$80K - $85K,"Simatree is searching for a Data Engineer Consultant to support our client’s operational requirements on a mission focused program within a challenging, dynamic setting. Successful candidates in this position will collaborate with a diverse group of individuals across the homeland security community and directly contribute to management and analysis of operational data using data science tools. This role supports a US Government contract. An US Citizenship is required.
The Role
The Data Engineer will work alongside a team to generate management tools that effectively control programs and generate reports while supporting the programmatic development of operational applications of the tools or analysis that are focused on operations research and document findings. They will conduct research into management, organizational, and business practices to implement improvements to existing processes. They will utilize commercial or open-source software to manage tasks and present findings to a broad audience, including agency executives. They will maintain a database to produce monthly financial reports, tracks processes, distribute deliverables, and monitor comment status. The Data Engineer will be required to use Microsoft Office to perform queries, analyses, and produce reports. Other responsibilities include:
Use R and Python to perform analyses and develop new tools using popular data visualization libraries such as ggplot2, plotly, matplotlib, or bokeh.
Use Databricks to Extract, Transform, and Load (ETL) data from different sources and transform it into a usable, reliable resource.
Manage data collection and transformation from federal law enforcement and other databases to be used in a wide spectrum of analyses including statistical modeling, performance metrics analysis, crosschecking data with other federal agencies, and risk analysis.
Use ArcGIS to perform analyses and develop new web map applications that summarize results from deep learning/AI algorithms.
Maintain and enhance business intelligence (BI) dashboards using best practices in UI/UX design
Develop and test new methodologies and algorithms which would increase the efficiency and accuracy of existing models, including pilot deployments of automated scripts
Requirements
This role supports a US Government contract. A US citizenship is required.
Strong Experience with SQL programming
Experience with R statistical programming and Python programming
Experience with BI software such as Qlik, Tableau or PowerBI
Demonstrated experience in performing data analysis and modeling on an independent basis
Demonstrated experience in developing and applying quantitative methods to find patterns and relationships in large data sets using statistical and graphical packages
Ability to translate complex, technical, or analytic findings into an easily understood narrative—tell a story with graphical, verbal or written form
3 years experience in supporting quantitative and analytical efforts
Bachelors degree in relevant fields such as statistics, mathematics, computer science, physical science, economics or engineering
Job Type: Full-time
Pay: $80,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Experience level:
1 year
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This question MUST be answered for consideration: This position supports a US Government contract which requires a US Citizenship. Are you a US Citizen?
Work Location: Hybrid remote in Washington, DC 20001",5.0,"Simatree
5.0","Washington, DC",1 to 50 Employees,2018,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
108,Data Communications Engineer - III,Employer Provided Salary:$91K - $181K,"Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person",4.6,"GTA (Global Technology Associates)
4.6","San Francisco, CA",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
109,Sr. Data Engineer,$83K - $115K (Glassdoor est.),"Provides application design, analysis, programming, testing and documentation for the development, integration, enhancement, and maintenance of information systems solutions
Job Description:
BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Job Responsibilities
Formulates and defines system scope and objectives through research and fact-finding to design, develop, modify, or integrate complex information systems.
Devises or modifies application systems and procedures to optimize functional requirements including capacity, operating time, response time, and form of desired results.
Designs, codes, tests, debugs, and documents programs, subroutines, and scripts.
Develops and implements program/system test plans. Devises data verification methods and standard system procedures.
Responds to system failures and performance events by taking appropriate measures to reduce system downtime and eliminate recurrence of problems.
May conduct research on software and hardware products to justify recommendations and support management in budgeting and purchasing efforts.
Acts as expert technical resource to development staff in all phases of the development and implementation process.
Job Qualifications
Education
Bachelors Degree in Computer Science or equivalent work experience required. Equivalent years of experience are determined as one year of technical experience for every year of college requested.
Experience
1 year - Experience with information technology concepts, application development methodology, terminology, and standards required
5 years - Systems and programming experience required
1 year - Expert working knowledge of at least one programming language in use at BCBST.
Skills\Certifications
Proven skill with information technology in use at BCBST is required
Proven analysis, design, and coding skills, and demonstrated success in leading large and complex projects
Top level technical experience in one or more specialized areas of applications systems analysis and programming
Demonstrated ability to interpret and translate technical and/or or complex concepts into information meaningful to project team members and/or business personnel.
Be organized, reliable, and able to manage multiple tasks with exceptional work ethic.
Leadership skills
Must be able to communicate effectively with both technical and non-technical co-workers
Job Specific Requirements:
Preference for candidates with Cloud experience (GCP specifically)
Python knowledge/experience a plus
Preferred Skills:
Number of Openings Available:
1
Worker Type:
Employee
Worker Sub-Type:
Employee
Company:
BCBST BlueCross BlueShield of Tennessee, Inc.
Applying for this job indicates your acknowledgement and understanding of the following statements:
BCBST is an Equal Opportunity employer (EEO), and all employees and applicants will be entitled to equal employment opportunities when employment decisions are made. BCBST will take affirmative action to recruit, hire, train and promote individuals in all job classifications without regard to race, religion, color, age, sex, national origin, citizenship, pregnancy, veteran status, sexual orientation, physical or mental disability, gender identity, or any other characteristic protected by applicable law.
Further information regarding BCBST's EEO Policies/Notices may be found by reviewing the following page:
BCBST's EEO Policies/Notices
BlueCross BlueShield of Tennessee is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at BlueCross BlueShield of Tennessee via-email, the Internet or any other method without a valid, written Direct Placement Agreement in place for this position from BlueCross BlueShield of Tennessee HR/Talent Acquisition will not be considered. No fee will be paid in the event the applicant is hired by BlueCross BlueShield of Tennessee as a result of the referral or through other means.
All applicants will be advised that BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Tobacco-Free Hiring Statement
To further our mission of peace of mind through better health, effective 2017, BlueCross BlueShield of Tennessee and its subsidiaries no longer hire individuals who use tobacco or nicotine products (including but not limited to cigarettes, cigars, pipe tobacco, snuff, chewing tobacco, gum, patch, lozenges and electronic or smokeless cigarettes) in any form in Tennessee and where state law permits. A tobacco or nicotine free hiring practice is part of an effort to combat serious diseases, as well as to promote health and wellness for our employees and our community. All offers of employment will be contingent upon passing a background check which includes an illegal drug and tobacco/nicotine test. An individual whose post offer screening result is positive for illegal drugs or tobacco/nicotine and/or whose background check is verified to be unsatisfactory, will be disqualified from employment, the job offer will be withdrawn, and they may be disqualified from applying for employment for six (6) months from the date of the post offer screening results.
Resources to help individuals discontinue the use of tobacco/nicotine products include smokefree.gov or 1-800-QUIT-NOW.",3.8,"BlueCross BlueShield of Tennessee
3.8","Chattanooga, TN",5001 to 10000 Employees,1945,Nonprofit Organization,Insurance Carriers,Insurance,$5 to $10 billion (USD)
110,Data Engineer,$81K - $114K (Glassdoor est.),"Headline: If you love to write code and have a genuine interest in developing customer-facing BI solutions, were looking for a passionate engineer who is driven to derive value from Enterprise systems by architecting systems that range from back-end data processing and machine learning, to front end presentation and dashboarding.

Position Summary: Data Engineer is responsible for designing, implementing and maintaining the enterprise data warehouse, data pipelines and enterprise reporting platform. The individual will support all members of the IT and Business teams on data initiatives and ensure optimal and consistent data delivery processes across all projects. The individual is also responsible for collecting, organizing and analyzing data from multiple platforms to inform the organization and provide actionable business insights.

Responsibilities:
Design, implement, and maintain robust data pipelines to efficiently ELT and ETL data from various sources
Integrate data from different internal and external sources, including but not limited to point of sale (POS) systems, enterprise resource planning (ERP) platforms, customer relationship management (CRM) platforms, inventory management systems, and others.
Design and develop data models, data sets and database structures that support efficient storage, retrieval, and analysis of all data.
Ensure data integrity, accuracy, and consistency throughout the environment. Implement quality checks, validation processes, and governance policies to maintain high-quality data.
Identify and resolve performance bottlenecks in the entire data chain by optimizing storage, indexes, and query execution to improve performance of data pipelines and analysis activities.
Design and maintain data warehousing solutions that enable efficient storage, organization, and retrieval of structured and unstructured data. This process must consider all technologies such as Data Lakes, Data Lakehouses, Data Marts and Data Warehouses.
Apply data transformations, data cleansing, and data preprocessing techniques to prepare raw data for analysis and reporting.
Implement and maintain data security measures to protect sensitive customer and business data. In conjunction with the Security team, ensure compliance with all data privacy regulations, such as CCPA, HIPAA, and Various State Data Privacy Laws.
Collaborate with analysts, and business stakeholders to understand data requirements and provide the necessary infrastructure and tools.
Monitor data pipelines, database systems, and data processing workflows to proactively identify issues and performance bottlenecks. Troubleshoot and resolve data-related problems to ensure the continuous availability and reliability of data systems.
Document data engineering processes, data flows, and system configurations. Share knowledge and best practices with team members and stakeholders to foster a culture of data-driven decision-making and data literacy within EG America.
Perform deep-dive analysis including the application of advanced analytical techniques to solve some of the more critical and complex business problems such as customer segmentation and targeting, and supply chain optimization.
Create new methods to visualize business metrics through dashboards, and analysis in Power BI to surface trends.
Develop standards for best practices for reporting, marketing, and educating the company on available reporting tools.
Design, Implement and Maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and big data technologies into a Data Warehouse based on internal process improvements, automation and optimization of data delivery.
Assemble large, complex data sets that meet functional / non-functional business requirements and design custom ETL and ELT processes in support of these processes

Excellent communication skills, particularly translating between technical and non-technical stakeholders
Advanced knowledge with SQL, DAX, M and Python.
Deep understanding of data integration and transformation patterns such as messaging, ETL
Machine learning knowledge/experience.
Experience working with both SQL and NoSQL databases and their query authoring (SQL) languages
Experience with Analysis Services, MS SQL Server Reporting Services (SSRS), Power BI, and other visualization tools.
Strong visual design skills used to create reports and dashboards
Background in Data Science with an understanding of the principles of machine learning.
Strong project management and organizational skills.
Bachelors in Computer Science, Engineering, Mathematics or a related technical discipline. (Advanced quantitative technical degree (MS or PhD) preferred).
At least 5 years of professional experience in data analysis including SQL, database modeling and design.",2.9,"EG America LLC
2.9","Westborough, MA",1001 to 5000 Employees,2001,Self-employed,Convenience Stores,Retail & Wholesale,$500 million to $1 billion (USD)
111,Data Engineer,$73K - $101K (Glassdoor est.),"POSITION OVERVIEW
NBME® is looking for a Data Engineer to join a highly skilled and growing team of data engineers and BI developers. In this role you will be using your data engineering skills and problem-solving skills to deliver meaningful insights to Internal and external customers. The Data Engineer will be responsible for modernizing, expanding, and optimizing NMBE data warehouse by building data integrations, supporting reporting and data visualization for internal and external customers, as well as optimizing new/existing data flows.
Diversity, Equity, and Inclusion Statement
At NBME, we continue to innovate and improve how we fulfill the evolving needs of the health care community. This commitment starts and ends with the people at NBME. By recruiting and empowering talented individuals from various disciplines and backgrounds, which includes professionals with diverse life experiences, abilities, and perspectives, NBME can take a well-informed, robust approach to advancing medical education and assessment for years to come. We also continue to focus on ensuring that our DEI work is impactful and ingrained in everything we do, including with our staff, workplace culture, products and services, the Philadelphia community and the broader medical education.
RESPONSIBILITIES
Participates in requirements gathering, analyzes and architects project and/or system data architectures. In collaboration with subject matter experts, data analysts, solutions and enterprise architects, ensures designs are consistent and coherent to the enterprise architecture standards and existing environment.
Be a critical part of a scrum team in an agile environment, ensuring the team successfully meets its deliverables each sprint.
Trains and mentors team members
Work independently and effectively to manage one's time across multiple priorities and projects.
Assures the integrity and accuracy of the corporate data, with particular attention to data security.
Responsible to ensure high data quality for Data Services, Analytics and Master Data Management.
Perform Logical and Physical data modeling with an agile mindset
Build automated, scalable, test-driven data pipelines.
Utilize software development practices such as version control via Git, CI/CD, and release management.
Build data products using various visualization, BI tools and data science tools.
Collaborate with Data Engineers, DevOps engineers and architects on improvement opportunities for DataOps tools and frameworks.
Be a trusted partner by providing agile capabilities and outstanding services through digital transformation
QUALIFICATIONS
Skills and Abilities
Expert in data modeling, database management and data profiling techniques, particularly in Data Warehouse, Master Data Management (MDM), and in the design and delivery of Business Intelligence solutions.
Deep understanding of industry standards and best practices
Strong analytic skills related to working with structured and unstructured datasets.
Critical thinker and creative problem-solving skills along with the ability to communicate well with stakeholders throughout the organization.
Highly proficient in PL/SQL
Experience with relational SQL and NoSQL databases, including Oracle, MS SQL Server and HBase.
Experience with data integration tools: Informatica, Boomi, Fivetran, MS Integration Services, Sqoop, etc.
Experience with one of the cloud vendors and services: AWS, Google, Microsoft, Snowflake
Experience consuming and building APIs
Experience with object-oriented/object function programming languages: Python, Java, C++, Scala, etc.
Experience with visual analytics tools: QlikView, Tableau, Power BI, Micro strategy etc.
Experience utilizing Agile methodology for development.
Experience with Singer and DBT is a plus
Experience
Minimum of 3 years of Data Engineering and Data Warehousing experience.
Minimum of 2 years of experience with python
Minimum of 2 years of experience with data integration tools like Informatica, Boomi, AWS - Glue etc.
Minimum 1 year of experience with AWS services like S3 and Redshift
Education
Bachelor's Degree in Computer Science or related technical field and 6 years of relevant work experience. Master's degree is a plus.

About NBME:
NBME offers a versatile selection of high-quality assessments and educational services for students, professionals, educators, regulators and institutions dedicated to the evolving needs of medical education and health care. To ensure our assessments meet the highest standards of quality, stay relevant and align to the current curriculum in medical schools and training programs, we rely on a wide network of collaborators. These include the volunteers who help develop our exam questions, the committees and panels who represent various groups within the medical education community, external researchers and health profession organizations.
We are committed to meeting the needs of educators and learners globally with assessment products and expert services such as NBME Subject Examinations, Customized Assessment Services, Self-Assessments, the International Foundations of Medicine Program and Item Writing Workshops. Together with the Federation of State Medical Boards, NBME develops and manages the United States Medical Licensing Examination, which measures the ability to apply knowledge and skills that form the basis of safe and effective patient care. Our Competency-based Assessment unit is focused on new methods as well as the optimization of assessment in the workplace and education.
As a result of leadership in ongoing research, innovative measurement practices and the exploration of forward-thinking assessment modalities and improvements, NBME advances assessment science. Our grant and funding opportunities further support this dedication to medical education and assessment science. We help develop the next generation of assessment professionals through our Summer Psychometric Internship Program. Through the Stemmler Fund, Strategic Educators Enhancement Fund and Latin America Grants Program, researchers and educators can continue to improve the assessment of health care professionals around the world.
NBME views diversity, equity and inclusion (DEI) as foundational and enduring to our strategy and vision. We continue to focus on ensuring that our DEI work is impactful and ingrained in everything we do, including with our staff, culture, products and services, the Philadelphia community and the broader medical education landscape. Our commitment manifests in our hiring and staff development, recruitment for committees, grants programs, design and review of our assessments, and involvement in our local and national communities.
Learn more about NBME at NBME.org.

The NBME offers competitive salaries, excellent benefits, and a rewarding work environment. Excellent Benefits include: Healthcare, Dental, Prescription, and Vision plans; 401(k) w/match; Retirement Income Plan, Tuition Reimbursement Plan, Commuter Benefit: Public Transit or Parking options. Remote Friendly Workplace.

NBME is an EEO employer as defined by the EEOC.",4.0,"NBME
4.0","Philadelphia, PA",201 to 500 Employees,1915,Nonprofit Organization,Education & Training Services,Education,$100 to $500 million (USD)
112,Data Engineer,Employer Provided Salary:$120K - $200K,"Verition Fund Management LLC (""Verition"") is a multi-strategy, multi-manager hedge fund founded in 2008. Verition focuses on global investment strategies including Global Credit, Global Convertible, Volatility & Capital Structure Arbitrage, Event-Driven Investing, Equity Long/Short & Capital Markets Trading, and Global Quantitative Trading. As a Data Engineer you would be responsible for building data pipelines and supporting Portfolio Managers and Risk teams.
Responsibilities:
Building data pipelines
Working closely with data vendors such as Bloomberg, Refinitiv, etc…
Taking this vendor data and normalizing/standardizing it or firm consumption
Taking the normalized data and customizing it to user specific needs.
Qualifications:
4+ years of experience in financial services
BS or MS in Computer Science or Computer Engineering
Strong technology/coding skills (Python)
Strong design skills to build extensible config/data-driven platforms
Good Financial data, specifically security master and/or knowledge of vendor datasets
Strong AWS data pipeline skills
Strong database skills – SQL and no-SQL
Strong problem solving skills
Ability work with datasets in Excel and other productivity tools
Nice to Have:
AWS Glue
Spark
Jupyter Notebook

Salary Range
$120,000—$200,000 USD",3.5,"Verition Group LLC
3.5","New York, NY",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
113,Data Engineer,$99K - $136K (Glassdoor est.),"The company

Decentralized data is the future. Data mesh is the right idea. We’re here to make it a reality. Nextdata OS is a data-mesh-native platform built to meet the challenge of decentralizing data at scale. We are inventing a new way for developers to work with data and share it responsibly via data product containers.

Our vision is to build a world where AI/ML and analytics are powered by decentralized, responsible, and equitable data ownership, across boundaries of organizations, technology, and most importantly boundaries of trust.

Our purpose is to change the experience of creating, sharing, discovering, and using data forever, to be connected, fast, and fair based on data mesh principles.

Our technology is designed to empower data developers, users and owners with a delightful experience where data products are a first-class primitive, with trust built-in.

We are here to accept the reality that the world of data is complex and messy; data models are out-of-date the moment they are created; data is owned across trust boundaries; data is stored on different platforms; data is used in many different modes and most importantly data can't protect itself. We are here to recognize that past approaches to tackle these complexities with centralized data collection, modeling and governance are ineffective at best and pathologically unfair at worst. We are here to reimagine, with you!

The role

You will be one of the first data product developers building data products on top of the Nextdata OS. You will work with Nextdata’s initial customers to translate their business needs into PoC data products that you will then build on top of our OS.
You will leverage your experience of the full lifecycle of analytics - translating business requirements to data models, to data pipelines, to analytics dashboards and even machine learning pipelines - to build data products and then provide feedback that will help us improve the Nextdata OS and tooling.

You will manage the full lifecycle of the PoC data products from ideation around business use case to successful deployment, including synthesizing realistic data for testing before deploying into production.

Our expectations

You have worked on complex data pipelines in large data organizations. You have dealt with the uncertainty of iterating from at times an under defined business use case to a concrete end-to-end analytics pipeline that drives value to the business. You may have had to help the business quantify the value of your analyses as well!
You are proficient in the modern data stack tools like airflow, dagster, prefect, dbt and ML development environments like PySpark and Pandas.
You are articulate and are able to present your findings in a compelling manner.
You have relied on continuous integration and continuous deployment to reduce software lead time, and have contributed to optimizing build and release processes as needed.
You have shown evidence that you aspire to test-first data pipeline development!

Our benefits

We are an early stage company, but we don't subsist on ramen! We are an experienced team with families. We provide $6000 for your home workspace setup, premium health, vision, dental insurance coverage for you and your family. And of course, early stage equity and market rate salary.",-1,Nextdata Technologies Inc,"San Francisco, CA",1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
114,Big Data Engineer,$87K - $120K (Glassdoor est.),"Supremus is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.

This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Supremus will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.

Duties & Responsibilities:

Supremus is seeking Big Data Engineers with technical experience in optimizing of management and deriving insights from large, non-structured, non-relational data.

In this role you will:

Apply semantic correlation, ontology structured data, and text analytics techniques and systems to analyze non-structured data and identify critical insights.
Apply Big Data technologies, such as Hadoop or Cassandra, with NoSQL data management and related programming languages, such as Jaql, HBase, Pig, or Hive.
Participate in all aspects of the software life cycle, including analysis, design, development, unit testing, production deployment and support.
Formulate approaches and gather data to solve business problems, develop conclusions and present solutions through formal deliverables.
Create Big Data accelerators to help deploy scalable solutions fast.
You will be successful in this role if you enjoy problem solving and utilizing consulting skills. Team leadership experience is preferred.

In this dynamic role, you will have the opportunity to interact directly with clients. As such, travel to client sites may be required, up to 4 days per week.

Qualified candidates are encouraged to please send resumes to careers@supremusglobal.com",-1,Supremus,"Philadelphia, PA",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
115,Data Engineer,$83K - $113K (Glassdoor est.),"ID: 6173 | 5-10 yrs | North Wales | careers
Job Description:
The Data Engineer is a critical role responsible for designing, developing, and maintaining data pipelines and infrastructure that enable efficient data processing, storage, and retrieval. This role involves working with large volumes of data from various sources, ensuring data quality, and creating scalable solutions to support data-driven decision-making within the organization. The Data Engineer collaborates with cross-functional teams to understand data needs and implement solutions that enable data analysis and reporting.
Responsibilities:
Data Pipeline Development:
Design and implement data pipelines to extract, transform, and load (ETL) data from various sources into data warehouses or data lakes.
Develop efficient and scalable ETL processes to ensure data quality, consistency, and accuracy.
Data Modeling and Architecture:
Design and implement data models that support the organization's analytical and reporting requirements.
Work with data architects to ensure proper data structure and architecture for optimal performance.
Data Integration:
Integrate data from different sources, including databases, APIs, and external systems.
Collaborate with application developers to embed data integration processes into applications.
Data Storage and Management:
Implement and manage data storage solutions, such as relational databases, NoSQL databases, and data lakes.
Optimize data storage for performance, scalability, and cost-effectiveness.
Data Transformation:
Develop data transformation processes to cleanse, enrich, and transform raw data into usable formats.
Utilize scripting and programming languages (e.g., Python, Java, Scala) to perform data transformations.
Data Quality and Governance:
Implement data quality checks and validation to ensure the accuracy and integrity of data.
Contribute to data governance initiatives by adhering to data security and privacy standards.
Performance Optimization:
Monitor and optimize data processing and query performance to ensure fast and efficient data retrieval.
Identify and address bottlenecks in data pipelines to improve overall system performance.
Collaboration and Communication:
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.
Communicate technical concepts and solutions to non-technical stakeholders effectively.
Version Control and Documentation:
Maintain version control for data pipelines and codebase.
Document data engineering processes, workflows, and data lineage.
Qualifications:
Bachelor's or Master's degree in Computer Science, Software Engineering, Information Technology, or a related field.
Strong proficiency in programming languages such as Python, Java, Scala, or similar.
Experience with data processing frameworks like Apache Spark, Apache Kafka, or similar.
Proficiency in SQL for querying and manipulation of data.
Knowledge of data modeling concepts and techniques.
Experience with ETL tools and data integration techniques.
Familiarity with data storage technologies such as relational databases (e.g., PostgreSQL, MySQL), NoSQL databases (e.g., MongoDB, Cassandra), and data lakes (e.g., AWS S3, Hadoop HDFS).
Understanding of cloud platforms (e.g., AWS, Azure, GCP) and related data services.
Strong problem-solving skills and attention to detail.
Ability to work in a collaborative team environment.
Excellent communication skills to work with technical and non-technical stakeholders.
The Data Engineer role involves designing, developing, and maintaining the data infrastructure necessary for effective data processing, integration, and storage, playing a key role in enabling data-driven decision-making within the organization.",3.7,"Jade Global
3.7","North Wales, PA",501 to 1000 Employees,2003,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
116,Data Science Engineer,-1,"Operations Technology brings data accessibility to the organization, with the overarching goal that all business problems have data driven answers.
Within the Operations Technology group, the Data Science Engineer’s purpose is to design, develop, and maintain data infrastructure, pipelines, and workflows. They are responsible for developing and merging modeling to ensure it stays consistent with data flowing across the organization.
They work closely with platform data owners, data analysts and consumers, and other stakeholders to ensure the data is properly collected, stored, processed, and analyzed to drive informed business decisions.
This role will become more ML focused as the data warehouse matures, with the added implementation of ML and advanced analytics models
SKILLS AND ABILITIES:
Proficiency in Azure Data Warehousing utilizing Azure Synapse (Data Factory also acceptable)
Strong understanding of database technologies and SQL queries
Strong experience with ETL processes, data integration, and data modeling
Experience with RESTful API connections in the Azure environment
Experience with cloud-based data storage and computing services
Experience with Azure networking
Excellent problem-solving and analytical skills
Experience with data visualization tools such as Tableau or Power BI
Excellent communication and collaboration skills
Ability to work independently and prioritize tasks in a fast-paced & dynamic environment
What sets you apart:
Certified in SQL, SQL Analytics, R, Python, Power BI
Experience with ML in Azure: Spark MLib, SynapseML, R, Python Anaconda, or the like
Experience with data queries to cloud ERP (Netsuite), CRM (Hubspot), Cloud Storage (Sharepoint, MS Project Online), On Prem Storage (SQL)
TREW’s story:
Business gets done working together. Successful business happens when trusted partners work together, to win together. At TREW we know that our customers buy solutions and technology built by people. With over 400 team members, we work fearlessly every day to do the right thing, even when no one is watching. From seasoned professionals to undergraduate co-ops, our team members enjoy seeing the impact of their contributions every day.
Trew/Hilmot/TKO is an equal opportunity employer. Applicants will be considered for employment without regard to age, race, creed, color, national origin, ancestry, marital status, affectional or sexual orientation, gender identity or expression, disability, nationality, sex, or veteran status.",3.5,"TREW LLC
3.5",Remote,51 to 200 Employees,-1,Company - Private,Taxi & Car Services,Transportation & Logistics,$25 to $100 million (USD)
117,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1,Radiant System,Remote,-1,-1,-1,-1,-1,-1
118,Data engineer,$97K - $136K (Glassdoor est.),"Qualification:
Master's Degree in Statistical Analytics, Data Science, or Bachelor's Degree in computer science engineering will be considered with at least three - five years of applicable work experience
Preferred Proficiency:

Web development experience (AngularJS, D3).
Experience in a statistical programming language like R or Python; applied machine learning techniques including dimensionality reduction strategies, supervised/unsupervised classification and natural language processing frameworks.
Experience in at least one data visualization tools (e.g. Tableau, QlikView) and data warehousing tools (e.g. Informatica) is preferred
Huge Advantage:
Building and scaling Machine Learning frameworks
Hadoop (Hive, Spark, UDF's)
Definite Plus:

Web development experience (AngularJS, D3).
Experience:

5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.
5+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets.
2+ years of experience in scripting languages like Python etc.",4.7,"Stacklogy
4.7","Fremont, CA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
119,Data Engineer,Employer Provided Salary:$130K - $175K,"Rightworks is hiring a Data Engineer in Fort Worth for a Multi-Billion dollar private equity fund in Fort Worth. This position is fully in-office in Fort Worth (client will pay relocation if needed). Compensation is aggressive & flexible, typical 10% above your previous compensation level with tremendous growth opportunities. (Expected compensation range for this data engineer will fall somewhere between 130 & 180,000 per year depending on experience). 50 hours per week, paid lunch, casual office atmosphere.
-Looking for data engineer with multiple years of SQL Server maintenance experience.
-Optimally would like a data engineer with formal education in either MIS, Mathematics, Software Engineering, or Computer Science
-SQL maintenance, Data Extraction, Transforms, Macros
-Database maintenance
Job Type: Full-time
Pay: $130,000.00 - $175,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Tuition reimbursement
Vision insurance
Schedule:
10 hour shift
Ability to commute/relocate:
Fort Worth, TX 76102: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What compensation range are you looking for?
Are you willing to work in-person in Fort Worth, TX 50 hrs a week?
Experience:
Microsoft SQL Server: 1 year (Required)
Work Location: In person",4.8,"RightWorks Inc.
4.8","Fort Worth, TX",51 to 200 Employees,2006,Company - Private,HR Consulting,Human Resources & Staffing,$25 to $100 million (USD)
120,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
121,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
122,Data Engineer,$74K - $107K (Glassdoor est.),"Analyze Business Requirement Documents and Implement Technical Solutions for privacy related applications.
Develop ETL process for supporting Data Extraction, transformations and loading.
Perform data conversions and aggregations using different transformations such as Merge, Merge join, Union condition split, sort, order by. Derived columns convert and cast transformations and row count and lookup and fuzzy lookup transformations.
Develop UNIX scripts to load the data from Source server to Teradata and validate the files between different servers.
Develop new process to implement state level privacy regulations based on each state law in Big Data Platform.
Create Temperory/Fact tables, loading with data and writing Teradata and Spark SQL queries.
Optimize/tune ETL objects, indexing and partitioning for better performance and efficiency.
Validate the performance metrics and work on performance tuning for SQL, HQL and Spark SQL queries.
Perform testing and Provide test support for various level of testing phases like Unit, User Acceptance, Regression, Parallel and System testing.
Promote the components to production environment through CI/CD process by using Git hub .
Script task and execute SQL tasks to execute SQL code. Work on containers for loop and for each loop container to run a group of tasks into a single container and repeating tasks.
Create the data flow to extract data from sources to OLEDB Source, Excel, XML, flat files sources and destination is SQL data warehouse.
Minimum Education Required:- All the responsibilities mentioned above are in line with the professional background and requires an absolute minimum of a Bachelor’s degree in computer science, computer information systems, technology management, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.",4.9,"Wise Equation Solutions
4.9","La Vista, NE",51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
123,Data Engineer,$67K - $96K (Glassdoor est.),"Massanutten Resort
Job Summary
The Data Engineer will play a crucial role in designing, developing, and maintaining the data infrastructure and systems that enable efficient and reliable data processing, storage, and analysis. The primary responsibility is to transform raw data into usable formats for various business applications and support data-informed decision-making processes. This role includes working in various Microsoft SQL Environments as well as supporting a Snowflake Data Warehouse. The Data Engineer is expected to have a broad understanding of data concepts and be avid about continuing education to keep up with the ever-changing data landscape.
Required
Degree in computer science or related file and 3+ years of experience with Python, SQL, and data visualization/exploration tools (in lieu of a degree, at least 5 years of experience in the above is equivalent)
Preferred
A degree in computer science or a related field
Certifications in database management and/or related technical certifications
Familiarity with advanced data concepts (i.e., AI and Machine Learning) and business intelligence/visualization tools
Typical Schedule
Days:
Monday-Friday, weekends and holidays as needed
Position will be required to be part of rotating on-call schedule
Hours:
8am-5pm, additional hours as needed
Core Responsibilities
Develop Data Pipeline: Design and implement scalable and efficient data pipelines, integrating diverse data sources and ensuring smooth data flow from ingestion to storage and processing
Transform and Model Data: Develop and maintain data transformation processes, such as ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform), to convert raw data into structured formats suitable for analysis and reporting. Create and manage data models to facilitate data integration and provide a consistent view of information across systems.
Implement and oversee Data Warehousing and Storage: Build and maintain data warehouses or data lakes, optimizing storage and retrieval processes to ensure high availability, reliability, and performance. Implement appropriate data governance and security measures to protect sensitive data.
Ensure Data Quality and Governance: Collaborate with data analysts and business stakeholders to understand data requirements and define data quality standards. Implement data validation, data profiling, and data cleansing techniques to ensure data accuracy, consistency, and integrity.
Optimize Performance: Identify and address performance bottlenecks in data processing and storage systems, leveraging optimization techniques and technologies to improve overall data engineering efficiency and reduce latency
Assist with Data Problem Solving: Support team with data related escalation issues, including help desk support tickets that are generated from our business users
Manage Data Integration and API Development: Establish connections and integrations between internal and external data sources, including APIs, databases, and third-party platforms. Develop and maintain data APIs to enable seamless data access and consumption by other applications and services.
Collaborate and Document: Work closely with cross-functional teams, including analysts, software engineers, and business stakeholders, to understand data requirements and ensure data solutions align with business objectives. Document data engineering processes, including data pipelines, transformations, and system architectures, to facilitate knowledge sharing and maintain a robust knowledge base.
Stay up-to-date with the latest advancements in data engineering, data management, and big data technologies. Evaluate and recommend new tools, frameworks, and methodologies to improve data engineering practices and enhance overall data infrastructure
Other duties as assigned by management
For more information, contact Ana at 540-289-4939.",3.6,"Great Eastern Resort Management, Inc.
3.6","McGaheysville, VA",1001 to 5000 Employees,-1,Self-employed,-1,-1,Unknown / Non-Applicable
124,Sr. Data Engineer,$83K - $115K (Glassdoor est.),"Provides application design, analysis, programming, testing and documentation for the development, integration, enhancement, and maintenance of information systems solutions
Job Description:
BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Job Responsibilities
Formulates and defines system scope and objectives through research and fact-finding to design, develop, modify, or integrate complex information systems.
Devises or modifies application systems and procedures to optimize functional requirements including capacity, operating time, response time, and form of desired results.
Designs, codes, tests, debugs, and documents programs, subroutines, and scripts.
Develops and implements program/system test plans. Devises data verification methods and standard system procedures.
Responds to system failures and performance events by taking appropriate measures to reduce system downtime and eliminate recurrence of problems.
May conduct research on software and hardware products to justify recommendations and support management in budgeting and purchasing efforts.
Acts as expert technical resource to development staff in all phases of the development and implementation process.
Job Qualifications
Education
Bachelors Degree in Computer Science or equivalent work experience required. Equivalent years of experience are determined as one year of technical experience for every year of college requested.
Experience
1 year - Experience with information technology concepts, application development methodology, terminology, and standards required
5 years - Systems and programming experience required
1 year - Expert working knowledge of at least one programming language in use at BCBST.
Skills\Certifications
Proven skill with information technology in use at BCBST is required
Proven analysis, design, and coding skills, and demonstrated success in leading large and complex projects
Top level technical experience in one or more specialized areas of applications systems analysis and programming
Demonstrated ability to interpret and translate technical and/or or complex concepts into information meaningful to project team members and/or business personnel.
Be organized, reliable, and able to manage multiple tasks with exceptional work ethic.
Leadership skills
Must be able to communicate effectively with both technical and non-technical co-workers
Job Specific Requirements:
Preference for candidates with Cloud experience (GCP specifically)
Python knowledge/experience a plus
Preferred Skills:
Number of Openings Available:
1
Worker Type:
Employee
Worker Sub-Type:
Employee
Company:
BCBST BlueCross BlueShield of Tennessee, Inc.
Applying for this job indicates your acknowledgement and understanding of the following statements:
BCBST is an Equal Opportunity employer (EEO), and all employees and applicants will be entitled to equal employment opportunities when employment decisions are made. BCBST will take affirmative action to recruit, hire, train and promote individuals in all job classifications without regard to race, religion, color, age, sex, national origin, citizenship, pregnancy, veteran status, sexual orientation, physical or mental disability, gender identity, or any other characteristic protected by applicable law.
Further information regarding BCBST's EEO Policies/Notices may be found by reviewing the following page:
BCBST's EEO Policies/Notices
BlueCross BlueShield of Tennessee is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at BlueCross BlueShield of Tennessee via-email, the Internet or any other method without a valid, written Direct Placement Agreement in place for this position from BlueCross BlueShield of Tennessee HR/Talent Acquisition will not be considered. No fee will be paid in the event the applicant is hired by BlueCross BlueShield of Tennessee as a result of the referral or through other means.
All applicants will be advised that BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Tobacco-Free Hiring Statement
To further our mission of peace of mind through better health, effective 2017, BlueCross BlueShield of Tennessee and its subsidiaries no longer hire individuals who use tobacco or nicotine products (including but not limited to cigarettes, cigars, pipe tobacco, snuff, chewing tobacco, gum, patch, lozenges and electronic or smokeless cigarettes) in any form in Tennessee and where state law permits. A tobacco or nicotine free hiring practice is part of an effort to combat serious diseases, as well as to promote health and wellness for our employees and our community. All offers of employment will be contingent upon passing a background check which includes an illegal drug and tobacco/nicotine test. An individual whose post offer screening result is positive for illegal drugs or tobacco/nicotine and/or whose background check is verified to be unsatisfactory, will be disqualified from employment, the job offer will be withdrawn, and they may be disqualified from applying for employment for six (6) months from the date of the post offer screening results.
Resources to help individuals discontinue the use of tobacco/nicotine products include smokefree.gov or 1-800-QUIT-NOW.",3.8,"BlueCross BlueShield of Tennessee
3.8","Chattanooga, TN",5001 to 10000 Employees,1945,Nonprofit Organization,Insurance Carriers,Insurance,$5 to $10 billion (USD)
125,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1,Radiant System,Remote,-1,-1,-1,-1,-1,-1
126,Data Engineer,Employer Provided Salary:$65.00 - $70.00 Per Hour,"Job title: Data Engineer ( USC GC )
Location: Dallas, TX ( Hybrid )
Client: Southwest Airlines
Looking for strong experience with Abinitio and AWS.
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Work Location: On the road",-1,Kommforcesolutions,"Dallas, TX",-1,-1,-1,-1,-1,-1
127,Data Engineer- Hadoop,Employer Provided Salary:$63.00 - $66.00 Per Hour,"Position Summary:
Synthetic data engineer is the experienced professional who can bring together skills and technologies to be able design and develop solutions to deliver data required for training models/test applications. Candidate will be working with a team of resources using Python, Sqoop, pyspark to create the data in a most efficient manner. Candidate should have prior experience & knowledge in highly scalable production grade applications with a strong engineering acumen. Should be able to work in a collaborative, fast paced environment. Candidate must have knowledge and/or experience working with Hadoop ecosystem.
Skills:
• Advanced knowledge in Python Programming
• Familiarity with Faker module
• pyspark, Sqoop, Hadoop, Hive, Shell script, Hadoop Shell commands
• Good experience dealing with bulk data extracts and bulk data loads
• Ability to translate requirements from stake holders into the technical requirements and the code
• Experience on statistical distribution, agent to model , deep learning is a plus
• Background of MF environment is a plus
Job Type: Contract
Pay: $63.00 - $66.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
Hadoop: 7 years (Required)
Python: 5 years (Required)
ETL: 3 years (Preferred)
Work Location: Hybrid remote in Charlotte, NC 28203",3.9,"Data Inc
3.9","Charlotte, NC",501 to 1000 Employees,1983,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
128,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
129,Data Science Engineer,-1,"Operations Technology brings data accessibility to the organization, with the overarching goal that all business problems have data driven answers.
Within the Operations Technology group, the Data Science Engineer’s purpose is to design, develop, and maintain data infrastructure, pipelines, and workflows. They are responsible for developing and merging modeling to ensure it stays consistent with data flowing across the organization.
They work closely with platform data owners, data analysts and consumers, and other stakeholders to ensure the data is properly collected, stored, processed, and analyzed to drive informed business decisions.
This role will become more ML focused as the data warehouse matures, with the added implementation of ML and advanced analytics models
SKILLS AND ABILITIES:
Proficiency in Azure Data Warehousing utilizing Azure Synapse (Data Factory also acceptable)
Strong understanding of database technologies and SQL queries
Strong experience with ETL processes, data integration, and data modeling
Experience with RESTful API connections in the Azure environment
Experience with cloud-based data storage and computing services
Experience with Azure networking
Excellent problem-solving and analytical skills
Experience with data visualization tools such as Tableau or Power BI
Excellent communication and collaboration skills
Ability to work independently and prioritize tasks in a fast-paced & dynamic environment
What sets you apart:
Certified in SQL, SQL Analytics, R, Python, Power BI
Experience with ML in Azure: Spark MLib, SynapseML, R, Python Anaconda, or the like
Experience with data queries to cloud ERP (Netsuite), CRM (Hubspot), Cloud Storage (Sharepoint, MS Project Online), On Prem Storage (SQL)
TREW’s story:
Business gets done working together. Successful business happens when trusted partners work together, to win together. At TREW we know that our customers buy solutions and technology built by people. With over 400 team members, we work fearlessly every day to do the right thing, even when no one is watching. From seasoned professionals to undergraduate co-ops, our team members enjoy seeing the impact of their contributions every day.
Trew/Hilmot/TKO is an equal opportunity employer. Applicants will be considered for employment without regard to age, race, creed, color, national origin, ancestry, marital status, affectional or sexual orientation, gender identity or expression, disability, nationality, sex, or veteran status.",3.5,"TREW LLC
3.5",Remote,51 to 200 Employees,-1,Company - Private,Taxi & Car Services,Transportation & Logistics,$25 to $100 million (USD)
130,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
131,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
132,Data Engineer,$81K - $114K (Glassdoor est.),"Position Overview:
The Novelis Corporate Digital team is seeking a Data Engineer. The person in this role will support the Data Services Manager. The ideal candidate will be responsible for delivery of the data pipelines feeding into machine learning models. The Data Engineer will aid in the optimization of operations by manipulating and aggregating the disparate operational and historian (L2) data sources into a format that is easily digestible by both data scientists and statistically adept colleagues. Their core responsibility will be to combine large volumes of disparate data, conduct quality checks on the data, manipulate the data and ensure continuous access to a clean format of the operational data for supporting functions and collaborators.

Novelis is the world leader in aluminum rolling and recycling, producing an estimated 19 percent of the world's flat-rolled aluminum products. We work alongside our customers to provide innovative solutions to the aerospace, beverage can, automotive and high-end specialty markets.

Headquartered in Atlanta, Georgia, USA, Novelis has approximately 13,000 employees in 33 operating facilities on four continents and is investing close to $2 billion in global expansions to meet the growing demand for our premium product.
Responsibilities & Qualifications:
Responsibilities:
Design and Develop data ingestion pipelines and processes based on requirements in Python and PySpark
Build error handing, exception management and data quality routines to expose the anomalies in the data
Profile and analyze data to identify gaps and potential data quality issues
Identifies relationships between disparate data sources
Uses Python, Databricks and Spark to code the data Engineering routines
Perform unit and integration testing
Works with the Data Science team and business SMEs to get the requirements and present the details in data.
Designs and jointly develops the data architecture with data architect and ensures security and maintenance
Explores suitable options, designs, and builds data pipeline (data lake / data warehouses) for specific analytical solutions
Identifies gaps and implements solutions for data security, quality and automation of processes
Builds data tools and products for effort automation and easy data accessibility
Supports maintenance, bug fixing and performance analysis along data pipeline
Diagnoses existing architecture and data maturity and identifies gaps
Minimum Qualifications:
Minimum of Bachelor of Science BS/MS degree in Computer Science, Engineering, and/or Background in Mathematics and Statistics
Over 3 years of work with data engineering, IT or related field
Experience in data engineering & data bases/warehouses
Used SQL, PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle etc in production environments
Experience with programming languages like UNIX shell scripting, Python etc.
Preferred Qualifications:
Certifications in Databricks, Pythod, Spark or other related subjects
Experience with Agile/Scrum methodology with the Product based implementation approach
Experience with CI/CD with DevOps frameworks
Experience on Big Data platforms (e.g. Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive)
Experience with cloud platforms (Azure, AWS, Databricks)
Experience with Historians (IBA, PI), Industry 4.0 or IoT environments
What We Offer:
This role offers a hybrid schedule. Novelis is committed to a Flex Work approach that empowers employees to maintain work-life balance while enjoying the benefits of a collaborative office environment.
Novelis benefits say a lot about how we care for each other. Our employees and their families have many different needs. As a result, our benefits offer choices on many levels and are high in quality, competitive in the marketplace, and affordable. These are a few of the benefits we offer to support you and your family’s well-being:
Medical, dental and vision insurance
Health savings accounts – Company Funded Health Savings Account (HSA) and Health Reimbursement Account (HRA)
Company-paid basic life insurance and Additional voluntary life coverage
Paid vacation and competitive personal time off
401(k) savings plan with company match
Retirement savings plans – medical and prescription drug coverage through private exchange
Employee assistance programs – available 24/7 to you and your family
Wellness and Work Life Support - career development and educational assistance
Location Profile:
Novelis’ Global Corporate Headquarters is located in the Buckhead neighborhood of Atlanta Georgia and employs approximately 250 people. It is co-located with Novelis’ North America regional office which employs approximately 225 people. Supporting its 24 operations worldwide Novelis’ corporate office is home to the executive leadership team and global functions that support the automotive beverage can and high-end specialties value streams. The City of Atlanta provides a diverse and family-friendly place to live with countless museums cultural organizations and educational institutions including the Georgia Aquarium Woodruff Arts Center CNN Center Georgia Tech and Mercedes-Benz Stadium. In the Atlanta area Novelis has strong community partnerships with Atlanta Habitat for Humanity GeorgiaFIRST and Agape Youth and Family Center in addition to many local museums and community groups. Novelis recognizes its talented and diverse workforce as a key competitive advantage. Novelis provides equal employment opportunities to all employees and applicants.All terms and conditions of employment at Novelis including recruiting hiring placement promotion termination layoffs recalls transfers leaves of absence compensation and training are without regard to race color religion age sex national origin disability status genetics protected veteran status sexual orientation gender identity or expression or any other characteristic protected by federal provincial or local laws.",3.8,"Novelis Corporate HQ
3.8","Atlanta, GA",10000+ Employees,2005,Subsidiary or Business Segment,Metal & Mineral Manufacturing,Manufacturing,$10+ billion (USD)
133,Sr. Data Engineer,Employer Provided Salary:$84K - $191K,"Job Summary:
The primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver modules, stable application systems, and Data or Platform solutions. This includes developing, configuring, or modifying complex integrated business and/or enterprise infrastructure or application solutions within various computing environments. This role facilitates the implementation and maintenance of complex business and enterprise Data or Platform solutions to ensure successful deployment of released applications.
Minimum Qualifications
Bachelor's Degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)
5 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering
4 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)
Preferred Qualifications
Master's Degree in Computer Science, CIS, or related field
5 years of IT experience developing and implementing business systems within an organization
5 years of experience working with defect or incident tracking software
5 years of experience writing technical documentation in a software development environment
3 years of experience working with an IT Infrastructure Library (ITIL) framework
3 years of experience leading teams, with or without direct reports
5 years of experience working with source code control systems
Experience working with Continuous Integration/Continuous Deployment tools
5 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutions
Job Type: Full-time
Salary: $84,002.64 - $190,979.59 per year
Experience:
Data, BI or Platform Engineering, Data Warehousing/ETL: 3 years (Preferred)
developing and implementing business systems: 3 years (Preferred)
Data Engineer: 4 years (Preferred)
Work Location: In person",-1,Market Tree Research,"Carolina, WV",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
134,Data Engineer,-1,"Description:
Who is Leafwell
Leafwell is a rapidly growing technology and data company that set out to increase access, education, and research into cannabis and to advance its application as medicine.
An exciting opportunity for a Data Engineer to join our growing team has arisen.
What to Expect as a Data Engineer at Leafwell
As a Data Engineer at Leafwell, you will be in charge of creating and orchestrating the Leafwell data pipeline, which means gathering data, creating & automating data transformations and producing actionable insights for Leafwell’s internal stakeholders. You will support critical testing and rollout of new data features. The Data Engineer will build and maintain systems that inform Leafwell’s business stakeholders about Key Performance Indicators (KPIs) and suggest data-driven strategies to optimize those metrics.
Essential Duties and Responsibilities
The Data Engineer will perform the following responsibilities:
Acquire, assemble, transform and analyze data
Create, manage and orchestrate the data pipeline and it’s infrastructure
Present findings, trends, and suggested optimizations
Identify new opportunities and threats to the company's business model
Update and revise reports, queries, and analytic procedures as necessary
Design and implement tracking so that optimization efforts can be measured
Identify inefficiencies in data processes and automate where appropriate
Write and update international SOPs and internal documentation
Support IT systems management with testing, validation, and user support
Proactively identify initiatives for data-related improvements
Why Leafwell
At Leafwell, we are passionate about our work and seek out employees who contribute the same level of dedication and enthusiasm. We are only as good as the people we hire, so we aim to be the best employer in order to attract the top talent in the industry.
Do we have your Interest?
Requirements:
Our Ideal Candidate
Our Ideal Candidate will possess the following:
Bachelor's degree in Computer Science, Mathematics, Economics, Information Systems, or another quantitative field
3-5 years of relevant professional experience in Data Engineering / Analytics
Advance working knowledge and experience in SQL and relational databases
Strong data management abilities, including data analysis, standardization, cleansing, querying, and consolidation of data
Data visualization experience (e.g Tableau, Looker, etc.)
Exercises self-reliance, initiative, and leadership while keeping stakeholders properly informed and involved
Reliably manage numerous duties during a workday, necessitating interactions with people located across the world
Technically competent, with the ability to quickly learn new processes and programs, and utilize various software applications
Excellent communication and interpersonal skills; ability to work with and appeal to a wide variety of personalities and professional tendencies
Bonus points if you have experience in the following:
Cannabis knowledge; industry experience is a plus
Used Data Orchestration tools like Dagster or Airflow
Used dbt or comparable data transformation software
Git
Python or R programming
Amazon Web Services
PostgreSQL and RedShift
CRM products
Data Visualization in Metabase
Effective project management skills and comfort utilizing a project management platform in collaboration with other team members
Data Science projects
Benefits Highlights
Our benefits include, but are not limited to:
Remote first - Most of our employees are 100% remote. Positions requiring travel enjoy a hybrid environment.
Rapid Growth - Our company is rapidly expanding, and our employees are advancing with us!
Training and Development - We foster a culture of learning. We encourage our employees to take part in educational, training, and development opportunities.
Leafwell is dedicated to bringing together people from diverse cultures and perspectives. We work hard to provide a welcoming atmosphere where everyone may flourish, have a sense of belonging, and collaborate effectively. As an equal opportunity employer, we do not tolerate any illegal discrimination against job applicants based on their race, color, religion, veteran status, sex, parental status, gender identity or expression, transgender status, sexual orientation, national origin, age, disability, or genetic information. We are committed to going above and above in creating diversity within our firm and follow the regulations upheld by the EEOC.",4.0,"LEAFWELL
4.0",Remote,51 to 200 Employees,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
135,Data Engineer,Employer Provided Salary:$100K - $145K,"Garner's mission is to transform the healthcare economy, delivering high quality and affordable care for all. By helping employers restructure their healthcare benefit to provide clear incentives and data-driven insights, we direct employees to higher quality and lower cost healthcare providers. The result is that patients get better health outcomes while doctors are rewarded for practicing well, not performing more procedures. We are backed by top-tier venture capital firms, are growing rapidly and looking to expand our team.
We are looking for a Data Engineer to support our technical teams by ensuring ease of access to data within our organization. The ideal candidate for this role will have strong technical skills, including Python, SQL, and AWS as well as a desire to be a hands-on contributor to building out a data platform from the ground up.
Main Responsibilities:
Build the data pipelines that power our business
Collaborate across disciplines to high-quality datasets
Protect our users' privacy and security through best practices
Support data pipelines in production
Our Tools:
Python, AWS, Snowflake, dbt, Terraform, Postgres
The ideal candidate has:
2+ years of experience building data pipelines in a fast-paced environment
Strong Python knowledge
Experience with Big Data technologies such as Snowflake, RedShift, BigQuery, or DataBricks
Ability to think in principles and frameworks to understand and decompose abstract problems
An aptitude to learn new technologies and tools quickly
Why You Should Join Our Team:
You are mission-driven and want to work at a company that can change the healthcare system
You want to be on a small, fast-paced team that nimbly moves to meet new challenges
You love ideating on new features and working with data to find new insights
You're excited about researching and working with the latest tools and technologies
The salary range for this position is $100,000 - $145,000. Compensation for this role will depend on a variety of factors including qualifications, skills and applicable laws. In addition to base compensation this role is eligible to participate in our equity incentive and competitive benefits plans.

Garner Health is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics.
Garner Health is committed to providing accommodations for qualified individuals with disabilities in our recruiting process. If you need assistance or an accommodation due to a disability, you may contact us at talent@getgarner.com.",3.5,"Garner Health
3.5","Dallas, TX",51 to 200 Employees,2019,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
136,Data Engineer,$88K - $120K (Glassdoor est.),"Data Engineer
Engineering
REMOTE OPPORTUNITY

About Catalytic Data Science (CDS):
Catalytic Data Science is a groundbreaking cloud R&D platform designed to integrate the volumes of scientific resources, data, and analytic tools while providing the ability to network with colleagues in one secure and scalable environment. By enabling R&D teams to work more collaboratively and improving productivity company-wide, the Catalytic platform helps teams achieve key R&D milestones faster and with greater accuracy. Our customers are passionate about making the world a better place, and we are inspired by the opportunity to help them.

The Role:
You are a Data Engineer with experience in processing terabytes of data. You have experience in creating and automating scalable, fault-tolerant and reproducible data pipelines using Amazon AWS technologies. You are interested in helping to create a platform completely built on top of AWS. You are eager to join a team of Life Scientists and Software Engineers that believe the brightest minds in research should have the best tools to drive innovation.

What You’ll Do:

Build & operate automated ETL pipelines that process terabytes of text data nightly
Develop service frontends around our various backend datastores (AWS Aurora MySQL, Elasticsearch, S3)
Perform technical analyses and requirements specification with our product team on data service integrations
Help customers bring their data to the platform

What You Know:

Must Haves:

Python 3 or Java programming experience, preferably both
Day-to-day experience using AWS technologies such as Lambda, ECS Fargate, SQS, & SNS
Experience building and operating cloud-native data pipelines
Experience extracting, processing, storing, and querying of petabyte-scale datasets
Familiarity with building and using containers
Familiarity with event-based microservices

Nice-to-Haves:

Prior experience with Elasticsearch (custom development and/or administration) is a huge plus
Prior work with text and natural-language processing
Knowledge of Graph databases

What do we love in team members?

Your specialization is less important than your ability to learn fast and adapt to shifting technologies. We’re especially fond of people who:

Focus on customer’s needs and our company’s goals, not just writing code
Iterate until customers love what you’ve built
Self-start and initiate
Self-organize
Strive to grow personally and professionally, beyond just expanding technical abilities
Love to experiment with new technology and share knowledge with the team

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.",4.2,"Catalytic Data Science
4.2","Boston, MA",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
137,Data Engineer - Python,-1,"Position: Data Engineer

Location: Fully Remote

Duration: 6 months assignment with the possibility of extension

AWS, Python , All Access, ServiceNow, Partner Gateway

Job Description:
Qualifications :
Bachelor’s degree or equivalent experience.
Experience in Data Engineering and Processing tools and services in AWS.
Experience with Python scripting.
Practical experience on a Data Migration effort from on-prem to AWS using DMS and/or similar data migration services.
Strong working knowledge in AWS covering networking, security and data services
Experience with Data lakes and S3 centric data processing designs.
Ability to research, design and implement DBOps paradigm and have it incorporated in CICD automation using tools like GitLab.
Experience working on agile projects and participate in daily scrums and updates
Strong analytical ability and technical skill, as well as the ability to provide innovative solutions to technical needs and business requirements.
Strong attention to detail with a high level of data integrity and accuracy.
Proficient oral and written communication, ability to interact on required information and concepts with people at all levels of the organization.
Proficient ability to translate highly technical information into non-technical terms.
Broad knowledge of the concepts, practices, and principles of programming including design, implementation, and testing.
Ability to interact with customers, understand business requirements and collaborate with team members to explore existing system, determine areas of complexity, identify potential risk to successful implementations.
Responsibilities :
Contribute in an agile and collaborative environment to the development, testing, implementation, and review and evaluation of complex solutions.
Work as part of a team to move on-prem applications to a Cloud environment.
Contribute to the design of technology infrastructure and configurations, recommend process improvements.
Compile and maintain technical documentation, including use cases and scripts; conduct technical research and maintain viable knowledge of technology trends.
Contribute in an agile and collaborative environment to the development, testing, implementation, and review and evaluation of complex solutions.
Work as part of a team to move on-prem applications to a Cloud environment.
Contribute to the design of technology infrastructure and configurations, recommend process improvements.
Compile and maintain technical documentation, including use cases and scripts; conduct technical research and maintain viable knowledge of technology trends.",3.8,"Ampcus Incorporated
3.8",Remote,501 to 1000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
138,Data Insights Engineer,Employer Provided Salary:$85K - $95K,"Who We Are
We're purpose-driven. With every ride, we aim to redefine health and happiness. It's all about being more than a workout: SoulCycle is a mind-body-soul experience, built on community, love, respect, acceptance, and a lot of fun. It comes to life through the ride, the relationships, and the unparalleled hospitality. And all of that comes from our people. Join us—we'd love to have you.
Our Mission
To foster an open, diverse, & inclusive community—while embracing each unique individual exactly as they are. We empower each other by listening with an open mind, finding ways to learn and grow together, and always nurturing a sanctuary of trust. To make a real, lasting impact, we'll work nonstop to embrace and create change. Because nobody is equal until everyone is equal.
Job Description
The Data Insights Engineer will play a pivotal role in driving data-driven decisions at SoulCycle. You'll be responsible for building and maintaining the data infrastructure that supports all business functions, from marketing and operations to finance and customer experience, in addition to providing analysis to each of these teams. By leveraging your technical expertise and analytical skills, you will empower stakeholders to derive insights from data, enabling them to make strategic and informed decisions that positively impact the business.
Roles and Responsibilities
Insights and Recommendations: collaborate with cross-functional teams to understand business requirements, provide analytical support, and identify opportunities for data-driven improvements
Visualizations and Dashboarding: design and develop ad-hoc and recurring Looker reports; create and monitor business metrics; identify patterns, trends, and opportunities for performance improvement
Data Modeling: build, optimize, and document LookML data models that support quick and efficient analysis
Prediction: build predictive models that forecast business outcomes, customer behavior, and other relevant metrics
Qualifications
1-3 years of professional experience transforming and analyzing data across platforms such as Looker, Tableau, Mode, Jupyter Notebooks, Excel, and GCP/AWS. Looker/LookML experience is a plus.
Expert in SQL (able to write structured and efficient queries on large data sets) and familiarity with Python
Ability to identify patterns and trends in data and solve problems
Excellent communication skills to work with stakeholders to translate business needs and ideas into analyses and recommendations
Top-notch organizational skills and ability to manage projects in a fast-paced environment
Creative problem solving skills to find solutions to vague questions
Experience with Python data analysis and visualization packages is a plus (pandas, tensorflow, matplotlib, etc.)
Pay Range: $85,000 - $95,000 per year. This role is on-site 4 days a week.",3.9,"SoulCycle HQ
3.9","New York, NY",1001 to 5000 Employees,2006,Company - Private,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
139,Data Engineer,Employer Provided Salary:$80K - $85K,"Simatree is searching for a Data Engineer Consultant to support our client’s operational requirements on a mission focused program within a challenging, dynamic setting. Successful candidates in this position will collaborate with a diverse group of individuals across the homeland security community and directly contribute to management and analysis of operational data using data science tools. This role supports a US Government contract. An US Citizenship is required.
The Role
The Data Engineer will work alongside a team to generate management tools that effectively control programs and generate reports while supporting the programmatic development of operational applications of the tools or analysis that are focused on operations research and document findings. They will conduct research into management, organizational, and business practices to implement improvements to existing processes. They will utilize commercial or open-source software to manage tasks and present findings to a broad audience, including agency executives. They will maintain a database to produce monthly financial reports, tracks processes, distribute deliverables, and monitor comment status. The Data Engineer will be required to use Microsoft Office to perform queries, analyses, and produce reports. Other responsibilities include:
Use R and Python to perform analyses and develop new tools using popular data visualization libraries such as ggplot2, plotly, matplotlib, or bokeh.
Use Databricks to Extract, Transform, and Load (ETL) data from different sources and transform it into a usable, reliable resource.
Manage data collection and transformation from federal law enforcement and other databases to be used in a wide spectrum of analyses including statistical modeling, performance metrics analysis, crosschecking data with other federal agencies, and risk analysis.
Use ArcGIS to perform analyses and develop new web map applications that summarize results from deep learning/AI algorithms.
Maintain and enhance business intelligence (BI) dashboards using best practices in UI/UX design
Develop and test new methodologies and algorithms which would increase the efficiency and accuracy of existing models, including pilot deployments of automated scripts
Requirements
This role supports a US Government contract. A US citizenship is required.
Strong Experience with SQL programming
Experience with R statistical programming and Python programming
Experience with BI software such as Qlik, Tableau or PowerBI
Demonstrated experience in performing data analysis and modeling on an independent basis
Demonstrated experience in developing and applying quantitative methods to find patterns and relationships in large data sets using statistical and graphical packages
Ability to translate complex, technical, or analytic findings into an easily understood narrative—tell a story with graphical, verbal or written form
3 years experience in supporting quantitative and analytical efforts
Bachelors degree in relevant fields such as statistics, mathematics, computer science, physical science, economics or engineering
Job Type: Full-time
Pay: $80,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Experience level:
1 year
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This question MUST be answered for consideration: This position supports a US Government contract which requires a US Citizenship. Are you a US Citizen?
Work Location: Hybrid remote in Washington, DC 20001",5.0,"Simatree
5.0","Washington, DC",1 to 50 Employees,2018,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
140,Data Science Engineer,-1,"Build the solution that transforms the real estate industry!
We are looking for a Data Science Engineer superhero with a sixth sense for data who can ignite our day-to-day activities with their creativity.
Want to infuse a $30B+ sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? Looking for a fully remote startup culture supported by a profitable business model? Join Mashvisor and help us build an entirely new type of real estate model.

Our Values

Customer Obsessed – We always put our customers first.
Solution Driven – We solve problems that other people are afraid to.
Product led: We are always one step ahead of our customer's needs and create / add features they love every time
One Team – We believe inclusion and teamwork produce the best results.
Open and Direct – We communicate with honesty and respect to our colleagues, customers, and partners.

What You’ll Do
Designing, developing, and researching Machine Learning systems, models, and schemes
Studying, transforming, and converting data science prototypes
Searching and selecting appropriate data sets
Performing statistical analysis and using results to improve models
Training and retraining ML systems and models as needed
Identifying differences in data distribution that could affect model performance in real-world situations
Visualizing data for deeper insights
Analyzing the use cases of ML algorithms and ranking them by their success probability
Understanding when your findings can be applied to business decisions
Enriching existing ML frameworks and libraries
Verifying data quality and/or ensuring it via data cleaning

What You’ll Need
BS or Masters degree in Mathematics, Statistics, Economics, Data Science or another quantitative field
3+ years of hands-on experience utilizing data science to manage, enhance and develop models and deploy solutions to solve complex business problems
Expertise in SQL and programming in SQL, R, Python, C++, Java, and beneficial to know Lisp and Prolog
Strong organizational, interpersonal, and communication skills (both written and verbal)
A bias towards solving problems from a customer-centric lens and an intuitive sense for how the work aligns closely with business objectives
Solid experience with managing databases and datasets and structuring and optimizing the framework
A thorough understanding of SQL databases
Bonus: background in US real estate data, insurance or financial markets analysis
The ideal candidate will be a creative problem solver with an excellent work history on data analytics projects.

We want the work you do here to be the best work of your life.
Compensation: We offer a great salary with a yearly bonus based on performance.
Attitude: Work with a Can-Do team across the world.
Freedom: Work anywhere, anytime.
Time Off: Yearly vacations and sick leaves.
Responsibility: Ability to excel in a fully remote work environment.
Are you the one?",3.7,"Mashvisor Inc.
3.7",Remote,1 to 50 Employees,2015,Company - Private,Real Estate,Real Estate,$1 to $5 million (USD)
141,Data Engineer,-1,"Remote
Contract
Opened 4 months ago
Job Description
Data Engineer (with Healthcare experience) Required Skills: SQL Databricks data engineering Snowflake data engineering QA experience for ETL experienced with data acquisition and ingestion using API and/or batch channels Experience with Healthcare",3.3,"Crackajack Solutions
3.3",Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
142,AI Data Engineer,$96K - $126K (Glassdoor est.),"AI Data Engineer
Forge - Newton, MA
www.forgeco.com
About Us
Forge (forgoco.com) is a newly formed startup based in Boston, MA. We are a technology-enabled trades company with mobile, web, AI, and smart-glasses software applications that enable our professionals in the field and create amazing experiences for our customers. We are innovating rapidly within the professional home services industry – an industry that has barely changed in the last 100 years. Why? Because hiring tradespeople or contractors to do even simple tasks is a frustrating and time-consuming process for customers. There are many reasons for this, but one of the biggest is the shortage of skilled tradespeople in the U.S. — the result of long-term trends that have pushed entry-level workers away from the trades.
At Forge, we are focused on building the next generation of trades professionals and the software that will help make them successful. We believe more skilled workers, enabled by modern technology, will power a wholly new (and vastly improved) customer experience for all.
About Your Role
Forge is growing rapidly and we're looking for a data-focused AI Engineer to join our team! This person will support our newly-formed AI Technologies Team building out solutions that will power our Pros in the field. You will help determine our data collection & engineering best practices to support our AI applications. You will work closely with our product managers, test engineers, and other technical leaders as we move at a rapid pace.
Responsibilities:
Strategize an overall data collection process (including labeling, annotations, etc.) for our overall AI efforts;
Work with different cross-functional teams to devise and deploy data collection approaches, as well as to discover opportunities for enabling AI / Computer Vision;
Develop, construct, test, and maintain schema designs, protocols, and data architectures necessary for the creation of data, image and video libraries to support ML/AI projects;
Enable scalable, reliable, and secure data processing systems (e.g., data lakes and workflows) for both model training and production;
Participate in deploying and validating ML models and monitor their performance;
Collect, clean, preprocess, and analyze large datasets to develop valuable insights and identify potential AI use cases;
Contribute to the development of patents, copyrights, or other forms of intellectual property protection for AI innovations;
Grow with the team as we learn and apply new technologies that are evolving rapidly;
Actively engage with our scrum process;
Work with product managers and designers to flesh out technical specifications and requirements;
Document and communicate database design, data flow and data dependencies;
Work closely with other departments: Pro team, Operations, Customer Service, etc;
Stay up to date on new advancements in the field of data & AI, participating in ongoing education, workshops, and conferences to maintain expertise and incorporate new developments into ongoing projects.
Qualifications:
5+ years of experience building solutions in the data engineering field;
Experience working with a variety of standard database technologies (e.g. DynamoDB, mySQL, Mongo, Oracle, PostgreSQL), generally comfort working with end-to-end data pipelines and streaming data;
Experience building machine learning data stacks (e.g., databricks, airflow, dbt);
Great communication skills. You must be able to communicate technical information to other software engineers, our testing team, and our product managers;
Good team player; ability to work closely with an experienced software team and collaborate effectively;
Bachelor's or Master's degree in a Computer Science-related field is desirable, but not a hard requirement. Must show very deep math & data-management skills;
You should be a quick learner who is comfortable working in a highly agile startup environment where rapid change is a constant;
Ability to get work done and meet deadlines with minimal direct supervision;
Empathy and appreciation for the Trades.
Added Bonus:
Machine Learning/data engineering research or academic background;
Experience using Python to build software in the ML space;
Development background working with Node.js, preferably in a microservices environment;
Experience with reinforcement learning algorithms (e.g., Q-learning, Deep Q-Networks);
Familiarity with big data technologies (e.g., Hadoop, Spark);
Experience working with R, C++;
Agile development experience, or experience setting up Agile processes;
Working experience in the Trades.
At Forge, we value innovation, teamwork, and a commitment to excellence. We're dedicated to creating a supportive and collaborative environment where individuals can grow professionally and make a real impact. We offer competitive salaries, equity, benefits, flexible working arrangements, and a dynamic culture of intelligent, hard-working, and creative individuals.",3.6,"Forge
3.6","Newton, MA",51 to 200 Employees,2020,Company - Private,Construction,"Construction, Repair & Maintenance Services",Unknown / Non-Applicable
143,Data Engineer,$79K - $108K (Glassdoor est.),"Role Description
Position Title: Data Engineer
Manager: Sr. Manager, Enterprise Solution
Position Location: Rehrig Technology Center, TX
Manager Once Removed: Director - IT & Security
Level of Work: Individual Contributor
Date Prepared: 08/09/2023
Brief Role Description
The Data Engineer will be responsible for developing and managing the enterprise overall data architecture, which includes the data models, data warehouses, data pipelines, and other systems that will be used to store, manage, and analyze data.
This role also requires a fair amount of knowledge on designing, implementing, and maintaining Microsoft SQL Servers, Data analytics using Power BI and azure data pipeline and implementing security and working with stakeholders to understand their data needs and deliver solutions that meet those needs.
Accountabilities
Database Management:
Design, create, and maintain optimal database systems for various applications and services.
Monitor and optimize database performance, security, and integrity.
Handle database backups, recovery, and regular updates to ensure data accuracy and availability.
Collaborate with IT teams to maintain server infrastructure and ensure database reliability.
Configuring, solving problems, and supporting SQL Servers in production and non-production environments running in cloud environments and Windows operating systems
Develop and maintain a disaster recovery plan to measure ensure that critical data systems are available in the event of outages.
Develop and maintain a capacity plan to measure utilization and plan for growth in demand for database resources.
Design scripts and utilities to automate day to day database administration.
Assist with the monitoring performance analysis of existing applications and ensuring that the system is consistent with ongoing need
Create technical documentation, operational procedures, Incident, and Problem communications and reporting.
Quickly restore service during system outages, provide troubleshooting support to team members, perform proactive problem prevention, and advance technical details to technical lead and/or to vendor support
Maintaining, detail and deliver Microsoft SQL 2012+ in accordance with certified specifications and security guidelines.
Data Engineering:
Design and construct scalable, maintainable, and efficient data pipelines.
Implement and manage ETL processes, ensuring the accuracy, reliability, and timeliness of data processes.
Design and maintain enterprise level data model and data flow
Cloud and Integration:
Design and implement solutions in Azure, with a particular focus on Azure data pipeline.
Integrate Power BI solutions, ensuring that data sources are accessible and optimized for visualization.
Stay updated with the latest Azure features and Power BI advancements to recommend and implement improvements.
Collaboration & Training:
Collaborate with cross-functional teams, assisting with data-related technical issues, and ensuring optimal data delivery architecture.
Provide training and support to junior team members and stakeholders on database management and data engineering best practices.
Qualifications
BS in Information Technology or a related technical field plus 4-6 or more years related overall experience.
3-5 years of experience in data engineering
Strong understanding of data engineering principles and practices
Experience with Power BI and Azure Data Pipeline
Experience with SQL, Python, and other programming languages
Strong expertise in Azure data pipeline, including Azure Data Factory, Azure Data Lake, and other Azure data services.
Proficiency in Power BI, including designing and deploying insightful dashboards and data visualizations.
Strong knowledge of SQL and MS SQL databases, including optimization techniques.
Excellent understanding of data pipeline and workflow management tools.
Strong analytical skills with a problem-solving attitude.
Good interpersonal and communication skills, with the ability to collaborate effectively across diverse teams.
Certifications in Cloud and/or Microsoft SQL Server highly desired
A minimum of 3 years of experience in on-premises and cloud-based Database Technologies - AWS RDS and/or Azure SQL PAAS
Devise Strategy and Engage with IT leadership and business partners to define a Database & Analytics strategy for Rehrig
Implement automation of database operations like upgrades and patching
Demonstrated Experience in Defining and Implementing Production Proactive Health Monitoring of Database Technologies
Strong Incident Root Cause Analysis on Database Execution and Application Interaction with Database Technology
Experience with Data migration and SQL upgrades across Microsoft Server versions
Ability to travel approximately 15%-20% of the time
General Responsibilities
Provide periodic reporting on workforce capacity and utilization.
Provide periodic reporting on the availability and health of software development process.
Other duties as assigned.",3.8,"Rehrig Pacific Company
3.8","Richardson, TX",501 to 1000 Employees,1913,Company - Private,Machinery Manufacturing,Manufacturing,$100 to $500 million (USD)
144,Data Engineer,Employer Provided Salary:$150K,"Data Engineer
We are looking for a Data Engineer that will architect and implement systems handling the ingestion, cleaning, and structuring of data. They will be a member of the Data Science team and will have direct interaction with the people using the data ingested. The hire will also be responsible for managing our group’s core infrastructure including our region redundant pipeline orchestration servers (Airflow) and webserver stack (NGINX + Gunicorn + Django). The ideal candidate is an experienced data pipeline builder and core infrastructure guru.
Responsibilities:
Create and maintain an optimal data pipelines in Python + SQL
Build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python and SQL (Snowflake & SQL Server) in both on premise and cloud environments
Manage and improve the user facing server infrastructure (web + api) including authorization and load balancing
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that that provide actionable insights on both the user interactions and infrastructure loads
Technical Qualifications:
Python expert, notably savvy with data science stack (Pandas, NumPy, SciPy)
Relational SQL databases, Microsoft SQL Server and Postgres preferred
Snowflake cloud database experience
Pipeline and workflow management tools: Airflow, Luigi, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Athena, SQS, etc.
2+ professional years’ experience with Masters or 3+ professional years with Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Highly Valued:
Data Science/Analysis background
Computer science understanding, data structures, processes, threading, memory usage
Unix/Linux command-line experience
Broad understanding of equities, derivatives, futures, FX, or other financial-services instruments
Other Qualifications:
Excellent listening, and communication (both oral and written) skills
Self-starter and critical thinker, takes ownership of own projects and makes improvement suggestions for the entire infrastructure.
Proactive, assertive and attentive to details.
Can work independently and in a collaborative environment.
Can handle several projects with different priorities at the same time in a fast-paced environment.
Excellent self-management and problem-solving skills.
Results-oriented, can deliver quality code quickly
Millennium pays a total compensation package which includes a base salary, discretionary performance bonus, and a comprehensive benefits package. The estimated base salary range for this position is $150,000, which is specific to New York and may change in the future. When finalizing an offer, we take into consideration an individual’s experience level and the qualifications they bring to the role to formulate a competitive total compensation package.",3.8,"Millennium Management LLC
3.8","New York, NY",1001 to 5000 Employees,1989,Company - Private,Investment & Asset Management,Financial Services,Unknown / Non-Applicable
145,Data Engineer,-1,"Who We Are
Land Intelligence is a software technology company serving the commercial real estate industry. We focus on Land Development. We have been recognized as an industry technology leader in providing solutions on a national scale. Our team are visionaries that see a better, faster, and more valuable way to research, finance, and trade land.

Our Culture
We are entrepreneurs first. Which means we manage the people, processes and product. We create new ways of doing things to drive value. We are builders and growth minded. Our leadership team has been recognized as a Best Places to work in the industry nationally. Our team drives for personal and professional development, as personal growth is instrumental to our success. Your learning will be supported by specialized in-house training programs and mentoring by the industry’s leading experts, many of whom are our investors and strategic partners.
Job Overview
Land Intelligence is seeking a savvy Data Engineer to join our growing team and help us continue to enhance our SaaS platform, LandSUITE®. The hire will be responsible for expanding and optimizing our data and data pipeline architecture to support product development and internal tools. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of re-designing our company's data architecture to support our next generation of products and data initiatives.
Responsibilities
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies
Build analytics tools that use the data pipeline to provide actionable insights into user behavior and market trends
Work with stakeholders including the executive and product development teams to assist with data-related technical issues and support their data infrastructure needs
Keep our data separated and secure across AWS regions
Create data tools for analytics and team members that assist them in building and optimizing LandSUITE® into an innovative industry leader
Qualifications
We are looking for a candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large, disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
Experience with the following software/tools:
Experience with relational SQL and NoSQL databases, including Postgres, MySQL, and Cassandra
Experience with data pipeline and workflow management tools
Experience with stream-processing systems
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java
We are a startup, but this isn't our first time doing this. As a result, you can get the thrill of working at a startup, with the resources of a publicly traded company. We offer a best-in-class benefits package, as we are a Professional Employment Organization (PEO) with our partner Insperity that includes medical, vision, dental and life insurance. Our 401 (k) program offers an employer match, along with a 401(k)-profit sharing and performance-based bonuses. • Generous paid time off

Land Intelligence is an EOE/Affirmative Action Employer M/F/D/V. If you are interested in applying for employment and need special assistance to apply for a posted position, please send an e-mail to careers@landintelligence.net.",1.8,"Land Intelligence Inc
1.8",Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
146,Data Visualization Engineer,$77K - $101K (Glassdoor est.),"Passionate about precision medicine and advancing the healthcare industry?
Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.
With the advent of genomic sequencing, we can finally decode and process our genetic makeup. We now have more data than ever before but providers don't have the infrastructure or expertise to make sense of this data. We're on a mission to connect an entire ecosystem to redefine how genomic data is used in clinical settings.
What You'll Do:
Own the design, development and administration of advanced analytics dashboards and reporting and business intelligence tools. Beginning with requirements gathering across cross-functional teams, to the delivery of analytic solutions into production.
Collaborate with stakeholders, providing guidance, data and analytical support for developing new datasets, analyses and tools in support of strategic initiatives.
Perform deep-dive root cause and problem-solving analyses for and with laboratory, business operations and engineering teams.
Design and build data views to support advanced analytical reporting, self-service data analysis and exploration, utilizing multiple data and visualization technologies.
Work closely with technical and product teams to identify and align changes to internal systems in support of future data analysis needs and requirements.
Develop and facilitate data governance initiatives to assure data integrity across distributed systems.
Solve new and interesting problems.
Qualifications:
Bachelor's degree in Information Systems, Business Administration, Finance, Economics, Mathematics, Operations Research, Marketing or similar
3+ years of experience developing data sets for analysis use-cases
3+ years of experience with direct stakeholder management and ownership of full lifecycle of analytics/data projects and presenting analysis results to stakeholders
3+ years report-development and data visualization experience with BI tools (Looker, Tableau, etc.)
Ability to deliver both high-level explanations of complex problems while also able to deep dive into complex problems
Advanced SQL skills for querying and analysis of large, complex datasets
Must work well in a team environment and be able to partner with both business and technical team members and translate technical language to the non-technical need
Creative problem solver, ability to handle multiple projects, and strong work ethic needed
Excellent written and verbal communication skills
Highly organized and systematic, superb attention to detail
Able to complete tasks with a high degree of accuracy and an eye towards process improvement
Persistent and resilient person who thrives in a fast-paced, highly collaborative environment
Bonus points for:
Advanced analytics experience - classification/predictive modeling, optimization models, forecasting
Experience with Python for data engineering or advanced data analysis projects/use-cases
Healthcare domain knowledge and experience
Expert knowledge of database/data modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others
Experience building cloud-native applications and supporting technologies / patterns / practices including: GCP, AWS, Docker, CI/CD, DevOps, and microservices

#LI-EH1
#LI-Hybrid
We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.1,"Tempus
3.1","Chicago, IL",501 to 1000 Employees,2015,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
147,Data Engineer,$80K - $109K (Glassdoor est.),"For more than 170 years, The Hanover has been committed to delivering on our promises and being there when it matters the most. We live our values every day, demonstrating we CARE through our values, ESG initiatives and IDE journey.

Our Claim Analytics dept is currently seeking a Data Engineer to join their growing team. This position is a remote work opportunity.

Position Overview:
Data engineering is the aspect of data science that focuses on practical applications of data collection and analysis. This role primarily will become proficient with all internal & external data produced and consumed by THG. The engineer will understand where the data is, basic data models and architecture, how to access and obtain data and how to manipulate and work with data to produce output – which may be reports, datasets or self-service reports.
This is a full time/ exempt role.

In this role you will:
This role will begin to work on more complex projects with minimal supervision. The Data Engineer will confer with business partners on data/reporting requests. The assignments will become broader in nature, usually requiring originality and ingenuity. This role will be assigned projects and be accountable for successful outcomes. Some Insurance knowledge required.

What you need to apply:
Education
2-5 years related experience and bachelor’s degree. Degrees and/or related experience in Applied Data Science, Data Science, Information and Data Science, Information Management Analytics, Data Mining, Predictive Analytics are most sought-after. Degrees in Statistics, Mathematics, Computer Science, Information Systems are also preferred.
Skills
Analytical Skills: Data Engineers work with large amounts of data that will include facts, figures, and number crunching. You will need to see through the data and analyze it to find conclusions.
Communication Skills: Data engineers are often called to present their findings or translate the data into an understandable document. You will need to write and speak clearly, easily communicating complex ideas.
Critical Thinking: Data engineers must look at the numbers, trends, and data and come to new conclusions based on the findings.
Attention to Detail: Data is precise. Data engineers must make sure they are vigilant in their analysis to come to correct conclusions.
Math Skills: Data engineers intermediate need math skills to estimate numerical data.
CAREER DEVELOPMENT:
It’s not just a job, it’s a career, and we are here to support you every step of the way. We want you to be successful and fulfilled. Through on-the-job experiences, personalized coaching and our robust learning and development programs, we encourage you – at every level – to grow and develop.

BENEFITS:
We offer comprehensive benefits to help you be healthy, build financial security, and balance work and home life. At The Hanover, you’ll enjoy what you do and have the support you need to succeed.

Benefits include:
Medical, dental, vision, life, and disability insurance
401K with a company match
Tuition reimbursement
PTO
Company paid holidays
Flexible work arrangements
Cultural Awareness Day in support of IDE
On-site medical/wellness center (Worcester only)
Click here for the full list of Benefits
EEO statement:
The Hanover values diversity in the workplace and among our customers. The company provides equal opportunity for employment and promotion to all qualified employees and applicants on the basis of experience, training, education, and ability to do the available work without regard to race, religion, color, age, sex/gender, sexual orientation, national origin, gender identity, disability, marital status, veteran status, genetic information, ancestry or any other status protected by law.

Furthermore, The Hanover Insurance Group is committed to providing an equal opportunity workplace that is free of discrimination and harassment based on national origin, race, color, religion, gender, ancestry, age, sexual orientation, gender identity, disability, marital status, veteran status, genetic information or any other status protected by law.”

As an equal opportunity employer, Hanover does not discriminate against qualified individuals with disabilities. Individuals with disabilities who wish to request a reasonable accommodation to participate in the job application or interview process, or to perform essential job functions, should contact us at: HRServices@hanover.com and include the link of the job posting in which you are interested.

Privacy Policy:
To view our privacy policy and online privacy statement, click here.

Applicants who are California residents: To see the types of information we may collect from applicants and employees and how we use it, please click here.",4.1,"The Hanover Insurance Group
4.1","Worcester, MA",5001 to 10000 Employees,1852,Company - Public,Insurance Carriers,Insurance,$5 to $10 billion (USD)
148,AWS Data Engineer Snowflake,-1,"Company Description

Orchestrated by adept technical architects with over fifty years of applied expertise, KYNITE is an advanced technology company specializing in the disciplines of: Blockchain, Cloud Services, Big Data & Analytics, Artificial Intelligence, Enterprise, Staff Augmentation and Managed Services
We are BigData Experts
We are Cloud Experts
We are Enterprise Architects
We are Artificial Intelligence Innovators
We are Technological Evangelists
We are Doers
We are Kynite

Additional Information

All your
This job is only for individuals residing in US
US Citizens, Green Card holders, EAD's can apply
W2, C2c,
Information will be kept confidential according to EEO guidelines.",-1,Kynite,"Myrtle Point, OR",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
149,Junior Data Engineer,$74K - $110K (Glassdoor est.),"Junior Data Engineer
Serenity Healthcare is hiring a Junior Data Engineer for our Lehi, UT headquarters. While previous ETL experience is preferred, we are open to exceptional entry-level talent for this role.
We intend to provide on the job training in data-skills: SQL, BI (PowerBI), ETL (SSIS), Warehousing (SQL Stored Procedures), Exploratory Data Analysis, etc. It’s our intention to train you in Microsoft’s new tool: PowerApps.
Desired skill sets:
Must be a quick learner
SSIS experience strongly preferred
Skills used in the role:
SQL 20%
SSIS 40%
PowerApps 40%
Day-to-day work description:
The Junior Data Engineer will be responsible for keeping the data flowing, building new data pipelines, and creating business applications using MS-PowerApps. You’ll need to be comfortable with SQL, SQL Server, and SSIS. You’ll be reading API documentation to establish new ETL flows, as well as automating report delivery.
Job Fit:
Capable of “Deep Work”
Problem Solver
Reliable and consistent
Attention to detail
What We Offer to You:
Competitive pay (DOE), including additional target compensation
Opportunity to work and grow your career in a fast-paced environment
Medical, Dental, Vision Insurance (90% coverage for you and codependents)
Life Insurance
Flexible spending account
Paid time off
Vision insurance
401k
Open and friendly, professional office environment
Who We Are:
We have helped thousands of patients take back their lives from mental illness with specialized clinical expertise and the foremost cutting-edge technology available in mental health today. Serenity’s approach to treating mental illnesses is to offer holistic options and treat the whole person by providing an atmosphere of positivity, support, and healing in an outpatient setting.
We believe people should live their best lives, and mental health is a substantial segment of total well-being. We bring the same passion we have for improving our patient’s lives to providing a work experience that will help you do your best work, enjoy the time you invest at work, and succeed in life outside of work. We take our people and culture seriously and make it a priority to invest in both.
Serenity Mental Health Centers is an equal opportunity employer. This position is contingent on successfully completing a criminal background check and drug screen upon hire.",2.8,"Serenity Healthcare
2.8","Lehi, UT",201 to 500 Employees,2017,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
150,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
151,Data Engineer,Employer Provided Salary:$110K - $130K,"Data Engineer
As a Data Engineer, your work at Tegria will center on strategic opportunities and implementation, process improvement, and growing Tegria as a company. This role will focus on developing, constructing, testing, and maintaining data structures, data pipelines, and architecture. You will recommend and implement ways to improve the readability, efficiency, and quality of data.  A Data Engineer at Tegria will also be focused on delivering data sets for data modeling, data mining, and production reporting.
The role you play
An effective Data Engineer will help the organization on a whole achieve success through:
Data Engineer-Specific Responsibilities
Create and maintain optimal data pipeline architecture while working within time and budget constraints (i.e. SSIS, Azure Data Factory, Apache Spark, Databricks, etc.)
Work with stakeholders to assist with data-related issues and support their data infrastructure needs
Consult, assess and provide recommendations for improvement in client’s current data architecture
Assemble large, complex, and disparate datasets to meet functional and non-functional business needs
Utilize multiple programming languages to develop the best solution for the client (i.e. SQL, R, Python, C++, etc.)
Research and develop processes for utilizing cloud-based services (i.e. Snowflake, Google Cloud Services, Amazon Web Services, and Microsoft Azure)
Create data tools for analytics and data science team members that will aid in the development and optimization of our current and future services into an industry leader in healthcare analytics
Client Engagement Delivery
Working independently or as part of a project team on a client engagement. Could be full-time on a single customer engagement or part-time across customers
Serving as a liaison between diverse IT and operations groups
Facilitating meetings and owning meeting scheduling and coordination, preparation, documentation, and follow-up
Utilizing, reviewing, and creating project tools and templates for assigned projects
Creating and maintaining project plans
Evaluating and documenting current-state processes through discovery and analysis. Presenting recommendations for improvements based on industry experience and best-practices
Facilitating future-state workflow, policy, and process design and planning
Building, testing, training, converting and/or deploying new infrastructure, workflows, policies, and processes
Participating in major milestone reviews and decision gates
Presenting to a wide variety of audiences
Documenting measurable outcomes resulting from initiatives through KPI analysis and impact tracking
Effectively utilizing communication, decision-making, and escalation pathways
Executing effective project wrap-up through outcomes documentation, lessons-learned, and leave-behind materials allowing customers to sustain ongoing operations
Mentoring Associate(s) on project activities and deliverables and collaborating with others on the same
Mentoring customer counterparts for successful, long-term ownership and growth
What we’re looking for
We expect:
5+ years of professional experience working as a Data Engineer, ETL Developer, Database Administrator, or similar positions
Experience working with SSIS and other data integration tools and a desire to stay current as the data engineering field advances
A deep conceptual knowledge and demonstrated practical understanding of data modeling techniques and best practices
In-depth understanding of data warehouse design with advanced knowledge in querying languages such as SQL
Experience working with unstructured and semi structured data (JSON, free-text entries, etc.)
Demonstrated ability in project management (waterfall and/or agile), and other organizational management such as risk management, or change management
Capable of and comfortable with working remotely
Capable of and comfortable with traveling to client sites as needed
We’d love to see:
Prior consulting experience
Experience with interface engines such as Epic Bridges, HL7 or FHIR
Some experience implementing, supporting, optimizing, and upgrading Epic
Certification in one or more Epic data model and/or application(s)
Formal project management certification – either PMP or CSM
Formal process improvement certification – ex: Lean Six Sigma or ITIL
Need a few more details?
Status: Exempt
Eligibility: Must be legally authorized to work in the US without sponsorship
Work Location: This position is remote. Must work in a location within the US.
Travel: Up to 25%
Benefits Eligibility: Eligible
Now, a little about us
At Tegria, we bring bold ideas and breakthroughs to improve care, technology, revenue, and operations in ways that move healthcare organizations from patient-centered to human-centered. We are helping healthcare put people first—both patients and those who dedicate their lives to delivering care.
And at the very core of this vital work is our incredibly talented people.
People with different backgrounds who welcome challenge and change. People who listen first, ask hard questions, and make decisions to cultivate a culture of equity and inclusion. People who chase after goals, growth, and generosity. We’re real. We’re nimble, and we believe in our mission to humanize healthcare.
Perks and benefits
Top talent deserves top rewards. We’ve carefully curated a best-in-class benefits package, meant to meet you wherever you are in your life and career.
Your health, holistically. We offer a choice of multiple health and dental plans with nationally recognized networks, as well as vision benefits, a total wellness program, and an employee assistance program for you and your family.
Your financial well-being. We offer competitive wages, retirement savings plans, company-paid disability and life insurance, pre-tax savings opportunities (HSA and/or FSA), and more.
And everything in between. Our lifestyle benefits are unrivaled, including professional development offerings, opportunities for remote work, and our favorite: a generous paid-time-off program, giving you the flexibility to plan a vacation, take time away for illness (or life’s important events), and shift your schedule to accommodate those unexpected curve balls thrown your way.
Tegria is an equal employment opportunity employer and provides equal employment opportunities (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. All qualified candidates are encouraged to apply.
Job Type: Full-time
Pay: $110,000.00 - $130,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Parental leave
Referral program
Vision insurance
Compensation package:
Bonus pay
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
COVID-19 considerations:
Remote role, Certain positions require individuals to meet their client’s COVID-19 requirements (typically being fully vaccinated, including a booster)
Experience:
Data Engineering: 3 years (Required)
Work Location: Remote",2.4,"Tegria
2.4",Remote,Unknown,2020,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
152,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
153,Data Engineer,$67K - $96K (Glassdoor est.),"Massanutten Resort
Job Summary
The Data Engineer will play a crucial role in designing, developing, and maintaining the data infrastructure and systems that enable efficient and reliable data processing, storage, and analysis. The primary responsibility is to transform raw data into usable formats for various business applications and support data-informed decision-making processes. This role includes working in various Microsoft SQL Environments as well as supporting a Snowflake Data Warehouse. The Data Engineer is expected to have a broad understanding of data concepts and be avid about continuing education to keep up with the ever-changing data landscape.
Required
Degree in computer science or related file and 3+ years of experience with Python, SQL, and data visualization/exploration tools (in lieu of a degree, at least 5 years of experience in the above is equivalent)
Preferred
A degree in computer science or a related field
Certifications in database management and/or related technical certifications
Familiarity with advanced data concepts (i.e., AI and Machine Learning) and business intelligence/visualization tools
Typical Schedule
Days:
Monday-Friday, weekends and holidays as needed
Position will be required to be part of rotating on-call schedule
Hours:
8am-5pm, additional hours as needed
Core Responsibilities
Develop Data Pipeline: Design and implement scalable and efficient data pipelines, integrating diverse data sources and ensuring smooth data flow from ingestion to storage and processing
Transform and Model Data: Develop and maintain data transformation processes, such as ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform), to convert raw data into structured formats suitable for analysis and reporting. Create and manage data models to facilitate data integration and provide a consistent view of information across systems.
Implement and oversee Data Warehousing and Storage: Build and maintain data warehouses or data lakes, optimizing storage and retrieval processes to ensure high availability, reliability, and performance. Implement appropriate data governance and security measures to protect sensitive data.
Ensure Data Quality and Governance: Collaborate with data analysts and business stakeholders to understand data requirements and define data quality standards. Implement data validation, data profiling, and data cleansing techniques to ensure data accuracy, consistency, and integrity.
Optimize Performance: Identify and address performance bottlenecks in data processing and storage systems, leveraging optimization techniques and technologies to improve overall data engineering efficiency and reduce latency
Assist with Data Problem Solving: Support team with data related escalation issues, including help desk support tickets that are generated from our business users
Manage Data Integration and API Development: Establish connections and integrations between internal and external data sources, including APIs, databases, and third-party platforms. Develop and maintain data APIs to enable seamless data access and consumption by other applications and services.
Collaborate and Document: Work closely with cross-functional teams, including analysts, software engineers, and business stakeholders, to understand data requirements and ensure data solutions align with business objectives. Document data engineering processes, including data pipelines, transformations, and system architectures, to facilitate knowledge sharing and maintain a robust knowledge base.
Stay up-to-date with the latest advancements in data engineering, data management, and big data technologies. Evaluate and recommend new tools, frameworks, and methodologies to improve data engineering practices and enhance overall data infrastructure
Other duties as assigned by management
For more information, contact Ana at 540-289-4939.",3.6,"Great Eastern Resort Management, Inc.
3.6","McGaheysville, VA",1001 to 5000 Employees,-1,Self-employed,-1,-1,Unknown / Non-Applicable
154,Data Visualization Engineer,$77K - $101K (Glassdoor est.),"Passionate about precision medicine and advancing the healthcare industry?
Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.
With the advent of genomic sequencing, we can finally decode and process our genetic makeup. We now have more data than ever before but providers don't have the infrastructure or expertise to make sense of this data. We're on a mission to connect an entire ecosystem to redefine how genomic data is used in clinical settings.
What You'll Do:
Own the design, development and administration of advanced analytics dashboards and reporting and business intelligence tools. Beginning with requirements gathering across cross-functional teams, to the delivery of analytic solutions into production.
Collaborate with stakeholders, providing guidance, data and analytical support for developing new datasets, analyses and tools in support of strategic initiatives.
Perform deep-dive root cause and problem-solving analyses for and with laboratory, business operations and engineering teams.
Design and build data views to support advanced analytical reporting, self-service data analysis and exploration, utilizing multiple data and visualization technologies.
Work closely with technical and product teams to identify and align changes to internal systems in support of future data analysis needs and requirements.
Develop and facilitate data governance initiatives to assure data integrity across distributed systems.
Solve new and interesting problems.
Qualifications:
Bachelor's degree in Information Systems, Business Administration, Finance, Economics, Mathematics, Operations Research, Marketing or similar
3+ years of experience developing data sets for analysis use-cases
3+ years of experience with direct stakeholder management and ownership of full lifecycle of analytics/data projects and presenting analysis results to stakeholders
3+ years report-development and data visualization experience with BI tools (Looker, Tableau, etc.)
Ability to deliver both high-level explanations of complex problems while also able to deep dive into complex problems
Advanced SQL skills for querying and analysis of large, complex datasets
Must work well in a team environment and be able to partner with both business and technical team members and translate technical language to the non-technical need
Creative problem solver, ability to handle multiple projects, and strong work ethic needed
Excellent written and verbal communication skills
Highly organized and systematic, superb attention to detail
Able to complete tasks with a high degree of accuracy and an eye towards process improvement
Persistent and resilient person who thrives in a fast-paced, highly collaborative environment
Bonus points for:
Advanced analytics experience - classification/predictive modeling, optimization models, forecasting
Experience with Python for data engineering or advanced data analysis projects/use-cases
Healthcare domain knowledge and experience
Expert knowledge of database/data modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others
Experience building cloud-native applications and supporting technologies / patterns / practices including: GCP, AWS, Docker, CI/CD, DevOps, and microservices

#LI-EH1
#LI-Hybrid
We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.1,"Tempus
3.1","Chicago, IL",501 to 1000 Employees,2015,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
155,Data Science Engineer,-1,"Build the solution that transforms the real estate industry!
We are looking for a Data Science Engineer superhero with a sixth sense for data who can ignite our day-to-day activities with their creativity.
Want to infuse a $30B+ sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? Looking for a fully remote startup culture supported by a profitable business model? Join Mashvisor and help us build an entirely new type of real estate model.

Our Values

Customer Obsessed – We always put our customers first.
Solution Driven – We solve problems that other people are afraid to.
Product led: We are always one step ahead of our customer's needs and create / add features they love every time
One Team – We believe inclusion and teamwork produce the best results.
Open and Direct – We communicate with honesty and respect to our colleagues, customers, and partners.

What You’ll Do
Designing, developing, and researching Machine Learning systems, models, and schemes
Studying, transforming, and converting data science prototypes
Searching and selecting appropriate data sets
Performing statistical analysis and using results to improve models
Training and retraining ML systems and models as needed
Identifying differences in data distribution that could affect model performance in real-world situations
Visualizing data for deeper insights
Analyzing the use cases of ML algorithms and ranking them by their success probability
Understanding when your findings can be applied to business decisions
Enriching existing ML frameworks and libraries
Verifying data quality and/or ensuring it via data cleaning

What You’ll Need
BS or Masters degree in Mathematics, Statistics, Economics, Data Science or another quantitative field
3+ years of hands-on experience utilizing data science to manage, enhance and develop models and deploy solutions to solve complex business problems
Expertise in SQL and programming in SQL, R, Python, C++, Java, and beneficial to know Lisp and Prolog
Strong organizational, interpersonal, and communication skills (both written and verbal)
A bias towards solving problems from a customer-centric lens and an intuitive sense for how the work aligns closely with business objectives
Solid experience with managing databases and datasets and structuring and optimizing the framework
A thorough understanding of SQL databases
Bonus: background in US real estate data, insurance or financial markets analysis
The ideal candidate will be a creative problem solver with an excellent work history on data analytics projects.

We want the work you do here to be the best work of your life.
Compensation: We offer a great salary with a yearly bonus based on performance.
Attitude: Work with a Can-Do team across the world.
Freedom: Work anywhere, anytime.
Time Off: Yearly vacations and sick leaves.
Responsibility: Ability to excel in a fully remote work environment.
Are you the one?",3.7,"Mashvisor Inc.
3.7",Remote,1 to 50 Employees,2015,Company - Private,Real Estate,Real Estate,$1 to $5 million (USD)
156,Data Engineer,$73K - $101K (Glassdoor est.),"POSITION OVERVIEW
NBME® is looking for a Data Engineer to join a highly skilled and growing team of data engineers and BI developers. In this role you will be using your data engineering skills and problem-solving skills to deliver meaningful insights to Internal and external customers. The Data Engineer will be responsible for modernizing, expanding, and optimizing NMBE data warehouse by building data integrations, supporting reporting and data visualization for internal and external customers, as well as optimizing new/existing data flows.
Diversity, Equity, and Inclusion Statement
At NBME, we continue to innovate and improve how we fulfill the evolving needs of the health care community. This commitment starts and ends with the people at NBME. By recruiting and empowering talented individuals from various disciplines and backgrounds, which includes professionals with diverse life experiences, abilities, and perspectives, NBME can take a well-informed, robust approach to advancing medical education and assessment for years to come. We also continue to focus on ensuring that our DEI work is impactful and ingrained in everything we do, including with our staff, workplace culture, products and services, the Philadelphia community and the broader medical education.
RESPONSIBILITIES
Participates in requirements gathering, analyzes and architects project and/or system data architectures. In collaboration with subject matter experts, data analysts, solutions and enterprise architects, ensures designs are consistent and coherent to the enterprise architecture standards and existing environment.
Be a critical part of a scrum team in an agile environment, ensuring the team successfully meets its deliverables each sprint.
Trains and mentors team members
Work independently and effectively to manage one's time across multiple priorities and projects.
Assures the integrity and accuracy of the corporate data, with particular attention to data security.
Responsible to ensure high data quality for Data Services, Analytics and Master Data Management.
Perform Logical and Physical data modeling with an agile mindset
Build automated, scalable, test-driven data pipelines.
Utilize software development practices such as version control via Git, CI/CD, and release management.
Build data products using various visualization, BI tools and data science tools.
Collaborate with Data Engineers, DevOps engineers and architects on improvement opportunities for DataOps tools and frameworks.
Be a trusted partner by providing agile capabilities and outstanding services through digital transformation
QUALIFICATIONS
Skills and Abilities
Expert in data modeling, database management and data profiling techniques, particularly in Data Warehouse, Master Data Management (MDM), and in the design and delivery of Business Intelligence solutions.
Deep understanding of industry standards and best practices
Strong analytic skills related to working with structured and unstructured datasets.
Critical thinker and creative problem-solving skills along with the ability to communicate well with stakeholders throughout the organization.
Highly proficient in PL/SQL
Experience with relational SQL and NoSQL databases, including Oracle, MS SQL Server and HBase.
Experience with data integration tools: Informatica, Boomi, Fivetran, MS Integration Services, Sqoop, etc.
Experience with one of the cloud vendors and services: AWS, Google, Microsoft, Snowflake
Experience consuming and building APIs
Experience with object-oriented/object function programming languages: Python, Java, C++, Scala, etc.
Experience with visual analytics tools: QlikView, Tableau, Power BI, Micro strategy etc.
Experience utilizing Agile methodology for development.
Experience with Singer and DBT is a plus
Experience
Minimum of 3 years of Data Engineering and Data Warehousing experience.
Minimum of 2 years of experience with python
Minimum of 2 years of experience with data integration tools like Informatica, Boomi, AWS - Glue etc.
Minimum 1 year of experience with AWS services like S3 and Redshift
Education
Bachelor's Degree in Computer Science or related technical field and 6 years of relevant work experience. Master's degree is a plus.

About NBME:
NBME offers a versatile selection of high-quality assessments and educational services for students, professionals, educators, regulators and institutions dedicated to the evolving needs of medical education and health care. To ensure our assessments meet the highest standards of quality, stay relevant and align to the current curriculum in medical schools and training programs, we rely on a wide network of collaborators. These include the volunteers who help develop our exam questions, the committees and panels who represent various groups within the medical education community, external researchers and health profession organizations.
We are committed to meeting the needs of educators and learners globally with assessment products and expert services such as NBME Subject Examinations, Customized Assessment Services, Self-Assessments, the International Foundations of Medicine Program and Item Writing Workshops. Together with the Federation of State Medical Boards, NBME develops and manages the United States Medical Licensing Examination, which measures the ability to apply knowledge and skills that form the basis of safe and effective patient care. Our Competency-based Assessment unit is focused on new methods as well as the optimization of assessment in the workplace and education.
As a result of leadership in ongoing research, innovative measurement practices and the exploration of forward-thinking assessment modalities and improvements, NBME advances assessment science. Our grant and funding opportunities further support this dedication to medical education and assessment science. We help develop the next generation of assessment professionals through our Summer Psychometric Internship Program. Through the Stemmler Fund, Strategic Educators Enhancement Fund and Latin America Grants Program, researchers and educators can continue to improve the assessment of health care professionals around the world.
NBME views diversity, equity and inclusion (DEI) as foundational and enduring to our strategy and vision. We continue to focus on ensuring that our DEI work is impactful and ingrained in everything we do, including with our staff, culture, products and services, the Philadelphia community and the broader medical education landscape. Our commitment manifests in our hiring and staff development, recruitment for committees, grants programs, design and review of our assessments, and involvement in our local and national communities.
Learn more about NBME at NBME.org.

The NBME offers competitive salaries, excellent benefits, and a rewarding work environment. Excellent Benefits include: Healthcare, Dental, Prescription, and Vision plans; 401(k) w/match; Retirement Income Plan, Tuition Reimbursement Plan, Commuter Benefit: Public Transit or Parking options. Remote Friendly Workplace.

NBME is an EEO employer as defined by the EEOC.",4.0,"NBME
4.0","Philadelphia, PA",201 to 500 Employees,1915,Nonprofit Organization,Education & Training Services,Education,$100 to $500 million (USD)
157,Data Engineer,$88K - $120K (Glassdoor est.),"Data Engineer
Engineering
REMOTE OPPORTUNITY

About Catalytic Data Science (CDS):
Catalytic Data Science is a groundbreaking cloud R&D platform designed to integrate the volumes of scientific resources, data, and analytic tools while providing the ability to network with colleagues in one secure and scalable environment. By enabling R&D teams to work more collaboratively and improving productivity company-wide, the Catalytic platform helps teams achieve key R&D milestones faster and with greater accuracy. Our customers are passionate about making the world a better place, and we are inspired by the opportunity to help them.

The Role:
You are a Data Engineer with experience in processing terabytes of data. You have experience in creating and automating scalable, fault-tolerant and reproducible data pipelines using Amazon AWS technologies. You are interested in helping to create a platform completely built on top of AWS. You are eager to join a team of Life Scientists and Software Engineers that believe the brightest minds in research should have the best tools to drive innovation.

What You’ll Do:

Build & operate automated ETL pipelines that process terabytes of text data nightly
Develop service frontends around our various backend datastores (AWS Aurora MySQL, Elasticsearch, S3)
Perform technical analyses and requirements specification with our product team on data service integrations
Help customers bring their data to the platform

What You Know:

Must Haves:

Python 3 or Java programming experience, preferably both
Day-to-day experience using AWS technologies such as Lambda, ECS Fargate, SQS, & SNS
Experience building and operating cloud-native data pipelines
Experience extracting, processing, storing, and querying of petabyte-scale datasets
Familiarity with building and using containers
Familiarity with event-based microservices

Nice-to-Haves:

Prior experience with Elasticsearch (custom development and/or administration) is a huge plus
Prior work with text and natural-language processing
Knowledge of Graph databases

What do we love in team members?

Your specialization is less important than your ability to learn fast and adapt to shifting technologies. We’re especially fond of people who:

Focus on customer’s needs and our company’s goals, not just writing code
Iterate until customers love what you’ve built
Self-start and initiate
Self-organize
Strive to grow personally and professionally, beyond just expanding technical abilities
Love to experiment with new technology and share knowledge with the team

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.",4.2,"Catalytic Data Science
4.2","Boston, MA",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
158,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1,Radiant System,Remote,-1,-1,-1,-1,-1,-1
159,Data Engineer,$78K - $112K (Glassdoor est.),"Translate business requirements and functional specifications into logical program designs and to deliver code modules, stable application systems, and software solutions.
Develop, configure, or modify complex integrated business and/or enterprise application solutions within various computing environments.
Implement and maintain complex business and enterprise software solutions to ensure successful deployment of released applications.
Translate complex cross-functional business requirements and functional specifications into logical program designs, code modules, stable application systems, and software solutions.
Partner with Product Team to understand business needs and functional specifications. .
Collaborate with cross-functional teams to ensure specifications are converted into flexible, scalable, and maintainable solution designs.
Evaluate project deliverables to ensure they meet specifications and architectural standards.
Coordinate, execute, and participate in component integration (CIT) scenarios, systems integration testing (SIT), and user acceptance testing (UAT) to identify application errors and to ensure quality software deployment.
Participate in all software development end-to-end product lifecycle phases by applying and sharing an in-depth understanding of complex company and industry methodologies, policies, standards, and controls.
Architect software design patterns and approaches, application level software architecture and make technical trade-off decisions at application level.
Automate and simplify team development, test, and operations processes.
Develop detailed architecture plans for large scale enterprise architecture projects and drives the plans to fruition.
Solve complex architecture/design and business problems; solutions are extensible; work to simplify, optimize, remove bottlenecks, etc.
Minimum Education Required:- All the responsibilities mentioned above are in line with the professional background and requires an absolute minimum of bachelor’s degree in computer science, computer information systems, information technology or a combination of education and experience equating to the U.S. equivalent of a bachelor's degree in one of the aforementioned subjects.",4.9,"Wise Equation Solutions
4.9","Mooresville, NC",51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
160,Data Engineer,-1,"We Breathe Life Into Data
At Komodo Health, our mission is to reduce the global burden of disease. And we believe that smarter use of data is essential to this mission. That's why we built the Healthcare Map — the industry's largest, most complete, precise view of the U.S. healthcare system — by combining de-identified, real-world patient data with innovative algorithms and decades of clinical experience. The Healthcare Map serves as our foundation for a powerful suite of software applications, helping us answer healthcare's most complex questions for our partners. Across the healthcare ecosystem, we're helping our clients unlock critical insights to track detailed patient behaviors and treatment patterns, identify gaps in care, address unmet patient needs, and reduce the global burden of disease.
As we pursue these goals, it remains essential to us that we stay grounded in our values: be awesome, seek growth, deliver ""wow,"" and enjoy the ride. At Komodo, you will be joining a team of ambitious, supportive Dragons with diverse backgrounds but a shared passion to deliver on our mission to reduce the burden of disease — and enjoy the journey along the way.
The Opportunity at Komodo Health
Komodo Health leverages the latest data engineering technology such as Spark, Airflow, and Snowflake to tackle some of healthcare's biggest challenges by transforming extraordinary amounts of data into rich and meaningful insights. As a Data Engineer on the Platform team, you will help lead the development of Komodo Health's platform-enabled workflow tools. The Komodo platform powers all of Komodo's current and future workflow analytical applications and enables 3rd party builders to integrate with, extend, customize, or build on the platform.
Reporting directly to the Engineering Manager, you will be solving complex data challenges while designing and implementing data processing and transformation at a scale that powers state-of-the-art interactive product experiences. You will enable smarter, more innovative uses of healthcare data by building robust data pipelines and implementing data best practices.
Looking back on your first 12 months at Komodo Health, you will have…
Worked on foundational pieces of our data platform architecture, pipelines, analytics, and services underlying our platform
Partnered with Engineering team members, Product Managers, Data Scientists, and customer-facing teams to understand and deliver python packages as well as web-based python services
Designed and developed reliable data pipelines that transform data at scale, orchestrated jobs via Airflow, using SQL and Python in Snowflake and/or Spark
Helped implement continuous improvements to our data governance practices and implemented data quality improvements
Implemented technical enhancements to our CI/CD processes and/or built tooling to ensure data consistency and quality
Gained an understanding of the broader Komodo Health data landscape and being part of architectural decisions for the Healthcare Analytics and Platform as a Service offerings
What you bring to Komodo:
Expertise in writing enterprise-level code and contributing to large data pipelining and API processing with Python
Experience with SQL and query design on large, complex datasets
Ability to use a variety of relational, NoSQL, Postgres, and/or MPP databases (ideally Snowflake on AWS) and leading data modeling, schema design, and data storage best practices
Demonstrated proficiency in designing and developing with distributed data processing platforms like Spark and pipeline orchestration tools like Airflow
A thirst for knowledge, willingness to learn, and a growth-oriented mindset
Committed to fostering an inclusive environment where your teammates feel motivated to succeed
Experience enhancing CI/CD build tooling in a containerized environment, from deployment pipelines (Jenkins, etc), infrastructure as code (Terraform, Cloudformation), and configuration management via Docker and Kubernetes
Excellent cross-team communication and collaboration skills
#LI-Remote
Compensation at Komodo Health
We are committed to providing competitive compensation for all roles at Komodo Health. We carefully consider multiple factors when determining compensation, including your skills, experience, and location while balancing internal equity relative to peers at the company.
The targeted base salary range for the this role is listed below and is accompanied by a competitive bonus and equity package.
$123,900—$145,800 USD
Where You'll Work
Komodo Health has a hybrid work model; we recognize the power of choice and importance of flexibility for the well-being of both our company and our individual Dragons. Roles may be completely remote based anywhere in the country listed, remote but based in a specific region, or local (commuting distance) to one of our hubs in San Francisco, New York City, or Chicago with remote work options.
What We Offer
On top of our commitment to providing competitive, fair pay for all roles at Komodo Health, we're proud to offer robust and inclusive benefits to all Dragons at Komodo Health. We offer global time off programs, extensive internal and external career development and learning opportunities, multiple affinity groups celebrating our team's diversity, and an annual wellness and productivity stipend to support you in being your healthiest, best self.
Equal Opportunity Statement
Komodo Health provides equal employment opportunities to all applicants and employees. We prohibit discrimination and harassment of any type with regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.",3.9,"Komodo Health
3.9",Remote,501 to 1000 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
161,Data Engineer,Employer Provided Salary:$100K - $145K,"Garner's mission is to transform the healthcare economy, delivering high quality and affordable care for all. By helping employers restructure their healthcare benefit to provide clear incentives and data-driven insights, we direct employees to higher quality and lower cost healthcare providers. The result is that patients get better health outcomes while doctors are rewarded for practicing well, not performing more procedures. We are backed by top-tier venture capital firms, are growing rapidly and looking to expand our team.
We are looking for a Data Engineer to support our technical teams by ensuring ease of access to data within our organization. The ideal candidate for this role will have strong technical skills, including Python, SQL, and AWS as well as a desire to be a hands-on contributor to building out a data platform from the ground up.
Main Responsibilities:
Build the data pipelines that power our business
Collaborate across disciplines to high-quality datasets
Protect our users' privacy and security through best practices
Support data pipelines in production
Our Tools:
Python, AWS, Snowflake, dbt, Terraform, Postgres
The ideal candidate has:
2+ years of experience building data pipelines in a fast-paced environment
Strong Python knowledge
Experience with Big Data technologies such as Snowflake, RedShift, BigQuery, or DataBricks
Ability to think in principles and frameworks to understand and decompose abstract problems
An aptitude to learn new technologies and tools quickly
Why You Should Join Our Team:
You are mission-driven and want to work at a company that can change the healthcare system
You want to be on a small, fast-paced team that nimbly moves to meet new challenges
You love ideating on new features and working with data to find new insights
You're excited about researching and working with the latest tools and technologies
The salary range for this position is $100,000 - $145,000. Compensation for this role will depend on a variety of factors including qualifications, skills and applicable laws. In addition to base compensation this role is eligible to participate in our equity incentive and competitive benefits plans.

Garner Health is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics.
Garner Health is committed to providing accommodations for qualified individuals with disabilities in our recruiting process. If you need assistance or an accommodation due to a disability, you may contact us at talent@getgarner.com.",3.5,"Garner Health
3.5","Dallas, TX",51 to 200 Employees,2019,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
162,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
163,Sr. Data Engineer,$83K - $115K (Glassdoor est.),"Provides application design, analysis, programming, testing and documentation for the development, integration, enhancement, and maintenance of information systems solutions
Job Description:
BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Job Responsibilities
Formulates and defines system scope and objectives through research and fact-finding to design, develop, modify, or integrate complex information systems.
Devises or modifies application systems and procedures to optimize functional requirements including capacity, operating time, response time, and form of desired results.
Designs, codes, tests, debugs, and documents programs, subroutines, and scripts.
Develops and implements program/system test plans. Devises data verification methods and standard system procedures.
Responds to system failures and performance events by taking appropriate measures to reduce system downtime and eliminate recurrence of problems.
May conduct research on software and hardware products to justify recommendations and support management in budgeting and purchasing efforts.
Acts as expert technical resource to development staff in all phases of the development and implementation process.
Job Qualifications
Education
Bachelors Degree in Computer Science or equivalent work experience required. Equivalent years of experience are determined as one year of technical experience for every year of college requested.
Experience
1 year - Experience with information technology concepts, application development methodology, terminology, and standards required
5 years - Systems and programming experience required
1 year - Expert working knowledge of at least one programming language in use at BCBST.
Skills\Certifications
Proven skill with information technology in use at BCBST is required
Proven analysis, design, and coding skills, and demonstrated success in leading large and complex projects
Top level technical experience in one or more specialized areas of applications systems analysis and programming
Demonstrated ability to interpret and translate technical and/or or complex concepts into information meaningful to project team members and/or business personnel.
Be organized, reliable, and able to manage multiple tasks with exceptional work ethic.
Leadership skills
Must be able to communicate effectively with both technical and non-technical co-workers
Job Specific Requirements:
Preference for candidates with Cloud experience (GCP specifically)
Python knowledge/experience a plus
Preferred Skills:
Number of Openings Available:
1
Worker Type:
Employee
Worker Sub-Type:
Employee
Company:
BCBST BlueCross BlueShield of Tennessee, Inc.
Applying for this job indicates your acknowledgement and understanding of the following statements:
BCBST is an Equal Opportunity employer (EEO), and all employees and applicants will be entitled to equal employment opportunities when employment decisions are made. BCBST will take affirmative action to recruit, hire, train and promote individuals in all job classifications without regard to race, religion, color, age, sex, national origin, citizenship, pregnancy, veteran status, sexual orientation, physical or mental disability, gender identity, or any other characteristic protected by applicable law.
Further information regarding BCBST's EEO Policies/Notices may be found by reviewing the following page:
BCBST's EEO Policies/Notices
BlueCross BlueShield of Tennessee is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at BlueCross BlueShield of Tennessee via-email, the Internet or any other method without a valid, written Direct Placement Agreement in place for this position from BlueCross BlueShield of Tennessee HR/Talent Acquisition will not be considered. No fee will be paid in the event the applicant is hired by BlueCross BlueShield of Tennessee as a result of the referral or through other means.
All applicants will be advised that BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Tobacco-Free Hiring Statement
To further our mission of peace of mind through better health, effective 2017, BlueCross BlueShield of Tennessee and its subsidiaries no longer hire individuals who use tobacco or nicotine products (including but not limited to cigarettes, cigars, pipe tobacco, snuff, chewing tobacco, gum, patch, lozenges and electronic or smokeless cigarettes) in any form in Tennessee and where state law permits. A tobacco or nicotine free hiring practice is part of an effort to combat serious diseases, as well as to promote health and wellness for our employees and our community. All offers of employment will be contingent upon passing a background check which includes an illegal drug and tobacco/nicotine test. An individual whose post offer screening result is positive for illegal drugs or tobacco/nicotine and/or whose background check is verified to be unsatisfactory, will be disqualified from employment, the job offer will be withdrawn, and they may be disqualified from applying for employment for six (6) months from the date of the post offer screening results.
Resources to help individuals discontinue the use of tobacco/nicotine products include smokefree.gov or 1-800-QUIT-NOW.",3.8,"BlueCross BlueShield of Tennessee
3.8","Chattanooga, TN",5001 to 10000 Employees,1945,Nonprofit Organization,Insurance Carriers,Insurance,$5 to $10 billion (USD)
164,Data Engineer,$71K - $99K (Glassdoor est.),"Data Engineer – Role Description & Requirements
Data is absolutely the lifeblood of ADVANA and our data engineers are the doctors administering it! Our data engineers support data collection, ingestion, validation, and loading of optimized data in the appropriate data stores. They work on a team made up of analyst(s), developer(s), data scientist(s), and a product lead, and everyone on the team collaborates in support of a specific mission. Working directly with the analyst(s) and the product lead, the data engineer identifies and implements solutions for the data requirements, including building pipelines to collect data from disparate, external sources, implementing rules to validate that expected data is received, cleansed, transformed, massaged and in an optimized output format for the data store. The Data Engineer performs validation and analytics corresponding with client requirements and evolves solutions through automation, optimizing performance with minimal human involvement. As pipelines are executed, the data engineer monitors their status, performance, and troubleshoots issues while working on improvements to ensure the solution is the very best version to address the customer's need.
As a Data Engineer, this role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis. The best athlete candidate for this position will be able to apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems. This candidate is able to work without considerable direction and may mentor or supervise other team members.
What we're looking for:
Someone with a solid background developing solutions for high volume, low latency applications and can operate in a fast-paced, highly collaborative environment.
A candidate with distributed computer understanding and experience with SQL, Spark, ETL.
A person who appreciates the opportunity to be independent, creative and challenged.
An individual with a curious mind, passionate about solving problems quickly and bringing innovative ideas to the table.
Basic Qualifications:
4+ years of experience with SQL
4+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets.
4+ years of experience with a modern programming language such as Python or Java
4 years of experience working in a big data and cloud environment
Secret Clearance or higher
Additional Qualifications:
2 years of experience working in an agile development environment
Ability to quickly learn technical concepts and communicate with multiple functional groups
Ability to display a positive, can-do attitude to solve the challenges of tomorrow
Possession of excellent verbal and written communication skills
Preferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes.",4.0,"Gray Tier Technologies
4.0","Crystal City, VA",1 to 50 Employees,-1,Company - Private,Enterprise Software & Network Solutions,Information Technology,$1 to $5 million (USD)
165,Data Engineer,-1,"Description:
Who is Leafwell
Leafwell is a rapidly growing technology and data company that set out to increase access, education, and research into cannabis and to advance its application as medicine.
An exciting opportunity for a Data Engineer to join our growing team has arisen.
What to Expect as a Data Engineer at Leafwell
As a Data Engineer at Leafwell, you will be in charge of creating and orchestrating the Leafwell data pipeline, which means gathering data, creating & automating data transformations and producing actionable insights for Leafwell’s internal stakeholders. You will support critical testing and rollout of new data features. The Data Engineer will build and maintain systems that inform Leafwell’s business stakeholders about Key Performance Indicators (KPIs) and suggest data-driven strategies to optimize those metrics.
Essential Duties and Responsibilities
The Data Engineer will perform the following responsibilities:
Acquire, assemble, transform and analyze data
Create, manage and orchestrate the data pipeline and it’s infrastructure
Present findings, trends, and suggested optimizations
Identify new opportunities and threats to the company's business model
Update and revise reports, queries, and analytic procedures as necessary
Design and implement tracking so that optimization efforts can be measured
Identify inefficiencies in data processes and automate where appropriate
Write and update international SOPs and internal documentation
Support IT systems management with testing, validation, and user support
Proactively identify initiatives for data-related improvements
Why Leafwell
At Leafwell, we are passionate about our work and seek out employees who contribute the same level of dedication and enthusiasm. We are only as good as the people we hire, so we aim to be the best employer in order to attract the top talent in the industry.
Do we have your Interest?
Requirements:
Our Ideal Candidate
Our Ideal Candidate will possess the following:
Bachelor's degree in Computer Science, Mathematics, Economics, Information Systems, or another quantitative field
3-5 years of relevant professional experience in Data Engineering / Analytics
Advance working knowledge and experience in SQL and relational databases
Strong data management abilities, including data analysis, standardization, cleansing, querying, and consolidation of data
Data visualization experience (e.g Tableau, Looker, etc.)
Exercises self-reliance, initiative, and leadership while keeping stakeholders properly informed and involved
Reliably manage numerous duties during a workday, necessitating interactions with people located across the world
Technically competent, with the ability to quickly learn new processes and programs, and utilize various software applications
Excellent communication and interpersonal skills; ability to work with and appeal to a wide variety of personalities and professional tendencies
Bonus points if you have experience in the following:
Cannabis knowledge; industry experience is a plus
Used Data Orchestration tools like Dagster or Airflow
Used dbt or comparable data transformation software
Git
Python or R programming
Amazon Web Services
PostgreSQL and RedShift
CRM products
Data Visualization in Metabase
Effective project management skills and comfort utilizing a project management platform in collaboration with other team members
Data Science projects
Benefits Highlights
Our benefits include, but are not limited to:
Remote first - Most of our employees are 100% remote. Positions requiring travel enjoy a hybrid environment.
Rapid Growth - Our company is rapidly expanding, and our employees are advancing with us!
Training and Development - We foster a culture of learning. We encourage our employees to take part in educational, training, and development opportunities.
Leafwell is dedicated to bringing together people from diverse cultures and perspectives. We work hard to provide a welcoming atmosphere where everyone may flourish, have a sense of belonging, and collaborate effectively. As an equal opportunity employer, we do not tolerate any illegal discrimination against job applicants based on their race, color, religion, veteran status, sex, parental status, gender identity or expression, transgender status, sexual orientation, national origin, age, disability, or genetic information. We are committed to going above and above in creating diversity within our firm and follow the regulations upheld by the EEOC.",4.0,"LEAFWELL
4.0",Remote,51 to 200 Employees,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
166,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
167,AWS Data Engineer Snowflake,-1,"Company Description

Orchestrated by adept technical architects with over fifty years of applied expertise, KYNITE is an advanced technology company specializing in the disciplines of: Blockchain, Cloud Services, Big Data & Analytics, Artificial Intelligence, Enterprise, Staff Augmentation and Managed Services
We are BigData Experts
We are Cloud Experts
We are Enterprise Architects
We are Artificial Intelligence Innovators
We are Technological Evangelists
We are Doers
We are Kynite

Additional Information

All your
This job is only for individuals residing in US
US Citizens, Green Card holders, EAD's can apply
W2, C2c,
Information will be kept confidential according to EEO guidelines.",-1,Kynite,"Myrtle Point, OR",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
168,Data Engineer,$79K - $108K (Glassdoor est.),"Role Description
Position Title: Data Engineer
Manager: Sr. Manager, Enterprise Solution
Position Location: Rehrig Technology Center, TX
Manager Once Removed: Director - IT & Security
Level of Work: Individual Contributor
Date Prepared: 08/09/2023
Brief Role Description
The Data Engineer will be responsible for developing and managing the enterprise overall data architecture, which includes the data models, data warehouses, data pipelines, and other systems that will be used to store, manage, and analyze data.
This role also requires a fair amount of knowledge on designing, implementing, and maintaining Microsoft SQL Servers, Data analytics using Power BI and azure data pipeline and implementing security and working with stakeholders to understand their data needs and deliver solutions that meet those needs.
Accountabilities
Database Management:
Design, create, and maintain optimal database systems for various applications and services.
Monitor and optimize database performance, security, and integrity.
Handle database backups, recovery, and regular updates to ensure data accuracy and availability.
Collaborate with IT teams to maintain server infrastructure and ensure database reliability.
Configuring, solving problems, and supporting SQL Servers in production and non-production environments running in cloud environments and Windows operating systems
Develop and maintain a disaster recovery plan to measure ensure that critical data systems are available in the event of outages.
Develop and maintain a capacity plan to measure utilization and plan for growth in demand for database resources.
Design scripts and utilities to automate day to day database administration.
Assist with the monitoring performance analysis of existing applications and ensuring that the system is consistent with ongoing need
Create technical documentation, operational procedures, Incident, and Problem communications and reporting.
Quickly restore service during system outages, provide troubleshooting support to team members, perform proactive problem prevention, and advance technical details to technical lead and/or to vendor support
Maintaining, detail and deliver Microsoft SQL 2012+ in accordance with certified specifications and security guidelines.
Data Engineering:
Design and construct scalable, maintainable, and efficient data pipelines.
Implement and manage ETL processes, ensuring the accuracy, reliability, and timeliness of data processes.
Design and maintain enterprise level data model and data flow
Cloud and Integration:
Design and implement solutions in Azure, with a particular focus on Azure data pipeline.
Integrate Power BI solutions, ensuring that data sources are accessible and optimized for visualization.
Stay updated with the latest Azure features and Power BI advancements to recommend and implement improvements.
Collaboration & Training:
Collaborate with cross-functional teams, assisting with data-related technical issues, and ensuring optimal data delivery architecture.
Provide training and support to junior team members and stakeholders on database management and data engineering best practices.
Qualifications
BS in Information Technology or a related technical field plus 4-6 or more years related overall experience.
3-5 years of experience in data engineering
Strong understanding of data engineering principles and practices
Experience with Power BI and Azure Data Pipeline
Experience with SQL, Python, and other programming languages
Strong expertise in Azure data pipeline, including Azure Data Factory, Azure Data Lake, and other Azure data services.
Proficiency in Power BI, including designing and deploying insightful dashboards and data visualizations.
Strong knowledge of SQL and MS SQL databases, including optimization techniques.
Excellent understanding of data pipeline and workflow management tools.
Strong analytical skills with a problem-solving attitude.
Good interpersonal and communication skills, with the ability to collaborate effectively across diverse teams.
Certifications in Cloud and/or Microsoft SQL Server highly desired
A minimum of 3 years of experience in on-premises and cloud-based Database Technologies - AWS RDS and/or Azure SQL PAAS
Devise Strategy and Engage with IT leadership and business partners to define a Database & Analytics strategy for Rehrig
Implement automation of database operations like upgrades and patching
Demonstrated Experience in Defining and Implementing Production Proactive Health Monitoring of Database Technologies
Strong Incident Root Cause Analysis on Database Execution and Application Interaction with Database Technology
Experience with Data migration and SQL upgrades across Microsoft Server versions
Ability to travel approximately 15%-20% of the time
General Responsibilities
Provide periodic reporting on workforce capacity and utilization.
Provide periodic reporting on the availability and health of software development process.
Other duties as assigned.",3.8,"Rehrig Pacific Company
3.8","Richardson, TX",501 to 1000 Employees,1913,Company - Private,Machinery Manufacturing,Manufacturing,$100 to $500 million (USD)
169,Data Engineer,-1,"Remote
Contract
Opened 4 months ago
Job Description
Data Engineer (with Healthcare experience) Required Skills: SQL Databricks data engineering Snowflake data engineering QA experience for ETL experienced with data acquisition and ingestion using API and/or batch channels Experience with Healthcare",3.3,"Crackajack Solutions
3.3",Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
170,"Data Engineer, Election Platforms (all-levels)",-1,"Job Description
The Washington Post is hiring a data engineer for our Election Platforms team, to work on our data pipelines, tools, and infrastructure ahead of the 2024 presidential election.
The Election Platforms team focuses on leveraging data, partnering with stakeholders, and pioneering serviceable tools to facilitate The Post’s election coverage. This role is part of Elections Engineering, a cross-functional group of full-stack engineers and data scientists, with varied technical and academic backgrounds, responsible for delivering live election results across The Post’s platforms.
The role will be focused on contributing to our technical stack and developing new technology to facilitate robust and lively election coverage. In particular, we’re looking for candidates who would be comfortable working on deploying high-impact code in production and have an interest in data and newsroom tooling. We also work on other data engineering projects related to elections, such as voter registration data and campaign finance, and occasionally write
open
source
software
. Most of our work is in Python and TypeScript/React/Node.
We typically work at a higher level of abstraction than a single story, and aim to create general purpose tools that are reusable and easy to deploy. Our ideal candidates are creative problem-solvers, who are comfortable writing production level code to solve real-world newsroom problems. This position is remote-friendly for work in the US.
Motivation
You have a desire to contribute to The Post’s journalism through technology.
You are passionate about refining and improving team process, culture, documentation and communication while tackling complex data science and engineering challenges.
You want to work at a deeper level of abstraction than a single story or beat.
You are excited about learning new technologies and applying them to the practice of journalism.
Responsibilities
Work on the software components of our live election architecture including deployment, observability, participating in code reviews, and more.
Work closely with our engineers and data scientists to develop and maintain pipelines and administrative apps/dashboards for election data.
Work on tooling to make data preparation easier and more efficient for our live election model.
Work with newsroom stakeholders to assess needs, build useful and reusable tooling, and contribute to storytelling.
Foster effective team collaboration and communication by improving and documenting internal tools, processes and guidelines.
Assist in attracting and hiring team members who want to work at the intersection of journalism and technology by actively participating in recruiting – and contributing feedback on our overall hiring process.
Qualifications
A minimum of one year of experience building and maintaining backend and data engineering applications.
Proficiency across some of the languages and frameworks used in our engineering stack such as Python, TypeScript/React/Node, and AWS services.
A desire to work on cutting edge, cross-functional tools to help with features, data science, and machine learning/big data work in a team setting.
Experience with election data is not necessary, but is a plus.
Experience developing and deploying newsroom tools and applications is not necessary, but is a plus.
Levels
If you have less than three years, but at least one year, we’d love to evaluate you for a data engineer role.
If you have more than three years but less than about seven years, we’d love to evaluate you for a senior data engineer role.
If you have more than about seven years, we’d love to talk to you about a principal data engineer role.
Wherever you are in your life or career, The Washington Post offers comprehensive and inclusive benefits for every step of your journey:
Competitive medical, dental and vision coverage
Company-paid pension and 401(k) match
Three weeks of vacation and up to three weeks of paid sick leave
Nine paid holidays and two personal days
20 weeks paid parental leave for any new parent
Robust mental health resources
Backup care and caregiver concierge services
Gender affirming services
Pet insurance
Free Post digital subscription
Leadership and career development programs
Benefits may vary based on the job, full-time or part-time schedule, location, and collectively bargained status.
The Post strives to provide its readers with high-quality, trustworthy news and information while constantly innovating. That mission is best served by a diverse, multi-generational workforce with varied life experiences and perspectives. All cultures and backgrounds are welcomed.
The innovation doesn’t end in the Newsroom – dozens of teams power The Washington Post. We are now hiring the next innovator – how will you Impact Tomorrow?
#washpostlife",4.2,"The Washington Post
4.2","Washington, DC",1001 to 5000 Employees,1877,Company - Private,Publishing,Media & Communication,Unknown / Non-Applicable
171,Data Engineer,Employer Provided Salary:$80K - $85K,"Simatree is searching for a Data Engineer Consultant to support our client’s operational requirements on a mission focused program within a challenging, dynamic setting. Successful candidates in this position will collaborate with a diverse group of individuals across the homeland security community and directly contribute to management and analysis of operational data using data science tools. This role supports a US Government contract. An US Citizenship is required.
The Role
The Data Engineer will work alongside a team to generate management tools that effectively control programs and generate reports while supporting the programmatic development of operational applications of the tools or analysis that are focused on operations research and document findings. They will conduct research into management, organizational, and business practices to implement improvements to existing processes. They will utilize commercial or open-source software to manage tasks and present findings to a broad audience, including agency executives. They will maintain a database to produce monthly financial reports, tracks processes, distribute deliverables, and monitor comment status. The Data Engineer will be required to use Microsoft Office to perform queries, analyses, and produce reports. Other responsibilities include:
Use R and Python to perform analyses and develop new tools using popular data visualization libraries such as ggplot2, plotly, matplotlib, or bokeh.
Use Databricks to Extract, Transform, and Load (ETL) data from different sources and transform it into a usable, reliable resource.
Manage data collection and transformation from federal law enforcement and other databases to be used in a wide spectrum of analyses including statistical modeling, performance metrics analysis, crosschecking data with other federal agencies, and risk analysis.
Use ArcGIS to perform analyses and develop new web map applications that summarize results from deep learning/AI algorithms.
Maintain and enhance business intelligence (BI) dashboards using best practices in UI/UX design
Develop and test new methodologies and algorithms which would increase the efficiency and accuracy of existing models, including pilot deployments of automated scripts
Requirements
This role supports a US Government contract. A US citizenship is required.
Strong Experience with SQL programming
Experience with R statistical programming and Python programming
Experience with BI software such as Qlik, Tableau or PowerBI
Demonstrated experience in performing data analysis and modeling on an independent basis
Demonstrated experience in developing and applying quantitative methods to find patterns and relationships in large data sets using statistical and graphical packages
Ability to translate complex, technical, or analytic findings into an easily understood narrative—tell a story with graphical, verbal or written form
3 years experience in supporting quantitative and analytical efforts
Bachelors degree in relevant fields such as statistics, mathematics, computer science, physical science, economics or engineering
Job Type: Full-time
Pay: $80,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Experience level:
1 year
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This question MUST be answered for consideration: This position supports a US Government contract which requires a US Citizenship. Are you a US Citizen?
Work Location: Hybrid remote in Washington, DC 20001",5.0,"Simatree
5.0","Washington, DC",1 to 50 Employees,2018,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
172,"Data Engineer, Sales",Employer Provided Salary:$161K - $220K,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Software Engineering
Job Details
About Salesforce
We’re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good – you’ve come to the right place.
Salesforce, Inc. seeks Data Engineer, Sales in San Francisco, CA:
Job Duties: Architect, design, implement and maintain data pipelines and data models enabling actional insights for Slack’s Sales and Corporate Systems key partners, data scientists, and data decision makers in the organization such as Sales, Customer Success, Marketing, and Finance. It requires the candidate to utilizing knowledge and expertise in Data Integration, Data Modeling, Optimization and Data Quality. Work cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data Warehouse model, as well as guaranteed compliance with data governance and data security requirements while creating, improving, and operationalizing these integrated and reusable data pipelines. This would enable faster data access, integrated data reuse and vastly improved time-to-solution for Slack’s data and analytics initiatives. Partner with Data Engineers, Data architects, domain experts, data analysts and other teams to build foundational data sets that are trusted, well understood, aligned with business strategy, and enable self-service. Work on the overall strategy for data governance, security, privacy, quality, and retention that will satisfy business policies and requirements. Own and document data pipelines and data lineage, as well as identify, document, and promote best practices of Data Engineering at Slack. ˆHQ address additionally encompasses the following Salesforce locations in San Francisco: 350 Mission Street, 415 Mission Street, and 50 Fremont Street. The permanent position may be offered at any of these locations in San Francisco. Fulltime telecommuting permitted.
Minimum Requirements: Master's degree, or foreign equivalent, in Computer Science, Information Technology and Data Science for Business Analytics, Engineering (any) or a related quantitative discipline and three (3) years of work experience in the job offered or in data-engineering-related or business-analytics-related position.
A related technical degree required (Computer Science, Engineering (any field)).
Special Skill Requirements: (1) Advanced SQL Programming; (2) Python; (3) Data Modeling; (4) Database & Data Warehouse Design; (5) ETL Development using tools such as Airflow, Alooma, Stitch, Segment; (6) Data Warehouses such as Redshift, Snowflake, MySQL; (7) AWS Services like S3, Aurora, DMS; (8) Dashboard/Report Development using Looker, Tableau; (9) Sales & Marketing platforms such as Salesforce Marketing cloud, Iterable, Swrve, Branch; (10) ERP and E-commerce platforms such as Netsuite, Shopify. Any suitable combination of education, training and/or experience is acceptable. Education, experience and criminal background checks will be conducted. Full-time telecommuting permitted.
Salary: $161,491.00 - $220,000.00 per annum.
Submit a resume using the apply button on this posting or by email at: onlinejobpostings@salesforce.com at Job #21-8965. Salesforce is an Equal Opportunity & Affirmative Action Employer. Education, experience, and criminal background checks will be conducted.
#LI-DNI
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form .
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com .
Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce .
Salesforce welcomes all.
Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, Salesforce will consider for employment qualified applicants with arrest and conviction records.
For New York-based roles, the base salary hiring range for this position is $161,491 to $220,000.
For Colorado-based roles, the base salary hiring range for this position is $161,491 to $220,000.
For Washington-based roles, the base salary hiring range for this position is $161,491 to $220,000.
For California-based roles, the base salary hiring range for this position is $161,491 to $220,000.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.",4.1,"Salesforce
4.1","San Francisco, CA",10000+ Employees,1999,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
173,Data Engineer,Employer Provided Salary:$150K,"Data Engineer
We are looking for a Data Engineer that will architect and implement systems handling the ingestion, cleaning, and structuring of data. They will be a member of the Data Science team and will have direct interaction with the people using the data ingested. The hire will also be responsible for managing our group’s core infrastructure including our region redundant pipeline orchestration servers (Airflow) and webserver stack (NGINX + Gunicorn + Django). The ideal candidate is an experienced data pipeline builder and core infrastructure guru.
Responsibilities:
Create and maintain an optimal data pipelines in Python + SQL
Build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python and SQL (Snowflake & SQL Server) in both on premise and cloud environments
Manage and improve the user facing server infrastructure (web + api) including authorization and load balancing
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that that provide actionable insights on both the user interactions and infrastructure loads
Technical Qualifications:
Python expert, notably savvy with data science stack (Pandas, NumPy, SciPy)
Relational SQL databases, Microsoft SQL Server and Postgres preferred
Snowflake cloud database experience
Pipeline and workflow management tools: Airflow, Luigi, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Athena, SQS, etc.
2+ professional years’ experience with Masters or 3+ professional years with Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Highly Valued:
Data Science/Analysis background
Computer science understanding, data structures, processes, threading, memory usage
Unix/Linux command-line experience
Broad understanding of equities, derivatives, futures, FX, or other financial-services instruments
Other Qualifications:
Excellent listening, and communication (both oral and written) skills
Self-starter and critical thinker, takes ownership of own projects and makes improvement suggestions for the entire infrastructure.
Proactive, assertive and attentive to details.
Can work independently and in a collaborative environment.
Can handle several projects with different priorities at the same time in a fast-paced environment.
Excellent self-management and problem-solving skills.
Results-oriented, can deliver quality code quickly
Millennium pays a total compensation package which includes a base salary, discretionary performance bonus, and a comprehensive benefits package. The estimated base salary range for this position is $150,000, which is specific to New York and may change in the future. When finalizing an offer, we take into consideration an individual’s experience level and the qualifications they bring to the role to formulate a competitive total compensation package.",3.8,"Millennium Management LLC
3.8","New York, NY",1001 to 5000 Employees,1989,Company - Private,Investment & Asset Management,Financial Services,Unknown / Non-Applicable
174,Quality Engineer Data Specialist,Employer Provided Salary:$148K - $208K,"Company
Federal Reserve Bank of New York
Working at the Federal Reserve Bank of New York positions you at the center of the financial world with a unique perspective on national and international markets and economies. You will work in an environment with a diverse group of experienced professionals to foster and support the safety, soundness, and vitality of our economic and financial systems.

This position will offer workplace flexibility e.g., working remotely or on site as needed/ desired during the week. Employees can expect to be in the office weekly as needed for meetings and team collaboration and should live within a commutable distance. The Bank believes in work flexibility to balance the demands of work and life while also connecting and collaborating with our colleagues in person a couple days a week.
What we do:
The Quality Engineering function within Enterprise Platforms and Capabilities organization is responsible for overall Quality Engineering practices in each squad. The Quality Engineers in the squad collaborate with Developers, Product Owners, and Scrum Master to create a quick feedback loop for Quality through Test Automation and Continuous Testing.
Your role as Quality Engineer Data Specialist:
Collaborating in Agile cross-functional teams with to deliver solutions with high quality across different technical environments in Trading domain.
Testing API, data profiling included end to end starting with data ingestion and all related data tasks. Ability to create analytical data view by gathering data from various sources based on the logical use cases and rules related to trading domain and underlying operations validate data, calculations at various stages including validation of attributes and transformation using data tools technology.
Validating existing user scenarios in old system, performing good debugging/analytical reasoning skills including application log validations.
Data Profiling and test data creation mapping back to the trade related use cases and rules.
Dissecting transformation rules and validating the data migration/onboarding on old versus new system.
Developing test automation and enabling DevOps through continuous testing.
Working knowledge of AWS based data sources such as Aurora Postgres and Athena, Data Glue, Snowflake, Data Bricks and Collibra data quality system to deliver strong Quality solutions.
What we are looking for:
Working experience in Investment bank preferably in auctions and trading domain.
Hands-on experience designing and masking test data based on the user scenarios/rules and building scripts to test data pipelines for data lake/ data warehouse ecosystem.
Data validations using SQL, strong understanding of the relational database concepts and models.
Experience building test automation have used automated frameworks, tools, and strong SQL, data management concepts including data profiling, data mapping and data integration.
Strong debugging/analytical skills including dissecting transformation rules, data inconsistencies, incompleteness of data.
Demonstrated ability of working with AWS services understanding such as Data Glue, Data Bricks, Snowflake, Aurora Postgres, Athena, and Collibra Data Quality Platform.
Experience working in a DevOps environment, SCD and hands on experience using CI/CD systems preferred to enable test data pipeline.
Experience in hands-on Python programming language experience used to build test automation.
Salary Range: $148400 – $207900 / year
We believe in transparency at the NY Fed. This salary range reflects a variety of skills and experiences candidates may bring to the job. We pay individuals along this range based on their unique backgrounds. Whether you’re stretching into the job or are a more seasoned candidate, we aim to pay competitively for your contributions.
Our Touchstone Behaviors—Communicate Authentically, Collaborate Inclusively, Drive Progress, Develop Others, and Take Ownership—help shape the culture of the Bank. They also provide a shared language for how we work together and achieve success, and they set clear expectations for leading with impact at every stage of your career with us.
Learn more.

Benefits:
Our organization offers benefits that are the best fit for you at every stage of your career:
Fully paid Pension plan and 401k with Generous Match
Comprehensive Insurance Plans (Medical, Dental and Vision including Flexible Spending Accounts and HSA)
Subsidized Public Transportation Program
Tuition Assistance Program
Onsite Fitness & Wellness Center
And
more
Please note that the position requires access to confidential supervisory information and/or FOMC information, which is limited to ""Protected Individuals"" as defined in the U.S. federal immigration law. Protected Individuals include, but are not limited to, U.S. citizens, U.S. nationals, and U.S. permanent residents who either are not yet eligible to apply for naturalization or who have applied for naturalization within the requisite timeframe. Candidates who are permanent residents may be eligible for the information access required for this position if they sign a declaration of intent to become a U.S. citizen and pursue a path to citizenship and meet other eligibility requirements.
In addition, all candidates must undergo an enhanced background check, comply with all applicable information handling rules, and will be tested for all controlled substances prohibited by federal law, to include marijuana.
The Federal Reserve Bank of New York is committed to a diverse workforce and to providing equal employment opportunity to all persons without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, age, genetic information, disability, or military service.
This is not necessarily an exhaustive list of all responsibilities, duties, performance standards or requirements, efforts, skills or working conditions associated with the job. While this is intended to be an accurate reflection of the current job, management reserves the right to revise the job or to require that other or different tasks be performed when circumstances change.
Full Time / Part Time
Full time
Regular / Temporary
Regular
Job Exempt (Yes / No)
Yes
Job Category
Information Technology
Work Shift
First (United States of America)
The Federal Reserve Banks believe that diversity and inclusion among our employees is critical to our success as an organization, and we seek to recruit, develop and retain the most talented people from a diverse candidate pool. The Federal Reserve Banks are committed to equal employment opportunity for employees and job applicants in compliance with applicable law and to an environment where employees are valued for their differences.
Privacy Notice",3.7,"Federal Reserve Bank of New York
3.7","New York, NY",1001 to 5000 Employees,1914,Company - Private,Banking & Lending,Financial Services,$10+ billion (USD)
175,Data Engineer,$99K - $140K (Glassdoor est.),"Azurity Pharmaceuticals, Inc. is a fast-growing pharmaceutical company focusing on the needs of patients requiring customized, user-friendly drug formulations, especially children and the elderly. Azurity’s products have benefited millions of patients whose needs are not served by other commercially available therapies. For more information, visit www.azurity.com.
Azurity's success is attributable to our incredibly talented, dedicated team that focuses on benefiting the lives of patients by bringing the best science and commitment to quality into everything that we do.
Mission:
Implement the data models and data structures needed for each use case as defined by the Data Architect, in the most convenient format to be used by the Data Scientist
Own the structural elements of data, e.g., data storage, data piping, interfacing with analytics platforms
Participate in data requirements, modelling and testing
Tasks & responsibilities:
Provide technical support related to data structures, data models and meta data management to relevant stakeholders
Creates data models, providing the right format and structure for the use case solutions
Participate in early data modeling and testing for use case development, provide input on how to improve proposed solutions and implement necessary changes
Extract relevant data to solve analytical problems; ensure development teams have the required data
Interact with the business ([function]) to understand all data requirements to develop business insights and translates them into data structures and data models, in close collaboration with Data Architect
Work closely with IT/IM teams on internal data acquisition (e.g., CRM, ERP) and with Data Architect for external data acquisition
Knowledge & experience:
5+ years’ experience with advanced data management systems (e.g., PostgreSQL, etc.)
Deep expertise in data modeling and structuring
Experience in high volume data environments
Ability to quickly learn new technologies
Developing and maintaining formal documentation that describes data and data structures including data modelling
Strong attention to detail and an ability to think critically and conceptually
Team oriented and flexible with proven track record in collaborating with multiple stakeholders
Strong verbal and written",3.0,"Azurity Pharmaceuticals, Inc.
3.0","Myrtle Point, OR",201 to 500 Employees,1999,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,$100 to $500 million (USD)
176,Data Engineer,$84K - $119K (Glassdoor est.),"Job Description

Proficiency with major tools such as SQL, Python, R, NiFi and Git. Demonstrated experience with ETL, cleaning, management, optimizing performance and processing large volumes of data. Familiarity with industry best-practices for software-hardware optimization when processing large sets of data. Experience with ML, with statistical modeling, time-series forecasting, and/or geospatial analytics. Experience with Hadoop, Spark, or other parallel storage/computing processes ETL programming experience.

Required Skills

Experience with SQL database structures and mapping of data structures

Experience with Nifi working with a parallel storage (ie. Hadoop/Spark or other), and the Extract-Transform-Load processes (ETL)

Data performance optimization-how to optimize analytical workloads

1. Use case and concurrent user volumes (Hardware)
2. Appropriate or transactional or analytical (databases)
3. Data model is optimized for queries and report generation (Data Model)
4. Increase performance by Limit by Date and Use Data Aggregation (Data Volume)

Ability to create operating system level scripts to perform ETL opeations on SQL data bases

Test-driven development of software solutions for the extraction, transacting, and loading of data using efficient languages, e.g. Nifi, Java, Python, SQL, and R.

Demonstrated success in developing software applications through all stages, including requirements generation/gathering, data engineering (data extract/transform/load), web service implementation, and user interface.

Map-Reduce technology experience for data management

Open source framework tools for statistical modeling and statical modeling experience

Experience with link/graph analysis

Work with users to define their data requirements and to assist with ad hoc data analysis.

Troubleshoot complex problems and provide support for software systems and application issues.

Undergraduate or Master’s Degree in Computer Science, Electrical or Computer Engineering or related technical discipline, or the equivalent combination of education, technical training, or work experience.

Tools:

Workflow tools-NiFi, Airflow, Prefect, Apache Flink

NoSQl Databases-ElasticSearch, DynamoDB, Redis

Relational Database Management Systems-PostgresSQL, and MySQL

Data Management technology-EMR, Spark

About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1,The Swift Group,"Chantilly, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
177,Data Engineer- Hadoop,Employer Provided Salary:$63.00 - $66.00 Per Hour,"Position Summary:
Synthetic data engineer is the experienced professional who can bring together skills and technologies to be able design and develop solutions to deliver data required for training models/test applications. Candidate will be working with a team of resources using Python, Sqoop, pyspark to create the data in a most efficient manner. Candidate should have prior experience & knowledge in highly scalable production grade applications with a strong engineering acumen. Should be able to work in a collaborative, fast paced environment. Candidate must have knowledge and/or experience working with Hadoop ecosystem.
Skills:
• Advanced knowledge in Python Programming
• Familiarity with Faker module
• pyspark, Sqoop, Hadoop, Hive, Shell script, Hadoop Shell commands
• Good experience dealing with bulk data extracts and bulk data loads
• Ability to translate requirements from stake holders into the technical requirements and the code
• Experience on statistical distribution, agent to model , deep learning is a plus
• Background of MF environment is a plus
Job Type: Contract
Pay: $63.00 - $66.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
Hadoop: 7 years (Required)
Python: 5 years (Required)
ETL: 3 years (Preferred)
Work Location: Hybrid remote in Charlotte, NC 28203",3.9,"Data Inc
3.9","Charlotte, NC",501 to 1000 Employees,1983,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
178,Data Engineer,$74K - $102K (Glassdoor est.),"Overview:
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.

Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.

Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities:
Position Summary:
The Data Governance/Data Quality Subject Matter Expert (SME) is a professional responsible for designing, implementing, and managing the client’s data governance strategy and ensuring high data quality standards across data-centric operations and decision-making.
The SME is in charge of developing and maintaining robust data pipelines, ensuring data observability, managing data within the Microsoft Azure environment, and implementing data governance principles in alignment with industry best practices and regulatory standards.

Primary Responsibilities and Accountabilities:
Data Governance & Quality: This includes developing and implementing data governance policies and procedures, monitoring data quality, managing data within the Microsoft Azure environment, and ensuring compliance with data privacy regulations.
Data Pipeline & Workflow Development: Designing and implementing efficient and secure data pipelines in Azure Data Factory and workflows, ensuring optimal performance and incorporating data quality principals.
Data Integration & Observability: Designing data integration strategies, integrating data from multiple sources, ensuring data quality and observability through tools like Azure Synapse.
Data Security: Defining and implementing data security policies and procedures, including access control and data encryption within the Azure environment.
Performance Tuning: Optimizing data access, indexing, and query performance within Azure Data Factory to ensure scalable and reliable data infrastructure.
Monitoring & Troubleshooting: Monitoring data systems, troubleshooting issues, identifying performance bottlenecks, and collaborating with team members to resolve problems.
Collaboration & Communication: Working closely with stakeholders to understand their data needs and develop solutions adhering to industry best practices and the needs of the business.
Automation & Tooling: Automating repetitive tasks and maintaining tools necessary for data processing and storage within the Azure environment.
Qualifications:
A Bachelor's degree in a related field such as computer science, mathematics, engineering, or information systems.
3+ years of hands-on experience programming in SQL.
2+ years of experience building and maintaining automated data pipelines and data assets using batch and/or streaming processes within the Microsoft Azure environment.
Experience in Data Governance OR Data Quality Management is a must.
Skills:
Competencies:
Expertise in Microsoft Azure data tools, with a focus on Azure Data Factory and Azure Synapse for data processing, storage, and analysis.
Strong SQL skills and knowledge of data warehousing concepts and ETL (Extract, Transform, Load) processes within the Azure environment.
Proficiency in programming languages such as Python, and experience with big data technologies such as Hadoop, Spark, and Kafka.
Ability to design, build, and maintain data pipelines, with a particular focus on Azure Data Factory.
Strong analytical and problem-solving skills, with the ability to identify and resolve data-related issues effectively.
Experience with data security and privacy regulations, and understanding of HIPAA compliance.
Strong experience with data governance, data quality, and data observability principles.
Excellent written and verbal communication skills, ability to explain complex technical concepts to non-technical stakeholders.
Excellent collaboration and teamwork skills, with the ability to work in a team involving client data engineers, data scientists, and data analysts.
Strong attention to detail and commitment to ensuring high data quality.
Continuous learning mindset and ability to stay up-to-date with new developments and technologies in data governance and data quality
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset",3.5,"Expleo
3.5","Minneapolis, MN",10000+ Employees,1966,Company - Private,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
179,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
180,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
181,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
182,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
183,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
184,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
185,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
186,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
187,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
188,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
189,Sr. Data Engineer,Employer Provided Salary:$118K - $142K,"Data Engineer
Egen Solutions Inc offers competitive compensation, benefits, and an excellent culture of teamwork and collaboration. We are an Equal Opportunity Employer that does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.
Job Summary
We have some major internal projects underway at Egen and need a highly skilled Data Engineer to join our team on a full-time basis. Below is the JD, kindly go through it and let me know your interest and if you are interested revert back with your updated resume
Our Data Engineering teams build scalable data pipelines using Python, Spark, and cloud services (GCP and AWS). The pipelines we build typically integrate with technologies such as Kafka, Storm, and Elasticsearch. We are working on a continuous deployment pipeline that leverages rapid on-demand releases. Our developers work in an agile process to efficiently deliver high value applications and product packages.
As a Staff Data Engineer at Egen, you will leverage Spark and GCP (preferred) to architect and implement cloud-native data pipelines and infrastructure to enable analytics and machine learning on rich datasets.
Required Experience:
Built and run resilient data pipelines in production and have implemented ETL/ELT to load a multi-terabyte enterprise data warehouse.
Implemented analytics applications using multiple database technologies, such as relational, multidimensional (OLAP), key-value, document, or graph.
Defined data contracts, and wrote specifications including REST APIs.
Transformed data between data models and formats with the most modern PySpark practices. Have built cloud-native applications and supporting technologies / patterns / practices including: Cloud Services, Docker, CI/CD, DevOps, and microservices.
Planned and designed artifacts that describe software architectures involving multiple systems and technologies
You've worked in agile environments and are comfortable iterating quickly.
Nice to have's (but not required):
GCP expertise is preferred but will consider AWS
Experience moving trained machine learning models into production data pipelines.
Experience in biotech, genomics, clinical research or precision medicine.
Expert knowledge of relational database modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others.
This is full-time, On-site, and Remote. We are based out in Naperville, IL.
Looking forward to striking up a conversation with you on the opportunity!, Kindly visit our website egen.solutions
equirements
Bachelor’s degree in Computer Science, Engineering, or related field.
3+ years of experience with data engineering and/or data analysis.
Strong analytical skills and ability to think analytically.
Ability to work independently with minimal supervision.
Experience with SQL and/or NoSQL databases is a plus.
Experience with ETL processes is a plus.
Experience with cloud-based solutions such as Azure Data Factory is a plus.
Experience with Agile software development methods such as Scrum is a plus.
Ability to work in a fast-paced environment and adapt to changing priorities.
Ability to work in a team environment while maintaining strong individual initiative.
Job Type: Full-time
Pay: $117,577.88 - $141,599.17 per year
Benefits:
401(k)
Dental insurance
Health insurance
Experience level:
5 years
Application Question(s):
Are you a personal who hold any of these work authorizations currently(OPT EAD/GC/F1 OPT/H4 EAD.
Work Location: In person",-1,Egen Solutions Inc,"Naperville, IL",-1,-1,-1,-1,-1,-1
190,Data Engineer,Employer Provided Salary:$115K - $135K,"**This position is a majority Remote role with occasional in-office meetings on an as-needed basis. Because of this, interested applicants must live within a reasonable driving distance of Symmetry Lending's office in Anaheim, CA.**
Job Description
This role will play a pivotal role in assisting the IT team with the evolution of the company data architecture. In addition to supporting extensions to the data warehouse design, this role will design and develop ETL required to onboard new types of business data. Finally, this role will also provide support for Analytics assignments as needed.
Responsibilities
Display sense of ownership over assigned work, requiring minimal direction and driving to completion in a sometimes fuzzy and uncharted environment.
Designing and building new data pipelines that support business requirements.
Work with IT and other business partners to support the evolution of the enterprise data architecture.
Work with IT to build new ETLs to take data from various operational systems and extend existing data warehouse data model for analytics and reporting.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business constituents.
Support the development of the company BI infrastructure, including the construction of scalable analytic solutions, and on-premises reporting tools.
Utilize Power BI development to build scalable reporting models to serve BI reporting needs.
Demonstrate the ability to gather requirements, extract and manipulate data residing in multiple disparate databases, and articulate solutions to support the business.
Design, develop, and maintain performant data models in Power BI utilizing best practices.
Serve as a subject matter expert for all data warehouse and enterprise systems
Use new or existing technologies to produce analytics solutions (in the form of excel spreadsheets, dashboards, etc.).
Work with IT and Analytics team to architect and build data pipelines to optimize for performance, data quality, scalability, ease of future development, and cost.
Gather requirements, assess gaps and build roadmaps to help the analytics driven organization achieve its goals.
Develop data-related Proof of Concepts in order to demonstrate feasibility and value to Symmetry business constituents.
Qualifications / Requirements
Bachelor’s degree in Information Systems, Computer Science, Finance, or similar education from an accredited college
2-3 years Accounting and Finance background preferred
Strong skillset in Microsoft Excel (2-3 years, financial formulas & VBA is a plus) and familiarity with Microsoft Office Productivity Suite (Excel, Word, etc.)
5+years’ experience with advanced SQL concepts and writing SQL statements (SQL Server or similar).
Prefer 2 years’ experience with Microsoft BI Suite, PowerBI, Power Query, PowerPivot, Reporting Service (SSRS), SharePoint
Experience in mentoring other team members in development best practice, and methodologies.
You are passionate about data quality control and know how and where to anticipate potential errors.
Knowledge of the software development lifecycle, agile methodologies, and structured software development methodologies.
Experience performing analysis with large datasets in a cloud-based environment.
Ability to work effectively with stakeholders at all levels within the organization
Strong communication and time management skills and a self-motivated approach
Ability to work independently, detail-oriented, and execution focused
Highly collaborative and team oriented
Tenacious (doesn’t give up easily)
Genuine passion for clean and reliable data
At least 5 years of work experience
About Symmetry
Symmetry Lending, specializes in providing mortgage fulfillment services to include origination, servicing, and capital markets needs to various Lenders across the country with whom we partner. We have offices in Atlanta, GA, Eden Prairie, MN, Denver, CO, Orlando, FL, and Anaheim, CA, and we do business from coast to coast. We take great pride in building a diverse team of motivated professionals that contribute to an exciting work atmosphere. We provide a competitive benefits package including medical, dental, and vision plan options, paid time off, and more.
California Disclosure - Employee Notice at Collection
This disclosure is intended to comply with the California Consumer Privacy Act (CCPA), which gives California residents who are applicants, employees, or contractors of Symmetry Lending (“Symmetry”) the right to know what categories of personal information Symmetry collects about them and the purposes for which Symmetry uses that information. As used in this Privacy Notice, “Personal Information” means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a particular individual or household. Personal Information includes, but is not limited to, the categories of personal information identified below if such information identifies, relates to, describes, is reasonably capable of being associated with, or could be reasonably linked, directly or indirectly, with a particular individual or household.
The following is a list of the categories of Personal Information that we may collect about consumers:
Identifiers. This may include a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, Social Security number, driver's license number, passport number, or other similar identifiers.
Personal information described in the California Customer Records Statute (Cal. Civ. Code § 1798.80(e)). This may include a name, signature, Social Security number, physical characteristics or description, address, telephone number, passport number, driver's license or state identification card number, insurance policy number, education, employment, employment history, bank account number, or any other financial information, medical information, or health insurance information.
Characteristics of Protected Classification under California or Federal Law. This may include age, race, color, ancestry, national or ethnic origin, citizenship status, religion or belief, marital status, a childbirth or related medical condition, physical or mental disability, sex (including gender, gender identity, gender expression, pregnancy or childbirth, and related medical conditions), sexual orientation, veteran or military status.
Biometric information. This may include voice and video recordings.
Sensory data. This may include audio, electronic, visual, or similar information, including photos.
Professional or employment-related information. This may include current or past job history, compensation data, performance evaluations, or employee benefits.
Beneficiaries, dependents, and emergency contact information. This may include the name, gender, phone number, and relationship of beneficiaries, dependents, and emergency contacts.
We may use the categories of Personal Information for the following business or commercial purposes:
To perform background checks necessary to comply with licensing requirements, to perform reference checks, to verify eligibility to work in the United States, for contact purposes, to assess your qualification for employment, to conduct performance evaluations, for payrolls and budgeting purposes, for implementation of employee benefits, for internal organizational purposes to establish proper accommodations for sick time, PTO, leaves of absences, or emergency situations, and to conduct health screenings as allowed by OSHA and the CDC to protect the safety of our employee during pandemic situations.
For internal use, such as tracking access into buildings, timekeeping, activity logs, etc.
To comply with laws and regulations, including but not limited to applicable tax, health and safety, anti-discrimination, immigration, labor and employment, and social welfare laws.
For security or the prevention, detection, or investigation of fraud, suspected or actual illegal activity, violations of company policy or rules, or other misconduct.
To comply with civil, criminal, judicial, or regulatory inquiries, investigations, subpoenas, or summons.
To comply with all licensing requirements necessary for our business operations, including state exam audits.
To exercise or defend the legal rights of Symmetry and its employees, affiliates, customers, contractors, and agents.
To seek advice from lawyers, auditors, or other professional advisors.
If Symmetry uses Personal Information of an applicant, employee, or contractor for a purpose materially different than those disclosed in this notice, Symmetry will notify the employee and obtain explicit consent from the employee to use the Personal Information for this new purpose.
Job Type: Full-time
Pay: $115,000.00 - $135,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Paid time off
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
Accounting and Finance: 2 years (Preferred)
advanced SQL concepts and writing SQL statements: 5 years (Required)
Work Location: Hybrid remote in Anaheim, CA 92807",3.9,"Symmetry Lending
3.9","Anaheim, CA",51 to 200 Employees,-1,Company - Public,Banking & Lending,Financial Services,$5 to $25 million (USD)
191,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
192,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
193,AWS Data Engineer Snowflake,-1,"Company Description

Orchestrated by adept technical architects with over fifty years of applied expertise, KYNITE is an advanced technology company specializing in the disciplines of: Blockchain, Cloud Services, Big Data & Analytics, Artificial Intelligence, Enterprise, Staff Augmentation and Managed Services
We are BigData Experts
We are Cloud Experts
We are Enterprise Architects
We are Artificial Intelligence Innovators
We are Technological Evangelists
We are Doers
We are Kynite

Additional Information

All your
This job is only for individuals residing in US
US Citizens, Green Card holders, EAD's can apply
W2, C2c,
Information will be kept confidential according to EEO guidelines.",-1,Kynite,"Myrtle Point, OR",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
194,Data Engineer,$100K - $144K (Glassdoor est.),"Position: Data Engineer
Duration: Contract
Location: Fremont, California
Skills:
Strong in SQL and Python
Data warehousing concepts , Data Modeling and RDBMS concepts
Good to have exposure to big data technologies – Apache Spark, Kafka, Airflow, NoSQL
Good to have exposure on Insurance & Finance business processes
Regards
Tejash Gupta
Recruiter
tejash@swifttechinc.com
Job Type: Contract
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Python: 7 years (Required)
SQL: 7 years (Required)
Data modeling: 6 years (Required)
RDBMS: 6 years (Required)
Work Location: In person",4.1,"Swift Technologies Inc
4.1","Fremont, CA",1 to 50 Employees,-1,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
195,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
196,Data Visualization Engineer,$77K - $101K (Glassdoor est.),"Passionate about precision medicine and advancing the healthcare industry?
Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.
With the advent of genomic sequencing, we can finally decode and process our genetic makeup. We now have more data than ever before but providers don't have the infrastructure or expertise to make sense of this data. We're on a mission to connect an entire ecosystem to redefine how genomic data is used in clinical settings.
What You'll Do:
Own the design, development and administration of advanced analytics dashboards and reporting and business intelligence tools. Beginning with requirements gathering across cross-functional teams, to the delivery of analytic solutions into production.
Collaborate with stakeholders, providing guidance, data and analytical support for developing new datasets, analyses and tools in support of strategic initiatives.
Perform deep-dive root cause and problem-solving analyses for and with laboratory, business operations and engineering teams.
Design and build data views to support advanced analytical reporting, self-service data analysis and exploration, utilizing multiple data and visualization technologies.
Work closely with technical and product teams to identify and align changes to internal systems in support of future data analysis needs and requirements.
Develop and facilitate data governance initiatives to assure data integrity across distributed systems.
Solve new and interesting problems.
Qualifications:
Bachelor's degree in Information Systems, Business Administration, Finance, Economics, Mathematics, Operations Research, Marketing or similar
3+ years of experience developing data sets for analysis use-cases
3+ years of experience with direct stakeholder management and ownership of full lifecycle of analytics/data projects and presenting analysis results to stakeholders
3+ years report-development and data visualization experience with BI tools (Looker, Tableau, etc.)
Ability to deliver both high-level explanations of complex problems while also able to deep dive into complex problems
Advanced SQL skills for querying and analysis of large, complex datasets
Must work well in a team environment and be able to partner with both business and technical team members and translate technical language to the non-technical need
Creative problem solver, ability to handle multiple projects, and strong work ethic needed
Excellent written and verbal communication skills
Highly organized and systematic, superb attention to detail
Able to complete tasks with a high degree of accuracy and an eye towards process improvement
Persistent and resilient person who thrives in a fast-paced, highly collaborative environment
Bonus points for:
Advanced analytics experience - classification/predictive modeling, optimization models, forecasting
Experience with Python for data engineering or advanced data analysis projects/use-cases
Healthcare domain knowledge and experience
Expert knowledge of database/data modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others
Experience building cloud-native applications and supporting technologies / patterns / practices including: GCP, AWS, Docker, CI/CD, DevOps, and microservices

#LI-EH1
#LI-Hybrid
We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.1,"Tempus
3.1","Chicago, IL",501 to 1000 Employees,2015,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
197,ETL Data Pipeline Engineer,Employer Provided Salary:$60.00 Per Hour,"ETL Data Pipeline Engineer
We do not work with 3rd party employers. Visa Sponsorship NOT available.
We are seeking a ETL DATA Pipeline Engineer for a consulting engagement with a major entertainment and media company. This person will be hands-on-date engineering development across multiple projects.
Required Skills:
10+ years of experience as Data Engineer with Large Data Pipelines
Strong SQL skills
Distributed Systems (Spark, Hadoop)
Cloud experience
STRONG ETL Experience
Python/Bash
Agile/Scrum
----------------------------------------
ABOUT MOORECROFT
A quality oriented national consulting firm. For more than two decades, providing expert IT professionals to our clients around the nation. Our core values are professionalism, honesty and integrity. We are only interested in working with professionals that subscribe to the same values and are driven to truly serve clients.
Job Type: Contract
Pay: From $60.00 per hour
Experience level:
10 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Santa Monica, CA 90404: Reliably commute or planning to relocate before starting work (Required)
Work Location: Hybrid remote in Santa Monica, CA 90404",-1,Moorecroft Systems,"Santa Monica, CA",Unknown,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
198,Data Engineer,$67K - $96K (Glassdoor est.),"Massanutten Resort
Job Summary
The Data Engineer will play a crucial role in designing, developing, and maintaining the data infrastructure and systems that enable efficient and reliable data processing, storage, and analysis. The primary responsibility is to transform raw data into usable formats for various business applications and support data-informed decision-making processes. This role includes working in various Microsoft SQL Environments as well as supporting a Snowflake Data Warehouse. The Data Engineer is expected to have a broad understanding of data concepts and be avid about continuing education to keep up with the ever-changing data landscape.
Required
Degree in computer science or related file and 3+ years of experience with Python, SQL, and data visualization/exploration tools (in lieu of a degree, at least 5 years of experience in the above is equivalent)
Preferred
A degree in computer science or a related field
Certifications in database management and/or related technical certifications
Familiarity with advanced data concepts (i.e., AI and Machine Learning) and business intelligence/visualization tools
Typical Schedule
Days:
Monday-Friday, weekends and holidays as needed
Position will be required to be part of rotating on-call schedule
Hours:
8am-5pm, additional hours as needed
Core Responsibilities
Develop Data Pipeline: Design and implement scalable and efficient data pipelines, integrating diverse data sources and ensuring smooth data flow from ingestion to storage and processing
Transform and Model Data: Develop and maintain data transformation processes, such as ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform), to convert raw data into structured formats suitable for analysis and reporting. Create and manage data models to facilitate data integration and provide a consistent view of information across systems.
Implement and oversee Data Warehousing and Storage: Build and maintain data warehouses or data lakes, optimizing storage and retrieval processes to ensure high availability, reliability, and performance. Implement appropriate data governance and security measures to protect sensitive data.
Ensure Data Quality and Governance: Collaborate with data analysts and business stakeholders to understand data requirements and define data quality standards. Implement data validation, data profiling, and data cleansing techniques to ensure data accuracy, consistency, and integrity.
Optimize Performance: Identify and address performance bottlenecks in data processing and storage systems, leveraging optimization techniques and technologies to improve overall data engineering efficiency and reduce latency
Assist with Data Problem Solving: Support team with data related escalation issues, including help desk support tickets that are generated from our business users
Manage Data Integration and API Development: Establish connections and integrations between internal and external data sources, including APIs, databases, and third-party platforms. Develop and maintain data APIs to enable seamless data access and consumption by other applications and services.
Collaborate and Document: Work closely with cross-functional teams, including analysts, software engineers, and business stakeholders, to understand data requirements and ensure data solutions align with business objectives. Document data engineering processes, including data pipelines, transformations, and system architectures, to facilitate knowledge sharing and maintain a robust knowledge base.
Stay up-to-date with the latest advancements in data engineering, data management, and big data technologies. Evaluate and recommend new tools, frameworks, and methodologies to improve data engineering practices and enhance overall data infrastructure
Other duties as assigned by management
For more information, contact Ana at 540-289-4939.",3.6,"Great Eastern Resort Management, Inc.
3.6","McGaheysville, VA",1001 to 5000 Employees,-1,Self-employed,-1,-1,Unknown / Non-Applicable
199,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
200,Senior Data Engineer,Employer Provided Salary:$105K - $186K,"**(UPDATED 8/21/2023) We invite applicants to express their interest in this role, but please note that we currently don't have a specific timeline for filling this position. This posting is intended for talent pooling. While there isn't a definitive hiring period, potential candidates should be aware that the role might be filled at some point in the future. We encourage you to start the process and discover more about this opportunity, Slalom, and potentially a career in consulting through discussions with our talent acquisition team.
Who You’ll Work With
As a modern technology company, we've never met a technical challenge we didn't like. We enable our clients to learn from their data, create incredible digital experiences, and make the most of new technologies. We blend design, engineering, and analytics expertise to build the future. We surround our technologists with interesting challenges, innovative minds and emerging technologies.
A Senior Data Engineer at Slalom is a technical team member who designs and develops data platform solutions. It uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g., data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc.
What You’ll Do
Work as part of a team and with clients to develop cloud data and analytics solutions
Make recommendations and prove data architecture concepts to team members and clients
Development of cloud data platforms and business intelligence solutions
Data wrangling of heterogeneous data sources for exploring and discovering new insights
Work with highly collaborative agile teams on modern data architecture projects
Key success measures of a Senior Data Engineer include technical proficiency in data processing technologies, collaboration with other teams, communication, and willingness to learn new things
What You'll Bring
Experience in one or more cloud technologies (AWS, Azure, Google Cloud, Snowflake)
Experience in data modeling, design, and development using complex SQL (SQL Server, Oracle, etc.)
Experience in Apache Airflow, Talend, SSIS, Informatica, Alteryx, or other data processing tool
Experience in one or more programming languages (Python, C#, Scala, R, PowerShell, Java, etc.)
Experience in cloud engineering, serverless architecture, and/or data streaming services
About Us
Slalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six countries and 43 markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 13,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.
Compensation and Benefits
Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance.
Slalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $105,000 to $186,000. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.
EEO and Accommodations
Slalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team if you require accommodations during the interview process.
#LI-JS8
#LI-Hybrid",4.3,"Slalom Consulting
4.3","Los Angeles, CA",10000+ Employees,2001,Company - Private,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
201,Data Engineer III,-1,"Overview:
Shift4 (NYSE: FOUR) is boldly redefining commerce by simplifying complex payments ecosystems across the world. As the leader in commerce-enabling technology, Shift4 powers billions of transactions annually for hundreds of thousands of businesses in virtually every industry. For more information, visit www.shift4.com.

Job Summary:
We are looking for a Data Engineer II to join our team of world-class in-house engineers. As a Data Engineer III, you will be responsible for designing, developing, and maintaining robust data infrastructure solutions to support the organization's data-driven initiatives. We are looking for self-driven and motivated individuals who take ownership of their projects. You will work closely with cross-functional teams, including project managers, business analysts, and software engineers, to ensure the availability, reliability, and scalability of our systems. This role requires expertise in data engineering and ETL processes.
Responsibilities:
Responsibilities:
Develop and maintain automated data pipelines and ETL processes to ingest, transform, and load large volumes of data from diverse sources into relational databases utilizing SQL, stored procedures, functions and other database technologies to move data.
Ensure the quality, integrity, and security of data by implementing data validation, data cleansing, and data governance processes.
Work on necessary new system developments, enhancements and bugs utilizing an agile software development lifecycle (SDLC).
Optimize and fine-tune data processes and systems for performance, scalability, and reliability.
Monitor and troubleshoot data pipelines, data processing jobs, and database performance issues including off hours support when necessary.
Aid in the design and implementation of efficient data models and schemas to support data analysis, reporting, and visualization needs.
Stay up-to-date with emerging data engineering technologies, tools, and best practices, and recommend improvements to existing data infrastructure.
Collaborate with software engineers to integrate data engineering solutions into applications and software products.
Document data engineering processes, data models, and system configurations for knowledge sharing and future reference.
Qualifications:
Qualifications:
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Minimum of 2 years of experience in data engineering or a similar role.
Advanced experience with relational databases like MSSQL, PostgreSQL, or AWS Redshift.
Advanced experience with SQL.
Experience with MSSQL SSIS or other ETL Tools.
Experience with reading data models.
Experience working in an agile software development lifecycle (SDLC).
Familiarity with data warehousing concepts and technologies.
Understanding of data governance, data security, and data privacy principles.
Strong problem-solving and analytical skills, with a keen attention to detail.
Ability to prioritize multiple tasks and easily adjust to changing priorities.
Excellent interpersonal, verbal and written skills including documentation of complex technical solutions.
Ability to identify problems, initiate solutions, and effectively collaborate and communicate with team members.
Relevant certifications in data engineering.

Shift4 Payments provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics

#LI-ASH",3.5,"Shift4 Payments
3.5","Center Valley, PA",1001 to 5000 Employees,1999,Company - Public,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
202,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
203,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
204,"Data Engineer - Python, Spark?",$91K - $131K (Glassdoor est.),"FlexIT client is looking for an immediate Data Engineer - Python, Sparkfor a 12-month remote contract.
The client is looking for great Engineers with talent and persistence who can leverage their existing skills and learn new ones. You should have some of the specific technical skills were looking for and be expert enough in one or two to help ramp others quickly.
Job Duties:
We are building petabyte-class solutions that consume fast-moving streams from eCommerce, retail, and partner channels and power the critical decisions that drive our business. We are building the Cloud Platform for Data and Analytics on AWS that fuels in digital transformation.
Focus areas include:
Data Streaming / Enrichment / Business Rules / MDM
Data Lake / Warehousing
Data Governance / GDPR / SOX
Data Strategy / Unified Access / IAM / RBAC
Be a great teammate on an agile/SCRUM team that sets and meets aggressive goals.
Mentor new and less experienced developers to advance their proficiency.
Leverage expert development skills and solid design skills to deliver reliable, scalable, performant solutions with modern tooling, data structures and algorithms.
Work with Product Owners, Engineering Managers and Principal Engineers to deliver solutions that enable digital transformation",4.0,"FlexIT Inc
4.0","Beaverton, OR",1 to 50 Employees,-1,Company - Private,-1,-1,$5 to $25 million (USD)
205,Data Engineer IV,Employer Provided Salary:$149K,"Who We Are
Invitation Homes is a fast-paced evolving publicly traded REIT that is pioneering a new industry with over 80,000 single family rental homes and a market capitalization of over $25 billion. We are a team of more than 1,400 associates who come from all walks of life. We call different communities “home,” but our shared values bind us together. Invitation Homes is a place where possibility lives.
THR Property Management LP / Data Engineer / Dallas, TX: Develop and maintain data pipelines into data warehouse and data lake, including design, and development. Develop and implement strategies to translate business requirements into feasible and acceptable data warehouse and data lake solutions. Architect and build new data models to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making. Design, build and launch data pipelines to move data to data lake and data warehouse build and maintain framework for auditing, error logging, and master data management for data pipelines. Build data expertise and own data quality for the data pipelines. Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes. Perform data analysis and assist in the resolution of data issues. Identify and resolve defects of complex scope using proper engineering tools and techniques. Provide support and maintain existing products and add new features. Mentor engineers by providing advice, coaching and educational opportunities. Telecommuting permitted from any location within the U.S. Salary: 148512/year.
The Senior Data Engineer is accountable for the maintenance, improvement, and movement of data in the Enterprise Data Warehouse, and other data sources used for enterprise reporting. A successful Senior Data Engineer delivers database design, implementation, and schematics that creates sustainable, competitive advantage for our company. Further, a Senior Data Engineer is responsible for the development and deployment of innovative analytics data platforms to support end-user enterprise reporting. Working with other data engineers, data analysts, and data scientists, a Senior Data Engineer must have a delivery first mentality and a firm grasp of Agile Development methodology. Lastly, the ideal candidate must demonstrate the ability to work as a team member and team leader.
What You’ll Do
Accountable for the overall performance and maintenance of our SQL Server environments:
Manage the EDW relational database for optimized performance
Design scalable ETL packages from the business source systems
Ensure all necessary testing and validation for all planned and unplanned releases
Mentor junior data engineers in best practices and documentation.
Act as our database evangelist in leading innovation activities through exploration, benchmarking and implementation of data technologies
Participate in solution-based activities; articulate your ideas through well thought out documentation and research best-in-class solutions
Lead and Develop and implement data architecture standards
Manage the necessary testing and validation to ensure proper compliance to data governance and quality
Prepare progress reports regarding our database environments’’ status and health
Manage troubleshooting data issues and present solutions to these issues.
Resolve tactical issues thoroughly while always looking for optimization and documenting lessons learned
Understand when tactical issues are really systematic issues requiring deeper level of analysis and strategic solutions
Cooperate with the support team to investigate and resolve data related defects
Proactively analyze and evaluate current state of data in order to identify and recommend improvements and optimization
Support our Agile Development methodology
Constantly prioritize user stories; balance feature development, defects, and tech debt according to business need
Be a champion of continuous improvement and delivery; define the minimum viable product, focus teams on key release milestones, and leverage fast follow-up sprints
Enable sprint progress; clarify story details, validate completed stories, and prepare stories for the next sprint
Who We’re Looking For
Bachelor’s degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field. An equivalent of the same in working experience is also accepted for the position.
A candidate for the position will have as at least 5 years of working experience as a database engineering or a database engineering administrator within a fast-paced a complex business setting.
Design, implement and maintain SQL Server databases
Design, implement and maintain ETL processes using SQL Server SSIS
Provide ongoing maintenance support through SQL query tuning and optimization
Strong working and conceptual knowledge of building and maintaining physical and logical data models
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience working in SaaS, IaaS, and PaaS
Excellent customer service and interpersonal skills; ability to relate to and get along with others
Professional verbal and written communication skills
Strong organizational and time-management skills
Ability to multi-task and maintain flexibility and creativity in a variety of situations
Ability to analyze and resolve problems
Ability to set and meet goals and consistently meet deadlines
Ability to maintain confidentiality
Why Invitation Homes
We stand for flexibility, opportunity, and a home that people can make their own. It’s as true for our associates as it is for our residents. Just like we help our residents live freer, we liberate our associates’ careers, too. Our associates know that at Invitation Homes goals matter, potential is unlocked, and careers thrive. Invitation Homes isn’t just a work place it is a possibility place. Invitation Homes offers the below to each new associate:
Competitive pay and an annual bonus program for all associates
Generous paid time off plans including vacation accrual, sick time, volunteer time, and standard and floating holidays
401k with matching company contributions
Awesome work environment with casual dress
Team events and gatherings
Employee resource groups: Together with Women, Asian Alliance, Black Collective, Juntos, Gen Next, and Open Invitation.
Invitation Homes truly is where possibility lives, pour a new foundation here!
Salary Range
$98,640.00 - $170,976.00
Compensation and Benefits
To attract and retain top talent, we're pleased to offer competitive compensation and benefits, including:
Annual bonus program
Health, dental, vision, and life insurance
Long-term and short-term disability insurance
Generous paid time off plans include vacation accrual, sick time, standard holidays and floating holidays
401(k) with company matching contributions
Awesome work environment with casual dress
Team events and gatherings (Pre- and Post-Covid)
Invitation Homes is an equal opportunity employer committed to fostering a diverse, inclusive and innovative environment with the best associates. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, Veteran status or any other factor protected by applicable federal, state or local law. If you have a disability or special need that requires accommodation, please contact us at
humanresources@invitationhomes.com
.
To all recruitment agencies: Invitation Homes does not accept agency resumes. Please do not forward resumes to Invitation Homes employees. Invitation Homes is not responsible for any fees related to unsolicited resumes.",3.9,"Invitation Homes
3.9","Dallas, TX",1001 to 5000 Employees,2012,Company - Public,Real Estate,Real Estate,$1 to $5 billion (USD)
206,Data Engineer,-1,"Description:
Who is Leafwell
Leafwell is a rapidly growing technology and data company that set out to increase access, education, and research into cannabis and to advance its application as medicine.
An exciting opportunity for a Data Engineer to join our growing team has arisen.
What to Expect as a Data Engineer at Leafwell
As a Data Engineer at Leafwell, you will be in charge of creating and orchestrating the Leafwell data pipeline, which means gathering data, creating & automating data transformations and producing actionable insights for Leafwell’s internal stakeholders. You will support critical testing and rollout of new data features. The Data Engineer will build and maintain systems that inform Leafwell’s business stakeholders about Key Performance Indicators (KPIs) and suggest data-driven strategies to optimize those metrics.
Essential Duties and Responsibilities
The Data Engineer will perform the following responsibilities:
Acquire, assemble, transform and analyze data
Create, manage and orchestrate the data pipeline and it’s infrastructure
Present findings, trends, and suggested optimizations
Identify new opportunities and threats to the company's business model
Update and revise reports, queries, and analytic procedures as necessary
Design and implement tracking so that optimization efforts can be measured
Identify inefficiencies in data processes and automate where appropriate
Write and update international SOPs and internal documentation
Support IT systems management with testing, validation, and user support
Proactively identify initiatives for data-related improvements
Why Leafwell
At Leafwell, we are passionate about our work and seek out employees who contribute the same level of dedication and enthusiasm. We are only as good as the people we hire, so we aim to be the best employer in order to attract the top talent in the industry.
Do we have your Interest?
Requirements:
Our Ideal Candidate
Our Ideal Candidate will possess the following:
Bachelor's degree in Computer Science, Mathematics, Economics, Information Systems, or another quantitative field
3-5 years of relevant professional experience in Data Engineering / Analytics
Advance working knowledge and experience in SQL and relational databases
Strong data management abilities, including data analysis, standardization, cleansing, querying, and consolidation of data
Data visualization experience (e.g Tableau, Looker, etc.)
Exercises self-reliance, initiative, and leadership while keeping stakeholders properly informed and involved
Reliably manage numerous duties during a workday, necessitating interactions with people located across the world
Technically competent, with the ability to quickly learn new processes and programs, and utilize various software applications
Excellent communication and interpersonal skills; ability to work with and appeal to a wide variety of personalities and professional tendencies
Bonus points if you have experience in the following:
Cannabis knowledge; industry experience is a plus
Used Data Orchestration tools like Dagster or Airflow
Used dbt or comparable data transformation software
Git
Python or R programming
Amazon Web Services
PostgreSQL and RedShift
CRM products
Data Visualization in Metabase
Effective project management skills and comfort utilizing a project management platform in collaboration with other team members
Data Science projects
Benefits Highlights
Our benefits include, but are not limited to:
Remote first - Most of our employees are 100% remote. Positions requiring travel enjoy a hybrid environment.
Rapid Growth - Our company is rapidly expanding, and our employees are advancing with us!
Training and Development - We foster a culture of learning. We encourage our employees to take part in educational, training, and development opportunities.
Leafwell is dedicated to bringing together people from diverse cultures and perspectives. We work hard to provide a welcoming atmosphere where everyone may flourish, have a sense of belonging, and collaborate effectively. As an equal opportunity employer, we do not tolerate any illegal discrimination against job applicants based on their race, color, religion, veteran status, sex, parental status, gender identity or expression, transgender status, sexual orientation, national origin, age, disability, or genetic information. We are committed to going above and above in creating diversity within our firm and follow the regulations upheld by the EEOC.",4.0,"LEAFWELL
4.0",Remote,51 to 200 Employees,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
207,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
208,Sr. Data Engineer,Employer Provided Salary:$84K - $191K,"Job Summary:
The primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver modules, stable application systems, and Data or Platform solutions. This includes developing, configuring, or modifying complex integrated business and/or enterprise infrastructure or application solutions within various computing environments. This role facilitates the implementation and maintenance of complex business and enterprise Data or Platform solutions to ensure successful deployment of released applications.
Minimum Qualifications
Bachelor's Degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)
5 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering
4 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)
Preferred Qualifications
Master's Degree in Computer Science, CIS, or related field
5 years of IT experience developing and implementing business systems within an organization
5 years of experience working with defect or incident tracking software
5 years of experience writing technical documentation in a software development environment
3 years of experience working with an IT Infrastructure Library (ITIL) framework
3 years of experience leading teams, with or without direct reports
5 years of experience working with source code control systems
Experience working with Continuous Integration/Continuous Deployment tools
5 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutions
Job Type: Full-time
Salary: $84,002.64 - $190,979.59 per year
Experience:
Data, BI or Platform Engineering, Data Warehousing/ETL: 3 years (Preferred)
developing and implementing business systems: 3 years (Preferred)
Data Engineer: 4 years (Preferred)
Work Location: In person",-1,Market Tree Research,"Carolina, WV",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
209,Data Engineer,$99K - $136K (Glassdoor est.),"The company

Decentralized data is the future. Data mesh is the right idea. We’re here to make it a reality. Nextdata OS is a data-mesh-native platform built to meet the challenge of decentralizing data at scale. We are inventing a new way for developers to work with data and share it responsibly via data product containers.

Our vision is to build a world where AI/ML and analytics are powered by decentralized, responsible, and equitable data ownership, across boundaries of organizations, technology, and most importantly boundaries of trust.

Our purpose is to change the experience of creating, sharing, discovering, and using data forever, to be connected, fast, and fair based on data mesh principles.

Our technology is designed to empower data developers, users and owners with a delightful experience where data products are a first-class primitive, with trust built-in.

We are here to accept the reality that the world of data is complex and messy; data models are out-of-date the moment they are created; data is owned across trust boundaries; data is stored on different platforms; data is used in many different modes and most importantly data can't protect itself. We are here to recognize that past approaches to tackle these complexities with centralized data collection, modeling and governance are ineffective at best and pathologically unfair at worst. We are here to reimagine, with you!

The role

You will be one of the first data product developers building data products on top of the Nextdata OS. You will work with Nextdata’s initial customers to translate their business needs into PoC data products that you will then build on top of our OS.
You will leverage your experience of the full lifecycle of analytics - translating business requirements to data models, to data pipelines, to analytics dashboards and even machine learning pipelines - to build data products and then provide feedback that will help us improve the Nextdata OS and tooling.

You will manage the full lifecycle of the PoC data products from ideation around business use case to successful deployment, including synthesizing realistic data for testing before deploying into production.

Our expectations

You have worked on complex data pipelines in large data organizations. You have dealt with the uncertainty of iterating from at times an under defined business use case to a concrete end-to-end analytics pipeline that drives value to the business. You may have had to help the business quantify the value of your analyses as well!
You are proficient in the modern data stack tools like airflow, dagster, prefect, dbt and ML development environments like PySpark and Pandas.
You are articulate and are able to present your findings in a compelling manner.
You have relied on continuous integration and continuous deployment to reduce software lead time, and have contributed to optimizing build and release processes as needed.
You have shown evidence that you aspire to test-first data pipeline development!

Our benefits

We are an early stage company, but we don't subsist on ramen! We are an experienced team with families. We provide $6000 for your home workspace setup, premium health, vision, dental insurance coverage for you and your family. And of course, early stage equity and market rate salary.",-1,Nextdata Technologies Inc,"San Francisco, CA",1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
210,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
211,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
212,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
213,Sr. Data Engineer,Employer Provided Salary:$118K - $142K,"Data Engineer
Egen Solutions Inc offers competitive compensation, benefits, and an excellent culture of teamwork and collaboration. We are an Equal Opportunity Employer that does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.
Job Summary
We have some major internal projects underway at Egen and need a highly skilled Data Engineer to join our team on a full-time basis. Below is the JD, kindly go through it and let me know your interest and if you are interested revert back with your updated resume
Our Data Engineering teams build scalable data pipelines using Python, Spark, and cloud services (GCP and AWS). The pipelines we build typically integrate with technologies such as Kafka, Storm, and Elasticsearch. We are working on a continuous deployment pipeline that leverages rapid on-demand releases. Our developers work in an agile process to efficiently deliver high value applications and product packages.
As a Staff Data Engineer at Egen, you will leverage Spark and GCP (preferred) to architect and implement cloud-native data pipelines and infrastructure to enable analytics and machine learning on rich datasets.
Required Experience:
Built and run resilient data pipelines in production and have implemented ETL/ELT to load a multi-terabyte enterprise data warehouse.
Implemented analytics applications using multiple database technologies, such as relational, multidimensional (OLAP), key-value, document, or graph.
Defined data contracts, and wrote specifications including REST APIs.
Transformed data between data models and formats with the most modern PySpark practices. Have built cloud-native applications and supporting technologies / patterns / practices including: Cloud Services, Docker, CI/CD, DevOps, and microservices.
Planned and designed artifacts that describe software architectures involving multiple systems and technologies
You've worked in agile environments and are comfortable iterating quickly.
Nice to have's (but not required):
GCP expertise is preferred but will consider AWS
Experience moving trained machine learning models into production data pipelines.
Experience in biotech, genomics, clinical research or precision medicine.
Expert knowledge of relational database modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others.
This is full-time, On-site, and Remote. We are based out in Naperville, IL.
Looking forward to striking up a conversation with you on the opportunity!, Kindly visit our website egen.solutions
equirements
Bachelor’s degree in Computer Science, Engineering, or related field.
3+ years of experience with data engineering and/or data analysis.
Strong analytical skills and ability to think analytically.
Ability to work independently with minimal supervision.
Experience with SQL and/or NoSQL databases is a plus.
Experience with ETL processes is a plus.
Experience with cloud-based solutions such as Azure Data Factory is a plus.
Experience with Agile software development methods such as Scrum is a plus.
Ability to work in a fast-paced environment and adapt to changing priorities.
Ability to work in a team environment while maintaining strong individual initiative.
Job Type: Full-time
Pay: $117,577.88 - $141,599.17 per year
Benefits:
401(k)
Dental insurance
Health insurance
Experience level:
5 years
Application Question(s):
Are you a personal who hold any of these work authorizations currently(OPT EAD/GC/F1 OPT/H4 EAD.
Work Location: In person",-1,Egen Solutions Inc,"Naperville, IL",-1,-1,-1,-1,-1,-1
214,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
215,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
216,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
217,AWS Data Engineer Snowflake,-1,"Company Description

Orchestrated by adept technical architects with over fifty years of applied expertise, KYNITE is an advanced technology company specializing in the disciplines of: Blockchain, Cloud Services, Big Data & Analytics, Artificial Intelligence, Enterprise, Staff Augmentation and Managed Services
We are BigData Experts
We are Cloud Experts
We are Enterprise Architects
We are Artificial Intelligence Innovators
We are Technological Evangelists
We are Doers
We are Kynite

Additional Information

All your
This job is only for individuals residing in US
US Citizens, Green Card holders, EAD's can apply
W2, C2c,
Information will be kept confidential according to EEO guidelines.",-1,Kynite,"Myrtle Point, OR",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
218,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
219,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
220,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
221,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
222,Data Engineer IV,Employer Provided Salary:$149K,"Who We Are
Invitation Homes is a fast-paced evolving publicly traded REIT that is pioneering a new industry with over 80,000 single family rental homes and a market capitalization of over $25 billion. We are a team of more than 1,400 associates who come from all walks of life. We call different communities “home,” but our shared values bind us together. Invitation Homes is a place where possibility lives.
THR Property Management LP / Data Engineer / Dallas, TX: Develop and maintain data pipelines into data warehouse and data lake, including design, and development. Develop and implement strategies to translate business requirements into feasible and acceptable data warehouse and data lake solutions. Architect and build new data models to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making. Design, build and launch data pipelines to move data to data lake and data warehouse build and maintain framework for auditing, error logging, and master data management for data pipelines. Build data expertise and own data quality for the data pipelines. Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes. Perform data analysis and assist in the resolution of data issues. Identify and resolve defects of complex scope using proper engineering tools and techniques. Provide support and maintain existing products and add new features. Mentor engineers by providing advice, coaching and educational opportunities. Telecommuting permitted from any location within the U.S. Salary: 148512/year.
The Senior Data Engineer is accountable for the maintenance, improvement, and movement of data in the Enterprise Data Warehouse, and other data sources used for enterprise reporting. A successful Senior Data Engineer delivers database design, implementation, and schematics that creates sustainable, competitive advantage for our company. Further, a Senior Data Engineer is responsible for the development and deployment of innovative analytics data platforms to support end-user enterprise reporting. Working with other data engineers, data analysts, and data scientists, a Senior Data Engineer must have a delivery first mentality and a firm grasp of Agile Development methodology. Lastly, the ideal candidate must demonstrate the ability to work as a team member and team leader.
What You’ll Do
Accountable for the overall performance and maintenance of our SQL Server environments:
Manage the EDW relational database for optimized performance
Design scalable ETL packages from the business source systems
Ensure all necessary testing and validation for all planned and unplanned releases
Mentor junior data engineers in best practices and documentation.
Act as our database evangelist in leading innovation activities through exploration, benchmarking and implementation of data technologies
Participate in solution-based activities; articulate your ideas through well thought out documentation and research best-in-class solutions
Lead and Develop and implement data architecture standards
Manage the necessary testing and validation to ensure proper compliance to data governance and quality
Prepare progress reports regarding our database environments’’ status and health
Manage troubleshooting data issues and present solutions to these issues.
Resolve tactical issues thoroughly while always looking for optimization and documenting lessons learned
Understand when tactical issues are really systematic issues requiring deeper level of analysis and strategic solutions
Cooperate with the support team to investigate and resolve data related defects
Proactively analyze and evaluate current state of data in order to identify and recommend improvements and optimization
Support our Agile Development methodology
Constantly prioritize user stories; balance feature development, defects, and tech debt according to business need
Be a champion of continuous improvement and delivery; define the minimum viable product, focus teams on key release milestones, and leverage fast follow-up sprints
Enable sprint progress; clarify story details, validate completed stories, and prepare stories for the next sprint
Who We’re Looking For
Bachelor’s degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field. An equivalent of the same in working experience is also accepted for the position.
A candidate for the position will have as at least 5 years of working experience as a database engineering or a database engineering administrator within a fast-paced a complex business setting.
Design, implement and maintain SQL Server databases
Design, implement and maintain ETL processes using SQL Server SSIS
Provide ongoing maintenance support through SQL query tuning and optimization
Strong working and conceptual knowledge of building and maintaining physical and logical data models
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience working in SaaS, IaaS, and PaaS
Excellent customer service and interpersonal skills; ability to relate to and get along with others
Professional verbal and written communication skills
Strong organizational and time-management skills
Ability to multi-task and maintain flexibility and creativity in a variety of situations
Ability to analyze and resolve problems
Ability to set and meet goals and consistently meet deadlines
Ability to maintain confidentiality
Why Invitation Homes
We stand for flexibility, opportunity, and a home that people can make their own. It’s as true for our associates as it is for our residents. Just like we help our residents live freer, we liberate our associates’ careers, too. Our associates know that at Invitation Homes goals matter, potential is unlocked, and careers thrive. Invitation Homes isn’t just a work place it is a possibility place. Invitation Homes offers the below to each new associate:
Competitive pay and an annual bonus program for all associates
Generous paid time off plans including vacation accrual, sick time, volunteer time, and standard and floating holidays
401k with matching company contributions
Awesome work environment with casual dress
Team events and gatherings
Employee resource groups: Together with Women, Asian Alliance, Black Collective, Juntos, Gen Next, and Open Invitation.
Invitation Homes truly is where possibility lives, pour a new foundation here!
Salary Range
$98,640.00 - $170,976.00
Compensation and Benefits
To attract and retain top talent, we're pleased to offer competitive compensation and benefits, including:
Annual bonus program
Health, dental, vision, and life insurance
Long-term and short-term disability insurance
Generous paid time off plans include vacation accrual, sick time, standard holidays and floating holidays
401(k) with company matching contributions
Awesome work environment with casual dress
Team events and gatherings (Pre- and Post-Covid)
Invitation Homes is an equal opportunity employer committed to fostering a diverse, inclusive and innovative environment with the best associates. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, Veteran status or any other factor protected by applicable federal, state or local law. If you have a disability or special need that requires accommodation, please contact us at
humanresources@invitationhomes.com
.
To all recruitment agencies: Invitation Homes does not accept agency resumes. Please do not forward resumes to Invitation Homes employees. Invitation Homes is not responsible for any fees related to unsolicited resumes.",3.9,"Invitation Homes
3.9","Dallas, TX",1001 to 5000 Employees,2012,Company - Public,Real Estate,Real Estate,$1 to $5 billion (USD)
223,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
224,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
225,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
226,Data Engineer,$99K - $136K (Glassdoor est.),"The company

Decentralized data is the future. Data mesh is the right idea. We’re here to make it a reality. Nextdata OS is a data-mesh-native platform built to meet the challenge of decentralizing data at scale. We are inventing a new way for developers to work with data and share it responsibly via data product containers.

Our vision is to build a world where AI/ML and analytics are powered by decentralized, responsible, and equitable data ownership, across boundaries of organizations, technology, and most importantly boundaries of trust.

Our purpose is to change the experience of creating, sharing, discovering, and using data forever, to be connected, fast, and fair based on data mesh principles.

Our technology is designed to empower data developers, users and owners with a delightful experience where data products are a first-class primitive, with trust built-in.

We are here to accept the reality that the world of data is complex and messy; data models are out-of-date the moment they are created; data is owned across trust boundaries; data is stored on different platforms; data is used in many different modes and most importantly data can't protect itself. We are here to recognize that past approaches to tackle these complexities with centralized data collection, modeling and governance are ineffective at best and pathologically unfair at worst. We are here to reimagine, with you!

The role

You will be one of the first data product developers building data products on top of the Nextdata OS. You will work with Nextdata’s initial customers to translate their business needs into PoC data products that you will then build on top of our OS.
You will leverage your experience of the full lifecycle of analytics - translating business requirements to data models, to data pipelines, to analytics dashboards and even machine learning pipelines - to build data products and then provide feedback that will help us improve the Nextdata OS and tooling.

You will manage the full lifecycle of the PoC data products from ideation around business use case to successful deployment, including synthesizing realistic data for testing before deploying into production.

Our expectations

You have worked on complex data pipelines in large data organizations. You have dealt with the uncertainty of iterating from at times an under defined business use case to a concrete end-to-end analytics pipeline that drives value to the business. You may have had to help the business quantify the value of your analyses as well!
You are proficient in the modern data stack tools like airflow, dagster, prefect, dbt and ML development environments like PySpark and Pandas.
You are articulate and are able to present your findings in a compelling manner.
You have relied on continuous integration and continuous deployment to reduce software lead time, and have contributed to optimizing build and release processes as needed.
You have shown evidence that you aspire to test-first data pipeline development!

Our benefits

We are an early stage company, but we don't subsist on ramen! We are an experienced team with families. We provide $6000 for your home workspace setup, premium health, vision, dental insurance coverage for you and your family. And of course, early stage equity and market rate salary.",-1,Nextdata Technologies Inc,"San Francisco, CA",1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
227,Intermediate Data Engineer,$67K - $99K (Glassdoor est.),"Summary of Responsibilities: Under the general direction of the Director of Application Services, this intermediate position will leverage their business and technical knowledge to architect and build a secure business intelligence infrastructure and develop secure production-ready data models to meet college goals and objectives.
Collaborate with cross-functional teams to identify and implement data solutions that meet college needs.
Analyze requirements and recognize alternative solutions that may be more efficient or effective for the college.
Design and develop advanced business intelligence reporting solutions based on input and analysis.
Integrate multiple campus data sources, analyze data, and develop scripts to transform/convert data.
Ensure data quality and accuracy by implementing data validation and testing processes.
Implement security and compliance measures to ensure the protection of sensitive data.
Design and develop scalable data solutions using cloud technologies.
Migrate existing data solutions to cloud platform technologies.

Essential Responsibilities: (These essential responsibilities are those the individual must be able to perform unaided or with the assistance of reasonable accommodation.)
Business Requirements and Process Analysis:
Partners with subject matter experts to learn and understand data and reporting requirements.
Conducts business requirements reviews and independently formulates logical statements of business requirements.
Systems Analysis:
Transform the logical statements of business requirements into technical/functional specifications.
Implements the functional/technical steps by revising existing data models or creating new data models.
Creates prototype data model designs.
Develops system architecture and assembles solutions for a modern business intelligence infrastructure.
Develop test transactions and use cases to verify the results of the data transformation (conversion) and identify data quality issues.
Deployment/Maintenance/Operations Analysis
Evaluates and documents operational performance as directed.
Diagnoses moderately complex to complex issues/incidents as directed.
Research and gather data/information, analyze information, and propose alternative solutions for moderately complex to complex data/reporting requirements.
Completes service requests/problem resolution for moderately complex to complex data/reporting requirements.
Additionally, this position may be required to maintain a standby status as part of a rotation within the team. This requires 24 hours per day, 7 days per week availability during the standby period. The frequency varies based upon the number of colleagues in the rotation.
Hosted Services Support
Works with vendor(s) for day-to-day systems operations support including related data architecture.
Reviews systems operations needs including software updates and upgrades, database server and instance maintenance including backup operations.
Report Writing
Develop and deliver reports based on college requirements.
Programming/Development:
Modify and/or write scripts as needed.
Test and debug script modifications for correctness.
Assist in Peer reviews.
Project Management
Works with project manager to define tasks and create teamwork plans with moderate supervision on moderately to complex projects.
Delegates work to others and monitor’s progress.
Identifies issues affecting work progress and recommends solutions.
Communicates schedule variances and potential scope changes in status reports.
Controls project costs, communicating any project-related expenses and recommends ways to control costs.
Manages small projects including Project scope, statement(s) of work, MS Project plan(s), estimates, and communications.
Educational/Knowledge Requirements:
Completes on-going training on-the-job, through courses, self-study, certifications and/or advanced degrees to maintain and enhance technical and business capabilities.
Demonstrates understanding of the fundamental tools and concepts of one of the information technology professional disciplines (i.e., Applications, Business Analysis, Development, etc.) and applies that understanding to make independent practical contributions to IT work within Application Services.
Miscellaneous:
Perform other duties as assigned.",4.1,"Geneva College
4.1","Beaver Falls, PA",201 to 500 Employees,1897,College / University,Colleges & Universities,Education,$25 to $100 million (USD)
228,Data Engineer,Employer Provided Salary:$115K - $135K,"**This position is a majority Remote role with occasional in-office meetings on an as-needed basis. Because of this, interested applicants must live within a reasonable driving distance of Symmetry Lending's office in Anaheim, CA.**
Job Description
This role will play a pivotal role in assisting the IT team with the evolution of the company data architecture. In addition to supporting extensions to the data warehouse design, this role will design and develop ETL required to onboard new types of business data. Finally, this role will also provide support for Analytics assignments as needed.
Responsibilities
Display sense of ownership over assigned work, requiring minimal direction and driving to completion in a sometimes fuzzy and uncharted environment.
Designing and building new data pipelines that support business requirements.
Work with IT and other business partners to support the evolution of the enterprise data architecture.
Work with IT to build new ETLs to take data from various operational systems and extend existing data warehouse data model for analytics and reporting.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business constituents.
Support the development of the company BI infrastructure, including the construction of scalable analytic solutions, and on-premises reporting tools.
Utilize Power BI development to build scalable reporting models to serve BI reporting needs.
Demonstrate the ability to gather requirements, extract and manipulate data residing in multiple disparate databases, and articulate solutions to support the business.
Design, develop, and maintain performant data models in Power BI utilizing best practices.
Serve as a subject matter expert for all data warehouse and enterprise systems
Use new or existing technologies to produce analytics solutions (in the form of excel spreadsheets, dashboards, etc.).
Work with IT and Analytics team to architect and build data pipelines to optimize for performance, data quality, scalability, ease of future development, and cost.
Gather requirements, assess gaps and build roadmaps to help the analytics driven organization achieve its goals.
Develop data-related Proof of Concepts in order to demonstrate feasibility and value to Symmetry business constituents.
Qualifications / Requirements
Bachelor’s degree in Information Systems, Computer Science, Finance, or similar education from an accredited college
2-3 years Accounting and Finance background preferred
Strong skillset in Microsoft Excel (2-3 years, financial formulas & VBA is a plus) and familiarity with Microsoft Office Productivity Suite (Excel, Word, etc.)
5+years’ experience with advanced SQL concepts and writing SQL statements (SQL Server or similar).
Prefer 2 years’ experience with Microsoft BI Suite, PowerBI, Power Query, PowerPivot, Reporting Service (SSRS), SharePoint
Experience in mentoring other team members in development best practice, and methodologies.
You are passionate about data quality control and know how and where to anticipate potential errors.
Knowledge of the software development lifecycle, agile methodologies, and structured software development methodologies.
Experience performing analysis with large datasets in a cloud-based environment.
Ability to work effectively with stakeholders at all levels within the organization
Strong communication and time management skills and a self-motivated approach
Ability to work independently, detail-oriented, and execution focused
Highly collaborative and team oriented
Tenacious (doesn’t give up easily)
Genuine passion for clean and reliable data
At least 5 years of work experience
About Symmetry
Symmetry Lending, specializes in providing mortgage fulfillment services to include origination, servicing, and capital markets needs to various Lenders across the country with whom we partner. We have offices in Atlanta, GA, Eden Prairie, MN, Denver, CO, Orlando, FL, and Anaheim, CA, and we do business from coast to coast. We take great pride in building a diverse team of motivated professionals that contribute to an exciting work atmosphere. We provide a competitive benefits package including medical, dental, and vision plan options, paid time off, and more.
California Disclosure - Employee Notice at Collection
This disclosure is intended to comply with the California Consumer Privacy Act (CCPA), which gives California residents who are applicants, employees, or contractors of Symmetry Lending (“Symmetry”) the right to know what categories of personal information Symmetry collects about them and the purposes for which Symmetry uses that information. As used in this Privacy Notice, “Personal Information” means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a particular individual or household. Personal Information includes, but is not limited to, the categories of personal information identified below if such information identifies, relates to, describes, is reasonably capable of being associated with, or could be reasonably linked, directly or indirectly, with a particular individual or household.
The following is a list of the categories of Personal Information that we may collect about consumers:
Identifiers. This may include a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, Social Security number, driver's license number, passport number, or other similar identifiers.
Personal information described in the California Customer Records Statute (Cal. Civ. Code § 1798.80(e)). This may include a name, signature, Social Security number, physical characteristics or description, address, telephone number, passport number, driver's license or state identification card number, insurance policy number, education, employment, employment history, bank account number, or any other financial information, medical information, or health insurance information.
Characteristics of Protected Classification under California or Federal Law. This may include age, race, color, ancestry, national or ethnic origin, citizenship status, religion or belief, marital status, a childbirth or related medical condition, physical or mental disability, sex (including gender, gender identity, gender expression, pregnancy or childbirth, and related medical conditions), sexual orientation, veteran or military status.
Biometric information. This may include voice and video recordings.
Sensory data. This may include audio, electronic, visual, or similar information, including photos.
Professional or employment-related information. This may include current or past job history, compensation data, performance evaluations, or employee benefits.
Beneficiaries, dependents, and emergency contact information. This may include the name, gender, phone number, and relationship of beneficiaries, dependents, and emergency contacts.
We may use the categories of Personal Information for the following business or commercial purposes:
To perform background checks necessary to comply with licensing requirements, to perform reference checks, to verify eligibility to work in the United States, for contact purposes, to assess your qualification for employment, to conduct performance evaluations, for payrolls and budgeting purposes, for implementation of employee benefits, for internal organizational purposes to establish proper accommodations for sick time, PTO, leaves of absences, or emergency situations, and to conduct health screenings as allowed by OSHA and the CDC to protect the safety of our employee during pandemic situations.
For internal use, such as tracking access into buildings, timekeeping, activity logs, etc.
To comply with laws and regulations, including but not limited to applicable tax, health and safety, anti-discrimination, immigration, labor and employment, and social welfare laws.
For security or the prevention, detection, or investigation of fraud, suspected or actual illegal activity, violations of company policy or rules, or other misconduct.
To comply with civil, criminal, judicial, or regulatory inquiries, investigations, subpoenas, or summons.
To comply with all licensing requirements necessary for our business operations, including state exam audits.
To exercise or defend the legal rights of Symmetry and its employees, affiliates, customers, contractors, and agents.
To seek advice from lawyers, auditors, or other professional advisors.
If Symmetry uses Personal Information of an applicant, employee, or contractor for a purpose materially different than those disclosed in this notice, Symmetry will notify the employee and obtain explicit consent from the employee to use the Personal Information for this new purpose.
Job Type: Full-time
Pay: $115,000.00 - $135,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Paid time off
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
Accounting and Finance: 2 years (Preferred)
advanced SQL concepts and writing SQL statements: 5 years (Required)
Work Location: Hybrid remote in Anaheim, CA 92807",3.9,"Symmetry Lending
3.9","Anaheim, CA",51 to 200 Employees,-1,Company - Public,Banking & Lending,Financial Services,$5 to $25 million (USD)
229,Data Engineer I - People Analytics,$73K - $111K (Glassdoor est.),"POSITION SUMMARY Our People Analytics team is expanding! The Data Engineer I is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.

This position can work in a flexible, hybrid model with 3 days on-site in Appleton, WI and 2 days remote.

JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.

QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming. Preferred experience in Python applied to data engineering and transformation.
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated

#LI-Hybid

Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101 .",4.1,"US Venture
4.1","Appleton, WI",1001 to 5000 Employees,1951,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
230,Senior Data Engineer,$87K - $114K (Glassdoor est.),"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.

This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration.
How will you make an impact?
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
What are we looking for?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework
You might be a good fit if you like to:
Leverage technology to deliver data solutions to drive business outcomes
Drive for continuous improvement of processes and solutions
Bring energy and commitment to excellence to drive delivery of high-quality solutions
Share knowledge and mentor team members on technologies and solutions
What you can learn on the job:
Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers
Grow practical understanding of agile work practices through application of the scrum framework to deliver work products
Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science
What does success look like?
First 30 days:
Understand the existing analytics data platform and technologies
Understand team’s work management processes
First 6 months:
Develop data solutions for the business including shared components and specific data sets
Learn business processes and develop necessary contacts throughout the organization
First year:
Continue to develop data solutions for the business
Become expert on the team for other data engineers to go to for mentoring in completing their work
Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,"AE Business Solutions
4.2","Milwaukee, WI",51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
231,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
232,Data Visualization Engineer,$77K - $101K (Glassdoor est.),"Passionate about precision medicine and advancing the healthcare industry?
Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.
With the advent of genomic sequencing, we can finally decode and process our genetic makeup. We now have more data than ever before but providers don't have the infrastructure or expertise to make sense of this data. We're on a mission to connect an entire ecosystem to redefine how genomic data is used in clinical settings.
What You'll Do:
Own the design, development and administration of advanced analytics dashboards and reporting and business intelligence tools. Beginning with requirements gathering across cross-functional teams, to the delivery of analytic solutions into production.
Collaborate with stakeholders, providing guidance, data and analytical support for developing new datasets, analyses and tools in support of strategic initiatives.
Perform deep-dive root cause and problem-solving analyses for and with laboratory, business operations and engineering teams.
Design and build data views to support advanced analytical reporting, self-service data analysis and exploration, utilizing multiple data and visualization technologies.
Work closely with technical and product teams to identify and align changes to internal systems in support of future data analysis needs and requirements.
Develop and facilitate data governance initiatives to assure data integrity across distributed systems.
Solve new and interesting problems.
Qualifications:
Bachelor's degree in Information Systems, Business Administration, Finance, Economics, Mathematics, Operations Research, Marketing or similar
3+ years of experience developing data sets for analysis use-cases
3+ years of experience with direct stakeholder management and ownership of full lifecycle of analytics/data projects and presenting analysis results to stakeholders
3+ years report-development and data visualization experience with BI tools (Looker, Tableau, etc.)
Ability to deliver both high-level explanations of complex problems while also able to deep dive into complex problems
Advanced SQL skills for querying and analysis of large, complex datasets
Must work well in a team environment and be able to partner with both business and technical team members and translate technical language to the non-technical need
Creative problem solver, ability to handle multiple projects, and strong work ethic needed
Excellent written and verbal communication skills
Highly organized and systematic, superb attention to detail
Able to complete tasks with a high degree of accuracy and an eye towards process improvement
Persistent and resilient person who thrives in a fast-paced, highly collaborative environment
Bonus points for:
Advanced analytics experience - classification/predictive modeling, optimization models, forecasting
Experience with Python for data engineering or advanced data analysis projects/use-cases
Healthcare domain knowledge and experience
Expert knowledge of database/data modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others
Experience building cloud-native applications and supporting technologies / patterns / practices including: GCP, AWS, Docker, CI/CD, DevOps, and microservices

#LI-EH1
#LI-Hybrid
We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.1,"Tempus
3.1","Chicago, IL",501 to 1000 Employees,2015,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
233,Senior Data Engineer,Employer Provided Salary:$105K - $186K,"**(UPDATED 8/21/2023) We invite applicants to express their interest in this role, but please note that we currently don't have a specific timeline for filling this position. This posting is intended for talent pooling. While there isn't a definitive hiring period, potential candidates should be aware that the role might be filled at some point in the future. We encourage you to start the process and discover more about this opportunity, Slalom, and potentially a career in consulting through discussions with our talent acquisition team.
Who You’ll Work With
As a modern technology company, we've never met a technical challenge we didn't like. We enable our clients to learn from their data, create incredible digital experiences, and make the most of new technologies. We blend design, engineering, and analytics expertise to build the future. We surround our technologists with interesting challenges, innovative minds and emerging technologies.
A Senior Data Engineer at Slalom is a technical team member who designs and develops data platform solutions. It uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g., data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc.
What You’ll Do
Work as part of a team and with clients to develop cloud data and analytics solutions
Make recommendations and prove data architecture concepts to team members and clients
Development of cloud data platforms and business intelligence solutions
Data wrangling of heterogeneous data sources for exploring and discovering new insights
Work with highly collaborative agile teams on modern data architecture projects
Key success measures of a Senior Data Engineer include technical proficiency in data processing technologies, collaboration with other teams, communication, and willingness to learn new things
What You'll Bring
Experience in one or more cloud technologies (AWS, Azure, Google Cloud, Snowflake)
Experience in data modeling, design, and development using complex SQL (SQL Server, Oracle, etc.)
Experience in Apache Airflow, Talend, SSIS, Informatica, Alteryx, or other data processing tool
Experience in one or more programming languages (Python, C#, Scala, R, PowerShell, Java, etc.)
Experience in cloud engineering, serverless architecture, and/or data streaming services
About Us
Slalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six countries and 43 markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 13,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.
Compensation and Benefits
Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance.
Slalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $105,000 to $186,000. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.
EEO and Accommodations
Slalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team if you require accommodations during the interview process.
#LI-JS8
#LI-Hybrid",4.3,"Slalom Consulting
4.3","Los Angeles, CA",10000+ Employees,2001,Company - Private,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
234,Data Science Engineer,$89K - $122K (Glassdoor est.),"Company Description

ClientSolv Technologies is an IT solution firm with over a decade of experience serving Fortune 1000 companies, public sector and small to medium sized companies. ClientSolv Technologies is a woman-owned and operated company that is certified as a WMBE, 8a firm by the Federal government's Small Business Administration.

Job Description

We are seeking a Data Science Engineer for a 12-month contract (with the option to extend further) in Gaithersburg, MD. This role will be onsite/ in the office Monday-Friday during normal business hours (there are no remote options) and will be working with the data science innovation team with:

Providing technical leadership and identifying solutions for complex problems.
Understand how to design and build a deployable application that uses any ML model that they design and recommend
Recommend, track, and report on the business goals and the resulting technical goals, projects, and technical tasks.
Perform proof of concepts, identify initial data wrangling, and then provide guidance for development of the productized version of the capabilities.
Analyze data and build predictive models based on stakeholder input.

Qualifications

Basic Requirements:
Bachelors degree in Computer Science, Data Science or related technical field (Masters in Data Science preferred)
10+ years of technical experience, preferably in computer networking and communications.
5+ years of experience performing data science
Preferred Qualifications:
Knowledge and experience in leading analysis efforts for large operational networks .
Advanced analytical and problem-solving abilities.
Ability to break down and understand complex business problems, define a solution and implement it using advanced quantitative methods.
Identify innovative solutions that apply data science, AI/ML technology to the problem so that a development team can then take that and build an application that leverages the DS approaches.
Must have strong DS design knowledge and experience in some ML/Ops/DevOps
Proficiency in programming for data analysis; ideally Python (or R) and SQL. Knowledge of Pandas is essential.
Knowledge of building machine learning models using Scikit-Learn, TensorFlow, Keras, or other open-source libraries.
Knowledge of data visualization tools and libraries in Python/R (Shiny preferably) Or Tableau.
Familiarity with software version control with git.
Strong work ethic and intellectual curiosity.
Knowledge of Cloud technologies such as AWS or Google
Knowledge of relational databases such as My SQL and BigQuery.
Team player with excellent communication and problem-solving skills. Solid oral and written communication skills, especially around analytical concepts and methods
Experience working across varying business and technical functional units

Additional Information

This 12-month contract (with the option to extend) will be located onsite/ in the office in Gaithersburg, MD during normal business hours, Monday- Friday (there are no remote options).",3.7,"ClientSolv Technologies
3.7","Gaithersburg, MD",Unknown,1994,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
235,Data Engineer,Employer Provided Salary:$150K,"Data Engineer
We are looking for a Data Engineer that will architect and implement systems handling the ingestion, cleaning, and structuring of data. They will be a member of the Data Science team and will have direct interaction with the people using the data ingested. The hire will also be responsible for managing our group’s core infrastructure including our region redundant pipeline orchestration servers (Airflow) and webserver stack (NGINX + Gunicorn + Django). The ideal candidate is an experienced data pipeline builder and core infrastructure guru.
Responsibilities:
Create and maintain an optimal data pipelines in Python + SQL
Build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python and SQL (Snowflake & SQL Server) in both on premise and cloud environments
Manage and improve the user facing server infrastructure (web + api) including authorization and load balancing
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that that provide actionable insights on both the user interactions and infrastructure loads
Technical Qualifications:
Python expert, notably savvy with data science stack (Pandas, NumPy, SciPy)
Relational SQL databases, Microsoft SQL Server and Postgres preferred
Snowflake cloud database experience
Pipeline and workflow management tools: Airflow, Luigi, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Athena, SQS, etc.
2+ professional years’ experience with Masters or 3+ professional years with Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Highly Valued:
Data Science/Analysis background
Computer science understanding, data structures, processes, threading, memory usage
Unix/Linux command-line experience
Broad understanding of equities, derivatives, futures, FX, or other financial-services instruments
Other Qualifications:
Excellent listening, and communication (both oral and written) skills
Self-starter and critical thinker, takes ownership of own projects and makes improvement suggestions for the entire infrastructure.
Proactive, assertive and attentive to details.
Can work independently and in a collaborative environment.
Can handle several projects with different priorities at the same time in a fast-paced environment.
Excellent self-management and problem-solving skills.
Results-oriented, can deliver quality code quickly
Millennium pays a total compensation package which includes a base salary, discretionary performance bonus, and a comprehensive benefits package. The estimated base salary range for this position is $150,000, which is specific to New York and may change in the future. When finalizing an offer, we take into consideration an individual’s experience level and the qualifications they bring to the role to formulate a competitive total compensation package.",3.8,"Millennium Management LLC
3.8","New York, NY",1001 to 5000 Employees,1989,Company - Private,Investment & Asset Management,Financial Services,Unknown / Non-Applicable
236,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
237,Senior Data Engineer,-1,"JOB DESCRIPTION:
Need to be expertise Coding on PySpark.
Supply Chain and Azure.
Experience in working with end-to-end technology implementation project, preferable those related to Data warehousing/Analytics/Reporting
Strong communication skills with the ability to collaborate effectively with business owners/users and technical teams.
Proven ability to gather product requirements from stakeholders and prioritize feature implementations based on business needs and product strategy.
Strong problem-solving skills and attention to detail.
Good to have experience with CPG/ Retail industry (preferable CPG companies such as Nestle, Unilever, P&G, Coca coal, PepsiCo etc)
Bachelor's/master's degree in business administration, computer science, engineering, or a related field.
Job Type: Contract
Experience:
PySpark: 5 years (Preferred)
SQL: 10 years (Preferred)
Data analytics: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Azure: 5 years (Preferred)
Data Bricks: 5 years (Preferred)
Data Lake: 5 years (Preferred)
Synapse: 5 years (Preferred)
Retail Domain: 8 years (Preferred)
CPG: 7 years (Preferred)
Work Location: Remote",4.0,"Quadrant Resource
4.0",Remote,1001 to 5000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
238,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
239,Cloud Data Engineer,Employer Provided Salary:$69.61 - $80.00 Per Hour,"Top skills:
Must-Haves (Concepts & Tools):
AWS cloud—KMS, S3, Glue, Lambda etc.
Deployed data pipelines
Java, Python or PySpark hands on development experience
Nice-to-Haves (Concepts & Tools):
Prior ETL migration experience from on prem. To cloud
Exposure to even driven streaming
Job Types: Full-time, Contract, Temporary
Pay: $69.61 - $80.00 per hour
Experience level:
7 years
8 years
9 years
Work Location: Remote",-1,Cloudrex,Remote,-1,-1,-1,-1,-1,-1
240,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
241,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
242,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
243,Senior Data Engineer,Employer Provided Salary:$105K - $186K,"**(UPDATED 8/21/2023) We invite applicants to express their interest in this role, but please note that we currently don't have a specific timeline for filling this position. This posting is intended for talent pooling. While there isn't a definitive hiring period, potential candidates should be aware that the role might be filled at some point in the future. We encourage you to start the process and discover more about this opportunity, Slalom, and potentially a career in consulting through discussions with our talent acquisition team.
Who You’ll Work With
As a modern technology company, we've never met a technical challenge we didn't like. We enable our clients to learn from their data, create incredible digital experiences, and make the most of new technologies. We blend design, engineering, and analytics expertise to build the future. We surround our technologists with interesting challenges, innovative minds and emerging technologies.
A Senior Data Engineer at Slalom is a technical team member who designs and develops data platform solutions. It uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g., data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc.
What You’ll Do
Work as part of a team and with clients to develop cloud data and analytics solutions
Make recommendations and prove data architecture concepts to team members and clients
Development of cloud data platforms and business intelligence solutions
Data wrangling of heterogeneous data sources for exploring and discovering new insights
Work with highly collaborative agile teams on modern data architecture projects
Key success measures of a Senior Data Engineer include technical proficiency in data processing technologies, collaboration with other teams, communication, and willingness to learn new things
What You'll Bring
Experience in one or more cloud technologies (AWS, Azure, Google Cloud, Snowflake)
Experience in data modeling, design, and development using complex SQL (SQL Server, Oracle, etc.)
Experience in Apache Airflow, Talend, SSIS, Informatica, Alteryx, or other data processing tool
Experience in one or more programming languages (Python, C#, Scala, R, PowerShell, Java, etc.)
Experience in cloud engineering, serverless architecture, and/or data streaming services
About Us
Slalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six countries and 43 markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 13,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.
Compensation and Benefits
Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance.
Slalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $105,000 to $186,000. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.
EEO and Accommodations
Slalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team if you require accommodations during the interview process.
#LI-JS8
#LI-Hybrid",4.3,"Slalom Consulting
4.3","Los Angeles, CA",10000+ Employees,2001,Company - Private,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
244,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
245,Sr. Data Engineer,Employer Provided Salary:$118K - $142K,"Data Engineer
Egen Solutions Inc offers competitive compensation, benefits, and an excellent culture of teamwork and collaboration. We are an Equal Opportunity Employer that does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.
Job Summary
We have some major internal projects underway at Egen and need a highly skilled Data Engineer to join our team on a full-time basis. Below is the JD, kindly go through it and let me know your interest and if you are interested revert back with your updated resume
Our Data Engineering teams build scalable data pipelines using Python, Spark, and cloud services (GCP and AWS). The pipelines we build typically integrate with technologies such as Kafka, Storm, and Elasticsearch. We are working on a continuous deployment pipeline that leverages rapid on-demand releases. Our developers work in an agile process to efficiently deliver high value applications and product packages.
As a Staff Data Engineer at Egen, you will leverage Spark and GCP (preferred) to architect and implement cloud-native data pipelines and infrastructure to enable analytics and machine learning on rich datasets.
Required Experience:
Built and run resilient data pipelines in production and have implemented ETL/ELT to load a multi-terabyte enterprise data warehouse.
Implemented analytics applications using multiple database technologies, such as relational, multidimensional (OLAP), key-value, document, or graph.
Defined data contracts, and wrote specifications including REST APIs.
Transformed data between data models and formats with the most modern PySpark practices. Have built cloud-native applications and supporting technologies / patterns / practices including: Cloud Services, Docker, CI/CD, DevOps, and microservices.
Planned and designed artifacts that describe software architectures involving multiple systems and technologies
You've worked in agile environments and are comfortable iterating quickly.
Nice to have's (but not required):
GCP expertise is preferred but will consider AWS
Experience moving trained machine learning models into production data pipelines.
Experience in biotech, genomics, clinical research or precision medicine.
Expert knowledge of relational database modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others.
This is full-time, On-site, and Remote. We are based out in Naperville, IL.
Looking forward to striking up a conversation with you on the opportunity!, Kindly visit our website egen.solutions
equirements
Bachelor’s degree in Computer Science, Engineering, or related field.
3+ years of experience with data engineering and/or data analysis.
Strong analytical skills and ability to think analytically.
Ability to work independently with minimal supervision.
Experience with SQL and/or NoSQL databases is a plus.
Experience with ETL processes is a plus.
Experience with cloud-based solutions such as Azure Data Factory is a plus.
Experience with Agile software development methods such as Scrum is a plus.
Ability to work in a fast-paced environment and adapt to changing priorities.
Ability to work in a team environment while maintaining strong individual initiative.
Job Type: Full-time
Pay: $117,577.88 - $141,599.17 per year
Benefits:
401(k)
Dental insurance
Health insurance
Experience level:
5 years
Application Question(s):
Are you a personal who hold any of these work authorizations currently(OPT EAD/GC/F1 OPT/H4 EAD.
Work Location: In person",-1,Egen Solutions Inc,"Naperville, IL",-1,-1,-1,-1,-1,-1
246,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
247,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
248,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
249,Data Engineer IV,Employer Provided Salary:$149K,"Who We Are
Invitation Homes is a fast-paced evolving publicly traded REIT that is pioneering a new industry with over 80,000 single family rental homes and a market capitalization of over $25 billion. We are a team of more than 1,400 associates who come from all walks of life. We call different communities “home,” but our shared values bind us together. Invitation Homes is a place where possibility lives.
THR Property Management LP / Data Engineer / Dallas, TX: Develop and maintain data pipelines into data warehouse and data lake, including design, and development. Develop and implement strategies to translate business requirements into feasible and acceptable data warehouse and data lake solutions. Architect and build new data models to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making. Design, build and launch data pipelines to move data to data lake and data warehouse build and maintain framework for auditing, error logging, and master data management for data pipelines. Build data expertise and own data quality for the data pipelines. Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes. Perform data analysis and assist in the resolution of data issues. Identify and resolve defects of complex scope using proper engineering tools and techniques. Provide support and maintain existing products and add new features. Mentor engineers by providing advice, coaching and educational opportunities. Telecommuting permitted from any location within the U.S. Salary: 148512/year.
The Senior Data Engineer is accountable for the maintenance, improvement, and movement of data in the Enterprise Data Warehouse, and other data sources used for enterprise reporting. A successful Senior Data Engineer delivers database design, implementation, and schematics that creates sustainable, competitive advantage for our company. Further, a Senior Data Engineer is responsible for the development and deployment of innovative analytics data platforms to support end-user enterprise reporting. Working with other data engineers, data analysts, and data scientists, a Senior Data Engineer must have a delivery first mentality and a firm grasp of Agile Development methodology. Lastly, the ideal candidate must demonstrate the ability to work as a team member and team leader.
What You’ll Do
Accountable for the overall performance and maintenance of our SQL Server environments:
Manage the EDW relational database for optimized performance
Design scalable ETL packages from the business source systems
Ensure all necessary testing and validation for all planned and unplanned releases
Mentor junior data engineers in best practices and documentation.
Act as our database evangelist in leading innovation activities through exploration, benchmarking and implementation of data technologies
Participate in solution-based activities; articulate your ideas through well thought out documentation and research best-in-class solutions
Lead and Develop and implement data architecture standards
Manage the necessary testing and validation to ensure proper compliance to data governance and quality
Prepare progress reports regarding our database environments’’ status and health
Manage troubleshooting data issues and present solutions to these issues.
Resolve tactical issues thoroughly while always looking for optimization and documenting lessons learned
Understand when tactical issues are really systematic issues requiring deeper level of analysis and strategic solutions
Cooperate with the support team to investigate and resolve data related defects
Proactively analyze and evaluate current state of data in order to identify and recommend improvements and optimization
Support our Agile Development methodology
Constantly prioritize user stories; balance feature development, defects, and tech debt according to business need
Be a champion of continuous improvement and delivery; define the minimum viable product, focus teams on key release milestones, and leverage fast follow-up sprints
Enable sprint progress; clarify story details, validate completed stories, and prepare stories for the next sprint
Who We’re Looking For
Bachelor’s degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field. An equivalent of the same in working experience is also accepted for the position.
A candidate for the position will have as at least 5 years of working experience as a database engineering or a database engineering administrator within a fast-paced a complex business setting.
Design, implement and maintain SQL Server databases
Design, implement and maintain ETL processes using SQL Server SSIS
Provide ongoing maintenance support through SQL query tuning and optimization
Strong working and conceptual knowledge of building and maintaining physical and logical data models
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience working in SaaS, IaaS, and PaaS
Excellent customer service and interpersonal skills; ability to relate to and get along with others
Professional verbal and written communication skills
Strong organizational and time-management skills
Ability to multi-task and maintain flexibility and creativity in a variety of situations
Ability to analyze and resolve problems
Ability to set and meet goals and consistently meet deadlines
Ability to maintain confidentiality
Why Invitation Homes
We stand for flexibility, opportunity, and a home that people can make their own. It’s as true for our associates as it is for our residents. Just like we help our residents live freer, we liberate our associates’ careers, too. Our associates know that at Invitation Homes goals matter, potential is unlocked, and careers thrive. Invitation Homes isn’t just a work place it is a possibility place. Invitation Homes offers the below to each new associate:
Competitive pay and an annual bonus program for all associates
Generous paid time off plans including vacation accrual, sick time, volunteer time, and standard and floating holidays
401k with matching company contributions
Awesome work environment with casual dress
Team events and gatherings
Employee resource groups: Together with Women, Asian Alliance, Black Collective, Juntos, Gen Next, and Open Invitation.
Invitation Homes truly is where possibility lives, pour a new foundation here!
Salary Range
$98,640.00 - $170,976.00
Compensation and Benefits
To attract and retain top talent, we're pleased to offer competitive compensation and benefits, including:
Annual bonus program
Health, dental, vision, and life insurance
Long-term and short-term disability insurance
Generous paid time off plans include vacation accrual, sick time, standard holidays and floating holidays
401(k) with company matching contributions
Awesome work environment with casual dress
Team events and gatherings (Pre- and Post-Covid)
Invitation Homes is an equal opportunity employer committed to fostering a diverse, inclusive and innovative environment with the best associates. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, Veteran status or any other factor protected by applicable federal, state or local law. If you have a disability or special need that requires accommodation, please contact us at
humanresources@invitationhomes.com
.
To all recruitment agencies: Invitation Homes does not accept agency resumes. Please do not forward resumes to Invitation Homes employees. Invitation Homes is not responsible for any fees related to unsolicited resumes.",3.9,"Invitation Homes
3.9","Dallas, TX",1001 to 5000 Employees,2012,Company - Public,Real Estate,Real Estate,$1 to $5 billion (USD)
250,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
251,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
252,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
253,Junior Data Engineer,$83K - $119K (Glassdoor est.),"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a junior data engineer, you will be part of a team that is responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.
The position:
Develop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Support various components of the data pipelines, including ingestion, validation, cleansing and curation
Manage and ensure the success of ongoing data pipeline routines
Collaborate with our implementation team to assist with the identification and reconciliation of data anomalies
Create and maintain documentation on data pipelines
Engage with our software engineering team to ensure precise data points per application specifications
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering, or applicable experience
Expertise with SQL, database design and data manipulation methodologies
Expertise with ETL/ELT and the development of automated validation and data pipelines
Strong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW, master data management and other database design principles
Comfortable working with high volume data in a variety of formats
Excellent verbal and written communication
Self-motivated
Passionate at learning
Familiarity with healthcare data is a plus
Experience with CI/CD and version control tools is a plus
Experience working within hybrid cloud environment such as AWS is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation
Social outings",5.0,"Decision Point Healthcare
5.0","Boston, MA",1 to 50 Employees,2013,Company - Public,-1,-1,Less than $1 million (USD)
254,Senior Data Engineer,-1,"JOB DESCRIPTION:
Need to be expertise Coding on PySpark.
Supply Chain and Azure.
Experience in working with end-to-end technology implementation project, preferable those related to Data warehousing/Analytics/Reporting
Strong communication skills with the ability to collaborate effectively with business owners/users and technical teams.
Proven ability to gather product requirements from stakeholders and prioritize feature implementations based on business needs and product strategy.
Strong problem-solving skills and attention to detail.
Good to have experience with CPG/ Retail industry (preferable CPG companies such as Nestle, Unilever, P&G, Coca coal, PepsiCo etc)
Bachelor's/master's degree in business administration, computer science, engineering, or a related field.
Job Type: Contract
Experience:
PySpark: 5 years (Preferred)
SQL: 10 years (Preferred)
Data analytics: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Azure: 5 years (Preferred)
Data Bricks: 5 years (Preferred)
Data Lake: 5 years (Preferred)
Synapse: 5 years (Preferred)
Retail Domain: 8 years (Preferred)
CPG: 7 years (Preferred)
Work Location: Remote",4.0,"Quadrant Resource
4.0",Remote,1001 to 5000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
255,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
256,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
257,Sr. Data Engineer,$83K - $115K (Glassdoor est.),"Provides application design, analysis, programming, testing and documentation for the development, integration, enhancement, and maintenance of information systems solutions
Job Description:
BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Job Responsibilities
Formulates and defines system scope and objectives through research and fact-finding to design, develop, modify, or integrate complex information systems.
Devises or modifies application systems and procedures to optimize functional requirements including capacity, operating time, response time, and form of desired results.
Designs, codes, tests, debugs, and documents programs, subroutines, and scripts.
Develops and implements program/system test plans. Devises data verification methods and standard system procedures.
Responds to system failures and performance events by taking appropriate measures to reduce system downtime and eliminate recurrence of problems.
May conduct research on software and hardware products to justify recommendations and support management in budgeting and purchasing efforts.
Acts as expert technical resource to development staff in all phases of the development and implementation process.
Job Qualifications
Education
Bachelors Degree in Computer Science or equivalent work experience required. Equivalent years of experience are determined as one year of technical experience for every year of college requested.
Experience
1 year - Experience with information technology concepts, application development methodology, terminology, and standards required
5 years - Systems and programming experience required
1 year - Expert working knowledge of at least one programming language in use at BCBST.
Skills\Certifications
Proven skill with information technology in use at BCBST is required
Proven analysis, design, and coding skills, and demonstrated success in leading large and complex projects
Top level technical experience in one or more specialized areas of applications systems analysis and programming
Demonstrated ability to interpret and translate technical and/or or complex concepts into information meaningful to project team members and/or business personnel.
Be organized, reliable, and able to manage multiple tasks with exceptional work ethic.
Leadership skills
Must be able to communicate effectively with both technical and non-technical co-workers
Job Specific Requirements:
Preference for candidates with Cloud experience (GCP specifically)
Python knowledge/experience a plus
Preferred Skills:
Number of Openings Available:
1
Worker Type:
Employee
Worker Sub-Type:
Employee
Company:
BCBST BlueCross BlueShield of Tennessee, Inc.
Applying for this job indicates your acknowledgement and understanding of the following statements:
BCBST is an Equal Opportunity employer (EEO), and all employees and applicants will be entitled to equal employment opportunities when employment decisions are made. BCBST will take affirmative action to recruit, hire, train and promote individuals in all job classifications without regard to race, religion, color, age, sex, national origin, citizenship, pregnancy, veteran status, sexual orientation, physical or mental disability, gender identity, or any other characteristic protected by applicable law.
Further information regarding BCBST's EEO Policies/Notices may be found by reviewing the following page:
BCBST's EEO Policies/Notices
BlueCross BlueShield of Tennessee is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at BlueCross BlueShield of Tennessee via-email, the Internet or any other method without a valid, written Direct Placement Agreement in place for this position from BlueCross BlueShield of Tennessee HR/Talent Acquisition will not be considered. No fee will be paid in the event the applicant is hired by BlueCross BlueShield of Tennessee as a result of the referral or through other means.
All applicants will be advised that BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Tobacco-Free Hiring Statement
To further our mission of peace of mind through better health, effective 2017, BlueCross BlueShield of Tennessee and its subsidiaries no longer hire individuals who use tobacco or nicotine products (including but not limited to cigarettes, cigars, pipe tobacco, snuff, chewing tobacco, gum, patch, lozenges and electronic or smokeless cigarettes) in any form in Tennessee and where state law permits. A tobacco or nicotine free hiring practice is part of an effort to combat serious diseases, as well as to promote health and wellness for our employees and our community. All offers of employment will be contingent upon passing a background check which includes an illegal drug and tobacco/nicotine test. An individual whose post offer screening result is positive for illegal drugs or tobacco/nicotine and/or whose background check is verified to be unsatisfactory, will be disqualified from employment, the job offer will be withdrawn, and they may be disqualified from applying for employment for six (6) months from the date of the post offer screening results.
Resources to help individuals discontinue the use of tobacco/nicotine products include smokefree.gov or 1-800-QUIT-NOW.",3.8,"BlueCross BlueShield of Tennessee
3.8","Chattanooga, TN",5001 to 10000 Employees,1945,Nonprofit Organization,Insurance Carriers,Insurance,$5 to $10 billion (USD)
258,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
259,Data Engineer I - People Analytics,$73K - $111K (Glassdoor est.),"POSITION SUMMARY Our People Analytics team is expanding! The Data Engineer I is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.

This position can work in a flexible, hybrid model with 3 days on-site in Appleton, WI and 2 days remote.

JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.

QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming. Preferred experience in Python applied to data engineering and transformation.
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated

#LI-Hybid

Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101 .",4.1,"US Venture
4.1","Appleton, WI",1001 to 5000 Employees,1951,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
260,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
261,Senior Data Engineer,$87K - $114K (Glassdoor est.),"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.

This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration.
How will you make an impact?
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
What are we looking for?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework
You might be a good fit if you like to:
Leverage technology to deliver data solutions to drive business outcomes
Drive for continuous improvement of processes and solutions
Bring energy and commitment to excellence to drive delivery of high-quality solutions
Share knowledge and mentor team members on technologies and solutions
What you can learn on the job:
Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers
Grow practical understanding of agile work practices through application of the scrum framework to deliver work products
Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science
What does success look like?
First 30 days:
Understand the existing analytics data platform and technologies
Understand team’s work management processes
First 6 months:
Develop data solutions for the business including shared components and specific data sets
Learn business processes and develop necessary contacts throughout the organization
First year:
Continue to develop data solutions for the business
Become expert on the team for other data engineers to go to for mentoring in completing their work
Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,"AE Business Solutions
4.2","Milwaukee, WI",51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
262,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
263,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
264,Senior Data Engineer,Employer Provided Salary:$166K - $232K,"Patreon is the best place for creators to build memberships by providing exclusive access to their work and a deeper connection with their communities. We're building a content and community platform where creators can engage directly with their fans and monetize their creativity, while maintaining full ownership over the work they make and the communities they create.
We're leaders in the membership space with 250,000+ active creators and over $3.5 billion paid directly to creators on our platform. Our team is building tools to optimize the creator-to-fan relationship, including native video, enhanced podcasting features, improved creation tools, and new community experiences. We're continuing to invest heavily in building the most talented team in the Creator Economy and are looking for a Data Engineer to support our mission.
This role is available to those wishing to work in our SF and NY offices on a hybrid work model or those wishing to be fully remote in the United States.
About the Role
Work on a tight-knit team of highly motivated and experienced data engineers with frequent collaboration with data scientists, product managers and product engineers.
Work on both ""data analytics"" and ""data infrastructure"" type projects in a fast-paced, high-growth startup environment.
Build core data sets and metrics to power analytics, reports and experimentation.
Write real-time and batch data pipelines to support a wide range of projects and features including our creator-facing analytics product, executive reporting, FP&A, marketing initiatives, model training, data science analytics, A/B testing, etc.
Help manage and build out our data platform and suite of data tools.
Be a driver of a data-centric culture at Patreon. Work autonomously on large green field initiatives and help define data best practices at the company.
About You
Expert in SQL, Spark and Python or Scala
Significant experience modeling data and developing core data sets and metrics to support analytics, reports and experimentation. Solid understanding of how to use data to inform the product roadmap.
Enjoy collaborating with Data Scientists, Product Managers and Product Engineers. Comfortable playing the role of a Project Manager in order to drive results.
Have previously built real-time and batch data pipelines using tooling such as Airflow, Spark, Kafka, S3, Fivetran, Census, etc. Experience working with event tracking frameworks, data observability frameworks and experimentation frameworks.
Experience managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc.
Highly motivated self-starter that is keen to make an impact and is unafraid of tackling large, complicated problems and putting in the work to ensure high craft deliverables.
About Patreon
Patreon powers creators to do what they love and get paid by the people who love what they do. Our team is passionate about making this mission and our core values come to life every day in our work. Through this work, our Patronauts:
Put Creators First | They're the reason we're here. When creators win, we win.****
Build with Craft | We sign our name to every deliverable, just like the creators we serve.
Make it Happen | We don't quit. We learn and deliver.
Win Together | We grow as individuals. We win as a team.
We hire talented and passionate people from different backgrounds because we want our company to represent the vast diversity of our creators. If you're excited about a role but your prior experience doesn't match with every bullet point outlined above, we strongly encourage you to apply anyway. If you're a creator at heart, are energized by our mission, and share our company values, we'd love to hear from you.
Patreon is proud to be an equal-opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.
Patreon offers a competitive benefits package including and not limited to salary, equity plans, healthcare, unlimited paid time off, company holidays and recharge days, commuter benefits, lifestyle stipends, learning and development stipends, patronage, parental leave, and 401k plan with matching.
The posted range represents the expected salary range for this job requisition and does not include any other potential components of the compensation package, benefits and perks previously outlined. Ultimately, in determining pay, we'll consider your experience, leveling, location and other job-related factors.
San Francisco Pay Range
$166,000—$231,500 USD",3.2,"Patreon
3.2","San Francisco, CA",51 to 200 Employees,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
265,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
266,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
267,Senior Data Engineer,$115K - $155K (Glassdoor est.),"At Compass, our mission is to help everyone find their place in the world. Founded in 2012, we're revolutionizing the real estate industry with our end-to-end platform that empowers residential real estate agents to deliver exceptional service to seller and buyer clients.
The Data Platform team at Compass is responsible for building and operating a unified, secure and scalable data platform, making high-quality data available for business intelligence, financial reporting, and data science use cases.
As a Senior Engineer of the Data Platform team, you will play a crucial role in building and operating the data infrastructure that serves the entire company. Your responsibilities will include delivering high-quality results to ensure the security, compliance, optimization, and automation of the data infrastructure. Additionally, you will take ownership and actively contribute to the architectural innovations while maintaining a high standard for quality.
To thrive in this role, effective collaboration with engineers and stakeholders across the company is essential. A strong sense of ownership for the product you work on is important to you. You have a genuine passion for learning and find joy in sharing your knowledge with others. You try to understand others before seeking to be understood. Every day you wake up excited about the new things you'll learn and the people you can inspire.
Responsibilities:
Data architecture: Evaluate data platform architecture. Build and optimize data services to improve compliance, security, reliability, performance, and resource utilization.
Compliance: Build process and tools to ensure that the data platform is compliant with SOX (Sarbanes-Oxley Act), GLBA (Gramm-Leach-Bliley Act ), and CCPA (California Consumer Privacy Act).
Security: Identify infrastructure security-relevant actions, tests, and key performance indicators. Enhances the security posture of the Compass data platform, ensuring the protection of data, systems, and assets.
Infrastructure automation: Increases reliability, enhances employee productivity, reduces security attack surfaces, and eliminates human errors through standardization of process and Infrastructure as Code.
Data infrastructure monitoring. Monitor the platform health, reliability, cost, and usage.
Best practices and guardrails: Champion the best practices and guardrails for using the data platform. Identify cost-saving opportunities and optimize the data platform for all stakeholders.
Operational support: Support the platform users for troubleshooting, configuration, version updates, and feature adoptions.
Qualifications:
8 years of development experience with a focus on architecting cloud data platforms, setting up, managing and automating the infrastructure of cloud data platforms, including Airflow, Databricks, Spark, and Kafka
5 years of developer experience using Infrastructure as Code (IaC) technologies, such as AWS Cloud Formation and Terraform
3 years of experience with data compliance and governance. Demonstration of collaboration with the internal compliance team and external auditors in planning and execution of all phases of SOX compliance including risk assessment & scoping, documentation of process walkthroughs, identifying controls and key reports, testing, and reporting results. Implement customer data deletion requests that are required by CCPA. Standardize and automate the compliance process.
5 years of experience maintaining and improving OE of data platforms - service alerting, monitoring,and automation.
8 years of experience and proficient programming skills in Python or Java/C#. 3 years of programming experience in SQL.
B.S., M.S., or PhD. in Computer Science or equivalent
Desirable to Have:
Subject matter expert (SME) in architecting cloud data platforms using Databricks, Snowflake, AWS Lake Formation or other relevant technologies.
SME in big data technologies including Spark, Presto, Airflow, Kafka
Work in a startup-like environment building agile products and services
Create clear and concise documentation, including diagrams, service descriptions, decisions, and runbooks.
Deep knowledge of cloud platform such as AWS, GCP, Azure
Proven records of improving services in your domain throughout their lifecycle. Identify and remove bottlenecks to address inefficiencies in the developer experience.
Check out Compass's Engineering blog!
Perks that You Need to Know About:
Participation in our incentive programs (which may include where eligible cash, equity, or commissions). Plus paid vacation, holidays, sick time, parental leave, marriage leave, and recharge leave; medical, tele-health, dental and vision benefits; 401(k) plan; flexible spending accounts (FSAs); commuter program; life and disability insurance; Maven (a support system for new parents); Carrot (fertility benefits); UrbanSitter (caregiver referral network); Employee Assistance Program; and pet insurance.

Do your best work, be your authentic self.
At Compass, we believe that everyone deserves to find their place in the world — a place where they feel like they belong, where they can be their authentic selves, where they can thrive. Our collaborative, energetic culture is grounded in our Compass Entrepreneurship Principles and our commitment to diversity, equity, inclusion, growth and mobility. As an equal opportunity employer, we offer competitive compensation packages, robust benefits and professional growth opportunities aimed at helping to improve our employees' lives and careers.
Notice for California Applicants",3.6,"Compass
3.6","Austin, TX",1001 to 5000 Employees,2012,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
268,Senior Data Engineer,$105K - $141K (Glassdoor est.),"Who You'll Work With:
Fixed Income Investment Technology (“FIIT”) group builds software that the Fixed Income business of AB uses in performing functions such as Fundamental Research, Quantitative research, Portfolio Management, Order Generation, Trading and Middle office and BackOffice operations. It partners with business to understand their challenges and help them by providing innovative technology solutions. We re-engineer the process where applicable in collaboration with business to help scale their business and be efficient in this dynamic market conditions. Our Data Warehouse ingests and egresses critical reference and transaction data from up-stream and down-stream systems.
We are seeking a Nashville based senior professional to join our Fixed Income Investment Technology team. With complex architecture which is core to how we service FI clients (and beyond), this is a critical role where someone with industry experience will be a good fit allowing us to scale our processes. Data is a constantly growing entity and we need to have a clean architecture to align our data with supporting investment team needs. The current data team has a lot of junior – mid level staff and needs someone with strong technical skills and leadership skills to design scalable infrastructure. The ideal candidate would have experience with designing, implementing, maintaining, monitoring and performance tuning ETL and ELT pipelines using technologies like Azure, Airflow, Python, Oracle and SQL server. They would be proficient in various data patterns like Gamma, Kappa, Data mesh / fabric, AKS, Data Lakes and Delta Lake

What You'll Do:
The primary project that the candidate will contribute to all modules within Fixed Income technology. These modules support Senior Portfolio Managers, Portfolio Managers, Assistant Portfolio Managers, Fundamental and Quantitative research analysts, Traders, Middle-office, and Operations functions.
Data Management Tools and Technologies:
Proficiency in data management tools such as SQL-based databases (e.g., SQL Server, MySQL, PostgreSQL, Oracle) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with data integration and ETL tools.
Knowledge of data warehousing concepts and technologies, including star schemas, data marts, and data lakes.
Familiarity with cloud-based data platforms (e.g., AWS Redshift, Google BigQuery, Azure SQL Data Warehouse) and related services.
Understanding of big data technologies such as Hadoop, Spark, and related ecosystem components (e.g., HDFS, Hive, Pig).
Data Modeling and Design:
Proficient with designing an optimal Data warehouse and Operational data store using modern cloud patterns.
Proficient in data modeling techniques and tools, such as ER modeling and UML diagrams.
Experience in designing and implementing data schemas, including conceptual, logical, and physical models.
Knowledge of dimensional modeling and familiarity with industry-standard frameworks such as Kimball or Data Vault.
Ability to optimize database performance through indexing, partitioning, and query tuning.
Data Governance and Compliance:
Understanding of data governance frameworks, data stewardship, and data ownership concepts.
Knowledge of data privacy regulations (e.g., GDPR, CCPA) and experience implementing data protection measures.
Familiarity with data security best practices, encryption methods, access controls, and user authentication mechanisms.
Data Quality Assurance:
Experience in data profiling techniques and tools to assess data quality and identify data anomalies.
Proficiency in data cleansing methods, data validation techniques, and data transformation rules.
Knowledge of data quality frameworks, metrics, and best practices for measuring and monitoring data quality.
Programming and Scripting:
Proficiency in one or more programming languages such as Python, Java, or Scala for data manipulation and transformation tasks.
Experience in writing complex SQL queries and stored procedures for data retrieval and manipulation.
Familiarity with scripting languages such as Bash or PowerShell for automation and process improvement.
Data Visualization and Reporting:
Knowledge of data visualization tools such as Tableau, Power BI, or QlikView for creating interactive and meaningful visualizations.
Ability to design and develop reports, dashboards, and data-driven presentations to communicate insights to stakeholders.
Cloud and Distributed Computing:
Understanding of cloud computing concepts and experience working with cloud platforms (e.g., AWS, Azure, or GCP).
Knowledge of distributed computing frameworks such as Apache Spark for large-scale data processing and analytics.
Software Development and Agile Practices:
Familiarity with software development methodologies, particularly Agile/Scrum, and experience working in Agile development teams.
Understanding of software development lifecycle (SDLC) processes and experience collaborating with development teams.
What We're Looking For:
Educational Background: A bachelor's or master's degree in Computer Science, Software Engineering, Data Science, or a related field is often required. Advanced degrees can be advantageous.
Programming Skills: Proficiency in programming languages commonly used in data engineering, such as Python, Java, Scala, or SQL.
Big Data Technologies: Strong knowledge of big data technologies and frameworks, such as Apache Hadoop, Apache Spark, Apache Kafka, Apache Hive, or Apache Flink.
Database Management: Experience with various database systems, both traditional relational databases (e.g., MySQL, PostgreSQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Data Modeling: Expertise in designing and implementing data models to facilitate efficient data storage, retrieval, and analysis.
Data Warehousing: Familiarity with data warehousing concepts and technologies, like Azure Synapse, Amazon Redshift, Google BigQuery, or Snowflake.
ETL/ ELT: Experience in designing and implementing ETL / ELT processes to extract, transform, and load data from various sources into the data warehouse.
Data Pipeline Management: Ability to create and manage data pipelines for data ingestion, integration, and transformation.
Experience with scheduling tools like Airflow, Control-M or Auto-sys
Cloud Services: Experience with cloud computing platforms like AWS, Azure, or Google Cloud, and familiarity with their data services.
Data Governance and Security: Knowledge of data governance principles and best practices, including data security and privacy measures.
Data Quality and Testing: Understanding of data quality assessment methods and proficiency in data testing techniques.
Performance Optimization: Skills to optimize data pipelines and queries for better performance and scalability.
Problem-Solving and Troubleshooting: Strong problem-solving abilities and the capability to identify and resolve complex data engineering issues.
Team Leadership: Leadership and mentoring skills, as senior data engineers often lead and guide other members of the data engineering team.
Communication: Effective communication skills to collaborate with cross-functional teams, communicate technical concepts to non-technical stakeholders, and document data engineering processes.
Who We Are:
We are a leading global investment management firm offering high-quality research and diversified investment services to institutional clients, retail investors, and private-wealth clients in major markets around the globe. With over 4,000 employees across 57 locations operating in 26 countries and jurisdictions, our ambition is simple: to be the most trusted investment firm in the world. We realize that it's our people who give us a competitive advantage and drive success in the market, and our goal is to create an inclusive culture that rewards hard work.
Our culture of intellectual curiosity and collaboration creates an environment where you can thrive and do your best work. Whether you're producing thought-provoking research, identifying compelling investment opportunities, infusing new technologies into our business or providing thoughtful advice to our clients, we are fully invested in you. If you're ready to challenge your limits and empower your career, join us!
AB does not discriminate against any employee or applicant for employment on the basis of race, color, religion, creed, ancestry, national origin, sex, age, disability, marital status, citizenship status, sexual orientation, gender identity, military or veteran status or any other basis that is prohibited by applicable law. AB’s policies, as well as practices, seek to ensure that employment opportunities are available to all employees and applicants, based solely on job-related criteria.
Nashville, Tennessee",3.7,"AllianceBernstein
3.7","Nashville, TN",1001 to 5000 Employees,1967,Company - Public,Investment & Asset Management,Financial Services,$1 to $5 billion (USD)
269,"Senior Data Engineer, AdTech",$121K - $165K (Glassdoor est.),"Engineering
Data
Our mission on the Advertising Product & Technology team is to build a next generation advertising platform that aligns with our unique value proposition for audio and video. We work to scale the user experience for hundreds of millions of fans and hundreds of thousands of advertisers. This scale brings unique challenges as well as tremendous opportunities for our artists and creators.
Location
New York or Remote Americas
Job type
Permanent
We are looking for Data Engineers to build and drive data engineering initiatives within our advertising and podcast monetization teams. In this role, you will be instrumental in streamlining data ingested from multiple sources to not only recognize value and insights, but to also set standards along the way. You will help us to create a user-first ad experience that's personalized and relevant, and develop and own software solutions on our fast-paced podcast and ad services technology. You will help us grow to billions of fans, increase engagement with our listeners, and provide better value to our advertisers. Above all, your work will impact the way the world experiences music and podcasts.
What You'll Do
Work closely with key partners across the ads organization, contributing to the improvement of many different pipelines and services
Build new distributed data pipelines across ad experience
Use best practices in continuous integration and delivery
Help drive optimization, testing and tooling to improve reliability and data quality
Apply a Data centric approach to all the platforms, pipelines and engineering activities
Jump into data pipelines, identify issues, and propose solutions across teams
Design, develop, and maintain Java services
Work in cross-functional agile teams to continuously experiment, iterate and deliver on new product objectives
Who You Are
You have a strong understanding of data systems
You are knowledgeable and passionate about improving and building new distributed data pipelines
You are knowledgeable about data modeling, data access, and data storage techniques
You are familiar with current engineering practices such as distributed architecture, and are curious about new technologies that help derive insights and value from data
You have experience working on and building distributed data pipelines that ingest huge amounts of data across multiple sources and brands
You are very comfortable working with datasets in SQL (we also use Scio!)
You are comfortable in at least one core language like Python, Java or Scala
You have experience working with Apache Beam, Hadoop or a similar streaming data technology
You have experience with GCP (preferred) or AWS
You enjoy close collaboration with backend and ML engineers, and are passionate about software architecture across the stack
Ad Tech and/or Podcast Monetization experience is a plus
Where You'll Be
We are a distributed workforce enabling our band members to find a work mode that is best for them!
Where in the world? For this role, it can be within the Americas region in which we have a work location and is within working hours: https://lifeatspotify.com/locations
Working hours? We operate within the Eastern Standard time zone for collaboration and ask that all be located in that time zone.
Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here.
Our global benefits
Extensive learning opportunities, through our dedicated team, GreenHouse.
Flexible share incentives letting you choose how you share in our success.
Global parental leave, six months off - fully paid - for all new parents.
All The Feels, our employee assistance program and self-care hub.
Flexible public holidays, swap days off according to your values and beliefs.
Learn about life at Spotify
The United States base range for this position is $156,275 - $223,25, plus equity. The benefits available for this position include health insurance, six month paid parental leave, 401(k) retirement plan, monthly meal allowance, 23 paid days off, 13 paid flexible holidays. This range encompasses multiple levels. Leveling is determined during the interview process. Placement in a level depends on relevant work history and interview performance. These ranges may be modified in the future.

Spotify is an equal opportunity employer. You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens.

Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service.",4.0,"Spotify
4.0","New York, NY",5001 to 10000 Employees,2006,Company - Public,Internet & Web Services,Information Technology,Unknown / Non-Applicable
270,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
271,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
272,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
273,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
274,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
275,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
276,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
277,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
278,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
279,Sr. Data Engineer,Employer Provided Salary:$118K - $142K,"Data Engineer
Egen Solutions Inc offers competitive compensation, benefits, and an excellent culture of teamwork and collaboration. We are an Equal Opportunity Employer that does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.
Job Summary
We have some major internal projects underway at Egen and need a highly skilled Data Engineer to join our team on a full-time basis. Below is the JD, kindly go through it and let me know your interest and if you are interested revert back with your updated resume
Our Data Engineering teams build scalable data pipelines using Python, Spark, and cloud services (GCP and AWS). The pipelines we build typically integrate with technologies such as Kafka, Storm, and Elasticsearch. We are working on a continuous deployment pipeline that leverages rapid on-demand releases. Our developers work in an agile process to efficiently deliver high value applications and product packages.
As a Staff Data Engineer at Egen, you will leverage Spark and GCP (preferred) to architect and implement cloud-native data pipelines and infrastructure to enable analytics and machine learning on rich datasets.
Required Experience:
Built and run resilient data pipelines in production and have implemented ETL/ELT to load a multi-terabyte enterprise data warehouse.
Implemented analytics applications using multiple database technologies, such as relational, multidimensional (OLAP), key-value, document, or graph.
Defined data contracts, and wrote specifications including REST APIs.
Transformed data between data models and formats with the most modern PySpark practices. Have built cloud-native applications and supporting technologies / patterns / practices including: Cloud Services, Docker, CI/CD, DevOps, and microservices.
Planned and designed artifacts that describe software architectures involving multiple systems and technologies
You've worked in agile environments and are comfortable iterating quickly.
Nice to have's (but not required):
GCP expertise is preferred but will consider AWS
Experience moving trained machine learning models into production data pipelines.
Experience in biotech, genomics, clinical research or precision medicine.
Expert knowledge of relational database modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others.
This is full-time, On-site, and Remote. We are based out in Naperville, IL.
Looking forward to striking up a conversation with you on the opportunity!, Kindly visit our website egen.solutions
equirements
Bachelor’s degree in Computer Science, Engineering, or related field.
3+ years of experience with data engineering and/or data analysis.
Strong analytical skills and ability to think analytically.
Ability to work independently with minimal supervision.
Experience with SQL and/or NoSQL databases is a plus.
Experience with ETL processes is a plus.
Experience with cloud-based solutions such as Azure Data Factory is a plus.
Experience with Agile software development methods such as Scrum is a plus.
Ability to work in a fast-paced environment and adapt to changing priorities.
Ability to work in a team environment while maintaining strong individual initiative.
Job Type: Full-time
Pay: $117,577.88 - $141,599.17 per year
Benefits:
401(k)
Dental insurance
Health insurance
Experience level:
5 years
Application Question(s):
Are you a personal who hold any of these work authorizations currently(OPT EAD/GC/F1 OPT/H4 EAD.
Work Location: In person",-1,Egen Solutions Inc,"Naperville, IL",-1,-1,-1,-1,-1,-1
280,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
281,Senior Data Engineer,Employer Provided Salary:$105K - $186K,"**(UPDATED 8/21/2023) We invite applicants to express their interest in this role, but please note that we currently don't have a specific timeline for filling this position. This posting is intended for talent pooling. While there isn't a definitive hiring period, potential candidates should be aware that the role might be filled at some point in the future. We encourage you to start the process and discover more about this opportunity, Slalom, and potentially a career in consulting through discussions with our talent acquisition team.
Who You’ll Work With
As a modern technology company, we've never met a technical challenge we didn't like. We enable our clients to learn from their data, create incredible digital experiences, and make the most of new technologies. We blend design, engineering, and analytics expertise to build the future. We surround our technologists with interesting challenges, innovative minds and emerging technologies.
A Senior Data Engineer at Slalom is a technical team member who designs and develops data platform solutions. It uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g., data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc.
What You’ll Do
Work as part of a team and with clients to develop cloud data and analytics solutions
Make recommendations and prove data architecture concepts to team members and clients
Development of cloud data platforms and business intelligence solutions
Data wrangling of heterogeneous data sources for exploring and discovering new insights
Work with highly collaborative agile teams on modern data architecture projects
Key success measures of a Senior Data Engineer include technical proficiency in data processing technologies, collaboration with other teams, communication, and willingness to learn new things
What You'll Bring
Experience in one or more cloud technologies (AWS, Azure, Google Cloud, Snowflake)
Experience in data modeling, design, and development using complex SQL (SQL Server, Oracle, etc.)
Experience in Apache Airflow, Talend, SSIS, Informatica, Alteryx, or other data processing tool
Experience in one or more programming languages (Python, C#, Scala, R, PowerShell, Java, etc.)
Experience in cloud engineering, serverless architecture, and/or data streaming services
About Us
Slalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six countries and 43 markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 13,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.
Compensation and Benefits
Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance.
Slalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $105,000 to $186,000. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.
EEO and Accommodations
Slalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team if you require accommodations during the interview process.
#LI-JS8
#LI-Hybrid",4.3,"Slalom Consulting
4.3","Los Angeles, CA",10000+ Employees,2001,Company - Private,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
282,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
283,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
284,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
285,Data Visualization Engineer,$77K - $101K (Glassdoor est.),"Passionate about precision medicine and advancing the healthcare industry?
Recent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.
With the advent of genomic sequencing, we can finally decode and process our genetic makeup. We now have more data than ever before but providers don't have the infrastructure or expertise to make sense of this data. We're on a mission to connect an entire ecosystem to redefine how genomic data is used in clinical settings.
What You'll Do:
Own the design, development and administration of advanced analytics dashboards and reporting and business intelligence tools. Beginning with requirements gathering across cross-functional teams, to the delivery of analytic solutions into production.
Collaborate with stakeholders, providing guidance, data and analytical support for developing new datasets, analyses and tools in support of strategic initiatives.
Perform deep-dive root cause and problem-solving analyses for and with laboratory, business operations and engineering teams.
Design and build data views to support advanced analytical reporting, self-service data analysis and exploration, utilizing multiple data and visualization technologies.
Work closely with technical and product teams to identify and align changes to internal systems in support of future data analysis needs and requirements.
Develop and facilitate data governance initiatives to assure data integrity across distributed systems.
Solve new and interesting problems.
Qualifications:
Bachelor's degree in Information Systems, Business Administration, Finance, Economics, Mathematics, Operations Research, Marketing or similar
3+ years of experience developing data sets for analysis use-cases
3+ years of experience with direct stakeholder management and ownership of full lifecycle of analytics/data projects and presenting analysis results to stakeholders
3+ years report-development and data visualization experience with BI tools (Looker, Tableau, etc.)
Ability to deliver both high-level explanations of complex problems while also able to deep dive into complex problems
Advanced SQL skills for querying and analysis of large, complex datasets
Must work well in a team environment and be able to partner with both business and technical team members and translate technical language to the non-technical need
Creative problem solver, ability to handle multiple projects, and strong work ethic needed
Excellent written and verbal communication skills
Highly organized and systematic, superb attention to detail
Able to complete tasks with a high degree of accuracy and an eye towards process improvement
Persistent and resilient person who thrives in a fast-paced, highly collaborative environment
Bonus points for:
Advanced analytics experience - classification/predictive modeling, optimization models, forecasting
Experience with Python for data engineering or advanced data analysis projects/use-cases
Healthcare domain knowledge and experience
Expert knowledge of database/data modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others
Experience building cloud-native applications and supporting technologies / patterns / practices including: GCP, AWS, Docker, CI/CD, DevOps, and microservices

#LI-EH1
#LI-Hybrid
We are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.1,"Tempus
3.1","Chicago, IL",501 to 1000 Employees,2015,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
286,Data Engineer,$100K - $144K (Glassdoor est.),"Position: Data Engineer
Duration: Contract
Location: Fremont, California
Skills:
Strong in SQL and Python
Data warehousing concepts , Data Modeling and RDBMS concepts
Good to have exposure to big data technologies – Apache Spark, Kafka, Airflow, NoSQL
Good to have exposure on Insurance & Finance business processes
Regards
Tejash Gupta
Recruiter
tejash@swifttechinc.com
Job Type: Contract
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Python: 7 years (Required)
SQL: 7 years (Required)
Data modeling: 6 years (Required)
RDBMS: 6 years (Required)
Work Location: In person",4.1,"Swift Technologies Inc
4.1","Fremont, CA",1 to 50 Employees,-1,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
287,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
288,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
289,Sr. Data Engineer,$83K - $115K (Glassdoor est.),"Provides application design, analysis, programming, testing and documentation for the development, integration, enhancement, and maintenance of information systems solutions
Job Description:
BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Job Responsibilities
Formulates and defines system scope and objectives through research and fact-finding to design, develop, modify, or integrate complex information systems.
Devises or modifies application systems and procedures to optimize functional requirements including capacity, operating time, response time, and form of desired results.
Designs, codes, tests, debugs, and documents programs, subroutines, and scripts.
Develops and implements program/system test plans. Devises data verification methods and standard system procedures.
Responds to system failures and performance events by taking appropriate measures to reduce system downtime and eliminate recurrence of problems.
May conduct research on software and hardware products to justify recommendations and support management in budgeting and purchasing efforts.
Acts as expert technical resource to development staff in all phases of the development and implementation process.
Job Qualifications
Education
Bachelors Degree in Computer Science or equivalent work experience required. Equivalent years of experience are determined as one year of technical experience for every year of college requested.
Experience
1 year - Experience with information technology concepts, application development methodology, terminology, and standards required
5 years - Systems and programming experience required
1 year - Expert working knowledge of at least one programming language in use at BCBST.
Skills\Certifications
Proven skill with information technology in use at BCBST is required
Proven analysis, design, and coding skills, and demonstrated success in leading large and complex projects
Top level technical experience in one or more specialized areas of applications systems analysis and programming
Demonstrated ability to interpret and translate technical and/or or complex concepts into information meaningful to project team members and/or business personnel.
Be organized, reliable, and able to manage multiple tasks with exceptional work ethic.
Leadership skills
Must be able to communicate effectively with both technical and non-technical co-workers
Job Specific Requirements:
Preference for candidates with Cloud experience (GCP specifically)
Python knowledge/experience a plus
Preferred Skills:
Number of Openings Available:
1
Worker Type:
Employee
Worker Sub-Type:
Employee
Company:
BCBST BlueCross BlueShield of Tennessee, Inc.
Applying for this job indicates your acknowledgement and understanding of the following statements:
BCBST is an Equal Opportunity employer (EEO), and all employees and applicants will be entitled to equal employment opportunities when employment decisions are made. BCBST will take affirmative action to recruit, hire, train and promote individuals in all job classifications without regard to race, religion, color, age, sex, national origin, citizenship, pregnancy, veteran status, sexual orientation, physical or mental disability, gender identity, or any other characteristic protected by applicable law.
Further information regarding BCBST's EEO Policies/Notices may be found by reviewing the following page:
BCBST's EEO Policies/Notices
BlueCross BlueShield of Tennessee is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at BlueCross BlueShield of Tennessee via-email, the Internet or any other method without a valid, written Direct Placement Agreement in place for this position from BlueCross BlueShield of Tennessee HR/Talent Acquisition will not be considered. No fee will be paid in the event the applicant is hired by BlueCross BlueShield of Tennessee as a result of the referral or through other means.
All applicants will be advised that BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Tobacco-Free Hiring Statement
To further our mission of peace of mind through better health, effective 2017, BlueCross BlueShield of Tennessee and its subsidiaries no longer hire individuals who use tobacco or nicotine products (including but not limited to cigarettes, cigars, pipe tobacco, snuff, chewing tobacco, gum, patch, lozenges and electronic or smokeless cigarettes) in any form in Tennessee and where state law permits. A tobacco or nicotine free hiring practice is part of an effort to combat serious diseases, as well as to promote health and wellness for our employees and our community. All offers of employment will be contingent upon passing a background check which includes an illegal drug and tobacco/nicotine test. An individual whose post offer screening result is positive for illegal drugs or tobacco/nicotine and/or whose background check is verified to be unsatisfactory, will be disqualified from employment, the job offer will be withdrawn, and they may be disqualified from applying for employment for six (6) months from the date of the post offer screening results.
Resources to help individuals discontinue the use of tobacco/nicotine products include smokefree.gov or 1-800-QUIT-NOW.",3.8,"BlueCross BlueShield of Tennessee
3.8","Chattanooga, TN",5001 to 10000 Employees,1945,Nonprofit Organization,Insurance Carriers,Insurance,$5 to $10 billion (USD)
290,Data Engineer,$99K - $136K (Glassdoor est.),"The company

Decentralized data is the future. Data mesh is the right idea. We’re here to make it a reality. Nextdata OS is a data-mesh-native platform built to meet the challenge of decentralizing data at scale. We are inventing a new way for developers to work with data and share it responsibly via data product containers.

Our vision is to build a world where AI/ML and analytics are powered by decentralized, responsible, and equitable data ownership, across boundaries of organizations, technology, and most importantly boundaries of trust.

Our purpose is to change the experience of creating, sharing, discovering, and using data forever, to be connected, fast, and fair based on data mesh principles.

Our technology is designed to empower data developers, users and owners with a delightful experience where data products are a first-class primitive, with trust built-in.

We are here to accept the reality that the world of data is complex and messy; data models are out-of-date the moment they are created; data is owned across trust boundaries; data is stored on different platforms; data is used in many different modes and most importantly data can't protect itself. We are here to recognize that past approaches to tackle these complexities with centralized data collection, modeling and governance are ineffective at best and pathologically unfair at worst. We are here to reimagine, with you!

The role

You will be one of the first data product developers building data products on top of the Nextdata OS. You will work with Nextdata’s initial customers to translate their business needs into PoC data products that you will then build on top of our OS.
You will leverage your experience of the full lifecycle of analytics - translating business requirements to data models, to data pipelines, to analytics dashboards and even machine learning pipelines - to build data products and then provide feedback that will help us improve the Nextdata OS and tooling.

You will manage the full lifecycle of the PoC data products from ideation around business use case to successful deployment, including synthesizing realistic data for testing before deploying into production.

Our expectations

You have worked on complex data pipelines in large data organizations. You have dealt with the uncertainty of iterating from at times an under defined business use case to a concrete end-to-end analytics pipeline that drives value to the business. You may have had to help the business quantify the value of your analyses as well!
You are proficient in the modern data stack tools like airflow, dagster, prefect, dbt and ML development environments like PySpark and Pandas.
You are articulate and are able to present your findings in a compelling manner.
You have relied on continuous integration and continuous deployment to reduce software lead time, and have contributed to optimizing build and release processes as needed.
You have shown evidence that you aspire to test-first data pipeline development!

Our benefits

We are an early stage company, but we don't subsist on ramen! We are an experienced team with families. We provide $6000 for your home workspace setup, premium health, vision, dental insurance coverage for you and your family. And of course, early stage equity and market rate salary.",-1,Nextdata Technologies Inc,"San Francisco, CA",1 to 50 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
291,Data Engineer I - People Analytics,$73K - $111K (Glassdoor est.),"POSITION SUMMARY Our People Analytics team is expanding! The Data Engineer I is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.

This position can work in a flexible, hybrid model with 3 days on-site in Appleton, WI and 2 days remote.

JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.

QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming. Preferred experience in Python applied to data engineering and transformation.
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated

#LI-Hybid

Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101 .",4.1,"US Venture
4.1","Appleton, WI",1001 to 5000 Employees,1951,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
292,"Manager, Data Engineer & ETL Processing",$83K - $118K (Glassdoor est.),"Company Description

About Spark Foundry:

Spark Foundry is a global media agency that exists to bring HEAT – Higher Engagement, Affinity, and Transactions – to brands. By combining flawless media fundamentals with aggressive innovation, Spark inspires consumers to pay more attention, to care more about our clients’ brands, and to buy more products and services from them.
Balancing the nimble spirit of a startup with the powerhouse soul of Publicis Media, Spark Foundry delivers the best of both worlds to a client roster that spans some of the world’s best and most beloved brands and companies. We combine boutique-caliber insights and service with the buying clout and first-look access of a global leader, bringing the heat to challenger brands that want to act like giants, and to giant brands that want to act like challengers.
With a bottom-up culture that celebrates diversity and aims for all voices to be heard, Spark has become a magnet for the industry’s best talent, with one of the best retention rates in the industry. And by applying a whole-person approach to professional and personal development, Spark develops a workforce that is well prepared for today’s challenges, and also poised to create meaningful careers in the years to come.
Because we know that heat arises the intersection of complementary forces, our professionals come from myriad disciplines and backgrounds: data, analytics, and insights, content and creative production, communications and strategy, finance and marketing, and sociology, psychology, and other liberal arts disciplines.

Job Description

Overview:
The Manager, Data Engineering & ETL Development is a key driver to build data platform leveraging best in class ETL practice and is also a strategic thinker and a talented data visualization expert with emphasis on dashboard reporting. The primary responsibility will be developing business intelligence platform to support media and marketing data for our clients. This position will allow you to be a significant contributor as part of the team to support easy access of data and visualizations to fuel stronger insights and media optimizations.
This position requires strong technical and tactical skill sets with an eye for numbers, intellectual curiosity, proficiency at problem solving, and a critical understanding of online media. The candidate must have a proven track record in managing ETL’s, APIs and business intelligence platforms. Candidate should be a team player. A “roll up the sleeves” approach is mandatory and a “get it done” attitude is a must. Specific responsibilities include coordination between the research, analytics & media teams ensuring high quality data projects are effectively delivered.
Successful candidates will be multi-dimensional ‘rising-stars’ who are able to employ complex problem solving skills, and are able to communicate these succinctly to a broad client and media stakeholder audience.
Role Objectives:
Responsible for loading and validating data into the data warehouse from various source systems
Analyze, develop, fix, test, review and deploy functionality, and bug fixes in ETL data pipelines
Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required
Building Data mappings between Source to Target systems
Provides support for technical issues and ensuring system availability
Work with business customers to identify and develop additional data and reporting needs
Understand how business intelligence platform/data technologies work and offer the ability to explain technical concepts in ordinary terms (be technically savvy - understand the opportunities and limitations)
Establish and manage data integrations utilizing a taxonomy nomenclature to make recommendations for data visualization to showcase media performance through the use of Datorama or Tableau
Perform regular quality assurance/quality control checks on assigned client campaigns to ensure the data is processing accurately
Design and build backend data streams and processes to automate reporting capability with data visualization tools
Contribute to client status and reporting calls, including presentation of reporting as required
Develop subject matter expertise in ETL, API development, and Business Intelligence platforms
Clearly define project deliverables, timelines, and dependencies for junior team members, internal stakeholders and clients
Collaborate on an inclusive team, where members openly communicate and collectively problem-solve
Strong ability to evaluate new technologies and present findings to team
Contribute to knowledge sharing efforts and mentorships
Complete other duties as assigned.

Qualifications
Bachelor’s degree or combination of education, and equivalent work experience is preferred in the field of computer science, management information systems or Information Technology
3+ years’ related experience ETL development and business intelligence platform management
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
3+ plus years’ development experience with database engines including Presto, Mongo db and Hadoop.
3+ years’ experience in ETL development, Strong Database (Modeling, SQL), SQL Server, Hadoop, AWS Redshift, Qubole.
Experience managing team of 2 or more associates/analysts preferred
Strong database modeling and SQL skills. Ability to write complex SQL statement, Procedures and data automation programs
Knowledge of ETL tools like Airflow, AWS Glue, Alteryx, SSIS, Qubole/Spark is a must
Knowledge of Python or similar programming languages required
Proficiency with Datorama, Tableau, or other data visualization tools is preferred
Experience working with AWS or other cloud technologies
Advanced user Microsoft Office Suite and reporting tools
Demonstrated expertise in core MS Excel functions (vlookup, pivot tables, data visualization)
Experience in designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.
Strong analytical and problem-solving skills
Strong verbal/written communication and interpersonal skills is required
Self-starter with ability to thrive in a fast-paced environment and able to function independently while providing status updates to a team of analysts
Cooperative, flexible, conscientious, dependable, resourceful, self-motivated, and team-oriented
Problem solving, time management, and critical thinking skills with a professional and positive attitude
Ability to work independently and as part of an agile team, participating in daily stand-ups, sprint planning and sprint review

Character:
The following qualities help drive success as member of the Spark Data and Analytics team:
Entrepreneurial, engaging, resourceful, curious, and self-directed spirit
Willing and easily roll sleeves up or down; love the nitty-gritty and the strategy
Collaborative approach to building cohesive, strong teams
Loving and living the intersections between brands, people, media, communications
Relentlessly passionate and resolute
Planning and time management excellence.
Embrace challenges
Proactive, especially in pushing for new opportunities, approaches, and ideas.
Keenly focused on action and solutions; thrives with deep critical thinking and analysis.
Pioneering insight attitude and research in-the-know.
Resourcefulness, flexibility and adaptability, strong ability to pivot when the need arises.
Inspired to be part of the insight journey/revolution with a growing, dedicated team

Additional Information

All your information will be kept confidential according to EEO guidelines.
23-2795",3.9,"Spark Foundry
3.9","Dallas, TX",51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,Unknown / Non-Applicable
293,Senior Data Engineer,-1,"JOB DESCRIPTION:
Need to be expertise Coding on PySpark.
Supply Chain and Azure.
Experience in working with end-to-end technology implementation project, preferable those related to Data warehousing/Analytics/Reporting
Strong communication skills with the ability to collaborate effectively with business owners/users and technical teams.
Proven ability to gather product requirements from stakeholders and prioritize feature implementations based on business needs and product strategy.
Strong problem-solving skills and attention to detail.
Good to have experience with CPG/ Retail industry (preferable CPG companies such as Nestle, Unilever, P&G, Coca coal, PepsiCo etc)
Bachelor's/master's degree in business administration, computer science, engineering, or a related field.
Job Type: Contract
Experience:
PySpark: 5 years (Preferred)
SQL: 10 years (Preferred)
Data analytics: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Azure: 5 years (Preferred)
Data Bricks: 5 years (Preferred)
Data Lake: 5 years (Preferred)
Synapse: 5 years (Preferred)
Retail Domain: 8 years (Preferred)
CPG: 7 years (Preferred)
Work Location: Remote",4.0,"Quadrant Resource
4.0",Remote,1001 to 5000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
294,Data Engineer IV,Employer Provided Salary:$149K,"Who We Are
Invitation Homes is a fast-paced evolving publicly traded REIT that is pioneering a new industry with over 80,000 single family rental homes and a market capitalization of over $25 billion. We are a team of more than 1,400 associates who come from all walks of life. We call different communities “home,” but our shared values bind us together. Invitation Homes is a place where possibility lives.
THR Property Management LP / Data Engineer / Dallas, TX: Develop and maintain data pipelines into data warehouse and data lake, including design, and development. Develop and implement strategies to translate business requirements into feasible and acceptable data warehouse and data lake solutions. Architect and build new data models to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making. Design, build and launch data pipelines to move data to data lake and data warehouse build and maintain framework for auditing, error logging, and master data management for data pipelines. Build data expertise and own data quality for the data pipelines. Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes. Perform data analysis and assist in the resolution of data issues. Identify and resolve defects of complex scope using proper engineering tools and techniques. Provide support and maintain existing products and add new features. Mentor engineers by providing advice, coaching and educational opportunities. Telecommuting permitted from any location within the U.S. Salary: 148512/year.
The Senior Data Engineer is accountable for the maintenance, improvement, and movement of data in the Enterprise Data Warehouse, and other data sources used for enterprise reporting. A successful Senior Data Engineer delivers database design, implementation, and schematics that creates sustainable, competitive advantage for our company. Further, a Senior Data Engineer is responsible for the development and deployment of innovative analytics data platforms to support end-user enterprise reporting. Working with other data engineers, data analysts, and data scientists, a Senior Data Engineer must have a delivery first mentality and a firm grasp of Agile Development methodology. Lastly, the ideal candidate must demonstrate the ability to work as a team member and team leader.
What You’ll Do
Accountable for the overall performance and maintenance of our SQL Server environments:
Manage the EDW relational database for optimized performance
Design scalable ETL packages from the business source systems
Ensure all necessary testing and validation for all planned and unplanned releases
Mentor junior data engineers in best practices and documentation.
Act as our database evangelist in leading innovation activities through exploration, benchmarking and implementation of data technologies
Participate in solution-based activities; articulate your ideas through well thought out documentation and research best-in-class solutions
Lead and Develop and implement data architecture standards
Manage the necessary testing and validation to ensure proper compliance to data governance and quality
Prepare progress reports regarding our database environments’’ status and health
Manage troubleshooting data issues and present solutions to these issues.
Resolve tactical issues thoroughly while always looking for optimization and documenting lessons learned
Understand when tactical issues are really systematic issues requiring deeper level of analysis and strategic solutions
Cooperate with the support team to investigate and resolve data related defects
Proactively analyze and evaluate current state of data in order to identify and recommend improvements and optimization
Support our Agile Development methodology
Constantly prioritize user stories; balance feature development, defects, and tech debt according to business need
Be a champion of continuous improvement and delivery; define the minimum viable product, focus teams on key release milestones, and leverage fast follow-up sprints
Enable sprint progress; clarify story details, validate completed stories, and prepare stories for the next sprint
Who We’re Looking For
Bachelor’s degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field. An equivalent of the same in working experience is also accepted for the position.
A candidate for the position will have as at least 5 years of working experience as a database engineering or a database engineering administrator within a fast-paced a complex business setting.
Design, implement and maintain SQL Server databases
Design, implement and maintain ETL processes using SQL Server SSIS
Provide ongoing maintenance support through SQL query tuning and optimization
Strong working and conceptual knowledge of building and maintaining physical and logical data models
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience working in SaaS, IaaS, and PaaS
Excellent customer service and interpersonal skills; ability to relate to and get along with others
Professional verbal and written communication skills
Strong organizational and time-management skills
Ability to multi-task and maintain flexibility and creativity in a variety of situations
Ability to analyze and resolve problems
Ability to set and meet goals and consistently meet deadlines
Ability to maintain confidentiality
Why Invitation Homes
We stand for flexibility, opportunity, and a home that people can make their own. It’s as true for our associates as it is for our residents. Just like we help our residents live freer, we liberate our associates’ careers, too. Our associates know that at Invitation Homes goals matter, potential is unlocked, and careers thrive. Invitation Homes isn’t just a work place it is a possibility place. Invitation Homes offers the below to each new associate:
Competitive pay and an annual bonus program for all associates
Generous paid time off plans including vacation accrual, sick time, volunteer time, and standard and floating holidays
401k with matching company contributions
Awesome work environment with casual dress
Team events and gatherings
Employee resource groups: Together with Women, Asian Alliance, Black Collective, Juntos, Gen Next, and Open Invitation.
Invitation Homes truly is where possibility lives, pour a new foundation here!
Salary Range
$98,640.00 - $170,976.00
Compensation and Benefits
To attract and retain top talent, we're pleased to offer competitive compensation and benefits, including:
Annual bonus program
Health, dental, vision, and life insurance
Long-term and short-term disability insurance
Generous paid time off plans include vacation accrual, sick time, standard holidays and floating holidays
401(k) with company matching contributions
Awesome work environment with casual dress
Team events and gatherings (Pre- and Post-Covid)
Invitation Homes is an equal opportunity employer committed to fostering a diverse, inclusive and innovative environment with the best associates. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, Veteran status or any other factor protected by applicable federal, state or local law. If you have a disability or special need that requires accommodation, please contact us at
humanresources@invitationhomes.com
.
To all recruitment agencies: Invitation Homes does not accept agency resumes. Please do not forward resumes to Invitation Homes employees. Invitation Homes is not responsible for any fees related to unsolicited resumes.",3.9,"Invitation Homes
3.9","Dallas, TX",1001 to 5000 Employees,2012,Company - Public,Real Estate,Real Estate,$1 to $5 billion (USD)
295,AWS Data Engineer II,$77K - $105K (Glassdoor est.),"Overview:
Shift4 (NYSE: FOUR) is boldly redefining commerce by simplifying complex payments ecosystems across the world. As the leader in commerce-enabling technology, Shift4 powers billions of transactions annually for hundreds of thousands of businesses in virtually every industry. For more information, visit www.shift4.com.

Shift4 Payments is seeking a Business Intelligence Engineer with an AWS Focus to join our growing internal Information Technology team. As a BI Engineer, you will help design and maintain the flow of information and analytics to the entire organization utilizing AWS capabilities to store, move and report on data. The position requires at least 3 years of prior experience as a Business Intelligence AWS engineer. We are looking for individuals that are extremely self-sufficient, available to work flexible hours and hold themselves to the highest standards of professionalism.Maintain and expand BI AWS architecture and content.
Responsibilities:
Support of BI AWS architecture and content up to and including off hours support when necessary.
Work on necessary bugs, enhancements and new system developments utilizing our Software Development Life Cycle (SDLC) standards.
Complete root cause analysis and provide recommendations to leadership to address opportunities.
Contribute to and enforce the use of the organization's Business Intelligence solution.
Develop software solutions that will provide solid foundations for the growth of the company.
Support key metrics and shared goals that determine success for a particular area.
Qualifications:
3-5+ years of professional experience in business intelligence AWS development or a related field.
Advanced experience working with AWS: S3, Glue, Lambda, Step Functions, Redshift, Athena.
Advanced experience working with various data sources such as AWS S3 and Redshift, Postgres, SQL Server, Amazon Athena, Excel, Flat Files, etc.
Advanced experience working with large data sets from sources such as AWS S3 and Redshift, Postgres, SQL Server, etc.
Strong PySpark, Python, CLI proficiency
Strong SQL proficiency (T-SQL, MySQL, and PostgreSQL).
Experience working with an Enterprise Data Warehouse and Dimensional Databases is a plus.
Advanced experience of the SDLC and how Development processes fit into KANBAN and Scrum Frameworks.
Experience with Jira and Confluence is a plus.
Ability to prioritize multiple tasks and easily adjust to changing priorities.
Ability to identify problems, initiate solutions, and effectively collaborate and communicate with team members.
Have excellent verbal and written communication skills.

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.",3.5,"Shift4 Payments
3.5","Allentown, PA",1001 to 5000 Employees,1999,Company - Public,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
296,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
297,Data Analytics Engineer,$99K - $134K (Glassdoor est.),"KAYAK, part of Booking Holdings (NASDAQ: BKNG), is the world's leading travel search engine. With billions of queries across our platforms, we help people find their perfect flight, stay, rental car, cruise, or vacation package. We're also transforming the in-travel experience with our app and new accommodation software. For more information, visit www.KAYAK.com.
As an employee of KAYAK, you will be part of a global network that includes OpenTable and KAYAK's portfolio of metasearch brands including Swoodoo, checkfelix, momondo, Cheapflights, Mundi and HotelsCombined. Many employees are dedicated to one brand, but we all have the benefit of using each others strengths and insights. Together, we're able to help people experience the world through dining and travel.
Please note, the position is required to work from our Concord, MA office at least 3 days a week.
In this role, you will:
Create dashboards, reports, and data visualizations to share insights into product performance;
Assist Product Managers with interpreting experiment results;
Build and maintain data pipelines (ETLs) using Python, SQL, Airflow, and Docker;
Design performant data models and optimize SQL queries;
Investigate data issues to identify patterns and root causes;
Automate repetitive tasks and build self-service tools to support data democratization;
Work closely with cross-functional stakeholders to understand business needs, propose solutions, and communicate results;
Please apply if:
2+ years of professional experience in the field of data engineering, analytics, or software engineering;
Considerable experience with SQL using relational databases (e.g. MySQL, Vertica, Snowflake, Hive);
Experience in building and maintaining ETLs preferred;
Ability to write clean and concise code, Python experience preferred;
Experience with Airflow is a plus.
Benefits and Perks
4 weeks paid vacation
Day off on your birthday
Generous retirement plans
Awesome health, dental and vision insurance plans
Flexible Spending Accounts
Headspace Subscription
No Meeting Fridays
Drinks, coffee, snacks, games etc.
Weekly catered lunches
Flexible hours
Regular team events/excursions
Universal Paid Parental leave
Diversity and Inclusion
We aspire to have a workplace that reflects all of the diverse communities we serve. We know that when we have diverse teams we produce more creative ideas, products, and better outcomes for our team members. OpenTable/KAYAK is proud to be an Equal Opportunity Employer and we welcome and encourage candidates from all backgrounds and experiences to apply for roles on our team. Whoever you are, just be you.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform job responsibilities, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",4.7,"KAYAK
4.7","Concord, MA",1001 to 5000 Employees,2004,Subsidiary or Business Segment,Internet & Web Services,Information Technology,$100 to $500 million (USD)
298,Data Engineer,$67K - $96K (Glassdoor est.),"Massanutten Resort
Job Summary
The Data Engineer will play a crucial role in designing, developing, and maintaining the data infrastructure and systems that enable efficient and reliable data processing, storage, and analysis. The primary responsibility is to transform raw data into usable formats for various business applications and support data-informed decision-making processes. This role includes working in various Microsoft SQL Environments as well as supporting a Snowflake Data Warehouse. The Data Engineer is expected to have a broad understanding of data concepts and be avid about continuing education to keep up with the ever-changing data landscape.
Required
Degree in computer science or related file and 3+ years of experience with Python, SQL, and data visualization/exploration tools (in lieu of a degree, at least 5 years of experience in the above is equivalent)
Preferred
A degree in computer science or a related field
Certifications in database management and/or related technical certifications
Familiarity with advanced data concepts (i.e., AI and Machine Learning) and business intelligence/visualization tools
Typical Schedule
Days:
Monday-Friday, weekends and holidays as needed
Position will be required to be part of rotating on-call schedule
Hours:
8am-5pm, additional hours as needed
Core Responsibilities
Develop Data Pipeline: Design and implement scalable and efficient data pipelines, integrating diverse data sources and ensuring smooth data flow from ingestion to storage and processing
Transform and Model Data: Develop and maintain data transformation processes, such as ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform), to convert raw data into structured formats suitable for analysis and reporting. Create and manage data models to facilitate data integration and provide a consistent view of information across systems.
Implement and oversee Data Warehousing and Storage: Build and maintain data warehouses or data lakes, optimizing storage and retrieval processes to ensure high availability, reliability, and performance. Implement appropriate data governance and security measures to protect sensitive data.
Ensure Data Quality and Governance: Collaborate with data analysts and business stakeholders to understand data requirements and define data quality standards. Implement data validation, data profiling, and data cleansing techniques to ensure data accuracy, consistency, and integrity.
Optimize Performance: Identify and address performance bottlenecks in data processing and storage systems, leveraging optimization techniques and technologies to improve overall data engineering efficiency and reduce latency
Assist with Data Problem Solving: Support team with data related escalation issues, including help desk support tickets that are generated from our business users
Manage Data Integration and API Development: Establish connections and integrations between internal and external data sources, including APIs, databases, and third-party platforms. Develop and maintain data APIs to enable seamless data access and consumption by other applications and services.
Collaborate and Document: Work closely with cross-functional teams, including analysts, software engineers, and business stakeholders, to understand data requirements and ensure data solutions align with business objectives. Document data engineering processes, including data pipelines, transformations, and system architectures, to facilitate knowledge sharing and maintain a robust knowledge base.
Stay up-to-date with the latest advancements in data engineering, data management, and big data technologies. Evaluate and recommend new tools, frameworks, and methodologies to improve data engineering practices and enhance overall data infrastructure
Other duties as assigned by management
For more information, contact Ana at 540-289-4939.",3.6,"Great Eastern Resort Management, Inc.
3.6","McGaheysville, VA",1001 to 5000 Employees,-1,Self-employed,-1,-1,Unknown / Non-Applicable
299,Data Engineer,Employer Provided Salary:$130K - $175K,"Rightworks is hiring a Data Engineer in Fort Worth for a Multi-Billion dollar private equity fund in Fort Worth. This position is fully in-office in Fort Worth (client will pay relocation if needed). Compensation is aggressive & flexible, typical 10% above your previous compensation level with tremendous growth opportunities. (Expected compensation range for this data engineer will fall somewhere between 130 & 180,000 per year depending on experience). 50 hours per week, paid lunch, casual office atmosphere.
-Looking for data engineer with multiple years of SQL Server maintenance experience.
-Optimally would like a data engineer with formal education in either MIS, Mathematics, Software Engineering, or Computer Science
-SQL maintenance, Data Extraction, Transforms, Macros
-Database maintenance
Job Type: Full-time
Pay: $130,000.00 - $175,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Tuition reimbursement
Vision insurance
Schedule:
10 hour shift
Ability to commute/relocate:
Fort Worth, TX 76102: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What compensation range are you looking for?
Are you willing to work in-person in Fort Worth, TX 50 hrs a week?
Experience:
Microsoft SQL Server: 1 year (Required)
Work Location: In person",4.8,"RightWorks Inc.
4.8","Fort Worth, TX",51 to 200 Employees,2006,Company - Private,HR Consulting,Human Resources & Staffing,$25 to $100 million (USD)
300,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
301,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
302,Sr. Data Engineer,Employer Provided Salary:$118K - $142K,"Data Engineer
Egen Solutions Inc offers competitive compensation, benefits, and an excellent culture of teamwork and collaboration. We are an Equal Opportunity Employer that does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.
Job Summary
We have some major internal projects underway at Egen and need a highly skilled Data Engineer to join our team on a full-time basis. Below is the JD, kindly go through it and let me know your interest and if you are interested revert back with your updated resume
Our Data Engineering teams build scalable data pipelines using Python, Spark, and cloud services (GCP and AWS). The pipelines we build typically integrate with technologies such as Kafka, Storm, and Elasticsearch. We are working on a continuous deployment pipeline that leverages rapid on-demand releases. Our developers work in an agile process to efficiently deliver high value applications and product packages.
As a Staff Data Engineer at Egen, you will leverage Spark and GCP (preferred) to architect and implement cloud-native data pipelines and infrastructure to enable analytics and machine learning on rich datasets.
Required Experience:
Built and run resilient data pipelines in production and have implemented ETL/ELT to load a multi-terabyte enterprise data warehouse.
Implemented analytics applications using multiple database technologies, such as relational, multidimensional (OLAP), key-value, document, or graph.
Defined data contracts, and wrote specifications including REST APIs.
Transformed data between data models and formats with the most modern PySpark practices. Have built cloud-native applications and supporting technologies / patterns / practices including: Cloud Services, Docker, CI/CD, DevOps, and microservices.
Planned and designed artifacts that describe software architectures involving multiple systems and technologies
You've worked in agile environments and are comfortable iterating quickly.
Nice to have's (but not required):
GCP expertise is preferred but will consider AWS
Experience moving trained machine learning models into production data pipelines.
Experience in biotech, genomics, clinical research or precision medicine.
Expert knowledge of relational database modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others.
This is full-time, On-site, and Remote. We are based out in Naperville, IL.
Looking forward to striking up a conversation with you on the opportunity!, Kindly visit our website egen.solutions
equirements
Bachelor’s degree in Computer Science, Engineering, or related field.
3+ years of experience with data engineering and/or data analysis.
Strong analytical skills and ability to think analytically.
Ability to work independently with minimal supervision.
Experience with SQL and/or NoSQL databases is a plus.
Experience with ETL processes is a plus.
Experience with cloud-based solutions such as Azure Data Factory is a plus.
Experience with Agile software development methods such as Scrum is a plus.
Ability to work in a fast-paced environment and adapt to changing priorities.
Ability to work in a team environment while maintaining strong individual initiative.
Job Type: Full-time
Pay: $117,577.88 - $141,599.17 per year
Benefits:
401(k)
Dental insurance
Health insurance
Experience level:
5 years
Application Question(s):
Are you a personal who hold any of these work authorizations currently(OPT EAD/GC/F1 OPT/H4 EAD.
Work Location: In person",-1,Egen Solutions Inc,"Naperville, IL",-1,-1,-1,-1,-1,-1
303,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
304,Senior Data Engineer,Employer Provided Salary:$105K - $186K,"**(UPDATED 8/21/2023) We invite applicants to express their interest in this role, but please note that we currently don't have a specific timeline for filling this position. This posting is intended for talent pooling. While there isn't a definitive hiring period, potential candidates should be aware that the role might be filled at some point in the future. We encourage you to start the process and discover more about this opportunity, Slalom, and potentially a career in consulting through discussions with our talent acquisition team.
Who You’ll Work With
As a modern technology company, we've never met a technical challenge we didn't like. We enable our clients to learn from their data, create incredible digital experiences, and make the most of new technologies. We blend design, engineering, and analytics expertise to build the future. We surround our technologists with interesting challenges, innovative minds and emerging technologies.
A Senior Data Engineer at Slalom is a technical team member who designs and develops data platform solutions. It uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g., data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc.
What You’ll Do
Work as part of a team and with clients to develop cloud data and analytics solutions
Make recommendations and prove data architecture concepts to team members and clients
Development of cloud data platforms and business intelligence solutions
Data wrangling of heterogeneous data sources for exploring and discovering new insights
Work with highly collaborative agile teams on modern data architecture projects
Key success measures of a Senior Data Engineer include technical proficiency in data processing technologies, collaboration with other teams, communication, and willingness to learn new things
What You'll Bring
Experience in one or more cloud technologies (AWS, Azure, Google Cloud, Snowflake)
Experience in data modeling, design, and development using complex SQL (SQL Server, Oracle, etc.)
Experience in Apache Airflow, Talend, SSIS, Informatica, Alteryx, or other data processing tool
Experience in one or more programming languages (Python, C#, Scala, R, PowerShell, Java, etc.)
Experience in cloud engineering, serverless architecture, and/or data streaming services
About Us
Slalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six countries and 43 markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 13,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.
Compensation and Benefits
Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance.
Slalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $105,000 to $186,000. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.
EEO and Accommodations
Slalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team if you require accommodations during the interview process.
#LI-JS8
#LI-Hybrid",4.3,"Slalom Consulting
4.3","Los Angeles, CA",10000+ Employees,2001,Company - Private,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
305,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
306,Senior Data Engineer,-1,"JOB DESCRIPTION:
Need to be expertise Coding on PySpark.
Supply Chain and Azure.
Experience in working with end-to-end technology implementation project, preferable those related to Data warehousing/Analytics/Reporting
Strong communication skills with the ability to collaborate effectively with business owners/users and technical teams.
Proven ability to gather product requirements from stakeholders and prioritize feature implementations based on business needs and product strategy.
Strong problem-solving skills and attention to detail.
Good to have experience with CPG/ Retail industry (preferable CPG companies such as Nestle, Unilever, P&G, Coca coal, PepsiCo etc)
Bachelor's/master's degree in business administration, computer science, engineering, or a related field.
Job Type: Contract
Experience:
PySpark: 5 years (Preferred)
SQL: 10 years (Preferred)
Data analytics: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Azure: 5 years (Preferred)
Data Bricks: 5 years (Preferred)
Data Lake: 5 years (Preferred)
Synapse: 5 years (Preferred)
Retail Domain: 8 years (Preferred)
CPG: 7 years (Preferred)
Work Location: Remote",4.0,"Quadrant Resource
4.0",Remote,1001 to 5000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
307,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
308,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
309,Senior Data Engineer,$87K - $114K (Glassdoor est.),"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.

This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration.
How will you make an impact?
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
What are we looking for?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework
You might be a good fit if you like to:
Leverage technology to deliver data solutions to drive business outcomes
Drive for continuous improvement of processes and solutions
Bring energy and commitment to excellence to drive delivery of high-quality solutions
Share knowledge and mentor team members on technologies and solutions
What you can learn on the job:
Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers
Grow practical understanding of agile work practices through application of the scrum framework to deliver work products
Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science
What does success look like?
First 30 days:
Understand the existing analytics data platform and technologies
Understand team’s work management processes
First 6 months:
Develop data solutions for the business including shared components and specific data sets
Learn business processes and develop necessary contacts throughout the organization
First year:
Continue to develop data solutions for the business
Become expert on the team for other data engineers to go to for mentoring in completing their work
Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,"AE Business Solutions
4.2","Milwaukee, WI",51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
310,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
311,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
312,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
313,Data Engineer IV,Employer Provided Salary:$149K,"Who We Are
Invitation Homes is a fast-paced evolving publicly traded REIT that is pioneering a new industry with over 80,000 single family rental homes and a market capitalization of over $25 billion. We are a team of more than 1,400 associates who come from all walks of life. We call different communities “home,” but our shared values bind us together. Invitation Homes is a place where possibility lives.
THR Property Management LP / Data Engineer / Dallas, TX: Develop and maintain data pipelines into data warehouse and data lake, including design, and development. Develop and implement strategies to translate business requirements into feasible and acceptable data warehouse and data lake solutions. Architect and build new data models to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making. Design, build and launch data pipelines to move data to data lake and data warehouse build and maintain framework for auditing, error logging, and master data management for data pipelines. Build data expertise and own data quality for the data pipelines. Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes. Perform data analysis and assist in the resolution of data issues. Identify and resolve defects of complex scope using proper engineering tools and techniques. Provide support and maintain existing products and add new features. Mentor engineers by providing advice, coaching and educational opportunities. Telecommuting permitted from any location within the U.S. Salary: 148512/year.
The Senior Data Engineer is accountable for the maintenance, improvement, and movement of data in the Enterprise Data Warehouse, and other data sources used for enterprise reporting. A successful Senior Data Engineer delivers database design, implementation, and schematics that creates sustainable, competitive advantage for our company. Further, a Senior Data Engineer is responsible for the development and deployment of innovative analytics data platforms to support end-user enterprise reporting. Working with other data engineers, data analysts, and data scientists, a Senior Data Engineer must have a delivery first mentality and a firm grasp of Agile Development methodology. Lastly, the ideal candidate must demonstrate the ability to work as a team member and team leader.
What You’ll Do
Accountable for the overall performance and maintenance of our SQL Server environments:
Manage the EDW relational database for optimized performance
Design scalable ETL packages from the business source systems
Ensure all necessary testing and validation for all planned and unplanned releases
Mentor junior data engineers in best practices and documentation.
Act as our database evangelist in leading innovation activities through exploration, benchmarking and implementation of data technologies
Participate in solution-based activities; articulate your ideas through well thought out documentation and research best-in-class solutions
Lead and Develop and implement data architecture standards
Manage the necessary testing and validation to ensure proper compliance to data governance and quality
Prepare progress reports regarding our database environments’’ status and health
Manage troubleshooting data issues and present solutions to these issues.
Resolve tactical issues thoroughly while always looking for optimization and documenting lessons learned
Understand when tactical issues are really systematic issues requiring deeper level of analysis and strategic solutions
Cooperate with the support team to investigate and resolve data related defects
Proactively analyze and evaluate current state of data in order to identify and recommend improvements and optimization
Support our Agile Development methodology
Constantly prioritize user stories; balance feature development, defects, and tech debt according to business need
Be a champion of continuous improvement and delivery; define the minimum viable product, focus teams on key release milestones, and leverage fast follow-up sprints
Enable sprint progress; clarify story details, validate completed stories, and prepare stories for the next sprint
Who We’re Looking For
Bachelor’s degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field. An equivalent of the same in working experience is also accepted for the position.
A candidate for the position will have as at least 5 years of working experience as a database engineering or a database engineering administrator within a fast-paced a complex business setting.
Design, implement and maintain SQL Server databases
Design, implement and maintain ETL processes using SQL Server SSIS
Provide ongoing maintenance support through SQL query tuning and optimization
Strong working and conceptual knowledge of building and maintaining physical and logical data models
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience working in SaaS, IaaS, and PaaS
Excellent customer service and interpersonal skills; ability to relate to and get along with others
Professional verbal and written communication skills
Strong organizational and time-management skills
Ability to multi-task and maintain flexibility and creativity in a variety of situations
Ability to analyze and resolve problems
Ability to set and meet goals and consistently meet deadlines
Ability to maintain confidentiality
Why Invitation Homes
We stand for flexibility, opportunity, and a home that people can make their own. It’s as true for our associates as it is for our residents. Just like we help our residents live freer, we liberate our associates’ careers, too. Our associates know that at Invitation Homes goals matter, potential is unlocked, and careers thrive. Invitation Homes isn’t just a work place it is a possibility place. Invitation Homes offers the below to each new associate:
Competitive pay and an annual bonus program for all associates
Generous paid time off plans including vacation accrual, sick time, volunteer time, and standard and floating holidays
401k with matching company contributions
Awesome work environment with casual dress
Team events and gatherings
Employee resource groups: Together with Women, Asian Alliance, Black Collective, Juntos, Gen Next, and Open Invitation.
Invitation Homes truly is where possibility lives, pour a new foundation here!
Salary Range
$98,640.00 - $170,976.00
Compensation and Benefits
To attract and retain top talent, we're pleased to offer competitive compensation and benefits, including:
Annual bonus program
Health, dental, vision, and life insurance
Long-term and short-term disability insurance
Generous paid time off plans include vacation accrual, sick time, standard holidays and floating holidays
401(k) with company matching contributions
Awesome work environment with casual dress
Team events and gatherings (Pre- and Post-Covid)
Invitation Homes is an equal opportunity employer committed to fostering a diverse, inclusive and innovative environment with the best associates. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, Veteran status or any other factor protected by applicable federal, state or local law. If you have a disability or special need that requires accommodation, please contact us at
humanresources@invitationhomes.com
.
To all recruitment agencies: Invitation Homes does not accept agency resumes. Please do not forward resumes to Invitation Homes employees. Invitation Homes is not responsible for any fees related to unsolicited resumes.",3.9,"Invitation Homes
3.9","Dallas, TX",1001 to 5000 Employees,2012,Company - Public,Real Estate,Real Estate,$1 to $5 billion (USD)
314,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
315,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
316,AWS Data Engineer Snowflake,-1,"Company Description

Orchestrated by adept technical architects with over fifty years of applied expertise, KYNITE is an advanced technology company specializing in the disciplines of: Blockchain, Cloud Services, Big Data & Analytics, Artificial Intelligence, Enterprise, Staff Augmentation and Managed Services
We are BigData Experts
We are Cloud Experts
We are Enterprise Architects
We are Artificial Intelligence Innovators
We are Technological Evangelists
We are Doers
We are Kynite

Additional Information

All your
This job is only for individuals residing in US
US Citizens, Green Card holders, EAD's can apply
W2, C2c,
Information will be kept confidential according to EEO guidelines.",-1,Kynite,"Myrtle Point, OR",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
317,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
318,Senior Data Engineer,Employer Provided Salary:$166K - $232K,"Patreon is the best place for creators to build memberships by providing exclusive access to their work and a deeper connection with their communities. We're building a content and community platform where creators can engage directly with their fans and monetize their creativity, while maintaining full ownership over the work they make and the communities they create.
We're leaders in the membership space with 250,000+ active creators and over $3.5 billion paid directly to creators on our platform. Our team is building tools to optimize the creator-to-fan relationship, including native video, enhanced podcasting features, improved creation tools, and new community experiences. We're continuing to invest heavily in building the most talented team in the Creator Economy and are looking for a Data Engineer to support our mission.
This role is available to those wishing to work in our SF and NY offices on a hybrid work model or those wishing to be fully remote in the United States.
About the Role
Work on a tight-knit team of highly motivated and experienced data engineers with frequent collaboration with data scientists, product managers and product engineers.
Work on both ""data analytics"" and ""data infrastructure"" type projects in a fast-paced, high-growth startup environment.
Build core data sets and metrics to power analytics, reports and experimentation.
Write real-time and batch data pipelines to support a wide range of projects and features including our creator-facing analytics product, executive reporting, FP&A, marketing initiatives, model training, data science analytics, A/B testing, etc.
Help manage and build out our data platform and suite of data tools.
Be a driver of a data-centric culture at Patreon. Work autonomously on large green field initiatives and help define data best practices at the company.
About You
Expert in SQL, Spark and Python or Scala
Significant experience modeling data and developing core data sets and metrics to support analytics, reports and experimentation. Solid understanding of how to use data to inform the product roadmap.
Enjoy collaborating with Data Scientists, Product Managers and Product Engineers. Comfortable playing the role of a Project Manager in order to drive results.
Have previously built real-time and batch data pipelines using tooling such as Airflow, Spark, Kafka, S3, Fivetran, Census, etc. Experience working with event tracking frameworks, data observability frameworks and experimentation frameworks.
Experience managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc.
Highly motivated self-starter that is keen to make an impact and is unafraid of tackling large, complicated problems and putting in the work to ensure high craft deliverables.
About Patreon
Patreon powers creators to do what they love and get paid by the people who love what they do. Our team is passionate about making this mission and our core values come to life every day in our work. Through this work, our Patronauts:
Put Creators First | They're the reason we're here. When creators win, we win.****
Build with Craft | We sign our name to every deliverable, just like the creators we serve.
Make it Happen | We don't quit. We learn and deliver.
Win Together | We grow as individuals. We win as a team.
We hire talented and passionate people from different backgrounds because we want our company to represent the vast diversity of our creators. If you're excited about a role but your prior experience doesn't match with every bullet point outlined above, we strongly encourage you to apply anyway. If you're a creator at heart, are energized by our mission, and share our company values, we'd love to hear from you.
Patreon is proud to be an equal-opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.
Patreon offers a competitive benefits package including and not limited to salary, equity plans, healthcare, unlimited paid time off, company holidays and recharge days, commuter benefits, lifestyle stipends, learning and development stipends, patronage, parental leave, and 401k plan with matching.
The posted range represents the expected salary range for this job requisition and does not include any other potential components of the compensation package, benefits and perks previously outlined. Ultimately, in determining pay, we'll consider your experience, leveling, location and other job-related factors.
San Francisco Pay Range
$166,000—$231,500 USD",3.2,"Patreon
3.2","San Francisco, CA",51 to 200 Employees,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
319,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
320,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
321,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
322,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
323,Senior Data Analytics Engineer,-1,"At Wistia our goal is to help every business to thrive with video. That's a big task, and we're super excited to help the world get there faster through simple and delightful video-focused products.
Wistia's Business Intelligence team is charged with leveraging data to empower strategic, insights-driven decision making across the organization. The Analytics Engineer role is a hybrid role with responsibilities that typically fall to Data Analysts, Data Engineers and Data Scientists. We are a small team and are looking for someone who is comfortable wearing different ""data"" hats. A successful candidate will combine strong technical and communication skills to manage the full data lifecycle at Wistia by creating and maintaining ETL pipelines, automating critical workflows, analyzing data to surface insights and translating those insights into clear recommendations for stakeholders.
The other members of the Business Intelligence team are curious, attentive to detail, impact-focused and approach problem solving with an engineering mindset. The team is centralized (not embedded) with each team member working with data from across the business. We strive to adopt the tools and best practices of the ""modern data stack"". For us that looks like: Redshift, dbt, Fivetran, Mode Analytics, Census, Snowplow Analytics. The Redshift warehouse we maintain serves as the single source of truth for the business.
Key Responsibilities
Maintain our 3rd-Party and custom ETL/reverse-ETL jobs and create new ones as required
Develop new predictive models as required to meet business outcomes
Generate easily interpretable analyses of complex datasets using SQL or python
Create persuasive narratives with data visualizations that enable self-serve exploration
Assist in the design and analysis of A/B testing to optimize business outcomes
Define version-controlled, modular business logic and datasets to be reused across the team
Document your findings, insights and recommendations for stakeholders
Meet business stakeholders ""where they live"" by becoming an expert in the data models and usage of their preferred SaaS tools, eg, Salesforce, HubSpot, Google Analytics, Amplitude, Stripe
Collaborate with:
Team members to share learnings, brainstorm new approaches and define best-practices
Stakeholders to scope, shape and prioritize their data requests
Qualifications
This section should talk about the competencies needed for the role, including qualities. Be sure to seek the input of key stakeholders and make sure we distinguish between must-haves and nice-to-haves because we will be using this to build our job positing and interview plan.
Technical
Highly proficient in SQL and python (or comparable general purpose programming language)
Facile with statistics and hypothesis testing
Skilled at constructing meaningful data visualizations
Comfortable with the command line and bash scripting
Knowledgeable about database architecture, data warehousing principles, REST APIs and basic ML algorithms
Non-technical
Experienced at scoping broad, loosely-defined requests into well-structured plans
Adept at translating data into actionable insights for stakeholders
Strong in written communication
Able to work independently with minimal oversight
Familiar with SaaS metrics and business practices
Values fit (for Wistia at large and BI specifically)
collaborative
learning oriented
attention to detail
simplicity as the starting point starts simple and add complexity only as needed
Key Metrics
This is a great place for more detail on the metrics we expect this person to own, and any that we measure success by.
Product, marketing, sales, and customer success teams have the data and insights they need to make decisions
Product, marketing, sales, and customer success teams have confidence in the data they use
Product, marketing, sales, and customer success teams are able to monitor business performance and spot problem areas and new opportunities
Responses to data requests are timely
Success Looks Like
This section should outline what success looks like. Knowing this helps inform the level and type of candidates we need, interview questions, onboarding plan, and coaching our new hires to success. It also makes it easier to spot performance issues early and hold ourselves accountable in hiring (and learn how we can improve).
30 Days
Complete Wistia onboarding program
Ramp up and learn the tools of the role (Mode, dbt, github, Fivetran, Redshift, Google Analytics, Salesforce, Pendo)
Ramp up and learn Wistia's data architecture, starting with Wistia app database
Ramp up and learn data analysis best practices and procedures
Complete ~5 basic data requests
90 Days
Confident in use of Mode, dbt, and GitHub for data analysis process
Confident in data architecture and able to self-source most data tables
Regularly handling basic data requests
Completed 1 complex data investigation
1 Year
Regularly initiating new data investigations and unearthing new insights
Regularly making improvements to our approach to data presentation
Regularly making improvements to our data models and data cleanliness
Working at Wistia
We try to ensure Wistia is an inclusive and diverse place where everyone feels happy, fulfilled, respected, comfortable, and welcome. We're proud to be an equal-opportunity workplace. We care a lot, so our benefits are actually benefits, not just the fun stuff like swag and snacks in the office (though we also have lots of those too!).
We know the biggest investment we can make is in our employees, so we provide:
A competitive compensation package that includes internal equity stock options
401k with 3% company contribution, regardless of whether you contribute
Fully paid healthcare, dental, and vision insurance (family plans included)
Pre-Tax FSA and Dependent Care Account
Fitness reimbursement
Flexible working hours – work at the times when you operate best, or set aside time for child and/or elderly care responsibilities during traditional ""office hours""
16 weeks paid parental leave for all new and expecting parents
Unlimited PTO (pretty common for people to take 4+ weeks off throughout the year)
Remote-first culture (work from anywhere in the U.S.)
Annual professional development stipend (courses, conferences, and more)
New hire bonus to enhance your home office set up
Pet insurance discount
Location/Remote Opportunities
Wistia is a remote-first company. Employees can work from our beautiful office in Cambridge, MA, or anywhere in the continental US",4.8,"Wistia
4.8","Cambridge, MA",51 to 200 Employees,2006,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
324,Sr. Data Engineer,$83K - $115K (Glassdoor est.),"Provides application design, analysis, programming, testing and documentation for the development, integration, enhancement, and maintenance of information systems solutions
Job Description:
BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Job Responsibilities
Formulates and defines system scope and objectives through research and fact-finding to design, develop, modify, or integrate complex information systems.
Devises or modifies application systems and procedures to optimize functional requirements including capacity, operating time, response time, and form of desired results.
Designs, codes, tests, debugs, and documents programs, subroutines, and scripts.
Develops and implements program/system test plans. Devises data verification methods and standard system procedures.
Responds to system failures and performance events by taking appropriate measures to reduce system downtime and eliminate recurrence of problems.
May conduct research on software and hardware products to justify recommendations and support management in budgeting and purchasing efforts.
Acts as expert technical resource to development staff in all phases of the development and implementation process.
Job Qualifications
Education
Bachelors Degree in Computer Science or equivalent work experience required. Equivalent years of experience are determined as one year of technical experience for every year of college requested.
Experience
1 year - Experience with information technology concepts, application development methodology, terminology, and standards required
5 years - Systems and programming experience required
1 year - Expert working knowledge of at least one programming language in use at BCBST.
Skills\Certifications
Proven skill with information technology in use at BCBST is required
Proven analysis, design, and coding skills, and demonstrated success in leading large and complex projects
Top level technical experience in one or more specialized areas of applications systems analysis and programming
Demonstrated ability to interpret and translate technical and/or or complex concepts into information meaningful to project team members and/or business personnel.
Be organized, reliable, and able to manage multiple tasks with exceptional work ethic.
Leadership skills
Must be able to communicate effectively with both technical and non-technical co-workers
Job Specific Requirements:
Preference for candidates with Cloud experience (GCP specifically)
Python knowledge/experience a plus
Preferred Skills:
Number of Openings Available:
1
Worker Type:
Employee
Worker Sub-Type:
Employee
Company:
BCBST BlueCross BlueShield of Tennessee, Inc.
Applying for this job indicates your acknowledgement and understanding of the following statements:
BCBST is an Equal Opportunity employer (EEO), and all employees and applicants will be entitled to equal employment opportunities when employment decisions are made. BCBST will take affirmative action to recruit, hire, train and promote individuals in all job classifications without regard to race, religion, color, age, sex, national origin, citizenship, pregnancy, veteran status, sexual orientation, physical or mental disability, gender identity, or any other characteristic protected by applicable law.
Further information regarding BCBST's EEO Policies/Notices may be found by reviewing the following page:
BCBST's EEO Policies/Notices
BlueCross BlueShield of Tennessee is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at BlueCross BlueShield of Tennessee via-email, the Internet or any other method without a valid, written Direct Placement Agreement in place for this position from BlueCross BlueShield of Tennessee HR/Talent Acquisition will not be considered. No fee will be paid in the event the applicant is hired by BlueCross BlueShield of Tennessee as a result of the referral or through other means.
All applicants will be advised that BlueCross, as a federal contractor, may be required to implement a COVID-19 vaccine mandate.
Tobacco-Free Hiring Statement
To further our mission of peace of mind through better health, effective 2017, BlueCross BlueShield of Tennessee and its subsidiaries no longer hire individuals who use tobacco or nicotine products (including but not limited to cigarettes, cigars, pipe tobacco, snuff, chewing tobacco, gum, patch, lozenges and electronic or smokeless cigarettes) in any form in Tennessee and where state law permits. A tobacco or nicotine free hiring practice is part of an effort to combat serious diseases, as well as to promote health and wellness for our employees and our community. All offers of employment will be contingent upon passing a background check which includes an illegal drug and tobacco/nicotine test. An individual whose post offer screening result is positive for illegal drugs or tobacco/nicotine and/or whose background check is verified to be unsatisfactory, will be disqualified from employment, the job offer will be withdrawn, and they may be disqualified from applying for employment for six (6) months from the date of the post offer screening results.
Resources to help individuals discontinue the use of tobacco/nicotine products include smokefree.gov or 1-800-QUIT-NOW.",3.8,"BlueCross BlueShield of Tennessee
3.8","Chattanooga, TN",5001 to 10000 Employees,1945,Nonprofit Organization,Insurance Carriers,Insurance,$5 to $10 billion (USD)
325,Data Engineer,Employer Provided Salary:$65K - $400K,"About Applied
Autonomy is one of the leading technological advances of this century that will come to impact our lives. The work you'll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting-edge technology while working with major players across the industry and the globe.
Applied Intuition provides software solutions to safely develop, test, and deploy autonomous vehicles at scale. The company's suite of simulation, validation, and drive log management software enables development teams to create thousands of scenarios in minutes, run simulations at scale, and verify and validate algorithms for production deployment. Headquartered in Silicon Valley with offices in Detroit, Washington, D.C., Munich, Stockholm, Seoul, and Tokyo, Applied consists of software, robotics, and automotive experts with experiences from top global companies. Leading autonomy programs and 17 of the top 20 global OEMs use Applied's solutions to bring autonomy to market faster.
About the role
We are looking for talented engineers who are excited about building products that wrangle AV data to supercharge our customers. You will drive the design and development of data processing across our products and internal tools. Handling massive volumes of data for Applied's platform needs is a critical area and we are looking for someone who can be hands-on in improving our data quality via data tools and processes.
We have multiple data engineer roles open for all levels of seniority, from mid-level to Lead.
At Applied, you will:
Design and develop systems for programmatically generating ground-truth-labeled data from a simulated world
Develop and deploy high-quality software using modern tooling and frameworks
Work with machine learning pipelines to understand and improve quality of datasets
Work with top autonomy companies to understand and solve unique challenges in perception with synthetic data
Work with products and teams across Applied Intuition
We're looking for someone who has:
Strong software engineering skills in programming languages (Python, C++, GoLang, etc.)
Experience with containerization and other modern software development workflows
Experience with synthetic data and its applications in perception systems
Nice to have:
Experience with applying synthetic data to machine learning tasks
Hands-on experience with characterization of models for Lidar, Radar, and Camera
The salary range for this position is $65,000 USD to $400,000 USD annually. This salary range is an estimate, and the actual salary may vary based on the Company's compensation practices
Don't meet every single requirement? If you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.
Applicants will be required to be fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable federal and state law. Applicants should be aware that for external-facing roles that involve close contact with Company employees or other third parties on the Company's premises, accommodations that involve remaining unvaccinated against COVID-19 may not be deemed reasonable. The Company will engage in the interactive process on an individualized basis taking into account the particular position.
Applied Intuition is an equal opportunity employer and federal contractor or subcontractor. Consequently, the parties agree that, as applicable, they will abide by the requirements of 41 CFR 60-1.4(a), 41 CFR 60-300.5(a) and 41 CFR 60-741.5(a) and that these laws are incorporated herein by reference. These regulations prohibit discrimination against qualified individuals based on their status as protected veterans
or individuals with disabilities, and prohibit discrimination against all individuals based on their race, color, religion, sex, sexual orientation, gender identity or national origin. These regulations require that covered prime contractors and subcontractors take affirmative action to employ and advance in employment individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status or disability. The parties also agree that, as applicable, they will abide by the requirements of Executive Order 13496 (29 CFR Part 471, Appendix A to Subpart A), relating to the notice of employee rights under federal labor laws.",4.0,"Applied Intuition
4.0","Mountain View, CA",51 to 200 Employees,2017,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
326,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
327,Senior Data Engineer,Employer Provided Salary:$143K - $300K,"The Data team leverages data from our autonomous vehicles and operations to determine autonomy and service readiness. We provide the foundation for strategic decision-making at Zoox. You will develop and implement the next generation of our data pipeline to ensure visibility into our business as we scale toward the launch of an autonomous mobility service. You will define the system and build the pipeline to enable Zoox to develop and scale with a data-first culture.

You will join a diverse, experienced team with rapidly growing scope and responsibility while also having access to one of the most unique data sets in the autonomous vehicle industry. Hence, we are seeking all skill levels to grow with the team.
Responsibilities
Designing, building, and maintaining the infrastructure that transforms autonomous vehicle data at scale to support analytics throughout the company
Defining and executing on how data from perception, prediction, planning and other parts of the autonomous stack is consumed to generate valuable insights by data scientists, engineers, and business users
Establishing robust data integrity monitoring so that company-wide metrics are based on accurate data
Partnering with engineering and product teams to define data consumption patterns and establish best practices
Qualifications
Experience designing and building complex data infrastructure at scale
Exceptional Python or Scala skills
Advanced SQL and data warehousing experience
Experience operating a workflow manager such as Airflow
Experience with large scale streaming platforms (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Hadoop) and storage engines (e.g. HDFS, HBase)
A strong DataOps mindset and opinions on next-generation warehousing tools
Bonus Qualifications
Basic fluency in C++
Familiarity with or exposure to experimentation platforms
Compensation
There are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights. The salary will range from $143,000 to $300,000. A sign-on bonus may be part of a compensation package. Compensation will vary based on geographic location, job-related knowledge, skills, and experience.

Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance.

ABOUT ZOOX

Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of artificial intelligence, robotics, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.

Follow us on LinkedIn

Vaccine Mandate
Employees working in this position will be required to have received a vaccine approved by the U.S. Food and Drug Administration and/or the World Health Organization. In addition, employees who are eligible for a COVID-19 booster vaccine (“Booster”) will be required to receive a Booster. Employees will be required to show proof of vaccination status upon receipt of a conditional offer of employment. That offer of employment will be conditioned upon, among other things, an Applicant’s ability to show proof of vaccination status. Please note the Company provides reasonable accommodations in accordance with applicable state, federal, and local laws.

About Zoox
Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of robotics, machine learning, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.

Follow us on LinkedIn

A Final Note:
You do not need to match every listed expectation to apply for this position. Here at Zoox, we know that diverse perspectives foster the innovation we need to be successful, and we are committed to building a team that encompasses a variety of backgrounds, experiences, and skills.",4.0,"Zoox
4.0","Foster City, CA",1001 to 5000 Employees,2014,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
328,Data Engineer 2,-1,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Exciting Full-Time Employee Benefits, Perks and Culture
Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S.
Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX.
Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually.
Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit.
Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service.
Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth.
Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters.
Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days.
Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make.
Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights.
Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills.
Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization.
Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey.
Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.

At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.

MyFitnessPal participates in E-Verify.",4.2,"MyFitnessPal
4.2",Remote,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
329,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
330,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
331,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
332,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
333,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
334,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
335,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
336,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
337,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
338,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
339,Senior Data Engineer,Employer Provided Salary:$105K - $186K,"**(UPDATED 8/21/2023) We invite applicants to express their interest in this role, but please note that we currently don't have a specific timeline for filling this position. This posting is intended for talent pooling. While there isn't a definitive hiring period, potential candidates should be aware that the role might be filled at some point in the future. We encourage you to start the process and discover more about this opportunity, Slalom, and potentially a career in consulting through discussions with our talent acquisition team.
Who You’ll Work With
As a modern technology company, we've never met a technical challenge we didn't like. We enable our clients to learn from their data, create incredible digital experiences, and make the most of new technologies. We blend design, engineering, and analytics expertise to build the future. We surround our technologists with interesting challenges, innovative minds and emerging technologies.
A Senior Data Engineer at Slalom is a technical team member who designs and develops data platform solutions. It uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g., data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc.
What You’ll Do
Work as part of a team and with clients to develop cloud data and analytics solutions
Make recommendations and prove data architecture concepts to team members and clients
Development of cloud data platforms and business intelligence solutions
Data wrangling of heterogeneous data sources for exploring and discovering new insights
Work with highly collaborative agile teams on modern data architecture projects
Key success measures of a Senior Data Engineer include technical proficiency in data processing technologies, collaboration with other teams, communication, and willingness to learn new things
What You'll Bring
Experience in one or more cloud technologies (AWS, Azure, Google Cloud, Snowflake)
Experience in data modeling, design, and development using complex SQL (SQL Server, Oracle, etc.)
Experience in Apache Airflow, Talend, SSIS, Informatica, Alteryx, or other data processing tool
Experience in one or more programming languages (Python, C#, Scala, R, PowerShell, Java, etc.)
Experience in cloud engineering, serverless architecture, and/or data streaming services
About Us
Slalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six countries and 43 markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 13,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.
Compensation and Benefits
Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance.
Slalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $105,000 to $186,000. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.
EEO and Accommodations
Slalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team if you require accommodations during the interview process.
#LI-JS8
#LI-Hybrid",4.3,"Slalom Consulting
4.3","Los Angeles, CA",10000+ Employees,2001,Company - Private,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
340,AWS Data Engineer II,$77K - $105K (Glassdoor est.),"Overview:
Shift4 (NYSE: FOUR) is boldly redefining commerce by simplifying complex payments ecosystems across the world. As the leader in commerce-enabling technology, Shift4 powers billions of transactions annually for hundreds of thousands of businesses in virtually every industry. For more information, visit www.shift4.com.

Shift4 Payments is seeking a Business Intelligence Engineer with an AWS Focus to join our growing internal Information Technology team. As a BI Engineer, you will help design and maintain the flow of information and analytics to the entire organization utilizing AWS capabilities to store, move and report on data. The position requires at least 3 years of prior experience as a Business Intelligence AWS engineer. We are looking for individuals that are extremely self-sufficient, available to work flexible hours and hold themselves to the highest standards of professionalism.Maintain and expand BI AWS architecture and content.
Responsibilities:
Support of BI AWS architecture and content up to and including off hours support when necessary.
Work on necessary bugs, enhancements and new system developments utilizing our Software Development Life Cycle (SDLC) standards.
Complete root cause analysis and provide recommendations to leadership to address opportunities.
Contribute to and enforce the use of the organization's Business Intelligence solution.
Develop software solutions that will provide solid foundations for the growth of the company.
Support key metrics and shared goals that determine success for a particular area.
Qualifications:
3-5+ years of professional experience in business intelligence AWS development or a related field.
Advanced experience working with AWS: S3, Glue, Lambda, Step Functions, Redshift, Athena.
Advanced experience working with various data sources such as AWS S3 and Redshift, Postgres, SQL Server, Amazon Athena, Excel, Flat Files, etc.
Advanced experience working with large data sets from sources such as AWS S3 and Redshift, Postgres, SQL Server, etc.
Strong PySpark, Python, CLI proficiency
Strong SQL proficiency (T-SQL, MySQL, and PostgreSQL).
Experience working with an Enterprise Data Warehouse and Dimensional Databases is a plus.
Advanced experience of the SDLC and how Development processes fit into KANBAN and Scrum Frameworks.
Experience with Jira and Confluence is a plus.
Ability to prioritize multiple tasks and easily adjust to changing priorities.
Ability to identify problems, initiate solutions, and effectively collaborate and communicate with team members.
Have excellent verbal and written communication skills.

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.",3.5,"Shift4 Payments
3.5","Allentown, PA",1001 to 5000 Employees,1999,Company - Public,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
341,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
342,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
343,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
344,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
345,Sr. Data Engineer,Employer Provided Salary:$118K - $142K,"Data Engineer
Egen Solutions Inc offers competitive compensation, benefits, and an excellent culture of teamwork and collaboration. We are an Equal Opportunity Employer that does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.
Job Summary
We have some major internal projects underway at Egen and need a highly skilled Data Engineer to join our team on a full-time basis. Below is the JD, kindly go through it and let me know your interest and if you are interested revert back with your updated resume
Our Data Engineering teams build scalable data pipelines using Python, Spark, and cloud services (GCP and AWS). The pipelines we build typically integrate with technologies such as Kafka, Storm, and Elasticsearch. We are working on a continuous deployment pipeline that leverages rapid on-demand releases. Our developers work in an agile process to efficiently deliver high value applications and product packages.
As a Staff Data Engineer at Egen, you will leverage Spark and GCP (preferred) to architect and implement cloud-native data pipelines and infrastructure to enable analytics and machine learning on rich datasets.
Required Experience:
Built and run resilient data pipelines in production and have implemented ETL/ELT to load a multi-terabyte enterprise data warehouse.
Implemented analytics applications using multiple database technologies, such as relational, multidimensional (OLAP), key-value, document, or graph.
Defined data contracts, and wrote specifications including REST APIs.
Transformed data between data models and formats with the most modern PySpark practices. Have built cloud-native applications and supporting technologies / patterns / practices including: Cloud Services, Docker, CI/CD, DevOps, and microservices.
Planned and designed artifacts that describe software architectures involving multiple systems and technologies
You've worked in agile environments and are comfortable iterating quickly.
Nice to have's (but not required):
GCP expertise is preferred but will consider AWS
Experience moving trained machine learning models into production data pipelines.
Experience in biotech, genomics, clinical research or precision medicine.
Expert knowledge of relational database modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others.
This is full-time, On-site, and Remote. We are based out in Naperville, IL.
Looking forward to striking up a conversation with you on the opportunity!, Kindly visit our website egen.solutions
equirements
Bachelor’s degree in Computer Science, Engineering, or related field.
3+ years of experience with data engineering and/or data analysis.
Strong analytical skills and ability to think analytically.
Ability to work independently with minimal supervision.
Experience with SQL and/or NoSQL databases is a plus.
Experience with ETL processes is a plus.
Experience with cloud-based solutions such as Azure Data Factory is a plus.
Experience with Agile software development methods such as Scrum is a plus.
Ability to work in a fast-paced environment and adapt to changing priorities.
Ability to work in a team environment while maintaining strong individual initiative.
Job Type: Full-time
Pay: $117,577.88 - $141,599.17 per year
Benefits:
401(k)
Dental insurance
Health insurance
Experience level:
5 years
Application Question(s):
Are you a personal who hold any of these work authorizations currently(OPT EAD/GC/F1 OPT/H4 EAD.
Work Location: In person",-1,Egen Solutions Inc,"Naperville, IL",-1,-1,-1,-1,-1,-1
346,Senior Data Engineer,Employer Provided Salary:$166K - $232K,"Patreon is the best place for creators to build memberships by providing exclusive access to their work and a deeper connection with their communities. We're building a content and community platform where creators can engage directly with their fans and monetize their creativity, while maintaining full ownership over the work they make and the communities they create.
We're leaders in the membership space with 250,000+ active creators and over $3.5 billion paid directly to creators on our platform. Our team is building tools to optimize the creator-to-fan relationship, including native video, enhanced podcasting features, improved creation tools, and new community experiences. We're continuing to invest heavily in building the most talented team in the Creator Economy and are looking for a Data Engineer to support our mission.
This role is available to those wishing to work in our SF and NY offices on a hybrid work model or those wishing to be fully remote in the United States.
About the Role
Work on a tight-knit team of highly motivated and experienced data engineers with frequent collaboration with data scientists, product managers and product engineers.
Work on both ""data analytics"" and ""data infrastructure"" type projects in a fast-paced, high-growth startup environment.
Build core data sets and metrics to power analytics, reports and experimentation.
Write real-time and batch data pipelines to support a wide range of projects and features including our creator-facing analytics product, executive reporting, FP&A, marketing initiatives, model training, data science analytics, A/B testing, etc.
Help manage and build out our data platform and suite of data tools.
Be a driver of a data-centric culture at Patreon. Work autonomously on large green field initiatives and help define data best practices at the company.
About You
Expert in SQL, Spark and Python or Scala
Significant experience modeling data and developing core data sets and metrics to support analytics, reports and experimentation. Solid understanding of how to use data to inform the product roadmap.
Enjoy collaborating with Data Scientists, Product Managers and Product Engineers. Comfortable playing the role of a Project Manager in order to drive results.
Have previously built real-time and batch data pipelines using tooling such as Airflow, Spark, Kafka, S3, Fivetran, Census, etc. Experience working with event tracking frameworks, data observability frameworks and experimentation frameworks.
Experience managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc.
Highly motivated self-starter that is keen to make an impact and is unafraid of tackling large, complicated problems and putting in the work to ensure high craft deliverables.
About Patreon
Patreon powers creators to do what they love and get paid by the people who love what they do. Our team is passionate about making this mission and our core values come to life every day in our work. Through this work, our Patronauts:
Put Creators First | They're the reason we're here. When creators win, we win.****
Build with Craft | We sign our name to every deliverable, just like the creators we serve.
Make it Happen | We don't quit. We learn and deliver.
Win Together | We grow as individuals. We win as a team.
We hire talented and passionate people from different backgrounds because we want our company to represent the vast diversity of our creators. If you're excited about a role but your prior experience doesn't match with every bullet point outlined above, we strongly encourage you to apply anyway. If you're a creator at heart, are energized by our mission, and share our company values, we'd love to hear from you.
Patreon is proud to be an equal-opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected class.
Patreon offers a competitive benefits package including and not limited to salary, equity plans, healthcare, unlimited paid time off, company holidays and recharge days, commuter benefits, lifestyle stipends, learning and development stipends, patronage, parental leave, and 401k plan with matching.
The posted range represents the expected salary range for this job requisition and does not include any other potential components of the compensation package, benefits and perks previously outlined. Ultimately, in determining pay, we'll consider your experience, leveling, location and other job-related factors.
San Francisco Pay Range
$166,000—$231,500 USD",3.2,"Patreon
3.2","San Francisco, CA",51 to 200 Employees,2013,Company - Private,Internet & Web Services,Information Technology,Unknown / Non-Applicable
347,Data Engineer,$78K - $112K (Glassdoor est.),"Translate business requirements and functional specifications into logical program designs and to deliver code modules, stable application systems, and software solutions.
Develop, configure, or modify complex integrated business and/or enterprise application solutions within various computing environments.
Implement and maintain complex business and enterprise software solutions to ensure successful deployment of released applications.
Translate complex cross-functional business requirements and functional specifications into logical program designs, code modules, stable application systems, and software solutions.
Partner with Product Team to understand business needs and functional specifications. .
Collaborate with cross-functional teams to ensure specifications are converted into flexible, scalable, and maintainable solution designs.
Evaluate project deliverables to ensure they meet specifications and architectural standards.
Coordinate, execute, and participate in component integration (CIT) scenarios, systems integration testing (SIT), and user acceptance testing (UAT) to identify application errors and to ensure quality software deployment.
Participate in all software development end-to-end product lifecycle phases by applying and sharing an in-depth understanding of complex company and industry methodologies, policies, standards, and controls.
Architect software design patterns and approaches, application level software architecture and make technical trade-off decisions at application level.
Automate and simplify team development, test, and operations processes.
Develop detailed architecture plans for large scale enterprise architecture projects and drives the plans to fruition.
Solve complex architecture/design and business problems; solutions are extensible; work to simplify, optimize, remove bottlenecks, etc.
Minimum Education Required:- All the responsibilities mentioned above are in line with the professional background and requires an absolute minimum of bachelor’s degree in computer science, computer information systems, information technology or a combination of education and experience equating to the U.S. equivalent of a bachelor's degree in one of the aforementioned subjects.",4.9,"Wise Equation Solutions
4.9","Mooresville, NC",51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
348,"Data Engineer, Sales",Employer Provided Salary:$161K - $220K,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Software Engineering
Job Details
About Salesforce
We’re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good – you’ve come to the right place.
Salesforce, Inc. seeks Data Engineer, Sales in San Francisco, CA:
Job Duties: Architect, design, implement and maintain data pipelines and data models enabling actional insights for Slack’s Sales and Corporate Systems key partners, data scientists, and data decision makers in the organization such as Sales, Customer Success, Marketing, and Finance. It requires the candidate to utilizing knowledge and expertise in Data Integration, Data Modeling, Optimization and Data Quality. Work cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data Warehouse model, as well as guaranteed compliance with data governance and data security requirements while creating, improving, and operationalizing these integrated and reusable data pipelines. This would enable faster data access, integrated data reuse and vastly improved time-to-solution for Slack’s data and analytics initiatives. Partner with Data Engineers, Data architects, domain experts, data analysts and other teams to build foundational data sets that are trusted, well understood, aligned with business strategy, and enable self-service. Work on the overall strategy for data governance, security, privacy, quality, and retention that will satisfy business policies and requirements. Own and document data pipelines and data lineage, as well as identify, document, and promote best practices of Data Engineering at Slack. ˆHQ address additionally encompasses the following Salesforce locations in San Francisco: 350 Mission Street, 415 Mission Street, and 50 Fremont Street. The permanent position may be offered at any of these locations in San Francisco. Fulltime telecommuting permitted.
Minimum Requirements: Master's degree, or foreign equivalent, in Computer Science, Information Technology and Data Science for Business Analytics, Engineering (any) or a related quantitative discipline and three (3) years of work experience in the job offered or in data-engineering-related or business-analytics-related position.
A related technical degree required (Computer Science, Engineering (any field)).
Special Skill Requirements: (1) Advanced SQL Programming; (2) Python; (3) Data Modeling; (4) Database & Data Warehouse Design; (5) ETL Development using tools such as Airflow, Alooma, Stitch, Segment; (6) Data Warehouses such as Redshift, Snowflake, MySQL; (7) AWS Services like S3, Aurora, DMS; (8) Dashboard/Report Development using Looker, Tableau; (9) Sales & Marketing platforms such as Salesforce Marketing cloud, Iterable, Swrve, Branch; (10) ERP and E-commerce platforms such as Netsuite, Shopify. Any suitable combination of education, training and/or experience is acceptable. Education, experience and criminal background checks will be conducted. Full-time telecommuting permitted.
Salary: $161,491.00 - $220,000.00 per annum.
Submit a resume using the apply button on this posting or by email at: onlinejobpostings@salesforce.com at Job #21-8965. Salesforce is an Equal Opportunity & Affirmative Action Employer. Education, experience, and criminal background checks will be conducted.
#LI-DNI
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form .
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com .
Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce .
Salesforce welcomes all.
Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, Salesforce will consider for employment qualified applicants with arrest and conviction records.
For New York-based roles, the base salary hiring range for this position is $161,491 to $220,000.
For Colorado-based roles, the base salary hiring range for this position is $161,491 to $220,000.
For Washington-based roles, the base salary hiring range for this position is $161,491 to $220,000.
For California-based roles, the base salary hiring range for this position is $161,491 to $220,000.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.",4.1,"Salesforce
4.1","San Francisco, CA",10000+ Employees,1999,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
349,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
350,Sr. Data Engineer- Contractor- Remote Work Eligible,-1,"Notes to applicants:
This position is eligible for full-time remote work in Texas, or North Carolina, or, in the alternative, to work in accordance with Dimensional’s best-of-both hybrid working model, which involves working in the office on Tuesdays, Wednesdays and Thursdays, and choosing to work in the office or remotely on Mondays and Fridays.
Resumes and portfolios (when applicable) are required as part of your application. When applying from a mobile device or tablet, you may not be able to attach a resume. If you cannot include an attachment at the time of your application, you will receive a follow up email asking you to attach your resume from a computer.
Here at Dimensional, we strive to be an inclusive workplace for all. Even if you do not match every qualification listed, if you are interested in who we are, what we do, and why we do it, we suggest and encourage you to apply.

Job Description:
About Dimensional:
Dimensional was built around a set of ideas bigger than the firm itself. With a confidence in markets, deep connections to the academic community, and a focus on implementation, we go where the science leads, and continue to pursue new insights, both large and small, that can benefit our clients.
The Technology Department at Dimensional leverages the rapidly evolving state of the art to engineer scalable, innovative, and research driven solutions to improve our client’s financial lives.
Software Engineers at Dimensional participate in the design and development of software solutions across an array of domains from Research and Investments to Sales and Marketing; collaboratively developing MVPs to test their ideas and rapidly iterate with constant feedback from users. Dimensional invests heavily in developer tools, platforms, paradigms and experience enabling teams to provide modern solutions that contribute profoundly to our client’s success.
We are looking for a Python Data Engineer to join our team and translate our customers’ goals into working software throughout the stack from automated configurations to model definitions, calculation APIs, and building robust data pipelines. The most important qualifications are a passion for quality software and enthusiasm for learning new technologies and approaches. The level of seniority for this position is negotiable based on experience.
You may be a fit for this role if you:
Are open-minded, curious, and resourceful
Are passionate about/stay current with modern technologies
Solve problems systematically and transparently
Share ideas, solicit/integrate feedback, design and solve collaboratively
Take a software engineering approach and demonstrate automation and security mindsets
What you might work on:
As a Data engineer at Dimensional, you will have the opportunity to understand the users’ needs and solve problems at all levels of the stack from automating infrastructure and deployments to building complex data pipelines to designing user friendly data applications.
Collaborate with subject matter experts in a variety of areas to drive the success of our clients
Perform software and data architecture and design
Develop complex software solutions using ETL and/or back-end technologies
Demonstrate and mentor software engineering best practices and participate in code reviews
Develop configurations and automations to enable testing, infrastructure and deployments
The successful candidate will be self-motivated and have a strong drive for learning and self-improvement.
Qualifications:
Bachelor’s degree in a technical field or equivalent practical experience.
5-10+ years of software development experience in a professional and/or academic setting (seniority of the role is negotiable).
5+ years of hands-on experience in developing ETL solutions using python.
Working knowledge of DevOps concepts, tools, and continuous delivery pipelines such as Octopus, TeamCity, Stash, Bitbucket, Jira, GIT, etc.
Advanced SQL knowledge and experience working with relational databases and working familiarity with various cloud data warehouses.
Experience in building processes supporting data transformation, data structures, metadata, dependency, and workload management.
Experience with data pipeline and workflow management tools: Airflow, etc.
Preferred Competencies:
Interest and ability to learn other coding languages as needed
Ability to write in English fluently and idiomatically
Advanced degree or equivalent experience in engineering, computer science or other technical related field
Experience with agile/scrum methodologies
Financial services industry experience
Experience with any of the following:
Redis, Postgresql, MongoDB, SQLServer
Airflow, Kafka, AWS, serverless/microservice architecture
TDD, BDD, Numpy/Scipy/Pandas, Ansible

#LI-Remote

Dimensional offers a variety of programs to help take care of you, your family, and your career, including comprehensive benefits, educational initiatives, and special celebrations of our history, culture, and growth.
It is the policy of the Company to provide equal employment opportunity for all applicants and employees. The Company does not unlawfully discriminate on the basis of race, color, religion, creed, sex, gender, gender identity, gender expression, national origin, age, disability, genetic information, ancestry, medical condition, marital status, covered veteran status, citizenship status, sexual orientation, or any other protected status. This policy applies to all areas of employment including recruitment, hiring, training, job assignment, promotion, compensation, benefits, transfer, discipline, termination, and social and recreational programs.",3.7,"Dimensional Fund Advisors
3.7",Texas,1001 to 5000 Employees,1981,Company - Private,Investment & Asset Management,Financial Services,$1 to $5 billion (USD)
351,Data Engineer IV,Employer Provided Salary:$149K,"Who We Are
Invitation Homes is a fast-paced evolving publicly traded REIT that is pioneering a new industry with over 80,000 single family rental homes and a market capitalization of over $25 billion. We are a team of more than 1,400 associates who come from all walks of life. We call different communities “home,” but our shared values bind us together. Invitation Homes is a place where possibility lives.
THR Property Management LP / Data Engineer / Dallas, TX: Develop and maintain data pipelines into data warehouse and data lake, including design, and development. Develop and implement strategies to translate business requirements into feasible and acceptable data warehouse and data lake solutions. Architect and build new data models to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making. Design, build and launch data pipelines to move data to data lake and data warehouse build and maintain framework for auditing, error logging, and master data management for data pipelines. Build data expertise and own data quality for the data pipelines. Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes. Perform data analysis and assist in the resolution of data issues. Identify and resolve defects of complex scope using proper engineering tools and techniques. Provide support and maintain existing products and add new features. Mentor engineers by providing advice, coaching and educational opportunities. Telecommuting permitted from any location within the U.S. Salary: 148512/year.
The Senior Data Engineer is accountable for the maintenance, improvement, and movement of data in the Enterprise Data Warehouse, and other data sources used for enterprise reporting. A successful Senior Data Engineer delivers database design, implementation, and schematics that creates sustainable, competitive advantage for our company. Further, a Senior Data Engineer is responsible for the development and deployment of innovative analytics data platforms to support end-user enterprise reporting. Working with other data engineers, data analysts, and data scientists, a Senior Data Engineer must have a delivery first mentality and a firm grasp of Agile Development methodology. Lastly, the ideal candidate must demonstrate the ability to work as a team member and team leader.
What You’ll Do
Accountable for the overall performance and maintenance of our SQL Server environments:
Manage the EDW relational database for optimized performance
Design scalable ETL packages from the business source systems
Ensure all necessary testing and validation for all planned and unplanned releases
Mentor junior data engineers in best practices and documentation.
Act as our database evangelist in leading innovation activities through exploration, benchmarking and implementation of data technologies
Participate in solution-based activities; articulate your ideas through well thought out documentation and research best-in-class solutions
Lead and Develop and implement data architecture standards
Manage the necessary testing and validation to ensure proper compliance to data governance and quality
Prepare progress reports regarding our database environments’’ status and health
Manage troubleshooting data issues and present solutions to these issues.
Resolve tactical issues thoroughly while always looking for optimization and documenting lessons learned
Understand when tactical issues are really systematic issues requiring deeper level of analysis and strategic solutions
Cooperate with the support team to investigate and resolve data related defects
Proactively analyze and evaluate current state of data in order to identify and recommend improvements and optimization
Support our Agile Development methodology
Constantly prioritize user stories; balance feature development, defects, and tech debt according to business need
Be a champion of continuous improvement and delivery; define the minimum viable product, focus teams on key release milestones, and leverage fast follow-up sprints
Enable sprint progress; clarify story details, validate completed stories, and prepare stories for the next sprint
Who We’re Looking For
Bachelor’s degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field. An equivalent of the same in working experience is also accepted for the position.
A candidate for the position will have as at least 5 years of working experience as a database engineering or a database engineering administrator within a fast-paced a complex business setting.
Design, implement and maintain SQL Server databases
Design, implement and maintain ETL processes using SQL Server SSIS
Provide ongoing maintenance support through SQL query tuning and optimization
Strong working and conceptual knowledge of building and maintaining physical and logical data models
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience working in SaaS, IaaS, and PaaS
Excellent customer service and interpersonal skills; ability to relate to and get along with others
Professional verbal and written communication skills
Strong organizational and time-management skills
Ability to multi-task and maintain flexibility and creativity in a variety of situations
Ability to analyze and resolve problems
Ability to set and meet goals and consistently meet deadlines
Ability to maintain confidentiality
Why Invitation Homes
We stand for flexibility, opportunity, and a home that people can make their own. It’s as true for our associates as it is for our residents. Just like we help our residents live freer, we liberate our associates’ careers, too. Our associates know that at Invitation Homes goals matter, potential is unlocked, and careers thrive. Invitation Homes isn’t just a work place it is a possibility place. Invitation Homes offers the below to each new associate:
Competitive pay and an annual bonus program for all associates
Generous paid time off plans including vacation accrual, sick time, volunteer time, and standard and floating holidays
401k with matching company contributions
Awesome work environment with casual dress
Team events and gatherings
Employee resource groups: Together with Women, Asian Alliance, Black Collective, Juntos, Gen Next, and Open Invitation.
Invitation Homes truly is where possibility lives, pour a new foundation here!
Salary Range
$98,640.00 - $170,976.00
Compensation and Benefits
To attract and retain top talent, we're pleased to offer competitive compensation and benefits, including:
Annual bonus program
Health, dental, vision, and life insurance
Long-term and short-term disability insurance
Generous paid time off plans include vacation accrual, sick time, standard holidays and floating holidays
401(k) with company matching contributions
Awesome work environment with casual dress
Team events and gatherings (Pre- and Post-Covid)
Invitation Homes is an equal opportunity employer committed to fostering a diverse, inclusive and innovative environment with the best associates. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, Veteran status or any other factor protected by applicable federal, state or local law. If you have a disability or special need that requires accommodation, please contact us at
humanresources@invitationhomes.com
.
To all recruitment agencies: Invitation Homes does not accept agency resumes. Please do not forward resumes to Invitation Homes employees. Invitation Homes is not responsible for any fees related to unsolicited resumes.",3.9,"Invitation Homes
3.9","Dallas, TX",1001 to 5000 Employees,2012,Company - Public,Real Estate,Real Estate,$1 to $5 billion (USD)
352,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
353,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
354,Senior Data Engineer,$87K - $114K (Glassdoor est.),"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.

This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration.
How will you make an impact?
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
What are we looking for?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework
You might be a good fit if you like to:
Leverage technology to deliver data solutions to drive business outcomes
Drive for continuous improvement of processes and solutions
Bring energy and commitment to excellence to drive delivery of high-quality solutions
Share knowledge and mentor team members on technologies and solutions
What you can learn on the job:
Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers
Grow practical understanding of agile work practices through application of the scrum framework to deliver work products
Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science
What does success look like?
First 30 days:
Understand the existing analytics data platform and technologies
Understand team’s work management processes
First 6 months:
Develop data solutions for the business including shared components and specific data sets
Learn business processes and develop necessary contacts throughout the organization
First year:
Continue to develop data solutions for the business
Become expert on the team for other data engineers to go to for mentoring in completing their work
Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,"AE Business Solutions
4.2","Milwaukee, WI",51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
355,"Senior Data Engineer, AdTech",$121K - $165K (Glassdoor est.),"Engineering
Data
Our mission on the Advertising Product & Technology team is to build a next generation advertising platform that aligns with our unique value proposition for audio and video. We work to scale the user experience for hundreds of millions of fans and hundreds of thousands of advertisers. This scale brings unique challenges as well as tremendous opportunities for our artists and creators.
Location
New York or Remote Americas
Job type
Permanent
We are looking for Data Engineers to build and drive data engineering initiatives within our advertising and podcast monetization teams. In this role, you will be instrumental in streamlining data ingested from multiple sources to not only recognize value and insights, but to also set standards along the way. You will help us to create a user-first ad experience that's personalized and relevant, and develop and own software solutions on our fast-paced podcast and ad services technology. You will help us grow to billions of fans, increase engagement with our listeners, and provide better value to our advertisers. Above all, your work will impact the way the world experiences music and podcasts.
What You'll Do
Work closely with key partners across the ads organization, contributing to the improvement of many different pipelines and services
Build new distributed data pipelines across ad experience
Use best practices in continuous integration and delivery
Help drive optimization, testing and tooling to improve reliability and data quality
Apply a Data centric approach to all the platforms, pipelines and engineering activities
Jump into data pipelines, identify issues, and propose solutions across teams
Design, develop, and maintain Java services
Work in cross-functional agile teams to continuously experiment, iterate and deliver on new product objectives
Who You Are
You have a strong understanding of data systems
You are knowledgeable and passionate about improving and building new distributed data pipelines
You are knowledgeable about data modeling, data access, and data storage techniques
You are familiar with current engineering practices such as distributed architecture, and are curious about new technologies that help derive insights and value from data
You have experience working on and building distributed data pipelines that ingest huge amounts of data across multiple sources and brands
You are very comfortable working with datasets in SQL (we also use Scio!)
You are comfortable in at least one core language like Python, Java or Scala
You have experience working with Apache Beam, Hadoop or a similar streaming data technology
You have experience with GCP (preferred) or AWS
You enjoy close collaboration with backend and ML engineers, and are passionate about software architecture across the stack
Ad Tech and/or Podcast Monetization experience is a plus
Where You'll Be
We are a distributed workforce enabling our band members to find a work mode that is best for them!
Where in the world? For this role, it can be within the Americas region in which we have a work location and is within working hours: https://lifeatspotify.com/locations
Working hours? We operate within the Eastern Standard time zone for collaboration and ask that all be located in that time zone.
Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here.
Our global benefits
Extensive learning opportunities, through our dedicated team, GreenHouse.
Flexible share incentives letting you choose how you share in our success.
Global parental leave, six months off - fully paid - for all new parents.
All The Feels, our employee assistance program and self-care hub.
Flexible public holidays, swap days off according to your values and beliefs.
Learn about life at Spotify
The United States base range for this position is $156,275 - $223,25, plus equity. The benefits available for this position include health insurance, six month paid parental leave, 401(k) retirement plan, monthly meal allowance, 23 paid days off, 13 paid flexible holidays. This range encompasses multiple levels. Leveling is determined during the interview process. Placement in a level depends on relevant work history and interview performance. These ranges may be modified in the future.

Spotify is an equal opportunity employer. You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens.

Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service.",4.0,"Spotify
4.0","New York, NY",5001 to 10000 Employees,2006,Company - Public,Internet & Web Services,Information Technology,Unknown / Non-Applicable
356,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
357,Data Engineer,$100K - $144K (Glassdoor est.),"Position: Data Engineer
Duration: Contract
Location: Fremont, California
Skills:
Strong in SQL and Python
Data warehousing concepts , Data Modeling and RDBMS concepts
Good to have exposure to big data technologies – Apache Spark, Kafka, Airflow, NoSQL
Good to have exposure on Insurance & Finance business processes
Regards
Tejash Gupta
Recruiter
tejash@swifttechinc.com
Job Type: Contract
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Python: 7 years (Required)
SQL: 7 years (Required)
Data modeling: 6 years (Required)
RDBMS: 6 years (Required)
Work Location: In person",4.1,"Swift Technologies Inc
4.1","Fremont, CA",1 to 50 Employees,-1,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
358,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
359,Data Engineer,Employer Provided Salary:$65K - $400K,"About Applied
Autonomy is one of the leading technological advances of this century that will come to impact our lives. The work you'll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting-edge technology while working with major players across the industry and the globe.
Applied Intuition provides software solutions to safely develop, test, and deploy autonomous vehicles at scale. The company's suite of simulation, validation, and drive log management software enables development teams to create thousands of scenarios in minutes, run simulations at scale, and verify and validate algorithms for production deployment. Headquartered in Silicon Valley with offices in Detroit, Washington, D.C., Munich, Stockholm, Seoul, and Tokyo, Applied consists of software, robotics, and automotive experts with experiences from top global companies. Leading autonomy programs and 17 of the top 20 global OEMs use Applied's solutions to bring autonomy to market faster.
About the role
We are looking for talented engineers who are excited about building products that wrangle AV data to supercharge our customers. You will drive the design and development of data processing across our products and internal tools. Handling massive volumes of data for Applied's platform needs is a critical area and we are looking for someone who can be hands-on in improving our data quality via data tools and processes.
We have multiple data engineer roles open for all levels of seniority, from mid-level to Lead.
At Applied, you will:
Design and develop systems for programmatically generating ground-truth-labeled data from a simulated world
Develop and deploy high-quality software using modern tooling and frameworks
Work with machine learning pipelines to understand and improve quality of datasets
Work with top autonomy companies to understand and solve unique challenges in perception with synthetic data
Work with products and teams across Applied Intuition
We're looking for someone who has:
Strong software engineering skills in programming languages (Python, C++, GoLang, etc.)
Experience with containerization and other modern software development workflows
Experience with synthetic data and its applications in perception systems
Nice to have:
Experience with applying synthetic data to machine learning tasks
Hands-on experience with characterization of models for Lidar, Radar, and Camera
The salary range for this position is $65,000 USD to $400,000 USD annually. This salary range is an estimate, and the actual salary may vary based on the Company's compensation practices
Don't meet every single requirement? If you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.
Applicants will be required to be fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable federal and state law. Applicants should be aware that for external-facing roles that involve close contact with Company employees or other third parties on the Company's premises, accommodations that involve remaining unvaccinated against COVID-19 may not be deemed reasonable. The Company will engage in the interactive process on an individualized basis taking into account the particular position.
Applied Intuition is an equal opportunity employer and federal contractor or subcontractor. Consequently, the parties agree that, as applicable, they will abide by the requirements of 41 CFR 60-1.4(a), 41 CFR 60-300.5(a) and 41 CFR 60-741.5(a) and that these laws are incorporated herein by reference. These regulations prohibit discrimination against qualified individuals based on their status as protected veterans
or individuals with disabilities, and prohibit discrimination against all individuals based on their race, color, religion, sex, sexual orientation, gender identity or national origin. These regulations require that covered prime contractors and subcontractors take affirmative action to employ and advance in employment individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status or disability. The parties also agree that, as applicable, they will abide by the requirements of Executive Order 13496 (29 CFR Part 471, Appendix A to Subpart A), relating to the notice of employee rights under federal labor laws.",4.0,"Applied Intuition
4.0","Mountain View, CA",51 to 200 Employees,2017,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
360,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
361,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
362,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
363,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
364,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
365,Data Engineer,Employer Provided Salary:$64K - $111K,"Responsibilities :
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications :
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Job Type: Contract
Pay: $64,162.18 - $110,860.98 per year
Work Location: In person",2.9,"AppsIntegration.Inc
2.9","Sturgis, MI",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$1 to $5 million (USD)
366,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
367,Data Engineer,Employer Provided Salary:$65K - $400K,"About Applied
Autonomy is one of the leading technological advances of this century that will come to impact our lives. The work you'll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting-edge technology while working with major players across the industry and the globe.
Applied Intuition provides software solutions to safely develop, test, and deploy autonomous vehicles at scale. The company's suite of simulation, validation, and drive log management software enables development teams to create thousands of scenarios in minutes, run simulations at scale, and verify and validate algorithms for production deployment. Headquartered in Silicon Valley with offices in Detroit, Washington, D.C., Munich, Stockholm, Seoul, and Tokyo, Applied consists of software, robotics, and automotive experts with experiences from top global companies. Leading autonomy programs and 17 of the top 20 global OEMs use Applied's solutions to bring autonomy to market faster.
About the role
We are looking for talented engineers who are excited about building products that wrangle AV data to supercharge our customers. You will drive the design and development of data processing across our products and internal tools. Handling massive volumes of data for Applied's platform needs is a critical area and we are looking for someone who can be hands-on in improving our data quality via data tools and processes.
We have multiple data engineer roles open for all levels of seniority, from mid-level to Lead.
At Applied, you will:
Design and develop systems for programmatically generating ground-truth-labeled data from a simulated world
Develop and deploy high-quality software using modern tooling and frameworks
Work with machine learning pipelines to understand and improve quality of datasets
Work with top autonomy companies to understand and solve unique challenges in perception with synthetic data
Work with products and teams across Applied Intuition
We're looking for someone who has:
Strong software engineering skills in programming languages (Python, C++, GoLang, etc.)
Experience with containerization and other modern software development workflows
Experience with synthetic data and its applications in perception systems
Nice to have:
Experience with applying synthetic data to machine learning tasks
Hands-on experience with characterization of models for Lidar, Radar, and Camera
The salary range for this position is $65,000 USD to $400,000 USD annually. This salary range is an estimate, and the actual salary may vary based on the Company's compensation practices
Don't meet every single requirement? If you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.
Applicants will be required to be fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable federal and state law. Applicants should be aware that for external-facing roles that involve close contact with Company employees or other third parties on the Company's premises, accommodations that involve remaining unvaccinated against COVID-19 may not be deemed reasonable. The Company will engage in the interactive process on an individualized basis taking into account the particular position.
Applied Intuition is an equal opportunity employer and federal contractor or subcontractor. Consequently, the parties agree that, as applicable, they will abide by the requirements of 41 CFR 60-1.4(a), 41 CFR 60-300.5(a) and 41 CFR 60-741.5(a) and that these laws are incorporated herein by reference. These regulations prohibit discrimination against qualified individuals based on their status as protected veterans
or individuals with disabilities, and prohibit discrimination against all individuals based on their race, color, religion, sex, sexual orientation, gender identity or national origin. These regulations require that covered prime contractors and subcontractors take affirmative action to employ and advance in employment individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status or disability. The parties also agree that, as applicable, they will abide by the requirements of Executive Order 13496 (29 CFR Part 471, Appendix A to Subpart A), relating to the notice of employee rights under federal labor laws.",4.0,"Applied Intuition
4.0","Mountain View, CA",51 to 200 Employees,2017,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
368,Senior Data Engineer,Employer Provided Salary:$105K - $186K,"**(UPDATED 8/21/2023) We invite applicants to express their interest in this role, but please note that we currently don't have a specific timeline for filling this position. This posting is intended for talent pooling. While there isn't a definitive hiring period, potential candidates should be aware that the role might be filled at some point in the future. We encourage you to start the process and discover more about this opportunity, Slalom, and potentially a career in consulting through discussions with our talent acquisition team.
Who You’ll Work With
As a modern technology company, we've never met a technical challenge we didn't like. We enable our clients to learn from their data, create incredible digital experiences, and make the most of new technologies. We blend design, engineering, and analytics expertise to build the future. We surround our technologists with interesting challenges, innovative minds and emerging technologies.
A Senior Data Engineer at Slalom is a technical team member who designs and develops data platform solutions. It uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g., data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc.
What You’ll Do
Work as part of a team and with clients to develop cloud data and analytics solutions
Make recommendations and prove data architecture concepts to team members and clients
Development of cloud data platforms and business intelligence solutions
Data wrangling of heterogeneous data sources for exploring and discovering new insights
Work with highly collaborative agile teams on modern data architecture projects
Key success measures of a Senior Data Engineer include technical proficiency in data processing technologies, collaboration with other teams, communication, and willingness to learn new things
What You'll Bring
Experience in one or more cloud technologies (AWS, Azure, Google Cloud, Snowflake)
Experience in data modeling, design, and development using complex SQL (SQL Server, Oracle, etc.)
Experience in Apache Airflow, Talend, SSIS, Informatica, Alteryx, or other data processing tool
Experience in one or more programming languages (Python, C#, Scala, R, PowerShell, Java, etc.)
Experience in cloud engineering, serverless architecture, and/or data streaming services
About Us
Slalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six countries and 43 markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 13,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.
Compensation and Benefits
Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance.
Slalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $105,000 to $186,000. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.
EEO and Accommodations
Slalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team if you require accommodations during the interview process.
#LI-JS8
#LI-Hybrid",4.3,"Slalom Consulting
4.3","Los Angeles, CA",10000+ Employees,2001,Company - Private,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
369,Data Engineer (Data Science),$99K - $131K (Glassdoor est.),"Job Description
The primary focus will be on the creation, enhancement and maintenance of mature tools that process and manipulate big-data, it is important to have senior and expert level Data Engineers/Software engineers to handle complex issues. The Data engineers will work independently or as part of a larger team on complex data science projects for various team centers.
Required Skills
Approximately 5 years of data engineering, data management or data science experience.
Proficiency with major data sciences tools such as SQL, Python, and Git.
Demonstrated experience with cleaning, management, optimizing performance and processing large volumes of data.
Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
Experience with machine learning, with statistical modeling, time-series forecasting, and/or geospatial analytics is preferred.
Experience working with different types of data (structured, unstructured, image, video, audio, speech, text, tabular, APIs, etc.)
Experience with ElasticSearch, Hadoop, Spark or other parallel storage/computing processes is a plus
Desired Skills
AWS/C2S
NiFi, ElasticSearch
Experience with data visualization (Kibana, Tableau, Seaborn, etc.)
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",-1,The Swift Group,"Reston, VA",201 to 500 Employees,2019,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
370,Senior Data Engineer,$87K - $114K (Glassdoor est.),"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.

This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration.
How will you make an impact?
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
What are we looking for?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework
You might be a good fit if you like to:
Leverage technology to deliver data solutions to drive business outcomes
Drive for continuous improvement of processes and solutions
Bring energy and commitment to excellence to drive delivery of high-quality solutions
Share knowledge and mentor team members on technologies and solutions
What you can learn on the job:
Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers
Grow practical understanding of agile work practices through application of the scrum framework to deliver work products
Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science
What does success look like?
First 30 days:
Understand the existing analytics data platform and technologies
Understand team’s work management processes
First 6 months:
Develop data solutions for the business including shared components and specific data sets
Learn business processes and develop necessary contacts throughout the organization
First year:
Continue to develop data solutions for the business
Become expert on the team for other data engineers to go to for mentoring in completing their work
Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,"AE Business Solutions
4.2","Milwaukee, WI",51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
371,Senior Data Engineer,-1,"JOB DESCRIPTION:
Need to be expertise Coding on PySpark.
Supply Chain and Azure.
Experience in working with end-to-end technology implementation project, preferable those related to Data warehousing/Analytics/Reporting
Strong communication skills with the ability to collaborate effectively with business owners/users and technical teams.
Proven ability to gather product requirements from stakeholders and prioritize feature implementations based on business needs and product strategy.
Strong problem-solving skills and attention to detail.
Good to have experience with CPG/ Retail industry (preferable CPG companies such as Nestle, Unilever, P&G, Coca coal, PepsiCo etc)
Bachelor's/master's degree in business administration, computer science, engineering, or a related field.
Job Type: Contract
Experience:
PySpark: 5 years (Preferred)
SQL: 10 years (Preferred)
Data analytics: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Azure: 5 years (Preferred)
Data Bricks: 5 years (Preferred)
Data Lake: 5 years (Preferred)
Synapse: 5 years (Preferred)
Retail Domain: 8 years (Preferred)
CPG: 7 years (Preferred)
Work Location: Remote",4.0,"Quadrant Resource
4.0",Remote,1001 to 5000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
372,Data Engineer IV,Employer Provided Salary:$149K,"Who We Are
Invitation Homes is a fast-paced evolving publicly traded REIT that is pioneering a new industry with over 80,000 single family rental homes and a market capitalization of over $25 billion. We are a team of more than 1,400 associates who come from all walks of life. We call different communities “home,” but our shared values bind us together. Invitation Homes is a place where possibility lives.
THR Property Management LP / Data Engineer / Dallas, TX: Develop and maintain data pipelines into data warehouse and data lake, including design, and development. Develop and implement strategies to translate business requirements into feasible and acceptable data warehouse and data lake solutions. Architect and build new data models to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making. Design, build and launch data pipelines to move data to data lake and data warehouse build and maintain framework for auditing, error logging, and master data management for data pipelines. Build data expertise and own data quality for the data pipelines. Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes. Perform data analysis and assist in the resolution of data issues. Identify and resolve defects of complex scope using proper engineering tools and techniques. Provide support and maintain existing products and add new features. Mentor engineers by providing advice, coaching and educational opportunities. Telecommuting permitted from any location within the U.S. Salary: 148512/year.
The Senior Data Engineer is accountable for the maintenance, improvement, and movement of data in the Enterprise Data Warehouse, and other data sources used for enterprise reporting. A successful Senior Data Engineer delivers database design, implementation, and schematics that creates sustainable, competitive advantage for our company. Further, a Senior Data Engineer is responsible for the development and deployment of innovative analytics data platforms to support end-user enterprise reporting. Working with other data engineers, data analysts, and data scientists, a Senior Data Engineer must have a delivery first mentality and a firm grasp of Agile Development methodology. Lastly, the ideal candidate must demonstrate the ability to work as a team member and team leader.
What You’ll Do
Accountable for the overall performance and maintenance of our SQL Server environments:
Manage the EDW relational database for optimized performance
Design scalable ETL packages from the business source systems
Ensure all necessary testing and validation for all planned and unplanned releases
Mentor junior data engineers in best practices and documentation.
Act as our database evangelist in leading innovation activities through exploration, benchmarking and implementation of data technologies
Participate in solution-based activities; articulate your ideas through well thought out documentation and research best-in-class solutions
Lead and Develop and implement data architecture standards
Manage the necessary testing and validation to ensure proper compliance to data governance and quality
Prepare progress reports regarding our database environments’’ status and health
Manage troubleshooting data issues and present solutions to these issues.
Resolve tactical issues thoroughly while always looking for optimization and documenting lessons learned
Understand when tactical issues are really systematic issues requiring deeper level of analysis and strategic solutions
Cooperate with the support team to investigate and resolve data related defects
Proactively analyze and evaluate current state of data in order to identify and recommend improvements and optimization
Support our Agile Development methodology
Constantly prioritize user stories; balance feature development, defects, and tech debt according to business need
Be a champion of continuous improvement and delivery; define the minimum viable product, focus teams on key release milestones, and leverage fast follow-up sprints
Enable sprint progress; clarify story details, validate completed stories, and prepare stories for the next sprint
Who We’re Looking For
Bachelor’s degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field. An equivalent of the same in working experience is also accepted for the position.
A candidate for the position will have as at least 5 years of working experience as a database engineering or a database engineering administrator within a fast-paced a complex business setting.
Design, implement and maintain SQL Server databases
Design, implement and maintain ETL processes using SQL Server SSIS
Provide ongoing maintenance support through SQL query tuning and optimization
Strong working and conceptual knowledge of building and maintaining physical and logical data models
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience working in SaaS, IaaS, and PaaS
Excellent customer service and interpersonal skills; ability to relate to and get along with others
Professional verbal and written communication skills
Strong organizational and time-management skills
Ability to multi-task and maintain flexibility and creativity in a variety of situations
Ability to analyze and resolve problems
Ability to set and meet goals and consistently meet deadlines
Ability to maintain confidentiality
Why Invitation Homes
We stand for flexibility, opportunity, and a home that people can make their own. It’s as true for our associates as it is for our residents. Just like we help our residents live freer, we liberate our associates’ careers, too. Our associates know that at Invitation Homes goals matter, potential is unlocked, and careers thrive. Invitation Homes isn’t just a work place it is a possibility place. Invitation Homes offers the below to each new associate:
Competitive pay and an annual bonus program for all associates
Generous paid time off plans including vacation accrual, sick time, volunteer time, and standard and floating holidays
401k with matching company contributions
Awesome work environment with casual dress
Team events and gatherings
Employee resource groups: Together with Women, Asian Alliance, Black Collective, Juntos, Gen Next, and Open Invitation.
Invitation Homes truly is where possibility lives, pour a new foundation here!
Salary Range
$98,640.00 - $170,976.00
Compensation and Benefits
To attract and retain top talent, we're pleased to offer competitive compensation and benefits, including:
Annual bonus program
Health, dental, vision, and life insurance
Long-term and short-term disability insurance
Generous paid time off plans include vacation accrual, sick time, standard holidays and floating holidays
401(k) with company matching contributions
Awesome work environment with casual dress
Team events and gatherings (Pre- and Post-Covid)
Invitation Homes is an equal opportunity employer committed to fostering a diverse, inclusive and innovative environment with the best associates. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, Veteran status or any other factor protected by applicable federal, state or local law. If you have a disability or special need that requires accommodation, please contact us at
humanresources@invitationhomes.com
.
To all recruitment agencies: Invitation Homes does not accept agency resumes. Please do not forward resumes to Invitation Homes employees. Invitation Homes is not responsible for any fees related to unsolicited resumes.",3.9,"Invitation Homes
3.9","Dallas, TX",1001 to 5000 Employees,2012,Company - Public,Real Estate,Real Estate,$1 to $5 billion (USD)
373,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
374,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
375,Data Engineer,$92K - $128K (Glassdoor est.),"Become a Part of the NIKE, Inc. Team

\r

\r

NIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it's about each person bringing skills and passion to a challenging and constantly evolving game.

Data Engineer-NIKE USA, Inc., Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.

Employer will accept a Bachelor's degree in Data Science, Computer Engineering, Computer Science, Computer Information Systems and 5 years of progressive post-baccalaureate experience in the job offered or in an engineering-related position.
Programming ability (Python, SQL);
Database related concept;
Big Data exposure;
Spark;
Airflow (Orchestration tools);
Cloud Solutions;
Software/Data design ability;
CI/CD understanding and implementation;
Code review;
Data Architecture; and
AWS, Azure

#LI-DNI

NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.

NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.

How We Hire

At NIKE, Inc. we promise to provide a premium, inclusive, compelling and authentic candidate experience. Delivering on this promise means we allow you to be at your best - and to do that, you need to understand how the hiring process works. Transparency is key.

This overview explains our hiring process for corporate roles. Note there may be different hiring steps involved for non-corporate roles.

Benefits

Whether it's transportation or financial health, we continually invest in our employees to help them achieve greatness - inside and outside of work. All who work here should be able to realize their full potential.",4.2,"Nike
4.2","Beaverton, OR",10000+ Employees,1972,Company - Public,Consumer Product Manufacturing,Manufacturing,$10+ billion (USD)
376,AWS Data Engineer II,$77K - $105K (Glassdoor est.),"Overview:
Shift4 (NYSE: FOUR) is boldly redefining commerce by simplifying complex payments ecosystems across the world. As the leader in commerce-enabling technology, Shift4 powers billions of transactions annually for hundreds of thousands of businesses in virtually every industry. For more information, visit www.shift4.com.

Shift4 Payments is seeking a Business Intelligence Engineer with an AWS Focus to join our growing internal Information Technology team. As a BI Engineer, you will help design and maintain the flow of information and analytics to the entire organization utilizing AWS capabilities to store, move and report on data. The position requires at least 3 years of prior experience as a Business Intelligence AWS engineer. We are looking for individuals that are extremely self-sufficient, available to work flexible hours and hold themselves to the highest standards of professionalism.Maintain and expand BI AWS architecture and content.
Responsibilities:
Support of BI AWS architecture and content up to and including off hours support when necessary.
Work on necessary bugs, enhancements and new system developments utilizing our Software Development Life Cycle (SDLC) standards.
Complete root cause analysis and provide recommendations to leadership to address opportunities.
Contribute to and enforce the use of the organization's Business Intelligence solution.
Develop software solutions that will provide solid foundations for the growth of the company.
Support key metrics and shared goals that determine success for a particular area.
Qualifications:
3-5+ years of professional experience in business intelligence AWS development or a related field.
Advanced experience working with AWS: S3, Glue, Lambda, Step Functions, Redshift, Athena.
Advanced experience working with various data sources such as AWS S3 and Redshift, Postgres, SQL Server, Amazon Athena, Excel, Flat Files, etc.
Advanced experience working with large data sets from sources such as AWS S3 and Redshift, Postgres, SQL Server, etc.
Strong PySpark, Python, CLI proficiency
Strong SQL proficiency (T-SQL, MySQL, and PostgreSQL).
Experience working with an Enterprise Data Warehouse and Dimensional Databases is a plus.
Advanced experience of the SDLC and how Development processes fit into KANBAN and Scrum Frameworks.
Experience with Jira and Confluence is a plus.
Ability to prioritize multiple tasks and easily adjust to changing priorities.
Ability to identify problems, initiate solutions, and effectively collaborate and communicate with team members.
Have excellent verbal and written communication skills.

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.",3.5,"Shift4 Payments
3.5","Allentown, PA",1001 to 5000 Employees,1999,Company - Public,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
377,Data Engineer,$100K - $144K (Glassdoor est.),"Position: Data Engineer
Duration: Contract
Location: Fremont, California
Skills:
Strong in SQL and Python
Data warehousing concepts , Data Modeling and RDBMS concepts
Good to have exposure to big data technologies – Apache Spark, Kafka, Airflow, NoSQL
Good to have exposure on Insurance & Finance business processes
Regards
Tejash Gupta
Recruiter
tejash@swifttechinc.com
Job Type: Contract
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Python: 7 years (Required)
SQL: 7 years (Required)
Data modeling: 6 years (Required)
RDBMS: 6 years (Required)
Work Location: In person",4.1,"Swift Technologies Inc
4.1","Fremont, CA",1 to 50 Employees,-1,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
378,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
379,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
380,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
381,Data Science Engineer,Employer Provided Salary:$122K - $223K,"Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

The Adobe 3D & Immersive team is seeking an experienced data science professional to drive our culture of data-driven decision-making.
At 3D&I we are revolutionizing the use of 3D in digital creation! We are obsessed with helping digital artists do their best 3D work. As a Data Scientist on our team, you will work with Product, Marketing and Engineering to build the data ecosystem and discover insights. Adding business context to the data insights will allow you to craft compelling narratives that drive decision making. You will have a direct impact on the 3D&I business and product decisions!
Join one of Adobe’s fastest and coolest organizations, and help us unlock 3D power across all digital creation through data and insight. What You'll Do:
Work across the 3D&I organization and our partners to uncover insights and guide business decisions
Collaborate with 3D&I leadership to define and implement the right metrics to track our business and product performance
Regularly monitor and contextualize reports to build a consistent narrative and insights What You'll Need to Succeed:
5+ years of professional experience in data science/analytics
BS or MS degree in an analytical field: statistics, applied mathematics, computer science, engineering, economics, etc. or 10+years of equivalent practical experience
Expert in storytelling and creating data visualizations to effectively communicate analysis results
Experience partnering with multiple teams across Adobe
Hands-on knowledge of descriptive and inferential statistics to understand usage behaviors and generate hypotheses
Strong proficiency in querying and manipulating large datasets using SQL-like languages on cloud (Databricks, Spark-SQL)
Familiarity with ML for propensity estimation and causal inference with observational data or experimental designs
Experience crafting dashboards in Tableau or PowerBI
Experience with modern data science workflows on the cloud (Databricks, Git, Jupyter, Python, AWS, Azure)
Excellent communication, relationship skills, and a strong standout colleague
Experience with Adobe Analytics or Adobe Customer Journey Analytics a plus
Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $122,400 -- $223,000 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.

Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.

Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.

Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.",4.4,"Adobe
4.4","New York, NY",10000+ Employees,1982,Company - Public,Computer Hardware Development,Information Technology,$5 to $10 billion (USD)
382,Data Engineer,$81K - $114K (Glassdoor est.),"Position Overview:
The Novelis Corporate Digital team is seeking a Data Engineer. The person in this role will support the Data Services Manager. The ideal candidate will be responsible for delivery of the data pipelines feeding into machine learning models. The Data Engineer will aid in the optimization of operations by manipulating and aggregating the disparate operational and historian (L2) data sources into a format that is easily digestible by both data scientists and statistically adept colleagues. Their core responsibility will be to combine large volumes of disparate data, conduct quality checks on the data, manipulate the data and ensure continuous access to a clean format of the operational data for supporting functions and collaborators.

Novelis is the world leader in aluminum rolling and recycling, producing an estimated 19 percent of the world's flat-rolled aluminum products. We work alongside our customers to provide innovative solutions to the aerospace, beverage can, automotive and high-end specialty markets.

Headquartered in Atlanta, Georgia, USA, Novelis has approximately 13,000 employees in 33 operating facilities on four continents and is investing close to $2 billion in global expansions to meet the growing demand for our premium product.
Responsibilities & Qualifications:
Responsibilities:
Design and Develop data ingestion pipelines and processes based on requirements in Python and PySpark
Build error handing, exception management and data quality routines to expose the anomalies in the data
Profile and analyze data to identify gaps and potential data quality issues
Identifies relationships between disparate data sources
Uses Python, Databricks and Spark to code the data Engineering routines
Perform unit and integration testing
Works with the Data Science team and business SMEs to get the requirements and present the details in data.
Designs and jointly develops the data architecture with data architect and ensures security and maintenance
Explores suitable options, designs, and builds data pipeline (data lake / data warehouses) for specific analytical solutions
Identifies gaps and implements solutions for data security, quality and automation of processes
Builds data tools and products for effort automation and easy data accessibility
Supports maintenance, bug fixing and performance analysis along data pipeline
Diagnoses existing architecture and data maturity and identifies gaps
Minimum Qualifications:
Minimum of Bachelor of Science BS/MS degree in Computer Science, Engineering, and/or Background in Mathematics and Statistics
Over 3 years of work with data engineering, IT or related field
Experience in data engineering & data bases/warehouses
Used SQL, PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle etc in production environments
Experience with programming languages like UNIX shell scripting, Python etc.
Preferred Qualifications:
Certifications in Databricks, Pythod, Spark or other related subjects
Experience with Agile/Scrum methodology with the Product based implementation approach
Experience with CI/CD with DevOps frameworks
Experience on Big Data platforms (e.g. Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive)
Experience with cloud platforms (Azure, AWS, Databricks)
Experience with Historians (IBA, PI), Industry 4.0 or IoT environments
What We Offer:
This role offers a hybrid schedule. Novelis is committed to a Flex Work approach that empowers employees to maintain work-life balance while enjoying the benefits of a collaborative office environment.
Novelis benefits say a lot about how we care for each other. Our employees and their families have many different needs. As a result, our benefits offer choices on many levels and are high in quality, competitive in the marketplace, and affordable. These are a few of the benefits we offer to support you and your family’s well-being:
Medical, dental and vision insurance
Health savings accounts – Company Funded Health Savings Account (HSA) and Health Reimbursement Account (HRA)
Company-paid basic life insurance and Additional voluntary life coverage
Paid vacation and competitive personal time off
401(k) savings plan with company match
Retirement savings plans – medical and prescription drug coverage through private exchange
Employee assistance programs – available 24/7 to you and your family
Wellness and Work Life Support - career development and educational assistance
Location Profile:
Novelis’ Global Corporate Headquarters is located in the Buckhead neighborhood of Atlanta Georgia and employs approximately 250 people. It is co-located with Novelis’ North America regional office which employs approximately 225 people. Supporting its 24 operations worldwide Novelis’ corporate office is home to the executive leadership team and global functions that support the automotive beverage can and high-end specialties value streams. The City of Atlanta provides a diverse and family-friendly place to live with countless museums cultural organizations and educational institutions including the Georgia Aquarium Woodruff Arts Center CNN Center Georgia Tech and Mercedes-Benz Stadium. In the Atlanta area Novelis has strong community partnerships with Atlanta Habitat for Humanity GeorgiaFIRST and Agape Youth and Family Center in addition to many local museums and community groups. Novelis recognizes its talented and diverse workforce as a key competitive advantage. Novelis provides equal employment opportunities to all employees and applicants.All terms and conditions of employment at Novelis including recruiting hiring placement promotion termination layoffs recalls transfers leaves of absence compensation and training are without regard to race color religion age sex national origin disability status genetics protected veteran status sexual orientation gender identity or expression or any other characteristic protected by federal provincial or local laws.",3.8,"Novelis Corporate HQ
3.8","Atlanta, GA",10000+ Employees,2005,Subsidiary or Business Segment,Metal & Mineral Manufacturing,Manufacturing,$10+ billion (USD)
383,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
384,Data Engineer,$77K - $104K (Glassdoor est.),"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Paid time off
Referral program
Vision insurance
Schedule:
Monday to Friday
No nights
No weekends
Ability to commute/relocate:
Atlanta, GA 30309: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 2 years (Required)
Language:
English (Required)
Work Location: Hybrid remote in Atlanta, GA 30309",3.7,"United Digestive
3.7","Atlanta, GA",501 to 1000 Employees,2018,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
385,Data Engineer,$98K - $147K (Glassdoor est.),"Greetings Everyone

Who are we?

For the past 20 years, we have powered many Digital Experiences for the Fortune 500. Since 1999, we have grown from a few people to more than 4000 team members across the globe that are engaged in various Digital Modernization. For a brief 1 minute video about us, you can check

https://youtu.be/uJWBWQZEA6o

.

What will you do?

List out the Instructions

What are we looking for?

Seeking for a strong data analyst who can perform independent.

Good to have Prior experience in banking domain is plus and understanding the finance various data transactions.

Should be strong in the below:
Python

Pyspark

SQL

Airflow

Trino

Hive

Offshore Cordination

LLD

Agile Scrum

Linux

Openshift

Kubernentes

Superset

Optional skill set:
Linux

Openshift

Kubernentes

Superset",3.8,"Photon
3.8","Las Vegas, NV",5001 to 10000 Employees,2007,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
386,"Data Engineer, Election Platforms (all-levels)",-1,"Job Description
The Washington Post is hiring a data engineer for our Election Platforms team, to work on our data pipelines, tools, and infrastructure ahead of the 2024 presidential election.
The Election Platforms team focuses on leveraging data, partnering with stakeholders, and pioneering serviceable tools to facilitate The Post’s election coverage. This role is part of Elections Engineering, a cross-functional group of full-stack engineers and data scientists, with varied technical and academic backgrounds, responsible for delivering live election results across The Post’s platforms.
The role will be focused on contributing to our technical stack and developing new technology to facilitate robust and lively election coverage. In particular, we’re looking for candidates who would be comfortable working on deploying high-impact code in production and have an interest in data and newsroom tooling. We also work on other data engineering projects related to elections, such as voter registration data and campaign finance, and occasionally write
open
source
software
. Most of our work is in Python and TypeScript/React/Node.
We typically work at a higher level of abstraction than a single story, and aim to create general purpose tools that are reusable and easy to deploy. Our ideal candidates are creative problem-solvers, who are comfortable writing production level code to solve real-world newsroom problems. This position is remote-friendly for work in the US.
Motivation
You have a desire to contribute to The Post’s journalism through technology.
You are passionate about refining and improving team process, culture, documentation and communication while tackling complex data science and engineering challenges.
You want to work at a deeper level of abstraction than a single story or beat.
You are excited about learning new technologies and applying them to the practice of journalism.
Responsibilities
Work on the software components of our live election architecture including deployment, observability, participating in code reviews, and more.
Work closely with our engineers and data scientists to develop and maintain pipelines and administrative apps/dashboards for election data.
Work on tooling to make data preparation easier and more efficient for our live election model.
Work with newsroom stakeholders to assess needs, build useful and reusable tooling, and contribute to storytelling.
Foster effective team collaboration and communication by improving and documenting internal tools, processes and guidelines.
Assist in attracting and hiring team members who want to work at the intersection of journalism and technology by actively participating in recruiting – and contributing feedback on our overall hiring process.
Qualifications
A minimum of one year of experience building and maintaining backend and data engineering applications.
Proficiency across some of the languages and frameworks used in our engineering stack such as Python, TypeScript/React/Node, and AWS services.
A desire to work on cutting edge, cross-functional tools to help with features, data science, and machine learning/big data work in a team setting.
Experience with election data is not necessary, but is a plus.
Experience developing and deploying newsroom tools and applications is not necessary, but is a plus.
Levels
If you have less than three years, but at least one year, we’d love to evaluate you for a data engineer role.
If you have more than three years but less than about seven years, we’d love to evaluate you for a senior data engineer role.
If you have more than about seven years, we’d love to talk to you about a principal data engineer role.
Wherever you are in your life or career, The Washington Post offers comprehensive and inclusive benefits for every step of your journey:
Competitive medical, dental and vision coverage
Company-paid pension and 401(k) match
Three weeks of vacation and up to three weeks of paid sick leave
Nine paid holidays and two personal days
20 weeks paid parental leave for any new parent
Robust mental health resources
Backup care and caregiver concierge services
Gender affirming services
Pet insurance
Free Post digital subscription
Leadership and career development programs
Benefits may vary based on the job, full-time or part-time schedule, location, and collectively bargained status.
The Post strives to provide its readers with high-quality, trustworthy news and information while constantly innovating. That mission is best served by a diverse, multi-generational workforce with varied life experiences and perspectives. All cultures and backgrounds are welcomed.
The innovation doesn’t end in the Newsroom – dozens of teams power The Washington Post. We are now hiring the next innovator – how will you Impact Tomorrow?
#washpostlife",4.2,"The Washington Post
4.2","Washington, DC",1001 to 5000 Employees,1877,Company - Private,Publishing,Media & Communication,Unknown / Non-Applicable
387,Senior Data Analytics Engineer,-1,"At Wistia our goal is to help every business to thrive with video. That's a big task, and we're super excited to help the world get there faster through simple and delightful video-focused products.
Wistia's Business Intelligence team is charged with leveraging data to empower strategic, insights-driven decision making across the organization. The Analytics Engineer role is a hybrid role with responsibilities that typically fall to Data Analysts, Data Engineers and Data Scientists. We are a small team and are looking for someone who is comfortable wearing different ""data"" hats. A successful candidate will combine strong technical and communication skills to manage the full data lifecycle at Wistia by creating and maintaining ETL pipelines, automating critical workflows, analyzing data to surface insights and translating those insights into clear recommendations for stakeholders.
The other members of the Business Intelligence team are curious, attentive to detail, impact-focused and approach problem solving with an engineering mindset. The team is centralized (not embedded) with each team member working with data from across the business. We strive to adopt the tools and best practices of the ""modern data stack"". For us that looks like: Redshift, dbt, Fivetran, Mode Analytics, Census, Snowplow Analytics. The Redshift warehouse we maintain serves as the single source of truth for the business.
Key Responsibilities
Maintain our 3rd-Party and custom ETL/reverse-ETL jobs and create new ones as required
Develop new predictive models as required to meet business outcomes
Generate easily interpretable analyses of complex datasets using SQL or python
Create persuasive narratives with data visualizations that enable self-serve exploration
Assist in the design and analysis of A/B testing to optimize business outcomes
Define version-controlled, modular business logic and datasets to be reused across the team
Document your findings, insights and recommendations for stakeholders
Meet business stakeholders ""where they live"" by becoming an expert in the data models and usage of their preferred SaaS tools, eg, Salesforce, HubSpot, Google Analytics, Amplitude, Stripe
Collaborate with:
Team members to share learnings, brainstorm new approaches and define best-practices
Stakeholders to scope, shape and prioritize their data requests
Qualifications
This section should talk about the competencies needed for the role, including qualities. Be sure to seek the input of key stakeholders and make sure we distinguish between must-haves and nice-to-haves because we will be using this to build our job positing and interview plan.
Technical
Highly proficient in SQL and python (or comparable general purpose programming language)
Facile with statistics and hypothesis testing
Skilled at constructing meaningful data visualizations
Comfortable with the command line and bash scripting
Knowledgeable about database architecture, data warehousing principles, REST APIs and basic ML algorithms
Non-technical
Experienced at scoping broad, loosely-defined requests into well-structured plans
Adept at translating data into actionable insights for stakeholders
Strong in written communication
Able to work independently with minimal oversight
Familiar with SaaS metrics and business practices
Values fit (for Wistia at large and BI specifically)
collaborative
learning oriented
attention to detail
simplicity as the starting point starts simple and add complexity only as needed
Key Metrics
This is a great place for more detail on the metrics we expect this person to own, and any that we measure success by.
Product, marketing, sales, and customer success teams have the data and insights they need to make decisions
Product, marketing, sales, and customer success teams have confidence in the data they use
Product, marketing, sales, and customer success teams are able to monitor business performance and spot problem areas and new opportunities
Responses to data requests are timely
Success Looks Like
This section should outline what success looks like. Knowing this helps inform the level and type of candidates we need, interview questions, onboarding plan, and coaching our new hires to success. It also makes it easier to spot performance issues early and hold ourselves accountable in hiring (and learn how we can improve).
30 Days
Complete Wistia onboarding program
Ramp up and learn the tools of the role (Mode, dbt, github, Fivetran, Redshift, Google Analytics, Salesforce, Pendo)
Ramp up and learn Wistia's data architecture, starting with Wistia app database
Ramp up and learn data analysis best practices and procedures
Complete ~5 basic data requests
90 Days
Confident in use of Mode, dbt, and GitHub for data analysis process
Confident in data architecture and able to self-source most data tables
Regularly handling basic data requests
Completed 1 complex data investigation
1 Year
Regularly initiating new data investigations and unearthing new insights
Regularly making improvements to our approach to data presentation
Regularly making improvements to our data models and data cleanliness
Working at Wistia
We try to ensure Wistia is an inclusive and diverse place where everyone feels happy, fulfilled, respected, comfortable, and welcome. We're proud to be an equal-opportunity workplace. We care a lot, so our benefits are actually benefits, not just the fun stuff like swag and snacks in the office (though we also have lots of those too!).
We know the biggest investment we can make is in our employees, so we provide:
A competitive compensation package that includes internal equity stock options
401k with 3% company contribution, regardless of whether you contribute
Fully paid healthcare, dental, and vision insurance (family plans included)
Pre-Tax FSA and Dependent Care Account
Fitness reimbursement
Flexible working hours – work at the times when you operate best, or set aside time for child and/or elderly care responsibilities during traditional ""office hours""
16 weeks paid parental leave for all new and expecting parents
Unlimited PTO (pretty common for people to take 4+ weeks off throughout the year)
Remote-first culture (work from anywhere in the U.S.)
Annual professional development stipend (courses, conferences, and more)
New hire bonus to enhance your home office set up
Pet insurance discount
Location/Remote Opportunities
Wistia is a remote-first company. Employees can work from our beautiful office in Cambridge, MA, or anywhere in the continental US",4.8,"Wistia
4.8","Cambridge, MA",51 to 200 Employees,2006,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
388,"Data Engineer / USA: Evanston, IL",$78K - $113K (Glassdoor est.),"Job Title: Data Engineer
Area: IT
Reports to: Product Manager, Data Platforms, Integrations & BI
Classification: Exempt
Location: Evanston, Illinois
This position is designated as Hybrid – Fixed Days and will regularly work in the office an average of 3+ days a week – with a set schedule. Managers may require team members to work on the same schedule to encourage collaboration. Rotary is under the jurisdiction of Illinois employment laws; we require all employees to live within reasonable daily commuting distance to Evanston.
Organization Overview
Rotary is a membership organization that unites people from all continents and cultures who take action to deliver real, long-term solutions to pressing issues facing our communities and the world. Each year, Rotary members expand their networks, build lasting relationships, and invest more than $300 million and 16 million volunteer hours to improve lives and create positive change in the world. In exchange for hard work and dedication in support of Rotary’s clubs, members and other participants, and their humanitarian service projects, our employees receive competitive salaries, flexible schedules, comprehensive benefits, and job enrichment. To learn more, visit http://www.rotary.org.
Rotary’s Commitment to Diversity
As a global network that strives to build a world where people unite and take action to create lasting change, Rotary values diversity and celebrates the contributions of people of all backgrounds, regardless of their age, ethnicity, race, color, abilities, religion, socioeconomic status, culture, sex, sexual orientation, and gender identity.
Overview
The Data Engineer is responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
You Will Have
Bachelor’s degree in Mathematics, Computer Science, Economics, or Statistics
3+ years experience mining data as a data analyst
Strong working knowledge of SQL and experience working with relational and analytical databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with cloud data lakehouses such as Snowflake
Experience building and optimizing data pipelines, architectures and data sets using popular tools such as Fivetran and Azure Data Factory
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Familiarity with Business Intelligence and data visualization software such as Microsoft PowerBI
Advanced experience with Microsoft Excel
Proven analytic skills, including mining, evaluation, analysis, and visualization
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Prior experience in model design and segmentation techniques
Technical writing experience in relevant areas, including queries, reports, and presentations
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. is a plus
You Are Good At
Excellent verbal and written communication skills.
Tackles problems with creativity and a positive attitude.
Possesses strong analytical and decision-making skills.
Ability to identify, analyze, and resolve market needs.
Act as a leader within the team, driving high quality customer experiences and exploring what is possible with evolving data management tools and processes.
You Will Be Responsible For
Strategizing, designing and developing patterns for data pipeline, ELT, and data consumption by analytic tools and models.
Classify, visualize, explain and document data insights for internal and external product consumption.
Mentor the development team, and provide advice and feedback on advanced analytics, data engineering and data visualization
Proactively identifies new improvements to platforms, leveraging your expertise and plan them in the roadmap
Ensuring services delivered in support of projects are being delivered on time and with quality and are meeting the needs of the product and project teams
Leadership Attributes
Communication: Be open to receiving ideas from diverse viewpoints and able to communicate messages so that they are universally understood.
Collaboration: Builds partnerships and works jointly with others to meet shared objectives.
Accountability: Have a clear sense of ownership and take personal responsibility for actions.
Innovation: Move Rotary beyond traditional ways of thinking
Strategy: Identify Rotary’s opportunities and design approaches that align with our strategic goals.
Why Work for Rotary?
Generous medical, dental, and vision benefits package
Progressive 401k matching contributions
Above market and generous paid time off package
Tuition reimbursement
Professional development opportunities
On-site cafeteria and coffee bar with special pricing for Rotary employees
Flextime-several different work schedules to choose from
Close to CTA, Metra, a variety of stores, and many food options
Please note:
Rotary does not provide VISA sponsorship or relocation assistance",4.0,"Rotary International
4.0","Evanston, IL",501 to 1000 Employees,1905,Nonprofit Organization,Membership Organizations,Management & Consulting,Unknown / Non-Applicable
389,"Senior Data Engineer, AdTech",$121K - $165K (Glassdoor est.),"Engineering
Data
Our mission on the Advertising Product & Technology team is to build a next generation advertising platform that aligns with our unique value proposition for audio and video. We work to scale the user experience for hundreds of millions of fans and hundreds of thousands of advertisers. This scale brings unique challenges as well as tremendous opportunities for our artists and creators.
Location
New York or Remote Americas
Job type
Permanent
We are looking for Data Engineers to build and drive data engineering initiatives within our advertising and podcast monetization teams. In this role, you will be instrumental in streamlining data ingested from multiple sources to not only recognize value and insights, but to also set standards along the way. You will help us to create a user-first ad experience that's personalized and relevant, and develop and own software solutions on our fast-paced podcast and ad services technology. You will help us grow to billions of fans, increase engagement with our listeners, and provide better value to our advertisers. Above all, your work will impact the way the world experiences music and podcasts.
What You'll Do
Work closely with key partners across the ads organization, contributing to the improvement of many different pipelines and services
Build new distributed data pipelines across ad experience
Use best practices in continuous integration and delivery
Help drive optimization, testing and tooling to improve reliability and data quality
Apply a Data centric approach to all the platforms, pipelines and engineering activities
Jump into data pipelines, identify issues, and propose solutions across teams
Design, develop, and maintain Java services
Work in cross-functional agile teams to continuously experiment, iterate and deliver on new product objectives
Who You Are
You have a strong understanding of data systems
You are knowledgeable and passionate about improving and building new distributed data pipelines
You are knowledgeable about data modeling, data access, and data storage techniques
You are familiar with current engineering practices such as distributed architecture, and are curious about new technologies that help derive insights and value from data
You have experience working on and building distributed data pipelines that ingest huge amounts of data across multiple sources and brands
You are very comfortable working with datasets in SQL (we also use Scio!)
You are comfortable in at least one core language like Python, Java or Scala
You have experience working with Apache Beam, Hadoop or a similar streaming data technology
You have experience with GCP (preferred) or AWS
You enjoy close collaboration with backend and ML engineers, and are passionate about software architecture across the stack
Ad Tech and/or Podcast Monetization experience is a plus
Where You'll Be
We are a distributed workforce enabling our band members to find a work mode that is best for them!
Where in the world? For this role, it can be within the Americas region in which we have a work location and is within working hours: https://lifeatspotify.com/locations
Working hours? We operate within the Eastern Standard time zone for collaboration and ask that all be located in that time zone.
Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here.
Our global benefits
Extensive learning opportunities, through our dedicated team, GreenHouse.
Flexible share incentives letting you choose how you share in our success.
Global parental leave, six months off - fully paid - for all new parents.
All The Feels, our employee assistance program and self-care hub.
Flexible public holidays, swap days off according to your values and beliefs.
Learn about life at Spotify
The United States base range for this position is $156,275 - $223,25, plus equity. The benefits available for this position include health insurance, six month paid parental leave, 401(k) retirement plan, monthly meal allowance, 23 paid days off, 13 paid flexible holidays. This range encompasses multiple levels. Leveling is determined during the interview process. Placement in a level depends on relevant work history and interview performance. These ranges may be modified in the future.

Spotify is an equal opportunity employer. You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens.

Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service.",4.0,"Spotify
4.0","New York, NY",5001 to 10000 Employees,2006,Company - Public,Internet & Web Services,Information Technology,Unknown / Non-Applicable
390,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
391,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
392,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
393,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
394,Data Engineer IV,Employer Provided Salary:$149K,"Who We Are
Invitation Homes is a fast-paced evolving publicly traded REIT that is pioneering a new industry with over 80,000 single family rental homes and a market capitalization of over $25 billion. We are a team of more than 1,400 associates who come from all walks of life. We call different communities “home,” but our shared values bind us together. Invitation Homes is a place where possibility lives.
THR Property Management LP / Data Engineer / Dallas, TX: Develop and maintain data pipelines into data warehouse and data lake, including design, and development. Develop and implement strategies to translate business requirements into feasible and acceptable data warehouse and data lake solutions. Architect and build new data models to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making. Design, build and launch data pipelines to move data to data lake and data warehouse build and maintain framework for auditing, error logging, and master data management for data pipelines. Build data expertise and own data quality for the data pipelines. Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes. Perform data analysis and assist in the resolution of data issues. Identify and resolve defects of complex scope using proper engineering tools and techniques. Provide support and maintain existing products and add new features. Mentor engineers by providing advice, coaching and educational opportunities. Telecommuting permitted from any location within the U.S. Salary: 148512/year.
The Senior Data Engineer is accountable for the maintenance, improvement, and movement of data in the Enterprise Data Warehouse, and other data sources used for enterprise reporting. A successful Senior Data Engineer delivers database design, implementation, and schematics that creates sustainable, competitive advantage for our company. Further, a Senior Data Engineer is responsible for the development and deployment of innovative analytics data platforms to support end-user enterprise reporting. Working with other data engineers, data analysts, and data scientists, a Senior Data Engineer must have a delivery first mentality and a firm grasp of Agile Development methodology. Lastly, the ideal candidate must demonstrate the ability to work as a team member and team leader.
What You’ll Do
Accountable for the overall performance and maintenance of our SQL Server environments:
Manage the EDW relational database for optimized performance
Design scalable ETL packages from the business source systems
Ensure all necessary testing and validation for all planned and unplanned releases
Mentor junior data engineers in best practices and documentation.
Act as our database evangelist in leading innovation activities through exploration, benchmarking and implementation of data technologies
Participate in solution-based activities; articulate your ideas through well thought out documentation and research best-in-class solutions
Lead and Develop and implement data architecture standards
Manage the necessary testing and validation to ensure proper compliance to data governance and quality
Prepare progress reports regarding our database environments’’ status and health
Manage troubleshooting data issues and present solutions to these issues.
Resolve tactical issues thoroughly while always looking for optimization and documenting lessons learned
Understand when tactical issues are really systematic issues requiring deeper level of analysis and strategic solutions
Cooperate with the support team to investigate and resolve data related defects
Proactively analyze and evaluate current state of data in order to identify and recommend improvements and optimization
Support our Agile Development methodology
Constantly prioritize user stories; balance feature development, defects, and tech debt according to business need
Be a champion of continuous improvement and delivery; define the minimum viable product, focus teams on key release milestones, and leverage fast follow-up sprints
Enable sprint progress; clarify story details, validate completed stories, and prepare stories for the next sprint
Who We’re Looking For
Bachelor’s degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field. An equivalent of the same in working experience is also accepted for the position.
A candidate for the position will have as at least 5 years of working experience as a database engineering or a database engineering administrator within a fast-paced a complex business setting.
Design, implement and maintain SQL Server databases
Design, implement and maintain ETL processes using SQL Server SSIS
Provide ongoing maintenance support through SQL query tuning and optimization
Strong working and conceptual knowledge of building and maintaining physical and logical data models
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience working in SaaS, IaaS, and PaaS
Excellent customer service and interpersonal skills; ability to relate to and get along with others
Professional verbal and written communication skills
Strong organizational and time-management skills
Ability to multi-task and maintain flexibility and creativity in a variety of situations
Ability to analyze and resolve problems
Ability to set and meet goals and consistently meet deadlines
Ability to maintain confidentiality
Why Invitation Homes
We stand for flexibility, opportunity, and a home that people can make their own. It’s as true for our associates as it is for our residents. Just like we help our residents live freer, we liberate our associates’ careers, too. Our associates know that at Invitation Homes goals matter, potential is unlocked, and careers thrive. Invitation Homes isn’t just a work place it is a possibility place. Invitation Homes offers the below to each new associate:
Competitive pay and an annual bonus program for all associates
Generous paid time off plans including vacation accrual, sick time, volunteer time, and standard and floating holidays
401k with matching company contributions
Awesome work environment with casual dress
Team events and gatherings
Employee resource groups: Together with Women, Asian Alliance, Black Collective, Juntos, Gen Next, and Open Invitation.
Invitation Homes truly is where possibility lives, pour a new foundation here!
Salary Range
$98,640.00 - $170,976.00
Compensation and Benefits
To attract and retain top talent, we're pleased to offer competitive compensation and benefits, including:
Annual bonus program
Health, dental, vision, and life insurance
Long-term and short-term disability insurance
Generous paid time off plans include vacation accrual, sick time, standard holidays and floating holidays
401(k) with company matching contributions
Awesome work environment with casual dress
Team events and gatherings (Pre- and Post-Covid)
Invitation Homes is an equal opportunity employer committed to fostering a diverse, inclusive and innovative environment with the best associates. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, Veteran status or any other factor protected by applicable federal, state or local law. If you have a disability or special need that requires accommodation, please contact us at
humanresources@invitationhomes.com
.
To all recruitment agencies: Invitation Homes does not accept agency resumes. Please do not forward resumes to Invitation Homes employees. Invitation Homes is not responsible for any fees related to unsolicited resumes.",3.9,"Invitation Homes
3.9","Dallas, TX",1001 to 5000 Employees,2012,Company - Public,Real Estate,Real Estate,$1 to $5 billion (USD)
395,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
396,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
397,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
398,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
399,Senior Data Engineer,Employer Provided Salary:$105K - $186K,"**(UPDATED 8/21/2023) We invite applicants to express their interest in this role, but please note that we currently don't have a specific timeline for filling this position. This posting is intended for talent pooling. While there isn't a definitive hiring period, potential candidates should be aware that the role might be filled at some point in the future. We encourage you to start the process and discover more about this opportunity, Slalom, and potentially a career in consulting through discussions with our talent acquisition team.
Who You’ll Work With
As a modern technology company, we've never met a technical challenge we didn't like. We enable our clients to learn from their data, create incredible digital experiences, and make the most of new technologies. We blend design, engineering, and analytics expertise to build the future. We surround our technologists with interesting challenges, innovative minds and emerging technologies.
A Senior Data Engineer at Slalom is a technical team member who designs and develops data platform solutions. It uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g., data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc.
What You’ll Do
Work as part of a team and with clients to develop cloud data and analytics solutions
Make recommendations and prove data architecture concepts to team members and clients
Development of cloud data platforms and business intelligence solutions
Data wrangling of heterogeneous data sources for exploring and discovering new insights
Work with highly collaborative agile teams on modern data architecture projects
Key success measures of a Senior Data Engineer include technical proficiency in data processing technologies, collaboration with other teams, communication, and willingness to learn new things
What You'll Bring
Experience in one or more cloud technologies (AWS, Azure, Google Cloud, Snowflake)
Experience in data modeling, design, and development using complex SQL (SQL Server, Oracle, etc.)
Experience in Apache Airflow, Talend, SSIS, Informatica, Alteryx, or other data processing tool
Experience in one or more programming languages (Python, C#, Scala, R, PowerShell, Java, etc.)
Experience in cloud engineering, serverless architecture, and/or data streaming services
About Us
Slalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six countries and 43 markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 13,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.
Compensation and Benefits
Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance.
Slalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $105,000 to $186,000. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.
EEO and Accommodations
Slalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team if you require accommodations during the interview process.
#LI-JS8
#LI-Hybrid",4.3,"Slalom Consulting
4.3","Los Angeles, CA",10000+ Employees,2001,Company - Private,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
400,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
401,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
402,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
403,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
404,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
405,Senior Data Analytics Engineer,-1,"At Wistia our goal is to help every business to thrive with video. That's a big task, and we're super excited to help the world get there faster through simple and delightful video-focused products.
Wistia's Business Intelligence team is charged with leveraging data to empower strategic, insights-driven decision making across the organization. The Analytics Engineer role is a hybrid role with responsibilities that typically fall to Data Analysts, Data Engineers and Data Scientists. We are a small team and are looking for someone who is comfortable wearing different ""data"" hats. A successful candidate will combine strong technical and communication skills to manage the full data lifecycle at Wistia by creating and maintaining ETL pipelines, automating critical workflows, analyzing data to surface insights and translating those insights into clear recommendations for stakeholders.
The other members of the Business Intelligence team are curious, attentive to detail, impact-focused and approach problem solving with an engineering mindset. The team is centralized (not embedded) with each team member working with data from across the business. We strive to adopt the tools and best practices of the ""modern data stack"". For us that looks like: Redshift, dbt, Fivetran, Mode Analytics, Census, Snowplow Analytics. The Redshift warehouse we maintain serves as the single source of truth for the business.
Key Responsibilities
Maintain our 3rd-Party and custom ETL/reverse-ETL jobs and create new ones as required
Develop new predictive models as required to meet business outcomes
Generate easily interpretable analyses of complex datasets using SQL or python
Create persuasive narratives with data visualizations that enable self-serve exploration
Assist in the design and analysis of A/B testing to optimize business outcomes
Define version-controlled, modular business logic and datasets to be reused across the team
Document your findings, insights and recommendations for stakeholders
Meet business stakeholders ""where they live"" by becoming an expert in the data models and usage of their preferred SaaS tools, eg, Salesforce, HubSpot, Google Analytics, Amplitude, Stripe
Collaborate with:
Team members to share learnings, brainstorm new approaches and define best-practices
Stakeholders to scope, shape and prioritize their data requests
Qualifications
This section should talk about the competencies needed for the role, including qualities. Be sure to seek the input of key stakeholders and make sure we distinguish between must-haves and nice-to-haves because we will be using this to build our job positing and interview plan.
Technical
Highly proficient in SQL and python (or comparable general purpose programming language)
Facile with statistics and hypothesis testing
Skilled at constructing meaningful data visualizations
Comfortable with the command line and bash scripting
Knowledgeable about database architecture, data warehousing principles, REST APIs and basic ML algorithms
Non-technical
Experienced at scoping broad, loosely-defined requests into well-structured plans
Adept at translating data into actionable insights for stakeholders
Strong in written communication
Able to work independently with minimal oversight
Familiar with SaaS metrics and business practices
Values fit (for Wistia at large and BI specifically)
collaborative
learning oriented
attention to detail
simplicity as the starting point starts simple and add complexity only as needed
Key Metrics
This is a great place for more detail on the metrics we expect this person to own, and any that we measure success by.
Product, marketing, sales, and customer success teams have the data and insights they need to make decisions
Product, marketing, sales, and customer success teams have confidence in the data they use
Product, marketing, sales, and customer success teams are able to monitor business performance and spot problem areas and new opportunities
Responses to data requests are timely
Success Looks Like
This section should outline what success looks like. Knowing this helps inform the level and type of candidates we need, interview questions, onboarding plan, and coaching our new hires to success. It also makes it easier to spot performance issues early and hold ourselves accountable in hiring (and learn how we can improve).
30 Days
Complete Wistia onboarding program
Ramp up and learn the tools of the role (Mode, dbt, github, Fivetran, Redshift, Google Analytics, Salesforce, Pendo)
Ramp up and learn Wistia's data architecture, starting with Wistia app database
Ramp up and learn data analysis best practices and procedures
Complete ~5 basic data requests
90 Days
Confident in use of Mode, dbt, and GitHub for data analysis process
Confident in data architecture and able to self-source most data tables
Regularly handling basic data requests
Completed 1 complex data investigation
1 Year
Regularly initiating new data investigations and unearthing new insights
Regularly making improvements to our approach to data presentation
Regularly making improvements to our data models and data cleanliness
Working at Wistia
We try to ensure Wistia is an inclusive and diverse place where everyone feels happy, fulfilled, respected, comfortable, and welcome. We're proud to be an equal-opportunity workplace. We care a lot, so our benefits are actually benefits, not just the fun stuff like swag and snacks in the office (though we also have lots of those too!).
We know the biggest investment we can make is in our employees, so we provide:
A competitive compensation package that includes internal equity stock options
401k with 3% company contribution, regardless of whether you contribute
Fully paid healthcare, dental, and vision insurance (family plans included)
Pre-Tax FSA and Dependent Care Account
Fitness reimbursement
Flexible working hours – work at the times when you operate best, or set aside time for child and/or elderly care responsibilities during traditional ""office hours""
16 weeks paid parental leave for all new and expecting parents
Unlimited PTO (pretty common for people to take 4+ weeks off throughout the year)
Remote-first culture (work from anywhere in the U.S.)
Annual professional development stipend (courses, conferences, and more)
New hire bonus to enhance your home office set up
Pet insurance discount
Location/Remote Opportunities
Wistia is a remote-first company. Employees can work from our beautiful office in Cambridge, MA, or anywhere in the continental US",4.8,"Wistia
4.8","Cambridge, MA",51 to 200 Employees,2006,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
406,Data Engineer,$92K - $128K (Glassdoor est.),"Become a Part of the NIKE, Inc. Team

\r

\r

NIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it's about each person bringing skills and passion to a challenging and constantly evolving game.

Data Engineer-NIKE USA, Inc., Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.

Employer will accept a Bachelor's degree in Data Science, Computer Engineering, Computer Science, Computer Information Systems and 5 years of progressive post-baccalaureate experience in the job offered or in an engineering-related position.
Programming ability (Python, SQL);
Database related concept;
Big Data exposure;
Spark;
Airflow (Orchestration tools);
Cloud Solutions;
Software/Data design ability;
CI/CD understanding and implementation;
Code review;
Data Architecture; and
AWS, Azure

#LI-DNI

NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.

NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.

How We Hire

At NIKE, Inc. we promise to provide a premium, inclusive, compelling and authentic candidate experience. Delivering on this promise means we allow you to be at your best - and to do that, you need to understand how the hiring process works. Transparency is key.

This overview explains our hiring process for corporate roles. Note there may be different hiring steps involved for non-corporate roles.

Benefits

Whether it's transportation or financial health, we continually invest in our employees to help them achieve greatness - inside and outside of work. All who work here should be able to realize their full potential.",4.2,"Nike
4.2","Beaverton, OR",10000+ Employees,1972,Company - Public,Consumer Product Manufacturing,Manufacturing,$10+ billion (USD)
407,Data Engineer I - People Analytics,$73K - $111K (Glassdoor est.),"POSITION SUMMARY Our People Analytics team is expanding! The Data Engineer I is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.

This position can work in a flexible, hybrid model with 3 days on-site in Appleton, WI and 2 days remote.

JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.

QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming. Preferred experience in Python applied to data engineering and transformation.
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated

#LI-Hybid

Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101 .",4.1,"US Venture
4.1","Appleton, WI",1001 to 5000 Employees,1951,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
408,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
409,Data Engineer,$98K - $147K (Glassdoor est.),"Greetings Everyone

Who are we?

For the past 20 years, we have powered many Digital Experiences for the Fortune 500. Since 1999, we have grown from a few people to more than 4000 team members across the globe that are engaged in various Digital Modernization. For a brief 1 minute video about us, you can check

https://youtu.be/uJWBWQZEA6o

.

What will you do?

List out the Instructions

What are we looking for?

Seeking for a strong data analyst who can perform independent.

Good to have Prior experience in banking domain is plus and understanding the finance various data transactions.

Should be strong in the below:
Python

Pyspark

SQL

Airflow

Trino

Hive

Offshore Cordination

LLD

Agile Scrum

Linux

Openshift

Kubernentes

Superset

Optional skill set:
Linux

Openshift

Kubernentes

Superset",3.8,"Photon
3.8","Las Vegas, NV",5001 to 10000 Employees,2007,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
410,Data Engineer,Employer Provided Salary:$65K - $400K,"About Applied
Autonomy is one of the leading technological advances of this century that will come to impact our lives. The work you'll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting-edge technology while working with major players across the industry and the globe.
Applied Intuition provides software solutions to safely develop, test, and deploy autonomous vehicles at scale. The company's suite of simulation, validation, and drive log management software enables development teams to create thousands of scenarios in minutes, run simulations at scale, and verify and validate algorithms for production deployment. Headquartered in Silicon Valley with offices in Detroit, Washington, D.C., Munich, Stockholm, Seoul, and Tokyo, Applied consists of software, robotics, and automotive experts with experiences from top global companies. Leading autonomy programs and 17 of the top 20 global OEMs use Applied's solutions to bring autonomy to market faster.
About the role
We are looking for talented engineers who are excited about building products that wrangle AV data to supercharge our customers. You will drive the design and development of data processing across our products and internal tools. Handling massive volumes of data for Applied's platform needs is a critical area and we are looking for someone who can be hands-on in improving our data quality via data tools and processes.
We have multiple data engineer roles open for all levels of seniority, from mid-level to Lead.
At Applied, you will:
Design and develop systems for programmatically generating ground-truth-labeled data from a simulated world
Develop and deploy high-quality software using modern tooling and frameworks
Work with machine learning pipelines to understand and improve quality of datasets
Work with top autonomy companies to understand and solve unique challenges in perception with synthetic data
Work with products and teams across Applied Intuition
We're looking for someone who has:
Strong software engineering skills in programming languages (Python, C++, GoLang, etc.)
Experience with containerization and other modern software development workflows
Experience with synthetic data and its applications in perception systems
Nice to have:
Experience with applying synthetic data to machine learning tasks
Hands-on experience with characterization of models for Lidar, Radar, and Camera
The salary range for this position is $65,000 USD to $400,000 USD annually. This salary range is an estimate, and the actual salary may vary based on the Company's compensation practices
Don't meet every single requirement? If you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.
Applicants will be required to be fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable federal and state law. Applicants should be aware that for external-facing roles that involve close contact with Company employees or other third parties on the Company's premises, accommodations that involve remaining unvaccinated against COVID-19 may not be deemed reasonable. The Company will engage in the interactive process on an individualized basis taking into account the particular position.
Applied Intuition is an equal opportunity employer and federal contractor or subcontractor. Consequently, the parties agree that, as applicable, they will abide by the requirements of 41 CFR 60-1.4(a), 41 CFR 60-300.5(a) and 41 CFR 60-741.5(a) and that these laws are incorporated herein by reference. These regulations prohibit discrimination against qualified individuals based on their status as protected veterans
or individuals with disabilities, and prohibit discrimination against all individuals based on their race, color, religion, sex, sexual orientation, gender identity or national origin. These regulations require that covered prime contractors and subcontractors take affirmative action to employ and advance in employment individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status or disability. The parties also agree that, as applicable, they will abide by the requirements of Executive Order 13496 (29 CFR Part 471, Appendix A to Subpart A), relating to the notice of employee rights under federal labor laws.",4.0,"Applied Intuition
4.0","Mountain View, CA",51 to 200 Employees,2017,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
411,Senior Data Engineer,Employer Provided Salary:$143K - $300K,"The Data team leverages data from our autonomous vehicles and operations to determine autonomy and service readiness. We provide the foundation for strategic decision-making at Zoox. You will develop and implement the next generation of our data pipeline to ensure visibility into our business as we scale toward the launch of an autonomous mobility service. You will define the system and build the pipeline to enable Zoox to develop and scale with a data-first culture.

You will join a diverse, experienced team with rapidly growing scope and responsibility while also having access to one of the most unique data sets in the autonomous vehicle industry. Hence, we are seeking all skill levels to grow with the team.
Responsibilities
Designing, building, and maintaining the infrastructure that transforms autonomous vehicle data at scale to support analytics throughout the company
Defining and executing on how data from perception, prediction, planning and other parts of the autonomous stack is consumed to generate valuable insights by data scientists, engineers, and business users
Establishing robust data integrity monitoring so that company-wide metrics are based on accurate data
Partnering with engineering and product teams to define data consumption patterns and establish best practices
Qualifications
Experience designing and building complex data infrastructure at scale
Exceptional Python or Scala skills
Advanced SQL and data warehousing experience
Experience operating a workflow manager such as Airflow
Experience with large scale streaming platforms (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Hadoop) and storage engines (e.g. HDFS, HBase)
A strong DataOps mindset and opinions on next-generation warehousing tools
Bonus Qualifications
Basic fluency in C++
Familiarity with or exposure to experimentation platforms
Compensation
There are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights. The salary will range from $143,000 to $300,000. A sign-on bonus may be part of a compensation package. Compensation will vary based on geographic location, job-related knowledge, skills, and experience.

Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance.

ABOUT ZOOX

Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of artificial intelligence, robotics, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.

Follow us on LinkedIn

Vaccine Mandate
Employees working in this position will be required to have received a vaccine approved by the U.S. Food and Drug Administration and/or the World Health Organization. In addition, employees who are eligible for a COVID-19 booster vaccine (“Booster”) will be required to receive a Booster. Employees will be required to show proof of vaccination status upon receipt of a conditional offer of employment. That offer of employment will be conditioned upon, among other things, an Applicant’s ability to show proof of vaccination status. Please note the Company provides reasonable accommodations in accordance with applicable state, federal, and local laws.

About Zoox
Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of robotics, machine learning, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.

Follow us on LinkedIn

A Final Note:
You do not need to match every listed expectation to apply for this position. Here at Zoox, we know that diverse perspectives foster the innovation we need to be successful, and we are committed to building a team that encompasses a variety of backgrounds, experiences, and skills.",4.0,"Zoox
4.0","Foster City, CA",1001 to 5000 Employees,2014,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
412,Data Engineer,-1,"Description:
Who is Leafwell
Leafwell is a rapidly growing technology and data company that set out to increase access, education, and research into cannabis and to advance its application as medicine.
An exciting opportunity for a Data Engineer to join our growing team has arisen.
What to Expect as a Data Engineer at Leafwell
As a Data Engineer at Leafwell, you will be in charge of creating and orchestrating the Leafwell data pipeline, which means gathering data, creating & automating data transformations and producing actionable insights for Leafwell’s internal stakeholders. You will support critical testing and rollout of new data features. The Data Engineer will build and maintain systems that inform Leafwell’s business stakeholders about Key Performance Indicators (KPIs) and suggest data-driven strategies to optimize those metrics.
Essential Duties and Responsibilities
The Data Engineer will perform the following responsibilities:
Acquire, assemble, transform and analyze data
Create, manage and orchestrate the data pipeline and it’s infrastructure
Present findings, trends, and suggested optimizations
Identify new opportunities and threats to the company's business model
Update and revise reports, queries, and analytic procedures as necessary
Design and implement tracking so that optimization efforts can be measured
Identify inefficiencies in data processes and automate where appropriate
Write and update international SOPs and internal documentation
Support IT systems management with testing, validation, and user support
Proactively identify initiatives for data-related improvements
Why Leafwell
At Leafwell, we are passionate about our work and seek out employees who contribute the same level of dedication and enthusiasm. We are only as good as the people we hire, so we aim to be the best employer in order to attract the top talent in the industry.
Do we have your Interest?
Requirements:
Our Ideal Candidate
Our Ideal Candidate will possess the following:
Bachelor's degree in Computer Science, Mathematics, Economics, Information Systems, or another quantitative field
3-5 years of relevant professional experience in Data Engineering / Analytics
Advance working knowledge and experience in SQL and relational databases
Strong data management abilities, including data analysis, standardization, cleansing, querying, and consolidation of data
Data visualization experience (e.g Tableau, Looker, etc.)
Exercises self-reliance, initiative, and leadership while keeping stakeholders properly informed and involved
Reliably manage numerous duties during a workday, necessitating interactions with people located across the world
Technically competent, with the ability to quickly learn new processes and programs, and utilize various software applications
Excellent communication and interpersonal skills; ability to work with and appeal to a wide variety of personalities and professional tendencies
Bonus points if you have experience in the following:
Cannabis knowledge; industry experience is a plus
Used Data Orchestration tools like Dagster or Airflow
Used dbt or comparable data transformation software
Git
Python or R programming
Amazon Web Services
PostgreSQL and RedShift
CRM products
Data Visualization in Metabase
Effective project management skills and comfort utilizing a project management platform in collaboration with other team members
Data Science projects
Benefits Highlights
Our benefits include, but are not limited to:
Remote first - Most of our employees are 100% remote. Positions requiring travel enjoy a hybrid environment.
Rapid Growth - Our company is rapidly expanding, and our employees are advancing with us!
Training and Development - We foster a culture of learning. We encourage our employees to take part in educational, training, and development opportunities.
Leafwell is dedicated to bringing together people from diverse cultures and perspectives. We work hard to provide a welcoming atmosphere where everyone may flourish, have a sense of belonging, and collaborate effectively. As an equal opportunity employer, we do not tolerate any illegal discrimination against job applicants based on their race, color, religion, veteran status, sex, parental status, gender identity or expression, transgender status, sexual orientation, national origin, age, disability, or genetic information. We are committed to going above and above in creating diversity within our firm and follow the regulations upheld by the EEOC.",4.0,"LEAFWELL
4.0",Remote,51 to 200 Employees,-1,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
413,Data Engineer,Employer Provided Salary:$120K - $200K,"Verition Fund Management LLC (""Verition"") is a multi-strategy, multi-manager hedge fund founded in 2008. Verition focuses on global investment strategies including Global Credit, Global Convertible, Volatility & Capital Structure Arbitrage, Event-Driven Investing, Equity Long/Short & Capital Markets Trading, and Global Quantitative Trading. As a Data Engineer you would be responsible for building data pipelines and supporting Portfolio Managers and Risk teams.
Responsibilities:
Building data pipelines
Working closely with data vendors such as Bloomberg, Refinitiv, etc…
Taking this vendor data and normalizing/standardizing it or firm consumption
Taking the normalized data and customizing it to user specific needs.
Qualifications:
4+ years of experience in financial services
BS or MS in Computer Science or Computer Engineering
Strong technology/coding skills (Python)
Strong design skills to build extensible config/data-driven platforms
Good Financial data, specifically security master and/or knowledge of vendor datasets
Strong AWS data pipeline skills
Strong database skills – SQL and no-SQL
Strong problem solving skills
Ability work with datasets in Excel and other productivity tools
Nice to Have:
AWS Glue
Spark
Jupyter Notebook

Salary Range
$120,000—$200,000 USD",3.5,"Verition Group LLC
3.5","New York, NY",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
414,Data Engineer,$81K - $114K (Glassdoor est.),"Position Overview:
The Novelis Corporate Digital team is seeking a Data Engineer. The person in this role will support the Data Services Manager. The ideal candidate will be responsible for delivery of the data pipelines feeding into machine learning models. The Data Engineer will aid in the optimization of operations by manipulating and aggregating the disparate operational and historian (L2) data sources into a format that is easily digestible by both data scientists and statistically adept colleagues. Their core responsibility will be to combine large volumes of disparate data, conduct quality checks on the data, manipulate the data and ensure continuous access to a clean format of the operational data for supporting functions and collaborators.

Novelis is the world leader in aluminum rolling and recycling, producing an estimated 19 percent of the world's flat-rolled aluminum products. We work alongside our customers to provide innovative solutions to the aerospace, beverage can, automotive and high-end specialty markets.

Headquartered in Atlanta, Georgia, USA, Novelis has approximately 13,000 employees in 33 operating facilities on four continents and is investing close to $2 billion in global expansions to meet the growing demand for our premium product.
Responsibilities & Qualifications:
Responsibilities:
Design and Develop data ingestion pipelines and processes based on requirements in Python and PySpark
Build error handing, exception management and data quality routines to expose the anomalies in the data
Profile and analyze data to identify gaps and potential data quality issues
Identifies relationships between disparate data sources
Uses Python, Databricks and Spark to code the data Engineering routines
Perform unit and integration testing
Works with the Data Science team and business SMEs to get the requirements and present the details in data.
Designs and jointly develops the data architecture with data architect and ensures security and maintenance
Explores suitable options, designs, and builds data pipeline (data lake / data warehouses) for specific analytical solutions
Identifies gaps and implements solutions for data security, quality and automation of processes
Builds data tools and products for effort automation and easy data accessibility
Supports maintenance, bug fixing and performance analysis along data pipeline
Diagnoses existing architecture and data maturity and identifies gaps
Minimum Qualifications:
Minimum of Bachelor of Science BS/MS degree in Computer Science, Engineering, and/or Background in Mathematics and Statistics
Over 3 years of work with data engineering, IT or related field
Experience in data engineering & data bases/warehouses
Used SQL, PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle etc in production environments
Experience with programming languages like UNIX shell scripting, Python etc.
Preferred Qualifications:
Certifications in Databricks, Pythod, Spark or other related subjects
Experience with Agile/Scrum methodology with the Product based implementation approach
Experience with CI/CD with DevOps frameworks
Experience on Big Data platforms (e.g. Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive)
Experience with cloud platforms (Azure, AWS, Databricks)
Experience with Historians (IBA, PI), Industry 4.0 or IoT environments
What We Offer:
This role offers a hybrid schedule. Novelis is committed to a Flex Work approach that empowers employees to maintain work-life balance while enjoying the benefits of a collaborative office environment.
Novelis benefits say a lot about how we care for each other. Our employees and their families have many different needs. As a result, our benefits offer choices on many levels and are high in quality, competitive in the marketplace, and affordable. These are a few of the benefits we offer to support you and your family’s well-being:
Medical, dental and vision insurance
Health savings accounts – Company Funded Health Savings Account (HSA) and Health Reimbursement Account (HRA)
Company-paid basic life insurance and Additional voluntary life coverage
Paid vacation and competitive personal time off
401(k) savings plan with company match
Retirement savings plans – medical and prescription drug coverage through private exchange
Employee assistance programs – available 24/7 to you and your family
Wellness and Work Life Support - career development and educational assistance
Location Profile:
Novelis’ Global Corporate Headquarters is located in the Buckhead neighborhood of Atlanta Georgia and employs approximately 250 people. It is co-located with Novelis’ North America regional office which employs approximately 225 people. Supporting its 24 operations worldwide Novelis’ corporate office is home to the executive leadership team and global functions that support the automotive beverage can and high-end specialties value streams. The City of Atlanta provides a diverse and family-friendly place to live with countless museums cultural organizations and educational institutions including the Georgia Aquarium Woodruff Arts Center CNN Center Georgia Tech and Mercedes-Benz Stadium. In the Atlanta area Novelis has strong community partnerships with Atlanta Habitat for Humanity GeorgiaFIRST and Agape Youth and Family Center in addition to many local museums and community groups. Novelis recognizes its talented and diverse workforce as a key competitive advantage. Novelis provides equal employment opportunities to all employees and applicants.All terms and conditions of employment at Novelis including recruiting hiring placement promotion termination layoffs recalls transfers leaves of absence compensation and training are without regard to race color religion age sex national origin disability status genetics protected veteran status sexual orientation gender identity or expression or any other characteristic protected by federal provincial or local laws.",3.8,"Novelis Corporate HQ
3.8","Atlanta, GA",10000+ Employees,2005,Subsidiary or Business Segment,Metal & Mineral Manufacturing,Manufacturing,$10+ billion (USD)
415,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
416,USA - Infrastructure Data Engineer (AWS),$104K - $148K (Glassdoor est.),"Job Title: Infrastructure Data Engineer (AWS)
Location: San Jose, California, United States
Type: Fulltime

The candidate has to be in San Jose, CA and it is 100% onsite job”.

Roles & Responsibilities:
Design, build and maintain data platform infrastructure on AWS environment.
Oversee design, build, and maintain data platform infrastructure on AWS environment.
Develop data pipelines to collect the metrics that is required to monitor data refreshes, reports deliveries and track SLAs.
Work with the DWH development team and business users in establishing SLAs for data refreshes and reports.
Build continuous integration/deployment (CI/CD) pipelines to accelerate development and improve team agility.
Oversee project.
Monitor all aspects of data platform system security, performance, storage, incidents, and usage for databases, data pipelines, applications, and infrastructure on AWS. Escalate to respective teams for fixes.
Ensure data pipelines meet intraday and daily SLAs, as per documented SLA definitions and escalate accordingly.
develop appropriate instrumentation to collect metrics on system performance, cost, data ingress/egress /storage processes.
Have a clear understanding of the reports/analyses/insights to be driven by data and build data driven solutions to optimally support the operational analytics needs.
Documents user stories, epics, and reports
Coordinate infrastructure enhancements and maintenance with the system/network engineering teams
Work with DWH development team and analytics team to do manual releases where required.
Onboard users to data analytics systems with appropriate approvals
Conduct system performance tests and collect metrics. Tune/add capacity.
Complete knowledge management processes
Own strategy and communicate potential major shifts in expected workload based on business, market, or operational changes.
Operate ongoing business relationship management sessions to review operational metrics, understand pain points, identify upcoming projects and engagement efforts.",-1,Avestacs,"San Jose, CA",51 to 200 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
417,Data Engineer,$100K - $144K (Glassdoor est.),"Position: Data Engineer
Duration: Contract
Location: Fremont, California
Skills:
Strong in SQL and Python
Data warehousing concepts , Data Modeling and RDBMS concepts
Good to have exposure to big data technologies – Apache Spark, Kafka, Airflow, NoSQL
Good to have exposure on Insurance & Finance business processes
Regards
Tejash Gupta
Recruiter
tejash@swifttechinc.com
Job Type: Contract
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Python: 7 years (Required)
SQL: 7 years (Required)
Data modeling: 6 years (Required)
RDBMS: 6 years (Required)
Work Location: In person",4.1,"Swift Technologies Inc
4.1","Fremont, CA",1 to 50 Employees,-1,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
418,Data Engineer,$74K - $107K (Glassdoor est.),"Analyze Business Requirement Documents and Implement Technical Solutions for privacy related applications.
Develop ETL process for supporting Data Extraction, transformations and loading.
Perform data conversions and aggregations using different transformations such as Merge, Merge join, Union condition split, sort, order by. Derived columns convert and cast transformations and row count and lookup and fuzzy lookup transformations.
Develop UNIX scripts to load the data from Source server to Teradata and validate the files between different servers.
Develop new process to implement state level privacy regulations based on each state law in Big Data Platform.
Create Temperory/Fact tables, loading with data and writing Teradata and Spark SQL queries.
Optimize/tune ETL objects, indexing and partitioning for better performance and efficiency.
Validate the performance metrics and work on performance tuning for SQL, HQL and Spark SQL queries.
Perform testing and Provide test support for various level of testing phases like Unit, User Acceptance, Regression, Parallel and System testing.
Promote the components to production environment through CI/CD process by using Git hub .
Script task and execute SQL tasks to execute SQL code. Work on containers for loop and for each loop container to run a group of tasks into a single container and repeating tasks.
Create the data flow to extract data from sources to OLEDB Source, Excel, XML, flat files sources and destination is SQL data warehouse.
Minimum Education Required:- All the responsibilities mentioned above are in line with the professional background and requires an absolute minimum of a Bachelor’s degree in computer science, computer information systems, technology management, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.",4.9,"Wise Equation Solutions
4.9","La Vista, NE",51 to 200 Employees,2006,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
419,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Data Engineer
Candidate must have AWS (Redshift, Glue, S3), Spark, Phython, Scala and Should be expert in SQL
Basic qualifications
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
Design, implementation and management of data processing systems dealing with large data sets.
Proficiency in, at least, one modern programming language such as Python, Java or Scala
Hands-on experience and advanced knowledge of SQL.
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and cloud computing
Experience working with Open-Source Big Data tools (Hadoop, Spark, Hive, Presto and etc.)
Preferred qualifications
Experience in working and delivering end-to-end projects independently
Relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics
Experience working with AWS Big Data Technologies (AWS Glue, EMR, Athena, Ec2, Redshift and S3 etc.)
Experience working with Oracle, PostgreSQL, SQL Server or MySQL.
Proven track record of delivering a big data solution
Experience working with both Batch and Real Time data processing systems
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Expected hours: 40 per week
Compensation package:
1099 contract
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Austin, TX 78703: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 8 years (Required)
Python: 8 years (Required)
AWS: 5 years (Preferred)
Work Location: Hybrid remote in Austin, TX 78703",-1,PRISM IT LLC,"Austin, TX",-1,-1,-1,-1,-1,-1
420,Data Engineer,Employer Provided Salary:$175K - $200K,"At Expa we're dedicated to being the best place for entrepreneurs to create and build companies. Expa was founded in 2013 by Uber co-founder Garrett Camp to help bring great ideas to life. We're builders, founders, and operators ourselves, know the struggles that come with starting a new company, and what it takes to succeed. We support entrepreneurs with more than just funding — we offer a valuable network, office space, and our advice and expertise. Expa Studio combines technical and design expertise, operational support, and capital to turn bold ideas into category-defining companies.
Your role
You will be a Data Engineer (ML, AI) at a new studio company tasked with building or leveraging machine learning solutions and developing applications utilizing AI frameworks and methods. This position will require you to design and collaborate with members across the organization to deliver product innovations leveraging your unique knowledge and skill set in Machine Learning and Artificial Intelligence in our Big Data Platform. You are required to use advanced programming and analytical skills to solve a variety of complex business problems across multiple industries and through all phases of the business lifecycle.
Your Responsibilities
Write and test product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Your Experience
Masters or PhD required, preferably in Statistics, Mathematics, Computer Science or related fields.
Minimum of 4+ years related experience, preferably in Financial Services.
Strong knowledge of statistical analysis software / data analysis experience including data manipulation; preferably Python, PySpark and R.
Proven track record in executing complex modeling/machine learning projects and new product development from concept to final delivery.
Expert analytical skills to evaluate, understand and interpret data.
Demonstrated ability to communicate ideas and analysis results effectively both verbally and in writing to technical and non-technical audiences.
Excellent communication, organization, interpersonal and presentation skills.
Demonstrated ability to simultaneously manage multiple projects of significant complexity.
This will be a full-time position with a salary range of $175,000-$200,000, which is determined by various factors, including, but not limited to, work experience, skills, location, depth and maturity of our project. We're open to remote work, but would prefer someone in the LA area so that we can work in person. We're a small team and we want to try to spend as much time iterating in these early days of the idea.",3.7,"Expa
3.7","Los Angeles, CA",1 to 50 Employees,2013,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
421,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
422,Senior Lead Data Engineer,Employer Provided Salary:$75.00 Per Hour,"Hello,
Hope you are doing well!!!
We are constantly on the lookout for professionals to fulfil the staffing needs of our clients, and we currently have a job opening that may interest you.
Job Title: Senior Big Data Engineer
Location: Malvern, PA (Hybrid)
Position type: Contract
Responsibilities
· Participate in agile team meetings, analyze requirements, design and build data pipelines using AWS infrastructure
Qualifications
· Pyspark, SQL, Python, AWS Glue ETL, AWS EMR, AWS S3, Control M
If you believe you are in the job market or interested in making a change, please email
Darshan More | New York Technology Partners
Job Type: Contract
Salary: From $75.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Malvern, PA 19355: Reliably commute or planning to relocate before starting work (Required)
Experience:
AWS Glue: 3 years (Preferred)
ETL: 3 years (Preferred)
Python: 3 years (Preferred)
PySpark: 2 years (Preferred)
SQL: 10 years (Preferred)
Work Location: In person",4.0,"New York Technology Partners
4.0","Malvern, PA",51 to 200 Employees,1999,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
423,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
424,Data Engineer IV,Employer Provided Salary:$149K,"Who We Are
Invitation Homes is a fast-paced evolving publicly traded REIT that is pioneering a new industry with over 80,000 single family rental homes and a market capitalization of over $25 billion. We are a team of more than 1,400 associates who come from all walks of life. We call different communities “home,” but our shared values bind us together. Invitation Homes is a place where possibility lives.
THR Property Management LP / Data Engineer / Dallas, TX: Develop and maintain data pipelines into data warehouse and data lake, including design, and development. Develop and implement strategies to translate business requirements into feasible and acceptable data warehouse and data lake solutions. Architect and build new data models to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making. Design, build and launch data pipelines to move data to data lake and data warehouse build and maintain framework for auditing, error logging, and master data management for data pipelines. Build data expertise and own data quality for the data pipelines. Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes. Perform data analysis and assist in the resolution of data issues. Identify and resolve defects of complex scope using proper engineering tools and techniques. Provide support and maintain existing products and add new features. Mentor engineers by providing advice, coaching and educational opportunities. Telecommuting permitted from any location within the U.S. Salary: 148512/year.
The Senior Data Engineer is accountable for the maintenance, improvement, and movement of data in the Enterprise Data Warehouse, and other data sources used for enterprise reporting. A successful Senior Data Engineer delivers database design, implementation, and schematics that creates sustainable, competitive advantage for our company. Further, a Senior Data Engineer is responsible for the development and deployment of innovative analytics data platforms to support end-user enterprise reporting. Working with other data engineers, data analysts, and data scientists, a Senior Data Engineer must have a delivery first mentality and a firm grasp of Agile Development methodology. Lastly, the ideal candidate must demonstrate the ability to work as a team member and team leader.
What You’ll Do
Accountable for the overall performance and maintenance of our SQL Server environments:
Manage the EDW relational database for optimized performance
Design scalable ETL packages from the business source systems
Ensure all necessary testing and validation for all planned and unplanned releases
Mentor junior data engineers in best practices and documentation.
Act as our database evangelist in leading innovation activities through exploration, benchmarking and implementation of data technologies
Participate in solution-based activities; articulate your ideas through well thought out documentation and research best-in-class solutions
Lead and Develop and implement data architecture standards
Manage the necessary testing and validation to ensure proper compliance to data governance and quality
Prepare progress reports regarding our database environments’’ status and health
Manage troubleshooting data issues and present solutions to these issues.
Resolve tactical issues thoroughly while always looking for optimization and documenting lessons learned
Understand when tactical issues are really systematic issues requiring deeper level of analysis and strategic solutions
Cooperate with the support team to investigate and resolve data related defects
Proactively analyze and evaluate current state of data in order to identify and recommend improvements and optimization
Support our Agile Development methodology
Constantly prioritize user stories; balance feature development, defects, and tech debt according to business need
Be a champion of continuous improvement and delivery; define the minimum viable product, focus teams on key release milestones, and leverage fast follow-up sprints
Enable sprint progress; clarify story details, validate completed stories, and prepare stories for the next sprint
Who We’re Looking For
Bachelor’s degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field. An equivalent of the same in working experience is also accepted for the position.
A candidate for the position will have as at least 5 years of working experience as a database engineering or a database engineering administrator within a fast-paced a complex business setting.
Design, implement and maintain SQL Server databases
Design, implement and maintain ETL processes using SQL Server SSIS
Provide ongoing maintenance support through SQL query tuning and optimization
Strong working and conceptual knowledge of building and maintaining physical and logical data models
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience working in SaaS, IaaS, and PaaS
Excellent customer service and interpersonal skills; ability to relate to and get along with others
Professional verbal and written communication skills
Strong organizational and time-management skills
Ability to multi-task and maintain flexibility and creativity in a variety of situations
Ability to analyze and resolve problems
Ability to set and meet goals and consistently meet deadlines
Ability to maintain confidentiality
Why Invitation Homes
We stand for flexibility, opportunity, and a home that people can make their own. It’s as true for our associates as it is for our residents. Just like we help our residents live freer, we liberate our associates’ careers, too. Our associates know that at Invitation Homes goals matter, potential is unlocked, and careers thrive. Invitation Homes isn’t just a work place it is a possibility place. Invitation Homes offers the below to each new associate:
Competitive pay and an annual bonus program for all associates
Generous paid time off plans including vacation accrual, sick time, volunteer time, and standard and floating holidays
401k with matching company contributions
Awesome work environment with casual dress
Team events and gatherings
Employee resource groups: Together with Women, Asian Alliance, Black Collective, Juntos, Gen Next, and Open Invitation.
Invitation Homes truly is where possibility lives, pour a new foundation here!
Salary Range
$98,640.00 - $170,976.00
Compensation and Benefits
To attract and retain top talent, we're pleased to offer competitive compensation and benefits, including:
Annual bonus program
Health, dental, vision, and life insurance
Long-term and short-term disability insurance
Generous paid time off plans include vacation accrual, sick time, standard holidays and floating holidays
401(k) with company matching contributions
Awesome work environment with casual dress
Team events and gatherings (Pre- and Post-Covid)
Invitation Homes is an equal opportunity employer committed to fostering a diverse, inclusive and innovative environment with the best associates. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, Veteran status or any other factor protected by applicable federal, state or local law. If you have a disability or special need that requires accommodation, please contact us at
humanresources@invitationhomes.com
.
To all recruitment agencies: Invitation Homes does not accept agency resumes. Please do not forward resumes to Invitation Homes employees. Invitation Homes is not responsible for any fees related to unsolicited resumes.",3.9,"Invitation Homes
3.9","Dallas, TX",1001 to 5000 Employees,2012,Company - Public,Real Estate,Real Estate,$1 to $5 billion (USD)
425,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
426,DATA ENGINEER,$81K - $117K (Glassdoor est.),"DATA ENGINEER
Upward Health is a home-based medical group specializing in primary medical and behavioral care for individuals with complex needs. We serve patients throughout their communities, and we diagnose, treat, and prescribe anywhere our patients call home. We reduce barriers to care such as long delays due to scheduling. We see patients when they need us, for as much time as they need, bringing care to them. Beyond medical supports, we also assist our patients with challenges that may affect their health, such as food insecurity, social isolation, housing needs, transportation and more. It’s no wonder 98% of patients report being fully satisfied with Upward Health!
Upward Health provides technology-enabled, integrated, and coordinated care delivery services that improve outcomes and reduce costs for patients with severe behavioral health diagnoses and co-morbid, chronic physical conditions. We are not your typical medical practice. At Upward Health, we see every day as an opportunity to make a difference in our patients' lives. We could tell you about our outcomes and patient satisfaction ratings. We could tell you about our commitment to our mission. Or you could join us and experience it all for yourself.
WHY IS THIS ROLE CRITICAL?
Are you an entrepreneurial data engineer who loves data and reporting but also wants to be more involved in the day-to-day business operations of a company? Do you get excited by solving problems, analyzing trends, creating solutions and do you have a unique ability to create order out of chaos? If you answered yes to these questions, then Upward Health has the perfect job for you. We have a unique opportunity for an energetic, smart, business savvy individual who is looking to join an entrepreneurial healthcare company and help change the lives of the patients we serve. He or she will be a dynamic team player with an outgoing personality that understands the importance of collaboration and communication. We are looking for a hard worker who is detail oriented and is excited by the opportunity to work for a company that values and rewards hard work, but that also prioritizes rewarding that hard work and having fun.
If you are excited by the thought of using your skills to positively affect how healthcare is delivered to patients, as well as the opportunity to join a rapidly growing, technology enabled healthcare company that values its employees and rewards excellent performance, then please apply to this job. You will be glad that you did.
KEY RESPONSIBILITIES:
Develop highly scalable reporting, data integration and web service features for technology deliverables and provide enterprise-level architectural design and oversight.
Develop, optimize and work with architects on data models for Data Warehouse and Operational databases.
Analyze functional needs and design, develop, integrate, and test software to meet those needs.
Design data integrations and analytics in accordance with architecture and security policies, procedures, and quality assurance best practices.
Create models and diagrams (such as flowcharts) that instruct other developers/programmers how integrations and data flows function.
Document integrations, applications and system components as a reference for future maintenance and upgrades.
Evaluate proposals to identify potential problem areas and make appropriate recommendations.
Research technologies for possible placement/adoption into UH's back-end system.
Ability to meet with users and business analysts to understand requirements, translate them into solutions and document technical designs.
Supervise and/or implement technology projects and support the resulting features in a Production environment.
Ability to test your code and provide quality deliverables.
TECHNICAL SKILLS MSUT HAVE:
4+ Years experience designing, developing and troubleshooting ETL services (SSIS or other)
4+ Years experience developing and optimizing Structured Query Language (SQL) to analyze and query data for reporting.
2+ Years experience with ETL services using Azure Data Factory
2+ Years experience with Java
2+ Years experience designing and writing web services/API code, micro services using API Frameworks and data structures (Rest, Json).
2+ Years experience in data modeling and database technologies.
2+ Years experience in Healthcare industry working w/ PHI
TECHNICAL SKILLS CONSIDERED A PLUS:
Experience w/ Power BI
MS Azure certifications
Knowledge of other Azure services
Coding ability using Visual Studio and C#
KNOWLEDGE & ABILITIES:
Highly ambitious, given role will grow as the company does.
Self-starter, very energetic and ability to change directions quickly.
High standard of quality and commitment to product top-notch work.
Ability to communicate ideas clearly and effectively in both technical and user-friendly language.
Proven ability to develop services according to healthcare industry security standards.
Strong curiosity, desire to learn new skills and acquire new knowledge.
Excellent written and verbal communication, ability to work well with people of different backgrounds and skill sets.
Ability to work independently and in a highly virtual environment, with colleagues all over the country.
Strong work ethic and willingness to work long hours, including nights and weekends when required.
Willingness to cover after-hours support when needed.
QUALIFICATIONS:
Bachelor’s degree from highly selective university
Industry experience in healthcare
Upward Health is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining, and maximizing the performance of a diverse and inclusive workforce.
This job description is a general outline of duties performed and is not to be misconstrued as encompassing all duties performed within the position.",3.6,"Upward Health
3.6","Hauppauge, NY",201 to 500 Employees,-1,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
427,Sr. Data Governance Engineer,$99K - $133K (Glassdoor est.),"Job Description
Job Description

The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection. The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.

Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services

Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Compensation for this role is expected to be between $128,300.00 and $146,300.00 Actual pay may be higher or lower depending on a number of permissible factors under applicable law, including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.",3.8,"Bose
3.8","Framingham, MA",5001 to 10000 Employees,1964,Company - Private,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
428,Data Engineer,$78K - $113K (Glassdoor est.),"Senior Data Engineer – Top Secret Clearance Required

Will implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Qualifications Required:
Minimum Active Top Secret Level Security Clearance
2+ years of professional experience.
2+ years of experience in full stack development, including front end and back end coding and a thorough knowledge of the SDLC
2+ years of experience designing and developing real time ETL architecture for real time predictive analytics.
2+ years of experience with Apache Hive, Apache Spark and Apache Yarn
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
Preferred:
5+ years of relevant consulting or industry experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation – desire to learn and apply new technologies, products, and libraries
Prior professional services or federal consulting experience",3.7,"ITR
3.7","Washington, DC",501 to 1000 Employees,-1,Company - Private,Aerospace & Defense,Aerospace & Defense,Unknown / Non-Applicable
429,Senior Data Engineer,Employer Provided Salary:$105K - $186K,"**(UPDATED 8/21/2023) We invite applicants to express their interest in this role, but please note that we currently don't have a specific timeline for filling this position. This posting is intended for talent pooling. While there isn't a definitive hiring period, potential candidates should be aware that the role might be filled at some point in the future. We encourage you to start the process and discover more about this opportunity, Slalom, and potentially a career in consulting through discussions with our talent acquisition team.
Who You’ll Work With
As a modern technology company, we've never met a technical challenge we didn't like. We enable our clients to learn from their data, create incredible digital experiences, and make the most of new technologies. We blend design, engineering, and analytics expertise to build the future. We surround our technologists with interesting challenges, innovative minds and emerging technologies.
A Senior Data Engineer at Slalom is a technical team member who designs and develops data platform solutions. It uses common data architecture practices to architect, design, and develop data/analytic platforms (e.g., data warehouses, data lakes) that are used to produce analytic products, like reports, dashboards, ML models, etc.
What You’ll Do
Work as part of a team and with clients to develop cloud data and analytics solutions
Make recommendations and prove data architecture concepts to team members and clients
Development of cloud data platforms and business intelligence solutions
Data wrangling of heterogeneous data sources for exploring and discovering new insights
Work with highly collaborative agile teams on modern data architecture projects
Key success measures of a Senior Data Engineer include technical proficiency in data processing technologies, collaboration with other teams, communication, and willingness to learn new things
What You'll Bring
Experience in one or more cloud technologies (AWS, Azure, Google Cloud, Snowflake)
Experience in data modeling, design, and development using complex SQL (SQL Server, Oracle, etc.)
Experience in Apache Airflow, Talend, SSIS, Informatica, Alteryx, or other data processing tool
Experience in one or more programming languages (Python, C#, Scala, R, PowerShell, Java, etc.)
Experience in cloud engineering, serverless architecture, and/or data streaming services
About Us
Slalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six countries and 43 markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 13,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.
Compensation and Benefits
Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance.
Slalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $105,000 to $186,000. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.
EEO and Accommodations
Slalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team if you require accommodations during the interview process.
#LI-JS8
#LI-Hybrid",4.3,"Slalom Consulting
4.3","Los Angeles, CA",10000+ Employees,2001,Company - Private,Business Consulting,Management & Consulting,$1 to $5 billion (USD)
430,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
431,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
432,Data Engineer,Employer Provided Salary:$65K - $400K,"About Applied
Autonomy is one of the leading technological advances of this century that will come to impact our lives. The work you'll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting-edge technology while working with major players across the industry and the globe.
Applied Intuition provides software solutions to safely develop, test, and deploy autonomous vehicles at scale. The company's suite of simulation, validation, and drive log management software enables development teams to create thousands of scenarios in minutes, run simulations at scale, and verify and validate algorithms for production deployment. Headquartered in Silicon Valley with offices in Detroit, Washington, D.C., Munich, Stockholm, Seoul, and Tokyo, Applied consists of software, robotics, and automotive experts with experiences from top global companies. Leading autonomy programs and 17 of the top 20 global OEMs use Applied's solutions to bring autonomy to market faster.
About the role
We are looking for talented engineers who are excited about building products that wrangle AV data to supercharge our customers. You will drive the design and development of data processing across our products and internal tools. Handling massive volumes of data for Applied's platform needs is a critical area and we are looking for someone who can be hands-on in improving our data quality via data tools and processes.
We have multiple data engineer roles open for all levels of seniority, from mid-level to Lead.
At Applied, you will:
Design and develop systems for programmatically generating ground-truth-labeled data from a simulated world
Develop and deploy high-quality software using modern tooling and frameworks
Work with machine learning pipelines to understand and improve quality of datasets
Work with top autonomy companies to understand and solve unique challenges in perception with synthetic data
Work with products and teams across Applied Intuition
We're looking for someone who has:
Strong software engineering skills in programming languages (Python, C++, GoLang, etc.)
Experience with containerization and other modern software development workflows
Experience with synthetic data and its applications in perception systems
Nice to have:
Experience with applying synthetic data to machine learning tasks
Hands-on experience with characterization of models for Lidar, Radar, and Camera
The salary range for this position is $65,000 USD to $400,000 USD annually. This salary range is an estimate, and the actual salary may vary based on the Company's compensation practices
Don't meet every single requirement? If you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.
Applicants will be required to be fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable federal and state law. Applicants should be aware that for external-facing roles that involve close contact with Company employees or other third parties on the Company's premises, accommodations that involve remaining unvaccinated against COVID-19 may not be deemed reasonable. The Company will engage in the interactive process on an individualized basis taking into account the particular position.
Applied Intuition is an equal opportunity employer and federal contractor or subcontractor. Consequently, the parties agree that, as applicable, they will abide by the requirements of 41 CFR 60-1.4(a), 41 CFR 60-300.5(a) and 41 CFR 60-741.5(a) and that these laws are incorporated herein by reference. These regulations prohibit discrimination against qualified individuals based on their status as protected veterans
or individuals with disabilities, and prohibit discrimination against all individuals based on their race, color, religion, sex, sexual orientation, gender identity or national origin. These regulations require that covered prime contractors and subcontractors take affirmative action to employ and advance in employment individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status or disability. The parties also agree that, as applicable, they will abide by the requirements of Executive Order 13496 (29 CFR Part 471, Appendix A to Subpart A), relating to the notice of employee rights under federal labor laws.",4.0,"Applied Intuition
4.0","Mountain View, CA",51 to 200 Employees,2017,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
433,Senior Cloud Data Engineer,$97K - $141K (Glassdoor est.),"Role: Senior Cloud Data Engineer
Location: Trenton, NJ (Hybrid,3 days onsite)
Contract Duration: 12+ Months
Interview: In person interviews on Thursday and Friday afternoon
The State of NJ is seeking a Cloud Data Engineer that will assist in maintaining, and monitoring infrastructure as well as build or assist in building data transformation pipelines. Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and numpy.
Experience with PySpark.
Experience using AWS Glue and EMR to construct data pipelines.
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets .
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large, disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
They should also have experience using the following software/tools:
Experience with big data tools: Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres. Experience with data pipeline and workflow management tools: AWS Glue, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
It is a strong possibility for extension beyond one year.
This will be onsite immediately. Schedule is hybrid, 3 days onsite and 2 days remote per week.
Please do not submit any candidates that are unable to relocate prior to start of the assignment.
The manager prefers Green Card and US Citizens at this point, based on a long-term desire to consider this position for employment directly with the State.
It is not inevitable, just a possibility a few years in the future.
Prior experience with writing and debugging python Required 5 Years
Prior experience with building data pipelines. Required 5 Years
Prior experience Data lakes in an aws environment Required 3 Years
Prior experience with Data warehouse technologies in an AWS environment Required 3 Years
Prior experience with AWS EMR Required 2 Years
Prior experience with Pyspark Highly desired 2 Years
Thanks and Regards,
David Jones
Phone: 908-360-8020;
Email: David@centillionz.com
Job Type: Contract
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Trenton, NJ 08608: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Lake: 5 years (Required)
Data Pipeline: 5 years (Required)
AWS / Azure: 5 years (Required)
Work Location: In person",5.0,"CentillionZ IT Solutions
5.0","Trenton, NJ",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
434,Data Engineer,$92K - $128K (Glassdoor est.),"Become a Part of the NIKE, Inc. Team

\r

\r

NIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it's about each person bringing skills and passion to a challenging and constantly evolving game.

Data Engineer-NIKE USA, Inc., Beaverton, OR. Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology. Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes. Evaluate and utilize new technologies/tools/frameworks centered around high-volume data processing. Translate product backlog items into engineering designs and logical units of work. Profile and analyze data for the purpose of designing scalable solutions. Define and apply appropriate data acquisition and consumption strategies for given technical scenarios. Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem. Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns. Implement complex automated routines using workflow orchestration tools. Anticipate, identify and tackle issues concerning data management to improve data quality. Build and incorporate automated unit tests and participate in integration testing efforts. Utilize and advance continuous integration and deployment frameworks.

Employer will accept a Bachelor's degree in Data Science, Computer Engineering, Computer Science, Computer Information Systems and 5 years of progressive post-baccalaureate experience in the job offered or in an engineering-related position.
Programming ability (Python, SQL);
Database related concept;
Big Data exposure;
Spark;
Airflow (Orchestration tools);
Cloud Solutions;
Software/Data design ability;
CI/CD understanding and implementation;
Code review;
Data Architecture; and
AWS, Azure

#LI-DNI

NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.

NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.

How We Hire

At NIKE, Inc. we promise to provide a premium, inclusive, compelling and authentic candidate experience. Delivering on this promise means we allow you to be at your best - and to do that, you need to understand how the hiring process works. Transparency is key.

This overview explains our hiring process for corporate roles. Note there may be different hiring steps involved for non-corporate roles.

Benefits

Whether it's transportation or financial health, we continually invest in our employees to help them achieve greatness - inside and outside of work. All who work here should be able to realize their full potential.",4.2,"Nike
4.2","Beaverton, OR",10000+ Employees,1972,Company - Public,Consumer Product Manufacturing,Manufacturing,$10+ billion (USD)
435,Senior Data Engineer,Employer Provided Salary:$143K - $300K,"The Data team leverages data from our autonomous vehicles and operations to determine autonomy and service readiness. We provide the foundation for strategic decision-making at Zoox. You will develop and implement the next generation of our data pipeline to ensure visibility into our business as we scale toward the launch of an autonomous mobility service. You will define the system and build the pipeline to enable Zoox to develop and scale with a data-first culture.

You will join a diverse, experienced team with rapidly growing scope and responsibility while also having access to one of the most unique data sets in the autonomous vehicle industry. Hence, we are seeking all skill levels to grow with the team.
Responsibilities
Designing, building, and maintaining the infrastructure that transforms autonomous vehicle data at scale to support analytics throughout the company
Defining and executing on how data from perception, prediction, planning and other parts of the autonomous stack is consumed to generate valuable insights by data scientists, engineers, and business users
Establishing robust data integrity monitoring so that company-wide metrics are based on accurate data
Partnering with engineering and product teams to define data consumption patterns and establish best practices
Qualifications
Experience designing and building complex data infrastructure at scale
Exceptional Python or Scala skills
Advanced SQL and data warehousing experience
Experience operating a workflow manager such as Airflow
Experience with large scale streaming platforms (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Hadoop) and storage engines (e.g. HDFS, HBase)
A strong DataOps mindset and opinions on next-generation warehousing tools
Bonus Qualifications
Basic fluency in C++
Familiarity with or exposure to experimentation platforms
Compensation
There are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights. The salary will range from $143,000 to $300,000. A sign-on bonus may be part of a compensation package. Compensation will vary based on geographic location, job-related knowledge, skills, and experience.

Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance.

ABOUT ZOOX

Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of artificial intelligence, robotics, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.

Follow us on LinkedIn

Vaccine Mandate
Employees working in this position will be required to have received a vaccine approved by the U.S. Food and Drug Administration and/or the World Health Organization. In addition, employees who are eligible for a COVID-19 booster vaccine (“Booster”) will be required to receive a Booster. Employees will be required to show proof of vaccination status upon receipt of a conditional offer of employment. That offer of employment will be conditioned upon, among other things, an Applicant’s ability to show proof of vaccination status. Please note the Company provides reasonable accommodations in accordance with applicable state, federal, and local laws.

About Zoox
Zoox is developing the first ground-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. Sitting at the intersection of robotics, machine learning, and design, Zoox aims to provide the next generation of mobility-as-a-service in urban environments. We’re looking for top talent that shares our passion and wants to be part of a fast-moving and highly execution-oriented team.

Follow us on LinkedIn

A Final Note:
You do not need to match every listed expectation to apply for this position. Here at Zoox, we know that diverse perspectives foster the innovation we need to be successful, and we are committed to building a team that encompasses a variety of backgrounds, experiences, and skills.",4.0,"Zoox
4.0","Foster City, CA",1001 to 5000 Employees,2014,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
436,Data Engineer,Employer Provided Salary:$125K - $179K,"Who We Are
Babylist is the leading digital destination for growing families. For over a decade, Babylist has been the technology solution for expecting parents and the community that supports them, expanding from baby registry into a full-service platform that helps parents make decisions with confidence, stay connected and build happy and healthy families. Every year Babylist helps over 9M people make purchases through its registry, app, ecommerce shop and comprehensive product guides. The Babylist ecosystem now includes Babylist Health, which provides access to products and services including insurance-covered breast pumps, Expectul, a new voice in health and wellness for pre-pregnancy through postpartum care, and The Push, a branded content studio that works with the biggest companies in the baby space. With over 59M monthly pageviews and 1.2M TikTok followers, Babylist is a generational brand leading the $88 billion baby product industry. To learn more about Babylist's registry options,editorial content and more, visit www.babylist.com
Our Ways of Working
We have team members located across the United States spanning multiple time zones. This means we put in extra effort to make sure we connect and collaborate in ways that make sense for us. We know how valuable the flexibility of remote work is for our employees.
We know that personal connection is the foundation for the great work we do together. In order to build those relationships with team members in other cities, we meet with coworkers in person two times a year at a full company offsite and a departmental offsite. These offsites are expected of employees and are great opportunities to meet the people you work with every day and to do some of the work that is much more difficult to do virtually.
Our Tech Stack
Snowflake
Airflow
DBT
AWS
What the Role Is
The Data Team at Babylist is looking for a Data Engineer to build and scale our core data platform. This work and the role is foundational for our team and strategic for our business. This person is a dedicated professional whose interests lie on the cusp of data engineering, data architecture and DevOps. You will work cross-functionally with our product managers, analytics engineers, data scientists, and business functions. As a Data Engineer, you will be joining a growing data engineering team at Babylist and will directly shape the data vision for the company through your work. You will hone your craft as a data professional while working on our state-of-the-art data stack. You will be responsible for building and scaling data tools for our analysts and data scientists that enable product growth at Babylist.
Who You Are
You have 3+ years hands-on experience writing and deploying production grade code
You have solid experience with a language like Python and familiarity with data orchestration tools like Airflow, dbt, etc
You have a strong understanding of data modeling/ETL principles and modern data warehousing systems like Snowflake
You are a DevOps in disguise: Comfortable with the AWS ecosystem including managing and deploying cloud data resources (EC2, S3, Lambda, EKS, Sagemaker)
How You Will Make An Impact
You will build custom and third-party data pipelines for data ingestion into the data warehouse
You will optimize our data platform as we continue to grow
You will improve data monitoring coverage for complex data flows between multiple systems
You will operationalize ML Models and build internal APIs to surface our machine learning products
You will ensure data quality, monitoring and SLA
Why You Will Love Working At Babylist
We invest in the infrastructure you'll need to be supported and successful: tools, opportunities to connect with colleagues, and a stipend to help you set up your office
We build products that have a positive impact on millions of people's lives
We work at a sustainable pace which means work/life balance is a real thing here
We believe technology and data can solve hard problems
We believe in exceptional management
We are an antiracist organization and doing the work to support differences of all kinds
We offer competitive pay and meaningful opportunities for career advancement
We have great benefits like company paid medical, dental, and vision, a generous paid parental leave policy, and 401k with company match
We care about employee wellbeing with perks for physical, mental and emotional health, parenting, childcare, and financial planning
Babylist takes a market-based approach to pay, and pay may vary depending on your location. Your actual base salary will depend on factors such as your skills, qualifications, experience, and work location.
The estimated pay range for this role is :
$125,000 - $179,000 USD
$126,000 - $183,000 CAD
In addition, Babylist offers equity, bonus, and benefits, including company paid health, dental and vision insurance, 401(k) matching, flexible spending account, and paid leave (including PTO and parental leave) in accordance with our applicable plans and policies.
If your experience is close to what we're looking for, please consider applying. Experience comes in many forms – skills are transferable, and passion goes a long way. We know that diversity makes for the best problem-solving and creative thinking, which is why we're dedicated to adding new perspectives to the team and encourage everyone to apply.

#bi-remote",4.2,"Babylist
4.2","Emeryville, CA",51 to 200 Employees,2011,Company - Private,Other Retail Stores,Retail & Wholesale,Unknown / Non-Applicable
437,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
438,Data Engineer I - People Analytics,$73K - $111K (Glassdoor est.),"POSITION SUMMARY Our People Analytics team is expanding! The Data Engineer I is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.

This position can work in a flexible, hybrid model with 3 days on-site in Appleton, WI and 2 days remote.

JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.

QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming. Preferred experience in Python applied to data engineering and transformation.
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated

#LI-Hybid

Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101 .",4.1,"US Venture
4.1","Appleton, WI",1001 to 5000 Employees,1951,Company - Private,Energy & Utilities,"Energy, Mining & Utilities",Unknown / Non-Applicable
439,Data Engineer,Employer Provided Salary:$120K - $200K,"Verition Fund Management LLC (""Verition"") is a multi-strategy, multi-manager hedge fund founded in 2008. Verition focuses on global investment strategies including Global Credit, Global Convertible, Volatility & Capital Structure Arbitrage, Event-Driven Investing, Equity Long/Short & Capital Markets Trading, and Global Quantitative Trading. As a Data Engineer you would be responsible for building data pipelines and supporting Portfolio Managers and Risk teams.
Responsibilities:
Building data pipelines
Working closely with data vendors such as Bloomberg, Refinitiv, etc…
Taking this vendor data and normalizing/standardizing it or firm consumption
Taking the normalized data and customizing it to user specific needs.
Qualifications:
4+ years of experience in financial services
BS or MS in Computer Science or Computer Engineering
Strong technology/coding skills (Python)
Strong design skills to build extensible config/data-driven platforms
Good Financial data, specifically security master and/or knowledge of vendor datasets
Strong AWS data pipeline skills
Strong database skills – SQL and no-SQL
Strong problem solving skills
Ability work with datasets in Excel and other productivity tools
Nice to Have:
AWS Glue
Spark
Jupyter Notebook

Salary Range
$120,000—$200,000 USD",3.5,"Verition Group LLC
3.5","New York, NY",1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
440,Data Engineer,$98K - $147K (Glassdoor est.),"Greetings Everyone

Who are we?

For the past 20 years, we have powered many Digital Experiences for the Fortune 500. Since 1999, we have grown from a few people to more than 4000 team members across the globe that are engaged in various Digital Modernization. For a brief 1 minute video about us, you can check

https://youtu.be/uJWBWQZEA6o

.

What will you do?

List out the Instructions

What are we looking for?

Seeking for a strong data analyst who can perform independent.

Good to have Prior experience in banking domain is plus and understanding the finance various data transactions.

Should be strong in the below:
Python

Pyspark

SQL

Airflow

Trino

Hive

Offshore Cordination

LLD

Agile Scrum

Linux

Openshift

Kubernentes

Superset

Optional skill set:
Linux

Openshift

Kubernentes

Superset",3.8,"Photon
3.8","Las Vegas, NV",5001 to 10000 Employees,2007,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
441,Senior Data Analytics Engineer,-1,"At Wistia our goal is to help every business to thrive with video. That's a big task, and we're super excited to help the world get there faster through simple and delightful video-focused products.
Wistia's Business Intelligence team is charged with leveraging data to empower strategic, insights-driven decision making across the organization. The Analytics Engineer role is a hybrid role with responsibilities that typically fall to Data Analysts, Data Engineers and Data Scientists. We are a small team and are looking for someone who is comfortable wearing different ""data"" hats. A successful candidate will combine strong technical and communication skills to manage the full data lifecycle at Wistia by creating and maintaining ETL pipelines, automating critical workflows, analyzing data to surface insights and translating those insights into clear recommendations for stakeholders.
The other members of the Business Intelligence team are curious, attentive to detail, impact-focused and approach problem solving with an engineering mindset. The team is centralized (not embedded) with each team member working with data from across the business. We strive to adopt the tools and best practices of the ""modern data stack"". For us that looks like: Redshift, dbt, Fivetran, Mode Analytics, Census, Snowplow Analytics. The Redshift warehouse we maintain serves as the single source of truth for the business.
Key Responsibilities
Maintain our 3rd-Party and custom ETL/reverse-ETL jobs and create new ones as required
Develop new predictive models as required to meet business outcomes
Generate easily interpretable analyses of complex datasets using SQL or python
Create persuasive narratives with data visualizations that enable self-serve exploration
Assist in the design and analysis of A/B testing to optimize business outcomes
Define version-controlled, modular business logic and datasets to be reused across the team
Document your findings, insights and recommendations for stakeholders
Meet business stakeholders ""where they live"" by becoming an expert in the data models and usage of their preferred SaaS tools, eg, Salesforce, HubSpot, Google Analytics, Amplitude, Stripe
Collaborate with:
Team members to share learnings, brainstorm new approaches and define best-practices
Stakeholders to scope, shape and prioritize their data requests
Qualifications
This section should talk about the competencies needed for the role, including qualities. Be sure to seek the input of key stakeholders and make sure we distinguish between must-haves and nice-to-haves because we will be using this to build our job positing and interview plan.
Technical
Highly proficient in SQL and python (or comparable general purpose programming language)
Facile with statistics and hypothesis testing
Skilled at constructing meaningful data visualizations
Comfortable with the command line and bash scripting
Knowledgeable about database architecture, data warehousing principles, REST APIs and basic ML algorithms
Non-technical
Experienced at scoping broad, loosely-defined requests into well-structured plans
Adept at translating data into actionable insights for stakeholders
Strong in written communication
Able to work independently with minimal oversight
Familiar with SaaS metrics and business practices
Values fit (for Wistia at large and BI specifically)
collaborative
learning oriented
attention to detail
simplicity as the starting point starts simple and add complexity only as needed
Key Metrics
This is a great place for more detail on the metrics we expect this person to own, and any that we measure success by.
Product, marketing, sales, and customer success teams have the data and insights they need to make decisions
Product, marketing, sales, and customer success teams have confidence in the data they use
Product, marketing, sales, and customer success teams are able to monitor business performance and spot problem areas and new opportunities
Responses to data requests are timely
Success Looks Like
This section should outline what success looks like. Knowing this helps inform the level and type of candidates we need, interview questions, onboarding plan, and coaching our new hires to success. It also makes it easier to spot performance issues early and hold ourselves accountable in hiring (and learn how we can improve).
30 Days
Complete Wistia onboarding program
Ramp up and learn the tools of the role (Mode, dbt, github, Fivetran, Redshift, Google Analytics, Salesforce, Pendo)
Ramp up and learn Wistia's data architecture, starting with Wistia app database
Ramp up and learn data analysis best practices and procedures
Complete ~5 basic data requests
90 Days
Confident in use of Mode, dbt, and GitHub for data analysis process
Confident in data architecture and able to self-source most data tables
Regularly handling basic data requests
Completed 1 complex data investigation
1 Year
Regularly initiating new data investigations and unearthing new insights
Regularly making improvements to our approach to data presentation
Regularly making improvements to our data models and data cleanliness
Working at Wistia
We try to ensure Wistia is an inclusive and diverse place where everyone feels happy, fulfilled, respected, comfortable, and welcome. We're proud to be an equal-opportunity workplace. We care a lot, so our benefits are actually benefits, not just the fun stuff like swag and snacks in the office (though we also have lots of those too!).
We know the biggest investment we can make is in our employees, so we provide:
A competitive compensation package that includes internal equity stock options
401k with 3% company contribution, regardless of whether you contribute
Fully paid healthcare, dental, and vision insurance (family plans included)
Pre-Tax FSA and Dependent Care Account
Fitness reimbursement
Flexible working hours – work at the times when you operate best, or set aside time for child and/or elderly care responsibilities during traditional ""office hours""
16 weeks paid parental leave for all new and expecting parents
Unlimited PTO (pretty common for people to take 4+ weeks off throughout the year)
Remote-first culture (work from anywhere in the U.S.)
Annual professional development stipend (courses, conferences, and more)
New hire bonus to enhance your home office set up
Pet insurance discount
Location/Remote Opportunities
Wistia is a remote-first company. Employees can work from our beautiful office in Cambridge, MA, or anywhere in the continental US",4.8,"Wistia
4.8","Cambridge, MA",51 to 200 Employees,2006,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
442,Sr. Data Engineer- Contractor- Remote Work Eligible,-1,"Notes to applicants:
This position is eligible for full-time remote work in Texas, or North Carolina, or, in the alternative, to work in accordance with Dimensional’s best-of-both hybrid working model, which involves working in the office on Tuesdays, Wednesdays and Thursdays, and choosing to work in the office or remotely on Mondays and Fridays.
Resumes and portfolios (when applicable) are required as part of your application. When applying from a mobile device or tablet, you may not be able to attach a resume. If you cannot include an attachment at the time of your application, you will receive a follow up email asking you to attach your resume from a computer.
Here at Dimensional, we strive to be an inclusive workplace for all. Even if you do not match every qualification listed, if you are interested in who we are, what we do, and why we do it, we suggest and encourage you to apply.

Job Description:
About Dimensional:
Dimensional was built around a set of ideas bigger than the firm itself. With a confidence in markets, deep connections to the academic community, and a focus on implementation, we go where the science leads, and continue to pursue new insights, both large and small, that can benefit our clients.
The Technology Department at Dimensional leverages the rapidly evolving state of the art to engineer scalable, innovative, and research driven solutions to improve our client’s financial lives.
Software Engineers at Dimensional participate in the design and development of software solutions across an array of domains from Research and Investments to Sales and Marketing; collaboratively developing MVPs to test their ideas and rapidly iterate with constant feedback from users. Dimensional invests heavily in developer tools, platforms, paradigms and experience enabling teams to provide modern solutions that contribute profoundly to our client’s success.
We are looking for a Python Data Engineer to join our team and translate our customers’ goals into working software throughout the stack from automated configurations to model definitions, calculation APIs, and building robust data pipelines. The most important qualifications are a passion for quality software and enthusiasm for learning new technologies and approaches. The level of seniority for this position is negotiable based on experience.
You may be a fit for this role if you:
Are open-minded, curious, and resourceful
Are passionate about/stay current with modern technologies
Solve problems systematically and transparently
Share ideas, solicit/integrate feedback, design and solve collaboratively
Take a software engineering approach and demonstrate automation and security mindsets
What you might work on:
As a Data engineer at Dimensional, you will have the opportunity to understand the users’ needs and solve problems at all levels of the stack from automating infrastructure and deployments to building complex data pipelines to designing user friendly data applications.
Collaborate with subject matter experts in a variety of areas to drive the success of our clients
Perform software and data architecture and design
Develop complex software solutions using ETL and/or back-end technologies
Demonstrate and mentor software engineering best practices and participate in code reviews
Develop configurations and automations to enable testing, infrastructure and deployments
The successful candidate will be self-motivated and have a strong drive for learning and self-improvement.
Qualifications:
Bachelor’s degree in a technical field or equivalent practical experience.
5-10+ years of software development experience in a professional and/or academic setting (seniority of the role is negotiable).
5+ years of hands-on experience in developing ETL solutions using python.
Working knowledge of DevOps concepts, tools, and continuous delivery pipelines such as Octopus, TeamCity, Stash, Bitbucket, Jira, GIT, etc.
Advanced SQL knowledge and experience working with relational databases and working familiarity with various cloud data warehouses.
Experience in building processes supporting data transformation, data structures, metadata, dependency, and workload management.
Experience with data pipeline and workflow management tools: Airflow, etc.
Preferred Competencies:
Interest and ability to learn other coding languages as needed
Ability to write in English fluently and idiomatically
Advanced degree or equivalent experience in engineering, computer science or other technical related field
Experience with agile/scrum methodologies
Financial services industry experience
Experience with any of the following:
Redis, Postgresql, MongoDB, SQLServer
Airflow, Kafka, AWS, serverless/microservice architecture
TDD, BDD, Numpy/Scipy/Pandas, Ansible

#LI-Remote

Dimensional offers a variety of programs to help take care of you, your family, and your career, including comprehensive benefits, educational initiatives, and special celebrations of our history, culture, and growth.
It is the policy of the Company to provide equal employment opportunity for all applicants and employees. The Company does not unlawfully discriminate on the basis of race, color, religion, creed, sex, gender, gender identity, gender expression, national origin, age, disability, genetic information, ancestry, medical condition, marital status, covered veteran status, citizenship status, sexual orientation, or any other protected status. This policy applies to all areas of employment including recruitment, hiring, training, job assignment, promotion, compensation, benefits, transfer, discipline, termination, and social and recreational programs.",3.7,"Dimensional Fund Advisors
3.7",Texas,1001 to 5000 Employees,1981,Company - Private,Investment & Asset Management,Financial Services,$1 to $5 billion (USD)
443,Data Engineer,$78K - $111K (Glassdoor est.),"AG Grace is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD. 75% onsite
Essential Job Responsibilities
The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past.
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation.
The candidate will work both independently and as part of a large team to accomplish client objectives.",4.0,"AG Grace Inc
4.0","Columbia, MD",51 to 200 Employees,-1,Company - Private,-1,-1,Unknown / Non-Applicable
444,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
445,Data Engineer IV,Employer Provided Salary:$126K - $180K,"Your Opportunity

At Schwab, you’re empowered to make an impact on your career. Here, innovative thought meets creative problem solving, helping us “challenge the status quo” and transform the finance industry together.

Deliver and maintain semantic data models and SAP BO Universes leveraging best practices. Collaborate directly with business and Product Owner stakeholders to define future-state business capabilities & requirements. Convert SAP BO applications from legacy Teradata databases to Google Big Query. Partner with solution architects and Business System Analysts to design and deliver self-service BI capabilities to the business. Ensure projects are progressing according to plan by resolving issues or obstacles. Build deep understanding of data sources required to support our business units. Leverage 360Suite to perform regression testing and manage metadata for SAP BO applications.

Must be available to work in office two days a week. Work from home is acceptable three days a week. Subject to Schwab’s internal approach to workplace flexibility.
What you have

Job Requirements: Bachelor’s degree (or foreign equivalent) in Computer Science or related field and five (5) years of progressive, post-baccalaureate experience in Business Intelligence or related role. SKILLS: Experience and/or education must include the following: SQL on Teradata, Oracle, SQL Server and Google Big Query platforms; Develop SAP BO Universes, Webi reports and dashboards; Design, build, and support Tableau dashboards; and Develop relational, non-relational, and dimensional data models.

Charles Schwab & Company, Inc. seeks Data Engineer IV in Westlake, TX.",3.8,"Charles Schwab
3.8","Westlake, TX",10000+ Employees,1973,Company - Public,Investment & Asset Management,Financial Services,$10+ billion (USD)
446,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Data Engineer
Candidate must have AWS (Redshift, Glue, S3), Spark, Phython, Scala and Should be expert in SQL
Basic qualifications
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
Design, implementation and management of data processing systems dealing with large data sets.
Proficiency in, at least, one modern programming language such as Python, Java or Scala
Hands-on experience and advanced knowledge of SQL.
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and cloud computing
Experience working with Open-Source Big Data tools (Hadoop, Spark, Hive, Presto and etc.)
Preferred qualifications
Experience in working and delivering end-to-end projects independently
Relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics
Experience working with AWS Big Data Technologies (AWS Glue, EMR, Athena, Ec2, Redshift and S3 etc.)
Experience working with Oracle, PostgreSQL, SQL Server or MySQL.
Proven track record of delivering a big data solution
Experience working with both Batch and Real Time data processing systems
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Expected hours: 40 per week
Compensation package:
1099 contract
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Austin, TX 78703: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 8 years (Required)
Python: 8 years (Required)
AWS: 5 years (Preferred)
Work Location: Hybrid remote in Austin, TX 78703",-1,PRISM IT LLC,"Austin, TX",-1,-1,-1,-1,-1,-1
447,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
448,USA - Infrastructure Data Engineer (AWS),$104K - $148K (Glassdoor est.),"Job Title: Infrastructure Data Engineer (AWS)
Location: San Jose, California, United States
Type: Fulltime

The candidate has to be in San Jose, CA and it is 100% onsite job”.

Roles & Responsibilities:
Design, build and maintain data platform infrastructure on AWS environment.
Oversee design, build, and maintain data platform infrastructure on AWS environment.
Develop data pipelines to collect the metrics that is required to monitor data refreshes, reports deliveries and track SLAs.
Work with the DWH development team and business users in establishing SLAs for data refreshes and reports.
Build continuous integration/deployment (CI/CD) pipelines to accelerate development and improve team agility.
Oversee project.
Monitor all aspects of data platform system security, performance, storage, incidents, and usage for databases, data pipelines, applications, and infrastructure on AWS. Escalate to respective teams for fixes.
Ensure data pipelines meet intraday and daily SLAs, as per documented SLA definitions and escalate accordingly.
develop appropriate instrumentation to collect metrics on system performance, cost, data ingress/egress /storage processes.
Have a clear understanding of the reports/analyses/insights to be driven by data and build data driven solutions to optimally support the operational analytics needs.
Documents user stories, epics, and reports
Coordinate infrastructure enhancements and maintenance with the system/network engineering teams
Work with DWH development team and analytics team to do manual releases where required.
Onboard users to data analytics systems with appropriate approvals
Conduct system performance tests and collect metrics. Tune/add capacity.
Complete knowledge management processes
Own strategy and communicate potential major shifts in expected workload based on business, market, or operational changes.
Operate ongoing business relationship management sessions to review operational metrics, understand pain points, identify upcoming projects and engagement efforts.",-1,Avestacs,"San Jose, CA",51 to 200 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
449,Senior Data Replication Engineer – Disaster Recovery (100% Remote),Employer Provided Salary:$81K - $135K,"OUTGROWN YOUR OWN BACKYARD? COME PLAY IN OURS.

At Columbia, we’re as passionate about the outdoors as you are. And while our gear is available worldwide, we’re proud to be based in the Pacific Northwest, where natural wonders are our playground.

Every product we make and every task we undertake is inspired by the famous words of our founder Gert Boyle: “It’s perfect. Now make it better.” As pioneers of relentless improvement, we are constantly evolving.

We believe the outdoors is ours to protect and strive to keep our planet healthy. We believe in empowering people to experience the outdoors to the fullest.

And we believe in you.
ABOUT THE POSITION
The infrastructure team is adding additional team members as Senior Data Replication Engineer with a focus on data resiliency across On Premises and Cloud.
This role will be managing our core database platforms, from On Premises SQL to Cloud data platforms to support our Multi-Cloud environments. The role will require building a deep understanding of Columbia systems and knowledge of how data is used by our internal customers while supporting the performance and recovery of said teams. Additionally, experience with global enterprise storage and replication systems is highly preferred.
HOW YOU’LL MAKE A DIFFERENCE
Manage and maintain our enterprise databases, including MSSQL, MySQL, MongoDB, CosmosDB, and Azure cloud SQL.
Enable efficient operations of the data platforms, while meeting requirements for database stakeholders, technical delivery and operational teams
Work with InfoSec teams to influence data security and data lifecycles within the data platforms
Support and implement backup and recovery plans for data platforms which meet requirements and targeted Service Levels
Refine and manage data platform monitoring that can be used by operational staff
Support platform capacity to ensure optimal performance and scalability.
Work closely with development teams to support database changes, deployments, and lifecycle of the platform
Troubleshoot escalated database issues with an eye towards preventing the issue via design or process changes
Participate in on-call rotation to provide 24/7 support for critical database systems.
YOU ARE
Self-motivated, initiative, Data driven, curious with a strong sense of collaboration.
Constantly learning new technology, skills and practices for a wide range of data products
Strong at solving complex problems with the ability to work independently and as part of a team.
Agile ways of working and mindset
YOU HAVE
Bachelor's degree in Computer Science or related field or equivalent work experience.
6 plus years of experience as a DBA or Engineer
Knowledge of resilient and efficient data platforms to enable Disaster Recovery and Business Continuity
Knowledge of performance tuning, monitoring, and capacity planning for databases.
Excellent analytical and problem-solving skills
Strong written and verbal communication skills.
Bonus knowledge:
Experience with enterprise storage and replication systems
Experience with cloud-based database platforms
Experience with MySQL, MongoDB, and CosmosDB.
Salary Range: 81,000.00 - 135,200.00 USD Annual
**Pay decisions are determined by multiple factors, including what the market is paying, a candidate’s capabilities and skills, years of experience, and internal equity.
State Exclude from Remote:
Alabama Arkansas Delaware Florida Georgia Hawaii Indiana Iowa Louisiana Maryland Mississippi Missouri Ohio Oklahoma Pennsylvania South Carolina Tennessee Virginia
#LI-CN1
#LI-Remote
Columbia Sportswear Company and our portfolio of brands, including Columbia, SOREL, Mountain Hardwear and prAna, know a thing or two about adventures. After all, we've been on one since 1938, working to perfect the art of enjoying the outdoors. Behind everything we make is an employee who's found that the greatest adventure starts with joining a company that strives to do the right thing.
This job description is not meant to be an all-inclusive list of duties and responsibilities, but constitutes a general definition of the position's scope and function in the company.
At Columbia Sportswear Company (CSC), we are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants and teammates without regard to race, color, religion, sex, pregnancy (including childbirth, lactation and related medical conditions), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, military and veteran status, and any other characteristic protected by applicable law. CSC believes that diversity and inclusion among our teammates is critical to our success as a global company, and we seek to recruit, develop and retain the most talented people from a diverse candidate pool. All employment is decided on the basis of qualifications, merit, and business need.",3.8,"Columbia Sportswear Company
3.8","Salem, OR",5001 to 10000 Employees,1938,Company - Public,Consumer Product Manufacturing,Manufacturing,$1 to $5 billion (USD)
450,"Data Engineer (AWS, Python)",Employer Provided Salary:$50.00 - $52.00 Per Hour,"Title: Data Engineer (AWS, Python)
Location: Remote
Duration: 12+ Months
Job Description:
Typical task breakdown:-
Daily standup, collaborate with peers, work independently to complete sprint commitments, participate in agile ceremonies.
Interaction with team:-
Daily stand up meetings with team – working mainly with Helios Platform adoption team.
Work environment:
Education & Experience Required:
Bachelor's in computer science required plus 7+ Working experience .
Masters degree with 6+ years working expeirence acceptable as well.
Preferred: AWS cloud certifications
Technical Skills
(Required)
Exp with Object oriented programming - Python (4+ years)
AWS (3+ years overall exp) lambda and/or glue, EMR, S3
SQL (3+ years)
2+ years exp in test driven development, unit test, integration testing, regression testing
(Desired)
Event driven framework experience.
Kinesis or Kafka experience
Familiar with working in Agile methodology
Soft Skills
(Required)
Strong written and verbal communication, experience working as a member of agile engineering team, ability to commit to and deliver work within allocated sprint window.
Job Type: Contract
Pay: $50.00 - $52.00 per hour
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
SQL: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",5.0,"DSMH LLC
5.0",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,$1 to $5 million (USD)
451,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
452,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
453,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
454,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
455,Data Engineer,Employer Provided Salary:$75.00 Per Hour,"8+ years of experience with Oracle and SQL Server databases
• 8+ years of experience with Informatica Power Center.
8+ years software development experience in one or more of the following areas: ETL, BI and DW
• 8+ years of experience in DW Multi-Dimensional modelling.
Understanding of the Project Delivery Framework and SDLC
Understanding of Waterfall and Agile Project Management Methodology
Strong analytical, communication and interpersonal skills.
Excellent and proven collaboration skills
Job Type: Full-time
Pay: $75.00 per hour
Expected hours: 40 per week
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Monday to Friday
Experience:
Informatica PowerCenter: 5 years (Required)
Multi-Dimensional modelling: 3 years (Required)
Work Location: Hybrid remote in New York, NY 10001",-1,Cybotic System,"New York, NY",-1,-1,-1,-1,-1,-1
456,Data Engineer,$73K - $101K (Glassdoor est.),"JMT Technology Group, a leading geospatial and information technology solutions provider, is looking for a responsible for architecting, documenting, and developing data architectures models and business reporting applications, providing technical leadership for database development efforts. Conducts complex data mapping and Extract, Transform and Load (ETL) efforts.
Essential functions and responsibilities
Conduct complex data mapping efforts between systems in a clear and well defined manner
Perform data analysis and develop the data models to support ETL development
Complete complex data migrations (ETL) processes to support development efforts
Collaborate with Software Architects, Software Developers, and BI Developers to design appropriate solutions for our internal team and external clients.
Mentor software developers in data architecture and database design best practices
Perform and assign database development tasks of medium complexity across multiple projects.
Work directly with third party solutions to design, document, and develop data integrations.
Create, contribute to, review and collaborate on data and database designs and implementations.
Work with IT to deploy solutions
Innovate and contribute to improving development standards, techniques, tools, and processes
Train and test for industry certifications
Identify, set, monitor, and achieve individual goals
Leverage mentorship and peer relationships to increase proficiency in development
Provide support to team members
Participate in the development personnel interview process
Collaborate with project managers, business systems analysts, UX designers, and application developers to deliver high-quality deliverables
Support production systems as urgent and critical issues arise; including non-business hours support on a rotating basis.
Nonessential functions and responsibilities
Perform other related duties as assigned



Required Skills
Excellent verbal and written communication skills



Required Experience
Bachelor’s Degree in computer science, Information Technology, or related fields
3+ years of experience developing data/database systems and integrating with third party solutions.
3+ years of Extract, Transform and Load (ETL) experience
2+ years of experience architecting data/database solutions and designing the integration with third party solutions.
3+ years of experience working in a hybrid agile development methodology
Preferred Experience
Experience with Microsoft’s Azure Technologies
Experience with a variety of databases and ETL solutions
Experience with Databricks
Working Conditions
Work is performed within a general hybrid office environment. Work is generally sedentary in nature, but may require occasional standing and walking. Lighting and temperature are adequate and there are no hazardous or unpleasant conditions caused by noise, dust, etc. within the office environment.
Job Competencies
Analytical Thinking/Problem Solving
Communication
Building Relationships and Teamwork
Results-Oriented
JMT is an Equal Opportunity Employer M/F/Disability/Vet/Sexual Orientation/Gender Identity, and we are proud to be building an inclusive and diverse workforce.
#LI-KW1",4.0,"Johnson, Mirmiran, and Thompson Inc.
4.0","Philadelphia, PA",1001 to 5000 Employees,-1,Private Practice / Firm,Architectural & Engineering Services,"Construction, Repair & Maintenance Services",$100 to $500 million (USD)
457,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
458,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
459,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
460,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
461,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
462,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the organization.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department. The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
463,Data Engineer,-1,"Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore.

Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office.
The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus.
Position Responsibilities
Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities
Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization
Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process
Qualifications & Desired Skills
Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance
Minimum 2 years' of development experience using either Python, C#/Java or C++
Minimum 2 years' years of database/SQL experience
Familiarity with object-oriented programming concepts
Must be comfortable with development across the application stack
Ability to complete complex projects independently
Detail-oriented with strong verbal and written communication skills
Ability to work effectively in a high-energy, time sensitive team environment
Interest or prior experience in the financial trading industry (particularly in fixed income) a plus
This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education.
Base Salary for this role is expected to be between:
$110,000—$150,000 USD",4.4,"Garda Capital Partners
4.4","New York, NY",51 to 200 Employees,-1,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
464,Data Engineer,-1,"SalesIntel is the top revenue intelligence platform on the market. Our unique approach to data collection, enhancement, verification and growth distinguishes us from the market, solidifying our position as the best B2B data partner.

The Data Engineer will be responsible for building and extending our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys working with big data and building systems from the ground up.

You will collaborate with our software engineers our software engineers, database architects, data analysts and data scientists to ensure our data delivery architecture is consistent throughout the platform. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What You’ll Be Doing:
Design and build parts of our data pipeline architecture for extraction, transformation, and loading of data from a wide variety of data sources using the latest Big Data technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with machine learning, data, and analytics experts to drive innovation, accuracy and greater functionality in our data system.
Qualifications:
Bachelor's degree in Engineering, Computer Science, or relevant field.
5+ years of experience in a Data Engineer role.
3+ years experience with Apache Spark and solid understanding of the fundamentals.
Deep understanding of Big Data concepts and distributed systems.
Strong coding skills with Scala, Python, Java and other languages and the ability to quickly switch between them with ease.
Advanced working SQL knowledge and experience working with a variety of relational databases such as Postgres and MySQL.
Experience working with data stored in many formats including TSV, CSV, JSON, Parquet.
Cloud Experience with DataBricks or similar service: S3, Athena, EC2, EMR, RDS, Redshift.
Comfortable working in a linux shell environment and writing scripts as needed.
Comfortable working in an Agile environment
Machine Learning knowledge is a plus.
Must be capable of working independently and delivering stable, efficient and reliable software.
Excellent written and verbal communication skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

This is a remote position.",-1,"SalesIntel Research, Inc.",Remote,51 to 200 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
465,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
466,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
467,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
468,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
469,Data Engineer,$82K - $112K (Glassdoor est.),"Overview
Trissential is a trusted partner for end-to-end quality services and management consulting for digital transformation. As a part of our parent company, Expleo, we are a global organization partnering with major corporations and leading non-profits in over 30 countries. Guided by our mission and values, Trissential puts people at the heart of our organization.
Come join an experience! Add your talent to a team of forward-thinking game changers that make an impact by driving innovative solutions.
Trissential is currently seeking a Data Engineer to join our dynamic team in Minneapolis, MN (Remote).
Responsibilities
Position Summary:
A Data Engineer is a professional responsible for designing, building, and maintaining the client’s data infrastructure that supports data-centric operations, and decision-making.
The Data Engineer is responsible for creating and maintaining data pipelines, data storage systems, and data processing systems, as well as ensuring data is accurate, secure, and easily accessible.
Primary Responsibilities and Accountabilities:
Data pipeline and workflow development: This includes designing and implementing client data pipelines and workflows to ensure that data is being collected, processed, and stored in an efficient and timely manner. Understand and incorporate data quality principals that ensure optimal performance, impact, and user experience.
Data storage and processing: This includes designing and implementing data storage systems and developing and maintaining code to process and analyze data.
Data integration: This includes working with the client on designing data integration strategies, integrating data from multiple sources, and ensuring data quality.
Data security: This includes defining and implementing client data security policies and procedures, including access control and data encryption.
Data governance: This includes defining data governance policies and procedures, monitoring data quality, and ensuring compliance with data privacy regulations.
Performance tuning: This includes optimizing data access, indexing, and query performance to ensure the data infrastructure can scale as required.
Monitoring and troubleshooting: This includes monitoring data systems and troubleshooting any issues that arise, identifying and resolving performance bottlenecks, and working with other team members to resolve any problems.
Collaboration and Communication: This includes working closely with client’s Data Architects, Data Analysts, Business Intelligence and Data Science team members, and other stakeholders to understand their data needs and develop solutions that meet those needs while adhering to industry best practices.
Qualifications
Extensive experience in ETL, preferably with transitioning from legacy systems to more advanced ones.
Strong data analysis skills, with the ability to identify areas of improvement in the current process.
In-depth knowledge of automation strategy development and implementation.
Ability to work in an advisory capacity, providing insights and recommendations to improve data management capabilities.
Excellent problem-solving skills to identify potential errors and propose solutions to enhance efficiency.
Skills
Works independently or well within a team
Wants to continuously grow knowledge base and skill set
Collaborative, consultative mindset
Works well in a fast paced environment
Strong technical background
Deep knowledge and curiosity about technology and systems
Agile mindset
Job Type: Contract
Work Location: Remote",4.1,"Trissential
4.1","Minneapolis, MN",201 to 500 Employees,2003,Subsidiary or Business Segment,Business Consulting,Management & Consulting,$25 to $100 million (USD)
470,Big Data Engineer,Employer Provided Salary:$48.00 - $52.00 Per Hour,"Greetings from KonnectingTree Inc.,
We are actively looking for a candidate with below mentioned experience,
Role : Bigdata Engineer
Location : Hybrid ( Bloomington, MN)
Experience : 3-5 years
Job Description:
* Good experience in build and deployment of Big data applications using Pyspark
* Good experience in Python.
* Experience in Hadoop file structure.
* Good Communication and team player
Job Type: Contract
Salary: $48.00 - $52.00 per hour
Experience level:
3 years
4 years
Schedule:
8 hour shift
Ability to commute/relocate:
Bloomington, MN 55425: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",-1,KONNECTINGTREE INC,"Bloomington, MN",-1,-1,-1,-1,-1,-1
471,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
472,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
473,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
474,AWS Data Engineer,Employer Provided Salary:$60.00 - $75.00 Per Hour,"Title: AWS Data Engineer
Location: Columbus, OH (Day 1 onsite / Hybrid Model)
Duration: Contract
Job Description:
Data Engineer: Open positions – 2
Skills:
Must Have:
BS/BA degree or equivalent experience
General: Strong organizational, problem-solving, and critical thinking skills; Strong documentation skills
Coding: Proficiency in Java
• Cluster Computing frameworks: Proficiency in Spark and Spark SQL
• AWS Data Services: Proficiency in Lake formation, Glue ETL (or) EMR, S3, Glue Catalog, Athena, Kinesis (or) MSK, Airflow (or) Lambda + Step Functions + Event Bridge
Data De/Serialization: Expertise in at least 2 of the formats: Parquet, Iceberg, AVRO, JSON-LD
AWS Data Security: Good Understanding of security concepts such as: Lake formation, IAM, Service roles, Encryption, KMS, Secrets Manager
Good to Have:
• Linux Scripting, Jenkins
• DevOps: Git, CI/CD, JIRA, TDD
Job Type: Contract
Salary: $60.00 - $75.00 per hour
Experience level:
10 years
11+ years
Ability to commute/relocate:
Columbus, OH 43085: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Required)
Java: 4 years (Required)
Data warehouse, Glue and SparkSQL: 8 years (Required)
Work Location: In person",4.5,"Synkriom Technology Pvt. Ltd.
4.5","Columbus, OH",201 to 500 Employees,2015,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
475,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
476,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
477,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
478,Cloud Data Engineer,Employer Provided Salary:$69.61 - $80.00 Per Hour,"Top skills:
Must-Haves (Concepts & Tools):
AWS cloud—KMS, S3, Glue, Lambda etc.
Deployed data pipelines
Java, Python or PySpark hands on development experience
Nice-to-Haves (Concepts & Tools):
Prior ETL migration experience from on prem. To cloud
Exposure to even driven streaming
Job Types: Full-time, Contract, Temporary
Pay: $69.61 - $80.00 per hour
Experience level:
7 years
8 years
9 years
Work Location: Remote",-1,Cloudrex,Remote,-1,-1,-1,-1,-1,-1
479,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
480,AWS Data Engineer/ Architect,Employer Provided Salary:$130K - $160K,"Working knowledge and experience with AWS Services including EMR, DynamoDB, Kinesis Data Stream, Lambda, S3, Salesforce
Focus on AWS and Salesforce – Sales & Service cloud.
Mastery and hands-on experience with Data Engineering technologies and scripting languages.
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Intimately involved in numerous end to end cloud migration projects.
Strong knowledge and experience in designing for and implementing solutions in the cloud (AWS). This can range from custom code on IaaS/PaaS to SaaS solution integrations.
Languages: Java, Angular, node.js, Python
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Working knowledge of application architectures, software development tools, and methodologies.
History of successful technical consulting, and architecture engagements with numerous customers simultaneously.
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Washington, DC 20001",-1,Omniware LLC,"Washington, DC",-1,-1,-1,-1,-1,-1
481,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
482,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
483,Data Engineer,Employer Provided Salary:$75.00 Per Hour,"8+ years of experience with Oracle and SQL Server databases
• 8+ years of experience with Informatica Power Center.
8+ years software development experience in one or more of the following areas: ETL, BI and DW
• 8+ years of experience in DW Multi-Dimensional modelling.
Understanding of the Project Delivery Framework and SDLC
Understanding of Waterfall and Agile Project Management Methodology
Strong analytical, communication and interpersonal skills.
Excellent and proven collaboration skills
Job Type: Full-time
Pay: $75.00 per hour
Expected hours: 40 per week
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Monday to Friday
Experience:
Informatica PowerCenter: 5 years (Required)
Multi-Dimensional modelling: 3 years (Required)
Work Location: Hybrid remote in New York, NY 10001",-1,Cybotic System,"New York, NY",-1,-1,-1,-1,-1,-1
484,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
485,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
486,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
487,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
488,Big Data Engineer,Employer Provided Salary:$48.00 - $52.00 Per Hour,"Greetings from KonnectingTree Inc.,
We are actively looking for a candidate with below mentioned experience,
Role : Bigdata Engineer
Location : Hybrid ( Bloomington, MN)
Experience : 3-5 years
Job Description:
* Good experience in build and deployment of Big data applications using Pyspark
* Good experience in Python.
* Experience in Hadoop file structure.
* Good Communication and team player
Job Type: Contract
Salary: $48.00 - $52.00 per hour
Experience level:
3 years
4 years
Schedule:
8 hour shift
Ability to commute/relocate:
Bloomington, MN 55425: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",-1,KONNECTINGTREE INC,"Bloomington, MN",-1,-1,-1,-1,-1,-1
489,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
490,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
491,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the organization.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department. The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
492,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
493,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
494,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
495,Energy Data Engineer - Mid Level,$81K - $121K (Glassdoor est.),"About the Organization
Now is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.

The Data Engineer will support Redhorse in its execution of specific task areas related to the Federal Government Carbon Pollution Free Electricity (CFE) Technical and Strategy Support contract. The program will establish technical frameworks and analytical tools and methods to inform CFE procurement decisions for the entirety of the federal government. Work will be conducted primarily at Redhorse office locations (hybrid remote work) and on government client site as required. The ideal candidate will live in the DC/Virginia/Maryland area and willing to undergo a background check to obtain a Public Trust.
Primary Duties and Responsibilities for this position include
Review and analyze the US Federal Government Executive Agencies electric load data
Develop and test modeling techniques to understand an agency’s hourly load profile to inform the agency’s hourly CFE procurement requirement as an off taker from a new CFE technology source.
Support the configuration and ingestion of designated structured, unstructured, and semi-structured data repositories into capabilities that satisfy client requirements and support a data analytics pipeline to drive rapid delivery of functionality to the client.
Maintain all operational aspects of data transfers while accounting for the security posture of the underlying infrastructure and the systems and applications that are supported and monitoring the health of the environment through a variety of health tracking capabilities.
Automate configuration management, leverage tools, and stay current on data extract, transfer, and load (ETL) technologies and services.
Work under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.
Apply comprehension of data engineering-specific technologies and services, leverage expertise in databases and a variety of approaches to structuring and retrieving of data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically using vendor consoles.
Engage with multiple functional groups to comprehend client challenges, prototype new ideas and new technologies, help to create solutions to drive the next wave of innovation, and design, implement, schedule, test, and deploy full features and components of solutions.
Identify and implement scalable and efficient coding solutions.
Minimum Requirements for skills, Experience, and Credentials
Master’s Degree in Engineering, Computer Science, Mathematics or Statistics
Four (4) years of experience in enterprise data analysis with a preferred focus on utility data, to include advanced metering infrastructure (AMI) electricity data
Local to Washington D.C. metro area preferred, remote work is an option
Willingness to travel in support of on-site customer meetings
Willingness to obtain a Public Trust
Desired Basic Requirements for Skills, Experience, and Credentials include
Knowledge of U.S. Federal Government Energy Legislation and Executive Orders (especially EO 14057)
Experience in or knowledge of utility interval data and electricity pricing
Executive Order Requirement:
While currently on HOLD, Redhorse recognizes that we may need to comply with Executive Order 14042 which requires Federal Contractors to ensure all U.S. new hires be fully vaccinated for COVID-19. As required by the Executive Order, Redhorse will work in coordination with applicable contract agencies to consider requests for Reasonable Accommodations.
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.
EOE/M/F/Vet/Disabled",3.5,"Redhorse
3.5","Arlington, VA",51 to 200 Employees,2008,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
496,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
497,Sr. Data Engineer,Employer Provided Salary:$84K - $191K,"Job Summary:
The primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver modules, stable application systems, and Data or Platform solutions. This includes developing, configuring, or modifying complex integrated business and/or enterprise infrastructure or application solutions within various computing environments. This role facilitates the implementation and maintenance of complex business and enterprise Data or Platform solutions to ensure successful deployment of released applications.
Minimum Qualifications
Bachelor's Degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)
5 years of experience in Data, BI or Platform Engineering, Data Warehousing/ETL, or Software Engineering
4 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)
Preferred Qualifications
Master's Degree in Computer Science, CIS, or related field
5 years of IT experience developing and implementing business systems within an organization
5 years of experience working with defect or incident tracking software
5 years of experience writing technical documentation in a software development environment
3 years of experience working with an IT Infrastructure Library (ITIL) framework
3 years of experience leading teams, with or without direct reports
5 years of experience working with source code control systems
Experience working with Continuous Integration/Continuous Deployment tools
5 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutions
Job Type: Full-time
Salary: $84,002.64 - $190,979.59 per year
Experience:
Data, BI or Platform Engineering, Data Warehousing/ETL: 3 years (Preferred)
developing and implementing business systems: 3 years (Preferred)
Data Engineer: 4 years (Preferred)
Work Location: In person",-1,Market Tree Research,"Carolina, WV",Unknown,-1,Company - Private,-1,-1,Unknown / Non-Applicable
498,Sr. Data Engineer,-1,"IntegraConnect is helping to improve patient outcomes by empowering physicians to have deep insight into the result of prescribed treatment plans. We are looking for a skilled data engineer to help us build new solutions, and improve on existing solutions that we offer to our customers. If you love working with data and are passionate about writing clean, maintainable, and testable code, we'd love to talk to you. We've got some exciting things coming up that we're positive you'll love to hear about!
Job Description:
A passion for writing clean and maintainable code.
The ability to understand software requirements and provide feedback on the impact on existing functionality
Participates in Scrum ceremonies with product management and architects and provides feedback on requirements and product milestones
Helps to establish coding and architectural standards and adheres to these standards
Enjoys working solo or pair programming to build assigned solutions.
Reviews pull requests and provide feedback for other developers on the team
Creates integration test scenarios and root cause analyses to replicate bugs and assist with debugging
Takes ownership of parts of the application you directly work on
Provides support for customer questions on new functionality or for escalations of software issues
Minimum of 7 years experience writing SQL and stored procedures
Minimum of 7 years experience in software engineering
BS degree in computer science or related field, or equivalent experience
Intermediate skill with engineering tools (IDEs, social, diagraming, compilation, build, testing)
Mastery in 1 or more TIOBE-Top-10 programming languages (SQL is preferred)
Demonstrable knowledge of ETL/Data Warehousing concepts
Excellent written/verbal skills and professional communication
Intermediate skill with contemporary engineering processes (e.g. agile, scrum, Kanban)
Benefits:
Integra Connect, LLC provides a comprehensive benefits plan
Medical/Dental/Vision Insurance beginning the 1st of the month following your date of hire
Paid Time Off
401k with employer match
Paid Holidays and Floating Holiday
Equal Opportunity Employer",-1,Integra Connect Careers,United States,-1,-1,-1,-1,-1,-1
499,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
500,Senior Data Engineer,$115K - $151K (Glassdoor est.),"Senior Data Engineer
Location: Various
Raleigh NC; Atlanta GA; Philadelphia PA; Charlottesville VA; others considered;
Hybrid, Remote, Travel required
Elder Research, Inc. (ERI) is a Data Science consulting firm specialized in providing analytic solutions to clients in Commercial and Government industries. The Commercial business unit is seeking to hire a Senior Data Engineer with strong engineering skills who will provide technical support across multiple project teams by leading, designing, and implementing the software and data architectures necessary to deliver analytics to our clients, as well as providing consulting and training support to client teams in the areas of architecture, data engineering, ML engineering and/or related areas. We are trusted advisors to our clients, building lasting relationships and partnering as preferred analytics providers. We use a variety of programming languages and tools to create analytic solutions, often fitting within our clients’ environment and needs.
We are looking to hire a Senior Data Engineer who can support robust and repeatable data manipulation, large-scale infrastructure for data ingestion, and data visualization for custom client applications. A Senior Data Engineer works collaboratively with Business Analysts, Data Scientists, Data Analysts, other Software/Data Engineers, and business stakeholders to create and deploy dynamic data applications that help our customers make meaningful business decisions.
Some of your responsibilities will include leading and implementing software engineering best practices, mentoring junior engineers, designing software architectures, wrangling and fusing large and disparate data sets, assisting in the deployment of models and algorithms, automating data pipelines in their entirety, and communicating model results through user-focused data visualizations. Candidates should have the ability and the willingness to tailor applications to a clients’ business goals using an iterative methodology. Candidates should also be able to consider both long-term stability and scalability while taking a user-focused approach to development and deployment. Excellent decision making skills are a must.
Desired Skills
Candidates should have a mix of the following experiences:
Set or contributed to data engineering and governance standards
Reviewed code/design/architecture and coached on adjustments (as needed)
Mentored junior engineers
Designed and implemented software architectures
Implemented a data pipeline that includes data transformations such as aggregations, joins, and cleaning.
Ingested data into SQL or NoSQL databases.
Interacted with SQL or NoSQL databases via Python, Java, or similar.
Provided access to transformed data for downstream applications or visualizations via REST APIs or similar.
Worked with visualization software such as Power BI, Tableau, or similar.
Skilled in both applying and teaching others to apply Git (or other distributed version control) for collaboration, record-keeping, and deployment scenarios
Skilled in both using and guiding others in using cloud platforms (AWS, Azure, Google Cloud) for building, deploying, evolving, and maintaining data pipelines
Skilled in both applying and guiding others in applying continuous integration and continuous deployment (CI/CD) techniques for surfacing situations where new work breaks existing functionality
Had exposure to modeling and/or data analysis.
Configured a technical service such as a database, version control system, or operating system.
Led and collaborated with others to accomplish a technical task.
Partnered on the development or deployment of a data strategy
Presented technical work to a non-technical audience via writing or presentation.
Desired Education and Experience
Education:
Master’s in a technical field and 4-5 years’ experience OR
Bachelor’s and 6+ years’ experience
Certifications:
AWS, Azure, or Google Cloud
Minimum Requirements
Bachelors/Master’s degree in Computer Science or related field requiring software development
4+ years’ of professional experience with Python, Java, or similar
Excellent written and verbal communication skills
Ability to parse project requirements, including asking questions to ensure full and correct understanding
Willingness and ability to learn new concepts and tools quickly
Willingness and ability to provide technical support across multiple projects
What You Would Do
Work on small teams in a highly collaborative environment.
Participate in both existing and new projects.
Contribute to consulting projects to solve interesting problems for multiple clients in various industries.
Communicate details of the technical architecture to fellow team members and clients in both technical and non-technical terms.
Work with Data Scientists to design and/or implement the technical architecture necessary to support analytics.
Work with Software/Data Engineers to improve the robustness and scalability of software products.
Participate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance.
Work on documentation of the technical architecture for internal and client use.
Design, create, and provision data stores.
Build trusted and lasting relationships with clients.
Provide value to our clients through analytics, software tools, and leadership.
Manage and monitor pipelines to extract, transform, and load data (ETL).
About Elder Research, Inc.
Headquartered in Charlottesville, VA with offices in Arlington, VA, Baltimore, MD, and Raleigh, NC, Elder Research, Inc. (ERI) is a fast-growing solutions and consulting firm specializing in predictive analytics. At ERI, you’ll be part of a fun, friendly community. In keeping with our entrepreneurial spirit, we want candidates that are self-motivated with an innate curiosity and strong team work ethic. We work hard to provide the best value to our clients and allow each person to contribute their ideas and put their skills to use immediately.
ERI believes in continuous learning; each week the entire company attends a “Tech Talk” and an office lunch. Individuals often share what they have learned at informal bag lunch talks or afternoon chats. ERI provides a supportive work environment with established parental, bereavement, and PTO policies. By prioritizing a healthy work-life balance - with reasonable hours, solid pay, low travel, and flexible time off - ERI enables and encourages its employees to serve others.
ERI provides analytic solutions to hundreds of companies across numerous industries. Our team enjoys great variety in the type of work we do and exposure to a wide range of techniques and tools. If you are passionate about integrating data, technology, and analytics in a team-based environment to solve problems, then ERI may be a good fit for you.",4.5,"Elder Research Inc
4.5","Charlottesville, VA",51 to 200 Employees,1995,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
501,Senior Data Analytics Engineer,-1,"At Wistia our goal is to help every business to thrive with video. That's a big task, and we're super excited to help the world get there faster through simple and delightful video-focused products.
Wistia's Business Intelligence team is charged with leveraging data to empower strategic, insights-driven decision making across the organization. The Analytics Engineer role is a hybrid role with responsibilities that typically fall to Data Analysts, Data Engineers and Data Scientists. We are a small team and are looking for someone who is comfortable wearing different ""data"" hats. A successful candidate will combine strong technical and communication skills to manage the full data lifecycle at Wistia by creating and maintaining ETL pipelines, automating critical workflows, analyzing data to surface insights and translating those insights into clear recommendations for stakeholders.
The other members of the Business Intelligence team are curious, attentive to detail, impact-focused and approach problem solving with an engineering mindset. The team is centralized (not embedded) with each team member working with data from across the business. We strive to adopt the tools and best practices of the ""modern data stack"". For us that looks like: Redshift, dbt, Fivetran, Mode Analytics, Census, Snowplow Analytics. The Redshift warehouse we maintain serves as the single source of truth for the business.
Key Responsibilities
Maintain our 3rd-Party and custom ETL/reverse-ETL jobs and create new ones as required
Develop new predictive models as required to meet business outcomes
Generate easily interpretable analyses of complex datasets using SQL or python
Create persuasive narratives with data visualizations that enable self-serve exploration
Assist in the design and analysis of A/B testing to optimize business outcomes
Define version-controlled, modular business logic and datasets to be reused across the team
Document your findings, insights and recommendations for stakeholders
Meet business stakeholders ""where they live"" by becoming an expert in the data models and usage of their preferred SaaS tools, eg, Salesforce, HubSpot, Google Analytics, Amplitude, Stripe
Collaborate with:
Team members to share learnings, brainstorm new approaches and define best-practices
Stakeholders to scope, shape and prioritize their data requests
Qualifications
This section should talk about the competencies needed for the role, including qualities. Be sure to seek the input of key stakeholders and make sure we distinguish between must-haves and nice-to-haves because we will be using this to build our job positing and interview plan.
Technical
Highly proficient in SQL and python (or comparable general purpose programming language)
Facile with statistics and hypothesis testing
Skilled at constructing meaningful data visualizations
Comfortable with the command line and bash scripting
Knowledgeable about database architecture, data warehousing principles, REST APIs and basic ML algorithms
Non-technical
Experienced at scoping broad, loosely-defined requests into well-structured plans
Adept at translating data into actionable insights for stakeholders
Strong in written communication
Able to work independently with minimal oversight
Familiar with SaaS metrics and business practices
Values fit (for Wistia at large and BI specifically)
collaborative
learning oriented
attention to detail
simplicity as the starting point starts simple and add complexity only as needed
Key Metrics
This is a great place for more detail on the metrics we expect this person to own, and any that we measure success by.
Product, marketing, sales, and customer success teams have the data and insights they need to make decisions
Product, marketing, sales, and customer success teams have confidence in the data they use
Product, marketing, sales, and customer success teams are able to monitor business performance and spot problem areas and new opportunities
Responses to data requests are timely
Success Looks Like
This section should outline what success looks like. Knowing this helps inform the level and type of candidates we need, interview questions, onboarding plan, and coaching our new hires to success. It also makes it easier to spot performance issues early and hold ourselves accountable in hiring (and learn how we can improve).
30 Days
Complete Wistia onboarding program
Ramp up and learn the tools of the role (Mode, dbt, github, Fivetran, Redshift, Google Analytics, Salesforce, Pendo)
Ramp up and learn Wistia's data architecture, starting with Wistia app database
Ramp up and learn data analysis best practices and procedures
Complete ~5 basic data requests
90 Days
Confident in use of Mode, dbt, and GitHub for data analysis process
Confident in data architecture and able to self-source most data tables
Regularly handling basic data requests
Completed 1 complex data investigation
1 Year
Regularly initiating new data investigations and unearthing new insights
Regularly making improvements to our approach to data presentation
Regularly making improvements to our data models and data cleanliness
Working at Wistia
We try to ensure Wistia is an inclusive and diverse place where everyone feels happy, fulfilled, respected, comfortable, and welcome. We're proud to be an equal-opportunity workplace. We care a lot, so our benefits are actually benefits, not just the fun stuff like swag and snacks in the office (though we also have lots of those too!).
We know the biggest investment we can make is in our employees, so we provide:
A competitive compensation package that includes internal equity stock options
401k with 3% company contribution, regardless of whether you contribute
Fully paid healthcare, dental, and vision insurance (family plans included)
Pre-Tax FSA and Dependent Care Account
Fitness reimbursement
Flexible working hours – work at the times when you operate best, or set aside time for child and/or elderly care responsibilities during traditional ""office hours""
16 weeks paid parental leave for all new and expecting parents
Unlimited PTO (pretty common for people to take 4+ weeks off throughout the year)
Remote-first culture (work from anywhere in the U.S.)
Annual professional development stipend (courses, conferences, and more)
New hire bonus to enhance your home office set up
Pet insurance discount
Location/Remote Opportunities
Wistia is a remote-first company. Employees can work from our beautiful office in Cambridge, MA, or anywhere in the continental US",4.8,"Wistia
4.8","Cambridge, MA",51 to 200 Employees,2006,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
502,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
503,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
504,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
505,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
506,Senior Software Engineer - Data Engineering,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: The merchant Reporting and Reconciliation team is looking for a senior Back-end engineer responsible for providing technical leadership to the team and executing the unification projects end to end. Your primary focus will be developing massively scalable, distributed software systems that require high availability to our business
Job Description:
Merchant reporting is crucial for our Merchants so that they can close their account books timely and accurately with complete payment data in core markets. We are looking for people who have a passion for developing massively scalable, distributed software systems that require high availability to our business. As a member on the Merchant Reporting and Reconciliation team, you thrive in a fast-paced environment and enjoy driving innovation through rapid prototyping and iterative development. You will work directly with our Product Owners and Domain Technical Leads to create outstanding solutions and deliver incredible reporting products. You will be involved from ideation to rollout.
Your day-to-day:
Work with Product Managers and other business partners to identify opportunities for improvement
Analyze data based on product requirements
Create reports for internal teams and/or external clients
Use graphs, infographics, and other methods to visualize data
Structure large data sets to find usable information
Work with a team of analysts and other associates to process information
Create presentations and reports based on recommendations and findings
Define validation queries when needed and how to identify discrepancies in the data as they arise
Write queries for runbooks that automate the discrepancy identification process
Implement the reporting data model
Deliver within schedule in an Agile software development using test-driven development methodologies.
Participate in development life cycle activities like design, coding, testing, and production release.
Be proactive with identifying areas for improvement and innovation to improve development productivity
What do you need to bring:
BS in EE/CS or equivalent work experience and successful completion of major projects for which you can show code examples.
5+ years of hands-on data/software engineering experience
Experience working with coding languages—preferably SQL, Java, Spark-SQL, Pyspark, Python
Experience working with SQL and NoSQL DataBase
High proficiency in MS Excel, MS PowerPoint, GIT, Apache Airflow
Have a passion for quality and writing clean and solid code that scales and performs well.
Strong desire to learn, push the envelope and share knowledge with others.
Excellent analytical and time management skills
Teamwork skills with a problem-solving attitude
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
507,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
508,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
509,Data Warehouse & Artificial Intelligence Engineer,$83K - $120K (Glassdoor est.),"Job Description:

Design, Implement and Maintain data warehouse.

Duties and Responsibilities:

Develop, test, and deploy extraction, transformation and loading (ETL) routines to load the data warehouse using SSIS, stored procedures, etc.
Develop analytics including reports, dashboards, and data visualizations
Troubleshoot BI-related problems and tune for performance
Implement and oversee a company-wide data governance program to support master data management and resolve data definition discrepancies among stakeholders
Plan the technical approach for implementing analytics strategy across multiple channels
Plan and implement activities to maintain data confidence
Provide technical direction and technical support for all member of the team
Architect and implement analytics solutions to meet data collection and reporting needs.
Work with team to recommend analytic software
Provide general troubleshooting assistance for analytic reports and break fixes
Wrangle data (a.k.a. data engineering) from a wide variety of complex data sets large volumes of data into normalized and enriched data sets.
Assemble and evaluate data such that new insights, solutions, and visualizations can be derived.
Perform data discoveries to understand data formats, source systems, file sizes, etc. and engage with internal and external business partners in this discovery process.
Deliver data acquisition, transformations, normalization, mapping, and loading of data into a variety of data models.
Collaborate extensively with Technology to design data ingestion, data models, and automated operational metrics for consistently high quality data
Supports the research, analysis and presentation of information by: producing reports, compiling and summarizing information, producing supporting documentation and exhibits, and verifying information received from external sources.
Assists other associates with preparation of reports and use of information systems, software and related sources of information. Trains other users on report preparation and database access.
Perform other duties as assigned.

Required Knowledge, skills and abilities:

Ability to manage change successfully in a fast-paced environment
A passion for learning and the ability to do so on the fly
Must be strategic, dynamic, and detail-oriented. The ideal candidate is an analytical thinker, a self-starter and a quick learner.
Ability to design and architect method of extracting data from its original source to another location (i.e. a data lake) to make that data consumable for a variety of downstream purposes such as applications, visualizations, analytics and other related data science activities.
Basic understanding of data science
Ability to transforming and map one ""raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.
Exceptional ability to clearly and effectively communicate with IT peers and users at all levels of technical and business acumen.
Strong time management skills and multi-tasking capabilities
Ability to understand business rules governing financial and budgeting process
Exceptional oral and written communication skills
Self-motivated, ability to set priorities and meet deadlines

Education and Experience:

Bachelor's degree in Computer Science, Business, Economics, Information Science, or Analytics
Minimum of 3 years of experience with data analytics across multiple channels
Experienced with the technologies required to implement Analytics across multiple channels, (for example, Data Layer, Custom JavaScript, JAVA, AJAX, HTML, Objective-C, Swift)
Knowledge of Tableau is a plus
Familiarity with Microsoft Power BI
Extensive experience with ETL and/or other Big Data processes.
Contract manufacturing experience is a plus.

Foxconn Assembly, LLC is an Equal Opportunity Employer (EOE). All qualified candidates will receive consideration without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or marital status in accordance with applicable federal, state and local laws.
Foxconn Assembly, LLC participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

NbXn0DyWY6",3.4,"Foxconn Corporation
3.4","Houston, TX",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$5 to $25 million (USD)
510,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
511,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
512,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
513,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
514,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
515,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
516,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the organization.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department. The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
517,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
518,Data Engineer,Employer Provided Salary:$75.00 Per Hour,"8+ years of experience with Oracle and SQL Server databases
• 8+ years of experience with Informatica Power Center.
8+ years software development experience in one or more of the following areas: ETL, BI and DW
• 8+ years of experience in DW Multi-Dimensional modelling.
Understanding of the Project Delivery Framework and SDLC
Understanding of Waterfall and Agile Project Management Methodology
Strong analytical, communication and interpersonal skills.
Excellent and proven collaboration skills
Job Type: Full-time
Pay: $75.00 per hour
Expected hours: 40 per week
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Monday to Friday
Experience:
Informatica PowerCenter: 5 years (Required)
Multi-Dimensional modelling: 3 years (Required)
Work Location: Hybrid remote in New York, NY 10001",-1,Cybotic System,"New York, NY",-1,-1,-1,-1,-1,-1
519,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
520,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
521,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
522,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
523,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
524,Senior Data Analytics Engineer,-1,"At Wistia our goal is to help every business to thrive with video. That's a big task, and we're super excited to help the world get there faster through simple and delightful video-focused products.
Wistia's Business Intelligence team is charged with leveraging data to empower strategic, insights-driven decision making across the organization. The Analytics Engineer role is a hybrid role with responsibilities that typically fall to Data Analysts, Data Engineers and Data Scientists. We are a small team and are looking for someone who is comfortable wearing different ""data"" hats. A successful candidate will combine strong technical and communication skills to manage the full data lifecycle at Wistia by creating and maintaining ETL pipelines, automating critical workflows, analyzing data to surface insights and translating those insights into clear recommendations for stakeholders.
The other members of the Business Intelligence team are curious, attentive to detail, impact-focused and approach problem solving with an engineering mindset. The team is centralized (not embedded) with each team member working with data from across the business. We strive to adopt the tools and best practices of the ""modern data stack"". For us that looks like: Redshift, dbt, Fivetran, Mode Analytics, Census, Snowplow Analytics. The Redshift warehouse we maintain serves as the single source of truth for the business.
Key Responsibilities
Maintain our 3rd-Party and custom ETL/reverse-ETL jobs and create new ones as required
Develop new predictive models as required to meet business outcomes
Generate easily interpretable analyses of complex datasets using SQL or python
Create persuasive narratives with data visualizations that enable self-serve exploration
Assist in the design and analysis of A/B testing to optimize business outcomes
Define version-controlled, modular business logic and datasets to be reused across the team
Document your findings, insights and recommendations for stakeholders
Meet business stakeholders ""where they live"" by becoming an expert in the data models and usage of their preferred SaaS tools, eg, Salesforce, HubSpot, Google Analytics, Amplitude, Stripe
Collaborate with:
Team members to share learnings, brainstorm new approaches and define best-practices
Stakeholders to scope, shape and prioritize their data requests
Qualifications
This section should talk about the competencies needed for the role, including qualities. Be sure to seek the input of key stakeholders and make sure we distinguish between must-haves and nice-to-haves because we will be using this to build our job positing and interview plan.
Technical
Highly proficient in SQL and python (or comparable general purpose programming language)
Facile with statistics and hypothesis testing
Skilled at constructing meaningful data visualizations
Comfortable with the command line and bash scripting
Knowledgeable about database architecture, data warehousing principles, REST APIs and basic ML algorithms
Non-technical
Experienced at scoping broad, loosely-defined requests into well-structured plans
Adept at translating data into actionable insights for stakeholders
Strong in written communication
Able to work independently with minimal oversight
Familiar with SaaS metrics and business practices
Values fit (for Wistia at large and BI specifically)
collaborative
learning oriented
attention to detail
simplicity as the starting point starts simple and add complexity only as needed
Key Metrics
This is a great place for more detail on the metrics we expect this person to own, and any that we measure success by.
Product, marketing, sales, and customer success teams have the data and insights they need to make decisions
Product, marketing, sales, and customer success teams have confidence in the data they use
Product, marketing, sales, and customer success teams are able to monitor business performance and spot problem areas and new opportunities
Responses to data requests are timely
Success Looks Like
This section should outline what success looks like. Knowing this helps inform the level and type of candidates we need, interview questions, onboarding plan, and coaching our new hires to success. It also makes it easier to spot performance issues early and hold ourselves accountable in hiring (and learn how we can improve).
30 Days
Complete Wistia onboarding program
Ramp up and learn the tools of the role (Mode, dbt, github, Fivetran, Redshift, Google Analytics, Salesforce, Pendo)
Ramp up and learn Wistia's data architecture, starting with Wistia app database
Ramp up and learn data analysis best practices and procedures
Complete ~5 basic data requests
90 Days
Confident in use of Mode, dbt, and GitHub for data analysis process
Confident in data architecture and able to self-source most data tables
Regularly handling basic data requests
Completed 1 complex data investigation
1 Year
Regularly initiating new data investigations and unearthing new insights
Regularly making improvements to our approach to data presentation
Regularly making improvements to our data models and data cleanliness
Working at Wistia
We try to ensure Wistia is an inclusive and diverse place where everyone feels happy, fulfilled, respected, comfortable, and welcome. We're proud to be an equal-opportunity workplace. We care a lot, so our benefits are actually benefits, not just the fun stuff like swag and snacks in the office (though we also have lots of those too!).
We know the biggest investment we can make is in our employees, so we provide:
A competitive compensation package that includes internal equity stock options
401k with 3% company contribution, regardless of whether you contribute
Fully paid healthcare, dental, and vision insurance (family plans included)
Pre-Tax FSA and Dependent Care Account
Fitness reimbursement
Flexible working hours – work at the times when you operate best, or set aside time for child and/or elderly care responsibilities during traditional ""office hours""
16 weeks paid parental leave for all new and expecting parents
Unlimited PTO (pretty common for people to take 4+ weeks off throughout the year)
Remote-first culture (work from anywhere in the U.S.)
Annual professional development stipend (courses, conferences, and more)
New hire bonus to enhance your home office set up
Pet insurance discount
Location/Remote Opportunities
Wistia is a remote-first company. Employees can work from our beautiful office in Cambridge, MA, or anywhere in the continental US",4.8,"Wistia
4.8","Cambridge, MA",51 to 200 Employees,2006,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
525,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
526,Data Warehouse & Artificial Intelligence Engineer,$83K - $120K (Glassdoor est.),"Job Description:

Design, Implement and Maintain data warehouse.

Duties and Responsibilities:

Develop, test, and deploy extraction, transformation and loading (ETL) routines to load the data warehouse using SSIS, stored procedures, etc.
Develop analytics including reports, dashboards, and data visualizations
Troubleshoot BI-related problems and tune for performance
Implement and oversee a company-wide data governance program to support master data management and resolve data definition discrepancies among stakeholders
Plan the technical approach for implementing analytics strategy across multiple channels
Plan and implement activities to maintain data confidence
Provide technical direction and technical support for all member of the team
Architect and implement analytics solutions to meet data collection and reporting needs.
Work with team to recommend analytic software
Provide general troubleshooting assistance for analytic reports and break fixes
Wrangle data (a.k.a. data engineering) from a wide variety of complex data sets large volumes of data into normalized and enriched data sets.
Assemble and evaluate data such that new insights, solutions, and visualizations can be derived.
Perform data discoveries to understand data formats, source systems, file sizes, etc. and engage with internal and external business partners in this discovery process.
Deliver data acquisition, transformations, normalization, mapping, and loading of data into a variety of data models.
Collaborate extensively with Technology to design data ingestion, data models, and automated operational metrics for consistently high quality data
Supports the research, analysis and presentation of information by: producing reports, compiling and summarizing information, producing supporting documentation and exhibits, and verifying information received from external sources.
Assists other associates with preparation of reports and use of information systems, software and related sources of information. Trains other users on report preparation and database access.
Perform other duties as assigned.

Required Knowledge, skills and abilities:

Ability to manage change successfully in a fast-paced environment
A passion for learning and the ability to do so on the fly
Must be strategic, dynamic, and detail-oriented. The ideal candidate is an analytical thinker, a self-starter and a quick learner.
Ability to design and architect method of extracting data from its original source to another location (i.e. a data lake) to make that data consumable for a variety of downstream purposes such as applications, visualizations, analytics and other related data science activities.
Basic understanding of data science
Ability to transforming and map one ""raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.
Exceptional ability to clearly and effectively communicate with IT peers and users at all levels of technical and business acumen.
Strong time management skills and multi-tasking capabilities
Ability to understand business rules governing financial and budgeting process
Exceptional oral and written communication skills
Self-motivated, ability to set priorities and meet deadlines

Education and Experience:

Bachelor's degree in Computer Science, Business, Economics, Information Science, or Analytics
Minimum of 3 years of experience with data analytics across multiple channels
Experienced with the technologies required to implement Analytics across multiple channels, (for example, Data Layer, Custom JavaScript, JAVA, AJAX, HTML, Objective-C, Swift)
Knowledge of Tableau is a plus
Familiarity with Microsoft Power BI
Extensive experience with ETL and/or other Big Data processes.
Contract manufacturing experience is a plus.

Foxconn Assembly, LLC is an Equal Opportunity Employer (EOE). All qualified candidates will receive consideration without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or marital status in accordance with applicable federal, state and local laws.
Foxconn Assembly, LLC participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

NbXn0DyWY6",3.4,"Foxconn Corporation
3.4","Houston, TX",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$5 to $25 million (USD)
527,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
528,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
529,Data Engineer,$67K - $93K (Glassdoor est.),"JMT Technology Group, a leading geospatial and information technology solutions provider, is looking for a Data Engineer. The Data Engineer will be responsible for architecting, documenting, and developing data architectures models and business reporting applications, providing technical leadership for database development efforts. Conducts complex data mapping and Extract, Transform and Load (ETL) efforts.
Essential functions and responsibilities
Conduct complex data mapping efforts between systems in a clear and well defined manner
Perform data analysis and develop the data models to support ETL development
Complete complex data migrations (ETL) processes to support development efforts
Collaborate with Software Architects, Software Developers, and BI Developers to design appropriate solutions for our internal team and external clients.
Mentor software developers in data architecture and database design best practices
Perform and assign database development tasks of medium complexity across multiple projects.
Work directly with third party solutions to design, document, and develop data integrations.
Create, contribute to, review and collaborate on data and database designs and implementations.
Work with IT to deploy solutions
Innovate and contribute to improving development standards, techniques, tools, and processes
Train and test for industry certifications
Identify, set, monitor, and achieve individual goals
Leverage mentorship and peer relationships to increase proficiency in development
Provide support to team members
Participate in the development personnel interview process
Collaborate with project managers, business systems analysts, UX designers, and application developers to deliver high-quality deliverables
Support production systems as urgent and critical issues arise; including non-business hours support on a rotating basis.
Nonessential functions and responsibilities
Perform other related duties as assigned



Required Skills
Excellent verbal and written communication skills



Required Experience
Bachelor’s Degree in computer science, Information Technology, or related fields
3+ years of experience developing data/database systems and integrating with third party solutions.
3+ years of Extract, Transform and Load (ETL) experience
2+ years of experience architecting data/database solutions and designing the integration with third party solutions.
3+ years of experience working in a hybrid agile development methodology
Preferred Experience
Experience with Microsoft’s Azure Technologies
Experience with a variety of databases and ETL solutions
Experience with Databricks
Working Conditions
Work is performed within a general hybrid office environment. Work is generally sedentary in nature, but may require occasional standing and walking. Lighting and temperature are adequate and there are no hazardous or unpleasant conditions caused by noise, dust, etc. within the office environment.
Job Competencies
Analytical Thinking/Problem Solving
Communication
Building Relationships and Teamwork
Results-Oriented
JMT is an Equal Opportunity Employer M/F/Disability/Vet/Sexual Orientation/Gender Identity, and we are proud to be building an inclusive and diverse workforce.
#LI-KW1",4.0,"Johnson, Mirmiran, and Thompson Inc.
4.0","Hunt Valley, MD",1001 to 5000 Employees,-1,Private Practice / Firm,Architectural & Engineering Services,"Construction, Repair & Maintenance Services",$100 to $500 million (USD)
530,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
531,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
532,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Data Engineer
Candidate must have AWS (Redshift, Glue, S3), Spark, Phython, Scala and Should be expert in SQL
Basic qualifications
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
Design, implementation and management of data processing systems dealing with large data sets.
Proficiency in, at least, one modern programming language such as Python, Java or Scala
Hands-on experience and advanced knowledge of SQL.
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and cloud computing
Experience working with Open-Source Big Data tools (Hadoop, Spark, Hive, Presto and etc.)
Preferred qualifications
Experience in working and delivering end-to-end projects independently
Relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics
Experience working with AWS Big Data Technologies (AWS Glue, EMR, Athena, Ec2, Redshift and S3 etc.)
Experience working with Oracle, PostgreSQL, SQL Server or MySQL.
Proven track record of delivering a big data solution
Experience working with both Batch and Real Time data processing systems
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Job Type: Contract
Pay: $70.00 - $75.00 per hour
Expected hours: 40 per week
Compensation package:
1099 contract
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Austin, TX 78703: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 8 years (Required)
Python: 8 years (Required)
AWS: 5 years (Preferred)
Work Location: Hybrid remote in Austin, TX 78703",-1,PRISM IT LLC,"Austin, TX",-1,-1,-1,-1,-1,-1
533,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
534,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
535,Data Engineer,-1,"Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore.

Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office.
The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus.
Position Responsibilities
Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities
Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization
Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process
Qualifications & Desired Skills
Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance
Minimum 2 years' of development experience using either Python, C#/Java or C++
Minimum 2 years' years of database/SQL experience
Familiarity with object-oriented programming concepts
Must be comfortable with development across the application stack
Ability to complete complex projects independently
Detail-oriented with strong verbal and written communication skills
Ability to work effectively in a high-energy, time sensitive team environment
Interest or prior experience in the financial trading industry (particularly in fixed income) a plus
This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education.
Base Salary for this role is expected to be between:
$110,000—$150,000 USD",4.4,"Garda Capital Partners
4.4","New York, NY",51 to 200 Employees,-1,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
536,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
537,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
538,USA - Infrastructure Data Engineer (AWS),$104K - $149K (Glassdoor est.),"Job Title: Infrastructure Data Engineer (AWS)
Location: San Jose, California, United States
Type: Fulltime

The candidate has to be in San Jose, CA and it is 100% onsite job”.

Roles & Responsibilities:
Design, build and maintain data platform infrastructure on AWS environment.
Oversee design, build, and maintain data platform infrastructure on AWS environment.
Develop data pipelines to collect the metrics that is required to monitor data refreshes, reports deliveries and track SLAs.
Work with the DWH development team and business users in establishing SLAs for data refreshes and reports.
Build continuous integration/deployment (CI/CD) pipelines to accelerate development and improve team agility.
Oversee project.
Monitor all aspects of data platform system security, performance, storage, incidents, and usage for databases, data pipelines, applications, and infrastructure on AWS. Escalate to respective teams for fixes.
Ensure data pipelines meet intraday and daily SLAs, as per documented SLA definitions and escalate accordingly.
develop appropriate instrumentation to collect metrics on system performance, cost, data ingress/egress /storage processes.
Have a clear understanding of the reports/analyses/insights to be driven by data and build data driven solutions to optimally support the operational analytics needs.
Documents user stories, epics, and reports
Coordinate infrastructure enhancements and maintenance with the system/network engineering teams
Work with DWH development team and analytics team to do manual releases where required.
Onboard users to data analytics systems with appropriate approvals
Conduct system performance tests and collect metrics. Tune/add capacity.
Complete knowledge management processes
Own strategy and communicate potential major shifts in expected workload based on business, market, or operational changes.
Operate ongoing business relationship management sessions to review operational metrics, understand pain points, identify upcoming projects and engagement efforts.",-1,Avestacs,"San Jose, CA",51 to 200 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
539,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
540,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
541,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
542,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the organization.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department. The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
543,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
544,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
545,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
546,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
547,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
548,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
549,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
550,Data Engineer,Employer Provided Salary:$75.00 Per Hour,"8+ years of experience with Oracle and SQL Server databases
• 8+ years of experience with Informatica Power Center.
8+ years software development experience in one or more of the following areas: ETL, BI and DW
• 8+ years of experience in DW Multi-Dimensional modelling.
Understanding of the Project Delivery Framework and SDLC
Understanding of Waterfall and Agile Project Management Methodology
Strong analytical, communication and interpersonal skills.
Excellent and proven collaboration skills
Job Type: Full-time
Pay: $75.00 per hour
Expected hours: 40 per week
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Monday to Friday
Experience:
Informatica PowerCenter: 5 years (Required)
Multi-Dimensional modelling: 3 years (Required)
Work Location: Hybrid remote in New York, NY 10001",-1,Cybotic System,"New York, NY",-1,-1,-1,-1,-1,-1
551,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
552,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
553,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
554,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
555,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
556,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
557,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
558,Data Visualization Engineer,$71K - $99K (Glassdoor est.),"At InnovaSystems, extraordinary solutions are born through innovative software.
A wholly owned subsidiary of Cydecor, InnovaSystems is a leading provider of information technology and enterprise-level solutions to the Department of Defense, federal, state, and local government agencies, delivering a vital service to support our nation’s National Security Strategy. We engage organizational effectiveness and readiness resulting in reduced costs, increased effectiveness, and accessibility.
As a member of the InnovaSystems team, you will be empowered to do the best work of your career with challenging assignments and collaborative teams that solve real-world problems.
Some of our perks & benefits include:
Flexible schedule & telecommute options, continuous learning opportunities including technical training and Leadership development programs, comprehensive benefits package, onsite gym/workout facilities, 401k plan w/ company match, competitive salary and a fantastic culture of collaboration.
Successful applicants will be asked to show proof of their U.S. Citizenship.

Job Role:
We are seeking a talented and creative Data Visualization Specialist to join our team. The ideal candidate will have experience using visualization tools such as Tableau, Power BI, and D3.js, and will have a strong understanding of data analysis and data management concepts.
As a Data Visualization Engineer, you will be responsible for:
Working with stakeholders to understand data needs and requirements.
Designing and develop visually compelling and informative dashboards, reports, and infographics.
Developing and maintain documentation, standards, and best practices for data visualization.
Performing data analysis to identify trends and insights, and use that knowledge to design effective visualizations.
Working closely with cross-functional teams to develop and implement data visualization solutions.
Continuously improve and innovate data visualization practices.

Skills Required:
2+ years of experience in data visualization or related field.
Proficiency in visualization tools such as Tableau, Power BI, and D3.js.
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Knowledge of data analysis and data management concepts.
Excellent communication and collaboration skills.
Strong portfolio demonstrating data visualization expertise.
Personal drive and initiative are qualities we seek

Skills desired:
Military/DoD background highly desirable

Our comprehensive employee benefits offerings include:
Flexible schedule
Company-paid employee development learning and licenses
10 paid holidays
3 weeks (120 hours) of paid leave annually - hours increase after 3 years of service
5% 401k plan w/ company match
Comprehensive benefit package to include health, dental, vision, pet and supplemental insurance plans
Recognition & reward programs including peer-to-peer, service awards, leadership and values awards
Onsite offices with fitness facilities (depending on location)
Social events
InnovaSystems is a proud supporter of community organizations including; Challenged Athletes Foundation (CAF), Support the Enlisted Program (STEP), and Salvation Army
InnovaSystems International is an EEO/AA employer M/F/D/V.",4.7,"Ironclad Technnology Services
4.7","Washington, DC",201 to 500 Employees,2008,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
559,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
560,Data Engineer (Onsite),Employer Provided Salary:$85K - $100K,"If you are seeking a career that has tremendous impact on manufacturing operations, then come be a part of Tokai Carbon GE's team to design, develop and implement our new manufacturing execution system. You'll play a critical role in our organization's success, providing easier access to data that engineers, analysts and decision-makers need to do their jobs. You will set the foundation of data for growth, relying on your programming and problem-solving skills to create scalable solutions. You will be part of the team building BI and Analytics tools that will take our operations to the next level. If you are excited about applying your knowledge and skills to meaningful challenges, apply today!
JOB SUMMARY
The Data Engineer will be responsible for designing, implementing, and supporting solutions used to collect, transform, store, and analyze data from a variety of company applications. They analyze and organize raw data, build data systems and pipelines, review and analyze data infrastructure, plan, and implement solutions to store and manage data. Also, they create reports, dashboards, and other analytics solutions. They will work closely with the lines of business and external partners, to ensure successful development and delivery of BI and data analytics solutions. This is a hands-on development and support position with ownership responsibility for data architecture and engineering for BI and analytics purposes.
ESSENTIAL FUNCTIONS
Work with business users and other technology teams to understand their data requirements, identify appropriate data sources, design, and develop data pipelines and design appropriate data structures to store the required data.
Design data pipelines using SQL, other programming languages and data integration tools.
Gather and combine data from a variety of sources to support actionable decision making. Prepare data for reporting, and predictive and prescriptive tools.
Address BI/EDW technical development working with operations, sales, supply chain, technical services, ERP applications, analytics, and finance & accounting teams.
Contribute to our suite of reports and data products while collaborating with business analysts, business process owners, and other team members, including UAT, and support for technical testing.
Work closely with senior team members defining data architecture, optimizing EDW performance, and updating EDW documentation, including outlining options to improve resilience and data quality monitoring.
Maintain the Enterprise Data Warehouse (EDW) and influence data quality and reliability.

COMPETENCIES
Data modeling, relational databases, data warehousing, business intelligence, stream processing.
SQL programming skills, including writing efficient and optimized SQL queries for data retrieval and manipulation.
ETL/ELT processes and data integration.
Strong analytical skills, development experience, and excellent verbal and written communications.
Manage all stages of the development lifecycle, including planning, requirements gathering, designing, developing, documenting, testing, training, deployment, and support.
Own individual and cross-functional team deliverables on small to large projects.
Project and change management methodologies including Agile/Scrum.
Willingness and availability to travel, and perform other duties as needed.
Must be a self-starter, manage workload autonomously, and balance competing priorities.
Ability and desire to continuously learn about recent technologies and Analytics trends.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Computer Engineering, Analytics, Information Systems, or equivalent experience.
1-3 years of experience working with SQL and relational databases (MS SQL Server preferred).
1-3 years of experience working with Microsoft Azure, particularly Azure Data Factory, Azure Synapse and Power Platform.
1-3 years of experience preferred in reporting, dashboards, and BI tools (Power BI preferred) for ERP and MES platforms.
1-3 years of experience preferred with Visual Studio, SSRS.
Experience with MS D365 and AVEVA or similar ERP and MES solutions preferred.
Proficient in coding and scripting in Python preferred.
Experience in Manufacturing, Supply Chain, Logistics, and Finance and Accounting solutions preferred.
How to stand out: Microsoft Azure Data Engineer Associate Certification
COMPENSATION AND BENEFITS
Available on your first day: Full medical, vision, dental benefits, short-term disability, and long-term disability
Paid time off
Monthly wellness reimbursement
Health Savings, Flexible Spending and Dependent Care accounts
401(k) retirement savings plan with employer match
Annual bonus",2.6,"Tokai Carbon GE LLC
2.6","Charlotte, NC",1001 to 5000 Employees,-1,Company - Public,Chemical Manufacturing,Manufacturing,$1 to $5 billion (USD)
561,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
562,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
563,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
564,Data Engineer,-1,"SalesIntel is the top revenue intelligence platform on the market. Our unique approach to data collection, enhancement, verification and growth distinguishes us from the market, solidifying our position as the best B2B data partner.

The Data Engineer will be responsible for building and extending our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys working with big data and building systems from the ground up.

You will collaborate with our software engineers our software engineers, database architects, data analysts and data scientists to ensure our data delivery architecture is consistent throughout the platform. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What You’ll Be Doing:
Design and build parts of our data pipeline architecture for extraction, transformation, and loading of data from a wide variety of data sources using the latest Big Data technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with machine learning, data, and analytics experts to drive innovation, accuracy and greater functionality in our data system.
Qualifications:
Bachelor's degree in Engineering, Computer Science, or relevant field.
5+ years of experience in a Data Engineer role.
3+ years experience with Apache Spark and solid understanding of the fundamentals.
Deep understanding of Big Data concepts and distributed systems.
Strong coding skills with Scala, Python, Java and other languages and the ability to quickly switch between them with ease.
Advanced working SQL knowledge and experience working with a variety of relational databases such as Postgres and MySQL.
Experience working with data stored in many formats including TSV, CSV, JSON, Parquet.
Cloud Experience with DataBricks or similar service: S3, Athena, EC2, EMR, RDS, Redshift.
Comfortable working in a linux shell environment and writing scripts as needed.
Comfortable working in an Agile environment
Machine Learning knowledge is a plus.
Must be capable of working independently and delivering stable, efficient and reliable software.
Excellent written and verbal communication skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

This is a remote position.",-1,"SalesIntel Research, Inc.",Remote,51 to 200 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
565,Senior Software Engineer - Data Engineering,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: The merchant Reporting and Reconciliation team is looking for a senior Back-end engineer responsible for providing technical leadership to the team and executing the unification projects end to end. Your primary focus will be developing massively scalable, distributed software systems that require high availability to our business
Job Description:
Merchant reporting is crucial for our Merchants so that they can close their account books timely and accurately with complete payment data in core markets. We are looking for people who have a passion for developing massively scalable, distributed software systems that require high availability to our business. As a member on the Merchant Reporting and Reconciliation team, you thrive in a fast-paced environment and enjoy driving innovation through rapid prototyping and iterative development. You will work directly with our Product Owners and Domain Technical Leads to create outstanding solutions and deliver incredible reporting products. You will be involved from ideation to rollout.
Your day-to-day:
Work with Product Managers and other business partners to identify opportunities for improvement
Analyze data based on product requirements
Create reports for internal teams and/or external clients
Use graphs, infographics, and other methods to visualize data
Structure large data sets to find usable information
Work with a team of analysts and other associates to process information
Create presentations and reports based on recommendations and findings
Define validation queries when needed and how to identify discrepancies in the data as they arise
Write queries for runbooks that automate the discrepancy identification process
Implement the reporting data model
Deliver within schedule in an Agile software development using test-driven development methodologies.
Participate in development life cycle activities like design, coding, testing, and production release.
Be proactive with identifying areas for improvement and innovation to improve development productivity
What do you need to bring:
BS in EE/CS or equivalent work experience and successful completion of major projects for which you can show code examples.
5+ years of hands-on data/software engineering experience
Experience working with coding languages—preferably SQL, Java, Spark-SQL, Pyspark, Python
Experience working with SQL and NoSQL DataBase
High proficiency in MS Excel, MS PowerPoint, GIT, Apache Airflow
Have a passion for quality and writing clean and solid code that scales and performs well.
Strong desire to learn, push the envelope and share knowledge with others.
Excellent analytical and time management skills
Teamwork skills with a problem-solving attitude
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
566,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
567,Data Engineer III,-1,"We are seeking a Data Engineer III to join our Data Platform team. This role will report to the director of data platform and be involved in the planning, design, and implementation of our centralized data lake solution supporting analytics, products and applications across the company.
Qualifications:
To be successful in this role the candidate needs to have the following qualifications:
Deep knowledge of Scala and Spark
Experience with Databricks is preferred.
Deep knowledge of modern orchestration frameworks such as Apache airflow
Experience working with SQL and NoSQL database systems.
Experience with cloud environments (Azure Preferred)
Experience with acquiring and preparing data from primary and secondary disparate data sources.
Experience with agile project management methodology
Healthcare industry experience preferred, including exposure to different EMR systems, revenue cycle management.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12 is a plus but is not required.
Responsibilities:
Be part of an engineering team in building data adaptors to expedite the data onboarding process from various health systems.
Work with product management and business analysts on design reusable and configurable data orchestration pipelines.
Work with data specialist in develop and design data transformation to standardize the data model and support data enrichment activities.
As a subject matter expert, hosts information sharing session with teams within data platform or larger R1 organization.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",3.4,"R1 RCM, Inc.
3.4",Remote,10000+ Employees,2003,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
568,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
569,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
570,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
571,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the organization.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department. The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
572,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
573,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
574,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
575,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
576,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
577,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
578,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
579,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
580,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
581,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
582,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
583,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
584,Senior Data Analytics Engineer,-1,"At Wistia our goal is to help every business to thrive with video. That's a big task, and we're super excited to help the world get there faster through simple and delightful video-focused products.
Wistia's Business Intelligence team is charged with leveraging data to empower strategic, insights-driven decision making across the organization. The Analytics Engineer role is a hybrid role with responsibilities that typically fall to Data Analysts, Data Engineers and Data Scientists. We are a small team and are looking for someone who is comfortable wearing different ""data"" hats. A successful candidate will combine strong technical and communication skills to manage the full data lifecycle at Wistia by creating and maintaining ETL pipelines, automating critical workflows, analyzing data to surface insights and translating those insights into clear recommendations for stakeholders.
The other members of the Business Intelligence team are curious, attentive to detail, impact-focused and approach problem solving with an engineering mindset. The team is centralized (not embedded) with each team member working with data from across the business. We strive to adopt the tools and best practices of the ""modern data stack"". For us that looks like: Redshift, dbt, Fivetran, Mode Analytics, Census, Snowplow Analytics. The Redshift warehouse we maintain serves as the single source of truth for the business.
Key Responsibilities
Maintain our 3rd-Party and custom ETL/reverse-ETL jobs and create new ones as required
Develop new predictive models as required to meet business outcomes
Generate easily interpretable analyses of complex datasets using SQL or python
Create persuasive narratives with data visualizations that enable self-serve exploration
Assist in the design and analysis of A/B testing to optimize business outcomes
Define version-controlled, modular business logic and datasets to be reused across the team
Document your findings, insights and recommendations for stakeholders
Meet business stakeholders ""where they live"" by becoming an expert in the data models and usage of their preferred SaaS tools, eg, Salesforce, HubSpot, Google Analytics, Amplitude, Stripe
Collaborate with:
Team members to share learnings, brainstorm new approaches and define best-practices
Stakeholders to scope, shape and prioritize their data requests
Qualifications
This section should talk about the competencies needed for the role, including qualities. Be sure to seek the input of key stakeholders and make sure we distinguish between must-haves and nice-to-haves because we will be using this to build our job positing and interview plan.
Technical
Highly proficient in SQL and python (or comparable general purpose programming language)
Facile with statistics and hypothesis testing
Skilled at constructing meaningful data visualizations
Comfortable with the command line and bash scripting
Knowledgeable about database architecture, data warehousing principles, REST APIs and basic ML algorithms
Non-technical
Experienced at scoping broad, loosely-defined requests into well-structured plans
Adept at translating data into actionable insights for stakeholders
Strong in written communication
Able to work independently with minimal oversight
Familiar with SaaS metrics and business practices
Values fit (for Wistia at large and BI specifically)
collaborative
learning oriented
attention to detail
simplicity as the starting point starts simple and add complexity only as needed
Key Metrics
This is a great place for more detail on the metrics we expect this person to own, and any that we measure success by.
Product, marketing, sales, and customer success teams have the data and insights they need to make decisions
Product, marketing, sales, and customer success teams have confidence in the data they use
Product, marketing, sales, and customer success teams are able to monitor business performance and spot problem areas and new opportunities
Responses to data requests are timely
Success Looks Like
This section should outline what success looks like. Knowing this helps inform the level and type of candidates we need, interview questions, onboarding plan, and coaching our new hires to success. It also makes it easier to spot performance issues early and hold ourselves accountable in hiring (and learn how we can improve).
30 Days
Complete Wistia onboarding program
Ramp up and learn the tools of the role (Mode, dbt, github, Fivetran, Redshift, Google Analytics, Salesforce, Pendo)
Ramp up and learn Wistia's data architecture, starting with Wistia app database
Ramp up and learn data analysis best practices and procedures
Complete ~5 basic data requests
90 Days
Confident in use of Mode, dbt, and GitHub for data analysis process
Confident in data architecture and able to self-source most data tables
Regularly handling basic data requests
Completed 1 complex data investigation
1 Year
Regularly initiating new data investigations and unearthing new insights
Regularly making improvements to our approach to data presentation
Regularly making improvements to our data models and data cleanliness
Working at Wistia
We try to ensure Wistia is an inclusive and diverse place where everyone feels happy, fulfilled, respected, comfortable, and welcome. We're proud to be an equal-opportunity workplace. We care a lot, so our benefits are actually benefits, not just the fun stuff like swag and snacks in the office (though we also have lots of those too!).
We know the biggest investment we can make is in our employees, so we provide:
A competitive compensation package that includes internal equity stock options
401k with 3% company contribution, regardless of whether you contribute
Fully paid healthcare, dental, and vision insurance (family plans included)
Pre-Tax FSA and Dependent Care Account
Fitness reimbursement
Flexible working hours – work at the times when you operate best, or set aside time for child and/or elderly care responsibilities during traditional ""office hours""
16 weeks paid parental leave for all new and expecting parents
Unlimited PTO (pretty common for people to take 4+ weeks off throughout the year)
Remote-first culture (work from anywhere in the U.S.)
Annual professional development stipend (courses, conferences, and more)
New hire bonus to enhance your home office set up
Pet insurance discount
Location/Remote Opportunities
Wistia is a remote-first company. Employees can work from our beautiful office in Cambridge, MA, or anywhere in the continental US",4.8,"Wistia
4.8","Cambridge, MA",51 to 200 Employees,2006,Company - Private,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
585,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
586,Data Engineer (Onsite),Employer Provided Salary:$85K - $100K,"If you are seeking a career that has tremendous impact on manufacturing operations, then come be a part of Tokai Carbon GE's team to design, develop and implement our new manufacturing execution system. You'll play a critical role in our organization's success, providing easier access to data that engineers, analysts and decision-makers need to do their jobs. You will set the foundation of data for growth, relying on your programming and problem-solving skills to create scalable solutions. You will be part of the team building BI and Analytics tools that will take our operations to the next level. If you are excited about applying your knowledge and skills to meaningful challenges, apply today!
JOB SUMMARY
The Data Engineer will be responsible for designing, implementing, and supporting solutions used to collect, transform, store, and analyze data from a variety of company applications. They analyze and organize raw data, build data systems and pipelines, review and analyze data infrastructure, plan, and implement solutions to store and manage data. Also, they create reports, dashboards, and other analytics solutions. They will work closely with the lines of business and external partners, to ensure successful development and delivery of BI and data analytics solutions. This is a hands-on development and support position with ownership responsibility for data architecture and engineering for BI and analytics purposes.
ESSENTIAL FUNCTIONS
Work with business users and other technology teams to understand their data requirements, identify appropriate data sources, design, and develop data pipelines and design appropriate data structures to store the required data.
Design data pipelines using SQL, other programming languages and data integration tools.
Gather and combine data from a variety of sources to support actionable decision making. Prepare data for reporting, and predictive and prescriptive tools.
Address BI/EDW technical development working with operations, sales, supply chain, technical services, ERP applications, analytics, and finance & accounting teams.
Contribute to our suite of reports and data products while collaborating with business analysts, business process owners, and other team members, including UAT, and support for technical testing.
Work closely with senior team members defining data architecture, optimizing EDW performance, and updating EDW documentation, including outlining options to improve resilience and data quality monitoring.
Maintain the Enterprise Data Warehouse (EDW) and influence data quality and reliability.

COMPETENCIES
Data modeling, relational databases, data warehousing, business intelligence, stream processing.
SQL programming skills, including writing efficient and optimized SQL queries for data retrieval and manipulation.
ETL/ELT processes and data integration.
Strong analytical skills, development experience, and excellent verbal and written communications.
Manage all stages of the development lifecycle, including planning, requirements gathering, designing, developing, documenting, testing, training, deployment, and support.
Own individual and cross-functional team deliverables on small to large projects.
Project and change management methodologies including Agile/Scrum.
Willingness and availability to travel, and perform other duties as needed.
Must be a self-starter, manage workload autonomously, and balance competing priorities.
Ability and desire to continuously learn about recent technologies and Analytics trends.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Computer Engineering, Analytics, Information Systems, or equivalent experience.
1-3 years of experience working with SQL and relational databases (MS SQL Server preferred).
1-3 years of experience working with Microsoft Azure, particularly Azure Data Factory, Azure Synapse and Power Platform.
1-3 years of experience preferred in reporting, dashboards, and BI tools (Power BI preferred) for ERP and MES platforms.
1-3 years of experience preferred with Visual Studio, SSRS.
Experience with MS D365 and AVEVA or similar ERP and MES solutions preferred.
Proficient in coding and scripting in Python preferred.
Experience in Manufacturing, Supply Chain, Logistics, and Finance and Accounting solutions preferred.
How to stand out: Microsoft Azure Data Engineer Associate Certification
COMPENSATION AND BENEFITS
Available on your first day: Full medical, vision, dental benefits, short-term disability, and long-term disability
Paid time off
Monthly wellness reimbursement
Health Savings, Flexible Spending and Dependent Care accounts
401(k) retirement savings plan with employer match
Annual bonus",2.6,"Tokai Carbon GE LLC
2.6","Charlotte, NC",1001 to 5000 Employees,-1,Company - Public,Chemical Manufacturing,Manufacturing,$1 to $5 billion (USD)
587,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
588,"Information Systems, IT, Cyber Engineer & Data Science (Recent Grad/Full Time)",-1,"Design solutions to drive safe living and quality of life
The future is what you make it.
Are you ready to help us make the future?
ABOUT THE ROLE:
Honeywell is hiring Recent Graduates from bachelor’s and master’s degree programs to join our technology teams. Are you ready to get inspired as you take on impactful projects, build business skills and interact with management? You’ll help solve real-world obstacles. Work with managers in your field on meaningful projects that directly relate to key Honeywell business goals.
This position is available in multiple locations across the United States.
MINIMUM QUALIFICATIONS:
Currently pursuing a Bachelor’s or Master’s degree program in Computer Science, Computer Engineering, Information Technology, Data Science, Cyber Security or MIS
Must have graduated from an accredited college or university since May 2023, or will graduate by June 2024
Must have obtained degree within 12 months of start date
WE VALUE:
Passion for using, developing, evaluating, and learning about new technologies
Foundational business acumen and desire to improve your business knowledge
Excellent oral and written communication skills
Ability to work with multi-functional teams to meet expectations of internal and external customers
Motivated problem solver
Validated academic excellence (3.0 GPA and higher)
HoneywellURNAM
Additional Information
JOB ID: req413265
Category: Engineering
Location: United States
Exempt
Honeywell is an equal opportunity employer. Qualified applicants will be considered without regard to age, race, creed, color, national origin, ancestry, marital status, affectional or sexual orientation, gender identity or expression, disability, nationality, sex, religion, or veteran status.",4.0,"Honeywell
4.0",United States,10000+ Employees,1885,Company - Public,Computer Hardware Development,Information Technology,$10+ billion (USD)
589,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
590,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
591,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
592,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
593,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
594,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
595,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
596,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
597,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
598,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
599,Data Engineer III,-1,"We are seeking a Data Engineer III to join our Data Platform team. This role will report to the director of data platform and be involved in the planning, design, and implementation of our centralized data lake solution supporting analytics, products and applications across the company.
Qualifications:
To be successful in this role the candidate needs to have the following qualifications:
Deep knowledge of Scala and Spark
Experience with Databricks is preferred.
Deep knowledge of modern orchestration frameworks such as Apache airflow
Experience working with SQL and NoSQL database systems.
Experience with cloud environments (Azure Preferred)
Experience with acquiring and preparing data from primary and secondary disparate data sources.
Experience with agile project management methodology
Healthcare industry experience preferred, including exposure to different EMR systems, revenue cycle management.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12 is a plus but is not required.
Responsibilities:
Be part of an engineering team in building data adaptors to expedite the data onboarding process from various health systems.
Work with product management and business analysts on design reusable and configurable data orchestration pipelines.
Work with data specialist in develop and design data transformation to standardize the data model and support data enrichment activities.
As a subject matter expert, hosts information sharing session with teams within data platform or larger R1 organization.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",3.4,"R1 RCM, Inc.
3.4",Remote,10000+ Employees,2003,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
600,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
601,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
602,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
603,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
604,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
605,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
606,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
607,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
608,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
609,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
610,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
611,AWS Data Engineer/ Architect,Employer Provided Salary:$130K - $160K,"Working knowledge and experience with AWS Services including EMR, DynamoDB, Kinesis Data Stream, Lambda, S3, Salesforce
Focus on AWS and Salesforce – Sales & Service cloud.
Mastery and hands-on experience with Data Engineering technologies and scripting languages.
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Intimately involved in numerous end to end cloud migration projects.
Strong knowledge and experience in designing for and implementing solutions in the cloud (AWS). This can range from custom code on IaaS/PaaS to SaaS solution integrations.
Languages: Java, Angular, node.js, Python
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Working knowledge of application architectures, software development tools, and methodologies.
History of successful technical consulting, and architecture engagements with numerous customers simultaneously.
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Washington, DC 20001",-1,Omniware LLC,"Washington, DC",-1,-1,-1,-1,-1,-1
612,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
613,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
614,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
615,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
616,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
617,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
618,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
619,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
620,Senior Software Engineer - Data Engineering,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: The merchant Reporting and Reconciliation team is looking for a senior Back-end engineer responsible for providing technical leadership to the team and executing the unification projects end to end. Your primary focus will be developing massively scalable, distributed software systems that require high availability to our business
Job Description:
Merchant reporting is crucial for our Merchants so that they can close their account books timely and accurately with complete payment data in core markets. We are looking for people who have a passion for developing massively scalable, distributed software systems that require high availability to our business. As a member on the Merchant Reporting and Reconciliation team, you thrive in a fast-paced environment and enjoy driving innovation through rapid prototyping and iterative development. You will work directly with our Product Owners and Domain Technical Leads to create outstanding solutions and deliver incredible reporting products. You will be involved from ideation to rollout.
Your day-to-day:
Work with Product Managers and other business partners to identify opportunities for improvement
Analyze data based on product requirements
Create reports for internal teams and/or external clients
Use graphs, infographics, and other methods to visualize data
Structure large data sets to find usable information
Work with a team of analysts and other associates to process information
Create presentations and reports based on recommendations and findings
Define validation queries when needed and how to identify discrepancies in the data as they arise
Write queries for runbooks that automate the discrepancy identification process
Implement the reporting data model
Deliver within schedule in an Agile software development using test-driven development methodologies.
Participate in development life cycle activities like design, coding, testing, and production release.
Be proactive with identifying areas for improvement and innovation to improve development productivity
What do you need to bring:
BS in EE/CS or equivalent work experience and successful completion of major projects for which you can show code examples.
5+ years of hands-on data/software engineering experience
Experience working with coding languages—preferably SQL, Java, Spark-SQL, Pyspark, Python
Experience working with SQL and NoSQL DataBase
High proficiency in MS Excel, MS PowerPoint, GIT, Apache Airflow
Have a passion for quality and writing clean and solid code that scales and performs well.
Strong desire to learn, push the envelope and share knowledge with others.
Excellent analytical and time management skills
Teamwork skills with a problem-solving attitude
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
621,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
622,Data Engineer,-1,"SalesIntel is the top revenue intelligence platform on the market. Our unique approach to data collection, enhancement, verification and growth distinguishes us from the market, solidifying our position as the best B2B data partner.

The Data Engineer will be responsible for building and extending our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys working with big data and building systems from the ground up.

You will collaborate with our software engineers our software engineers, database architects, data analysts and data scientists to ensure our data delivery architecture is consistent throughout the platform. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What You’ll Be Doing:
Design and build parts of our data pipeline architecture for extraction, transformation, and loading of data from a wide variety of data sources using the latest Big Data technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with machine learning, data, and analytics experts to drive innovation, accuracy and greater functionality in our data system.
Qualifications:
Bachelor's degree in Engineering, Computer Science, or relevant field.
5+ years of experience in a Data Engineer role.
3+ years experience with Apache Spark and solid understanding of the fundamentals.
Deep understanding of Big Data concepts and distributed systems.
Strong coding skills with Scala, Python, Java and other languages and the ability to quickly switch between them with ease.
Advanced working SQL knowledge and experience working with a variety of relational databases such as Postgres and MySQL.
Experience working with data stored in many formats including TSV, CSV, JSON, Parquet.
Cloud Experience with DataBricks or similar service: S3, Athena, EC2, EMR, RDS, Redshift.
Comfortable working in a linux shell environment and writing scripts as needed.
Comfortable working in an Agile environment
Machine Learning knowledge is a plus.
Must be capable of working independently and delivering stable, efficient and reliable software.
Excellent written and verbal communication skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

This is a remote position.",-1,"SalesIntel Research, Inc.",Remote,51 to 200 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
623,GCP Data Engineer - Only W2,Employer Provided Salary:$52.73 Per Hour,"Job description:
We are seeking Data Engineer to work on analyzing and manipulating large datasets supporting the enterprise by activating data assets to support Enabling Platforms and analytics.
At least 10+ years of experience as a Data Engineer.
Data Engineers will be responsible for designing the transformation and modernization on Google Cloud Platform cloud.
Experience with large scale solutioning and operationalization of data warehouses, data lakes and analytics platforms on Google Cloud Platform is a must.
Design and build production data engineering solutions to deliver our pipeline patterns using Google Cloud Platform (Google Cloud Platform) services: BigQuery, DataFlow, Pub/Sub, BigTable, Data Fusion
BigQuery experience is a must.
Strong and in depth knowledge of SQL is essential
Communication skills are most important.
Job Type: Contract
Pay: From $52.73 per hour
Experience level:
10 years
11+ years
9 years
Schedule:
8 hour shift
Experience:
Google Cloud Platform: 8 years (Preferred)
BigQuery: 9 years (Preferred)
SQL: 8 years (Preferred)
Work Location: Remote",4.6,"Infomatics Corp
4.6",Remote,51 to 200 Employees,2005,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
624,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
625,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
626,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
627,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
628,Cloud Data Engineer,Employer Provided Salary:$69.61 - $80.00 Per Hour,"Top skills:
Must-Haves (Concepts & Tools):
AWS cloud—KMS, S3, Glue, Lambda etc.
Deployed data pipelines
Java, Python or PySpark hands on development experience
Nice-to-Haves (Concepts & Tools):
Prior ETL migration experience from on prem. To cloud
Exposure to even driven streaming
Job Types: Full-time, Contract, Temporary
Pay: $69.61 - $80.00 per hour
Experience level:
7 years
8 years
9 years
Work Location: Remote",-1,Cloudrex,Remote,-1,-1,-1,-1,-1,-1
629,Data Engineer III,-1,"We are seeking a Data Engineer III to join our Data Platform team. This role will report to the director of data platform and be involved in the planning, design, and implementation of our centralized data lake solution supporting analytics, products and applications across the company.
Qualifications:
To be successful in this role the candidate needs to have the following qualifications:
Deep knowledge of Scala and Spark
Experience with Databricks is preferred.
Deep knowledge of modern orchestration frameworks such as Apache airflow
Experience working with SQL and NoSQL database systems.
Experience with cloud environments (Azure Preferred)
Experience with acquiring and preparing data from primary and secondary disparate data sources.
Experience with agile project management methodology
Healthcare industry experience preferred, including exposure to different EMR systems, revenue cycle management.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12 is a plus but is not required.
Responsibilities:
Be part of an engineering team in building data adaptors to expedite the data onboarding process from various health systems.
Work with product management and business analysts on design reusable and configurable data orchestration pipelines.
Work with data specialist in develop and design data transformation to standardize the data model and support data enrichment activities.
As a subject matter expert, hosts information sharing session with teams within data platform or larger R1 organization.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",3.4,"R1 RCM, Inc.
3.4",Remote,10000+ Employees,2003,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
630,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
631,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
632,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
633,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
634,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
635,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
636,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
637,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
638,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
639,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
640,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
641,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
642,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
643,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
644,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
645,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
646,AWS Data Engineer/ Architect,Employer Provided Salary:$130K - $160K,"Working knowledge and experience with AWS Services including EMR, DynamoDB, Kinesis Data Stream, Lambda, S3, Salesforce
Focus on AWS and Salesforce – Sales & Service cloud.
Mastery and hands-on experience with Data Engineering technologies and scripting languages.
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Intimately involved in numerous end to end cloud migration projects.
Strong knowledge and experience in designing for and implementing solutions in the cloud (AWS). This can range from custom code on IaaS/PaaS to SaaS solution integrations.
Languages: Java, Angular, node.js, Python
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Working knowledge of application architectures, software development tools, and methodologies.
History of successful technical consulting, and architecture engagements with numerous customers simultaneously.
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Washington, DC 20001",-1,Omniware LLC,"Washington, DC",-1,-1,-1,-1,-1,-1
647,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
648,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
649,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
650,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
651,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
652,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
653,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
654,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
655,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
656,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
657,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the organization.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department. The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
658,Data Engineer,-1,"SalesIntel is the top revenue intelligence platform on the market. Our unique approach to data collection, enhancement, verification and growth distinguishes us from the market, solidifying our position as the best B2B data partner.

The Data Engineer will be responsible for building and extending our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys working with big data and building systems from the ground up.

You will collaborate with our software engineers our software engineers, database architects, data analysts and data scientists to ensure our data delivery architecture is consistent throughout the platform. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What You’ll Be Doing:
Design and build parts of our data pipeline architecture for extraction, transformation, and loading of data from a wide variety of data sources using the latest Big Data technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with machine learning, data, and analytics experts to drive innovation, accuracy and greater functionality in our data system.
Qualifications:
Bachelor's degree in Engineering, Computer Science, or relevant field.
5+ years of experience in a Data Engineer role.
3+ years experience with Apache Spark and solid understanding of the fundamentals.
Deep understanding of Big Data concepts and distributed systems.
Strong coding skills with Scala, Python, Java and other languages and the ability to quickly switch between them with ease.
Advanced working SQL knowledge and experience working with a variety of relational databases such as Postgres and MySQL.
Experience working with data stored in many formats including TSV, CSV, JSON, Parquet.
Cloud Experience with DataBricks or similar service: S3, Athena, EC2, EMR, RDS, Redshift.
Comfortable working in a linux shell environment and writing scripts as needed.
Comfortable working in an Agile environment
Machine Learning knowledge is a plus.
Must be capable of working independently and delivering stable, efficient and reliable software.
Excellent written and verbal communication skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

This is a remote position.",-1,"SalesIntel Research, Inc.",Remote,51 to 200 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
659,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
660,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
661,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
662,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
663,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
664,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
665,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
666,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
667,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
668,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
669,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
670,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
671,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
672,Data Engineer,Employer Provided Salary:$65.00 Per Hour,"Job Title: Data Engineer (23-32722)
Location: Boston, MA (2-3 day onsite a week)
Duration: Contract
Job Description:
Implementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL
Querying Snowflake using SQL.
Development of scripts using Unix, Python, etc. for loading, extracting, and transforming data.
Knowledge of Amazon Web Services Redshift
Assist with production issues in Data Warehouses like reloading data, transformations, and translations
Develop a Database Design and Reporting Design based on Business Intelligence and Reporting requirements
Skills: Snowflake, Databrcks, Azure Synapse
Job Type: Contract
Pay: Up to $65.00 per hour
Ability to commute/relocate:
Boston, MA 02108: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Are you local to Boston, MA?
Experience:
Data Engineer: 7 years (Required)
ETL: 5 years (Required)
data warehouse: 5 years (Required)
Python: 5 years (Required)
Snowflakes: 4 years (Required)
Databrcks: 4 years (Required)
Azure Synapse: 4 years (Required)
Work Location: Hybrid remote in Boston, MA 02108",-1,"Hayward Group, LLC","Boston, MA",1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
673,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
674,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
675,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
676,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
677,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
678,Data Engineer,Employer Provided Salary:$77K - $113K,"Eaton’s Electrical division is currently seeking a Business Analyst to work out of our Raleigh, NC location.

Relocation within the United States will be offered.

The expected annual salary range for this role is $77,249 - $113,299 a year.
Please note the salary information shown above is a general guideline only. Salaries are based upon candidate skills, experience, and qualifications, as well as market and business considerations.
What you’ll do:

The Data Engineer will collaborate with cross-functional teams to understand data requirements, design and implement efficient data pipelines, and ensure the availability, reliability, and scalability of our data solutions. This position necessitates proficiency in data manipulation, data analysis, SQL (writing queries), data modeling, and visualization skills.

Job responsibilities
Gather data from various sources. Design and write SQL queries to retrieve and analyze data stored in various databases.
Develop ETL processes to clean, transform, and enrich data in PowerQuery, ensuring data quality, consistency, and accuracy for use in the PowerBI's Data model.
Design and develop data model, transform raw data into meaningful insights, create relationships, hierarchies, and calculated measures in PowerBI’s Data Model
Document data integration and transformation processes for future reference and troubleshooting.
Assists in the implementation of data governance processes and systems.
Qualifications:
Basic Qualifications:
Bachelor's degree from an accredited educational institution
Minimum three (3) years of SQL and Power BI experience
Must be legally authorized to work in the United States without company sponsorship now or in the future

Preferred Qualifications:
Bachelor’s degree in computer science or software engineering
PowerBI (PowerQuery, Modelling, DAX, creating interactive visuals), Relational databases, Data Governance, Visualization
Problem-solving, Communication, Attention to detail
#LI-JM3
We are committed to ensuring equal employment opportunities for all job applicants and employees. Employment decisions are based upon job-related reasons regardless of an applicant's race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, marital status, genetic information, protected veteran status, or any other status protected by law.
Eaton considers qualified applicants regardless of criminal histories, consistent with local laws. To request a disability-related reasonable accommodation to assist you in your job search, application or interview process, please call us at 1-800-836-6345 to discuss your specific need. Only accommodation requests will be accepted by this phone number.
We know that good benefit programs are important to employees and their families. Eaton provides various Health and Welfare benefits as well as Retirement benefits, and several programs that provide for paid and unpaid time away from work. Click here for more detail: Eaton Benefits Overview. Please note that specific programs and options available to an employee may depend on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.",3.9,"Eaton
3.9","Raleigh, NC",10000+ Employees,1911,Company - Public,Electronics Manufacturing,Manufacturing,$10+ billion (USD)
679,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
680,"Data Engineer, Google Nest",-1,"Minimum qualifications:
Bachelor's degree in Computer Science, a related technical field, or equivalent practical experience.
Experience analyzing data and creating reports with database query (e.g., SQL) and visualization tools (e.g., Tableau, Dashboards).
Experience with one or more general purpose programming languages: Python, C/C++, or Java.

Preferred qualifications:
Knowledge of Machine Learning (ML) and Big Data Infrastructure.
Excellent communication and presentation skills.
About the job
The Business Strategy & Operations organization provides business critical insights using analytics, ensures cross functional alignment of goals and execution, and helps teams drive strategic partnerships and new initiatives forward. We stay focused on aligning the highest-level company priorities with effective day-to-day operations, and help evolve early stage ideas into future-growth initiatives.
The Google Nest team focuses on hardware, software, and services for the home, ranging from Nest thermostats to Nest smart displays. The Google Nest team develops, designs, and develops new technologies and hardware to make users’ homes more helpful. Our mission is the helpful home: to create a home that cares for the people inside it and the world around it.
The US base salary range for this full-time position is $115,000-$169,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.
Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google.

Responsibilities
Collaborate with Google Home App (GHA), Home Graph and other Platform engineering team to ensure infrastructure supports key analyses by identifying logging gaps. Build or Modify new/existing data pipelines for tracking GHA metrics.
Conduct end-to-end analysis that includes data gathering and requirements specification, processing, analysis, ongoing deliverables, and presentations.
Act as a thought partner to produce insights and metrics for various technical and business stakeholders across the Smart Home Platforms teams.
Deliver presentations of findings and recommendations to leadership, creating visual displays of quantitative information.
Make business recommendations with the presentations of findings at multiple levels of stakeholders through visual displays of quantitative information.
Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form.",4.4,"Google
4.4","Mountain View, CA",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
681,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the organization.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department. The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
682,Data Engineer (Onsite),Employer Provided Salary:$85K - $100K,"If you are seeking a career that has tremendous impact on manufacturing operations, then come be a part of Tokai Carbon GE's team to design, develop and implement our new manufacturing execution system. You'll play a critical role in our organization's success, providing easier access to data that engineers, analysts and decision-makers need to do their jobs. You will set the foundation of data for growth, relying on your programming and problem-solving skills to create scalable solutions. You will be part of the team building BI and Analytics tools that will take our operations to the next level. If you are excited about applying your knowledge and skills to meaningful challenges, apply today!
JOB SUMMARY
The Data Engineer will be responsible for designing, implementing, and supporting solutions used to collect, transform, store, and analyze data from a variety of company applications. They analyze and organize raw data, build data systems and pipelines, review and analyze data infrastructure, plan, and implement solutions to store and manage data. Also, they create reports, dashboards, and other analytics solutions. They will work closely with the lines of business and external partners, to ensure successful development and delivery of BI and data analytics solutions. This is a hands-on development and support position with ownership responsibility for data architecture and engineering for BI and analytics purposes.
ESSENTIAL FUNCTIONS
Work with business users and other technology teams to understand their data requirements, identify appropriate data sources, design, and develop data pipelines and design appropriate data structures to store the required data.
Design data pipelines using SQL, other programming languages and data integration tools.
Gather and combine data from a variety of sources to support actionable decision making. Prepare data for reporting, and predictive and prescriptive tools.
Address BI/EDW technical development working with operations, sales, supply chain, technical services, ERP applications, analytics, and finance & accounting teams.
Contribute to our suite of reports and data products while collaborating with business analysts, business process owners, and other team members, including UAT, and support for technical testing.
Work closely with senior team members defining data architecture, optimizing EDW performance, and updating EDW documentation, including outlining options to improve resilience and data quality monitoring.
Maintain the Enterprise Data Warehouse (EDW) and influence data quality and reliability.

COMPETENCIES
Data modeling, relational databases, data warehousing, business intelligence, stream processing.
SQL programming skills, including writing efficient and optimized SQL queries for data retrieval and manipulation.
ETL/ELT processes and data integration.
Strong analytical skills, development experience, and excellent verbal and written communications.
Manage all stages of the development lifecycle, including planning, requirements gathering, designing, developing, documenting, testing, training, deployment, and support.
Own individual and cross-functional team deliverables on small to large projects.
Project and change management methodologies including Agile/Scrum.
Willingness and availability to travel, and perform other duties as needed.
Must be a self-starter, manage workload autonomously, and balance competing priorities.
Ability and desire to continuously learn about recent technologies and Analytics trends.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Computer Engineering, Analytics, Information Systems, or equivalent experience.
1-3 years of experience working with SQL and relational databases (MS SQL Server preferred).
1-3 years of experience working with Microsoft Azure, particularly Azure Data Factory, Azure Synapse and Power Platform.
1-3 years of experience preferred in reporting, dashboards, and BI tools (Power BI preferred) for ERP and MES platforms.
1-3 years of experience preferred with Visual Studio, SSRS.
Experience with MS D365 and AVEVA or similar ERP and MES solutions preferred.
Proficient in coding and scripting in Python preferred.
Experience in Manufacturing, Supply Chain, Logistics, and Finance and Accounting solutions preferred.
How to stand out: Microsoft Azure Data Engineer Associate Certification
COMPENSATION AND BENEFITS
Available on your first day: Full medical, vision, dental benefits, short-term disability, and long-term disability
Paid time off
Monthly wellness reimbursement
Health Savings, Flexible Spending and Dependent Care accounts
401(k) retirement savings plan with employer match
Annual bonus",2.6,"Tokai Carbon GE LLC
2.6","Charlotte, NC",1001 to 5000 Employees,-1,Company - Public,Chemical Manufacturing,Manufacturing,$1 to $5 billion (USD)
683,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
684,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
685,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
686,AWS Data Engineer/ Architect,Employer Provided Salary:$130K - $160K,"Working knowledge and experience with AWS Services including EMR, DynamoDB, Kinesis Data Stream, Lambda, S3, Salesforce
Focus on AWS and Salesforce – Sales & Service cloud.
Mastery and hands-on experience with Data Engineering technologies and scripting languages.
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Intimately involved in numerous end to end cloud migration projects.
Strong knowledge and experience in designing for and implementing solutions in the cloud (AWS). This can range from custom code on IaaS/PaaS to SaaS solution integrations.
Languages: Java, Angular, node.js, Python
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Working knowledge of application architectures, software development tools, and methodologies.
History of successful technical consulting, and architecture engagements with numerous customers simultaneously.
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Washington, DC 20001",-1,Omniware LLC,"Washington, DC",-1,-1,-1,-1,-1,-1
687,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
688,Data Engineer,-1,"Duration: 11+ months

Job Description:

Aviation connects the world and Connected Aviation Solutions (CAS) connects Aviation. Sustainably. Seamlessly. Securely. The Data Management & Data Science (DM&DS) team is tasked with the end to end responsibility to ensure that CAS data assets are managed with integrity and quality prior to consumption by our critical customer facing applications - whether via APIs, analytics and/or data visualizations. As a senior data engineer on the DM&DS team, you will be responsible for the design, development and maintenance of data processes and pipelines supporting critical CAS Strategic Business Unit (SBU) Data initiatives in support of the Digital Transformation of CAS as well as for the cross-SBU Connected Ecosystem. In this endeavor, you will be working closely with data architecture, data analytics & visualization teams leaders across CAS, SBU and Digital Technology (DT) teams to ensure the technical solutions are efficient, scalable and meet long term Connected Ecosystem needs.

Primary Responsibilities:
Design, develop and support the processes and pipelines for moving data throughout the CAS and cross SBU environments.
Develop automation and monitoring processes that support the data pipelines
Work closely with the architecture team to implement modern data repositories that support the CAS use cases (Pipelines, APIs, Data Science, Applications and Visualizations)
Work with internal business customers and software development teams to gather and document requirements for data publishing and data consumption via data warehouse, data lake, and analytics solutions
Support the operation of the CAS and Digital Technology owned Data Platforms, Data Warehouse and Data Lakes with a view to leveraging capabilities and resources over-time
Work with the CAS and DT Enterprise Data Architects to automate cloud deployments, as well as build CI/CD pipeline to support Cloud-Based workloads. Develop views, materialized views, and SQL scripts
Work with the CAS and DT Enterprise Data Architects to recommend investments or changes in technology, resources, procedures, equipment, systems, or other assets to improve the quality of the organizations projects.
May travel domestically and internationally up to 15%.

Qualifications / Required Skills:
Bachelors degree and 5 years of prior relevant experience OR Advanced Degree in a related technical field and minimum years 3 experience OR In absence of a degree, 10 years of relevant experience is required
3+ years of demonstrated engineering leadership in a relevant engineering function, such as software/service development and deployment, system design and integration, or data analytics.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.",3.8,"Capgemini
3.8",Remote,10000+ Employees,1967,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
689,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
690,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
691,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
692,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
693,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
694,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
695,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
696,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
697,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
698,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
699,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
700,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
701,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
702,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
703,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
704,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
705,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
706,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
707,Data Engineer,-1,"Blackwell Security Inc. is a start-up backed by venture capital, focused on bridging the technology gap in healthcare. Our purpose-built ecosystem provides comprehensive cybersecurity managed services for life sciences and healthcare. We are building a customizable product that ensures health systems have access to a suite of security solutions, with built-in visualization and optimization to ensure the safety of patient information.
As we continue to build out our core team, we are adding a Data Engineer to our Engineering Team. You will be working cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data models. You will design, implement, and scale data pipelines that transform billions of records into action and insight.
This is a unique opportunity to jump into an early-stage start-up at a pivotal time and make a meaningful impact. If you thrive in a small, growing environment and love the energy of start-ups, this is the role for you!
While our headquarters are in Detroit, Michigan, this is a remote role but ideally a candidate would live in Detroit or Minneapolis, location of our core Engineering Team. This role is not eligible for visa sponsorship.
What you will do in the Data Engineer role:
Collaborate with engineering to build and maintain an enterprise data ecosystem including ingestion, storage, organization and interface.
Analyze the business and technical requirements for data systems and applications; Coordinate the integration of IT policies, procedures and development practices.
Translate business requirements into data models that are easy to understand and used by different disciplines across the company.
Design, implement, build/enhance pipelines that deliver data with measurable quality under the SLA.
Partner with business domain experts, data analysts and engineering teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service.
Champion the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements.
Own and document foundational company metrics and benchmarks with clear definition and data lineage.
Identify, document and promote best practices.
Design and architect data systems, focusing not only on performance and scalability, but also on crafting a beautiful user experience.
Define/Implement data visualizations & UX for external/internal customers.
Taking a thoughtful approach to decision making; balance speed and quality, with a focus on tangible results.
Explore Blackwell’s data to discover trends and opportunities, identify what questions we should be asking of our data.
Analyze & evaluate transactional system data for transformation and use in reporting, analytics, and AI/ML.
Evaluate and establish early strategies and usage of AI (machine learning, generative AI, etc.).
Qualities and skills for success in the Data Engineer role:
Bachelor's degree in Computer Science, Engineering, or related technical or business field.
Attention to detail, and Agile development experience.
Experience with Python and AWS services.
Experience with various data storage systems, RDBMS, Document/NoSQL DBs, etc.
Experience implementing data pipelines via methods such as ETL, ELT, EL/TL, DaaS, Data Lake or ODS.
Experience experimenting with and applying AI (machine learning, generative AI, etc.) in an enterprise environment.
Experience working with multi-customer multi-tenant environments preferred. Experience with cybersecurity data is not required but is a plus.
Adaptable and focused on solutions.
Equal Employment Opportunity
We’re proud to be an equal opportunity employer and welcome our employee’s differences, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or Veteran status. Difference makes us better. Join us.",2.8,"Blackwell Security, Inc.
2.8",Remote,51 to 200 Employees,1999,Company - Private,Security & Protective,Management & Consulting,$1 to $5 million (USD)
708,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the organization.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department. The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
709,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
710,Data Engineer,-1,"SalesIntel is the top revenue intelligence platform on the market. Our unique approach to data collection, enhancement, verification and growth distinguishes us from the market, solidifying our position as the best B2B data partner.

The Data Engineer will be responsible for building and extending our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys working with big data and building systems from the ground up.

You will collaborate with our software engineers our software engineers, database architects, data analysts and data scientists to ensure our data delivery architecture is consistent throughout the platform. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What You’ll Be Doing:
Design and build parts of our data pipeline architecture for extraction, transformation, and loading of data from a wide variety of data sources using the latest Big Data technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with machine learning, data, and analytics experts to drive innovation, accuracy and greater functionality in our data system.
Qualifications:
Bachelor's degree in Engineering, Computer Science, or relevant field.
5+ years of experience in a Data Engineer role.
3+ years experience with Apache Spark and solid understanding of the fundamentals.
Deep understanding of Big Data concepts and distributed systems.
Strong coding skills with Scala, Python, Java and other languages and the ability to quickly switch between them with ease.
Advanced working SQL knowledge and experience working with a variety of relational databases such as Postgres and MySQL.
Experience working with data stored in many formats including TSV, CSV, JSON, Parquet.
Cloud Experience with DataBricks or similar service: S3, Athena, EC2, EMR, RDS, Redshift.
Comfortable working in a linux shell environment and writing scripts as needed.
Comfortable working in an Agile environment
Machine Learning knowledge is a plus.
Must be capable of working independently and delivering stable, efficient and reliable software.
Excellent written and verbal communication skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

This is a remote position.",-1,"SalesIntel Research, Inc.",Remote,51 to 200 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
711,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
712,Data Engineer,-1,"Duration: 11+ months

Job Description:

Aviation connects the world and Connected Aviation Solutions (CAS) connects Aviation. Sustainably. Seamlessly. Securely. The Data Management & Data Science (DM&DS) team is tasked with the end to end responsibility to ensure that CAS data assets are managed with integrity and quality prior to consumption by our critical customer facing applications - whether via APIs, analytics and/or data visualizations. As a senior data engineer on the DM&DS team, you will be responsible for the design, development and maintenance of data processes and pipelines supporting critical CAS Strategic Business Unit (SBU) Data initiatives in support of the Digital Transformation of CAS as well as for the cross-SBU Connected Ecosystem. In this endeavor, you will be working closely with data architecture, data analytics & visualization teams leaders across CAS, SBU and Digital Technology (DT) teams to ensure the technical solutions are efficient, scalable and meet long term Connected Ecosystem needs.

Primary Responsibilities:
Design, develop and support the processes and pipelines for moving data throughout the CAS and cross SBU environments.
Develop automation and monitoring processes that support the data pipelines
Work closely with the architecture team to implement modern data repositories that support the CAS use cases (Pipelines, APIs, Data Science, Applications and Visualizations)
Work with internal business customers and software development teams to gather and document requirements for data publishing and data consumption via data warehouse, data lake, and analytics solutions
Support the operation of the CAS and Digital Technology owned Data Platforms, Data Warehouse and Data Lakes with a view to leveraging capabilities and resources over-time
Work with the CAS and DT Enterprise Data Architects to automate cloud deployments, as well as build CI/CD pipeline to support Cloud-Based workloads. Develop views, materialized views, and SQL scripts
Work with the CAS and DT Enterprise Data Architects to recommend investments or changes in technology, resources, procedures, equipment, systems, or other assets to improve the quality of the organizations projects.
May travel domestically and internationally up to 15%.

Qualifications / Required Skills:
Bachelors degree and 5 years of prior relevant experience OR Advanced Degree in a related technical field and minimum years 3 experience OR In absence of a degree, 10 years of relevant experience is required
3+ years of demonstrated engineering leadership in a relevant engineering function, such as software/service development and deployment, system design and integration, or data analytics.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.",3.8,"Capgemini
3.8",Remote,10000+ Employees,1967,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
713,Data Engineer,$80K - $113K (Glassdoor est.),"Data Scientist / Data Engineer - AWS Tools Specialist
Job Description:
We are looking for a versatile and experienced professional to fill the role of Data Scientist / Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.
Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.
Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and QuickSight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development.
Experience with machine learning frameworks and libraries (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.
If you're a self-driven individual with a passion for data, analytics, and cloud technologies, this role offers an exciting opportunity to contribute to our data-driven initiatives while utilizing the power of AWS tools to their fullest potential.",4.2,"MST Solutions
4.2","Phoenix, AZ",201 to 500 Employees,2012,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
714,Data Engineer,Employer Provided Salary:$80K - $85K,"Simatree is searching for a Data Engineer Consultant to support our client’s operational requirements on a mission focused program within a challenging, dynamic setting. Successful candidates in this position will collaborate with a diverse group of individuals across the homeland security community and directly contribute to management and analysis of operational data using data science tools. This role supports a US Government contract. An US Citizenship is required.
The Role
The Data Engineer will work alongside a team to generate management tools that effectively control programs and generate reports while supporting the programmatic development of operational applications of the tools or analysis that are focused on operations research and document findings. They will conduct research into management, organizational, and business practices to implement improvements to existing processes. They will utilize commercial or open-source software to manage tasks and present findings to a broad audience, including agency executives. They will maintain a database to produce monthly financial reports, tracks processes, distribute deliverables, and monitor comment status. The Data Engineer will be required to use Microsoft Office to perform queries, analyses, and produce reports. Other responsibilities include:
Use R and Python to perform analyses and develop new tools using popular data visualization libraries such as ggplot2, plotly, matplotlib, or bokeh.
Use Databricks to Extract, Transform, and Load (ETL) data from different sources and transform it into a usable, reliable resource.
Manage data collection and transformation from federal law enforcement and other databases to be used in a wide spectrum of analyses including statistical modeling, performance metrics analysis, crosschecking data with other federal agencies, and risk analysis.
Use ArcGIS to perform analyses and develop new web map applications that summarize results from deep learning/AI algorithms.
Maintain and enhance business intelligence (BI) dashboards using best practices in UI/UX design
Develop and test new methodologies and algorithms which would increase the efficiency and accuracy of existing models, including pilot deployments of automated scripts
Requirements
This role supports a US Government contract. A US citizenship is required.
Strong Experience with SQL programming
Experience with R statistical programming and Python programming
Experience with BI software such as Qlik, Tableau or PowerBI
Demonstrated experience in performing data analysis and modeling on an independent basis
Demonstrated experience in developing and applying quantitative methods to find patterns and relationships in large data sets using statistical and graphical packages
Ability to translate complex, technical, or analytic findings into an easily understood narrative—tell a story with graphical, verbal or written form
3 years experience in supporting quantitative and analytical efforts
Bachelors degree in relevant fields such as statistics, mathematics, computer science, physical science, economics or engineering
Job Type: Full-time
Pay: $80,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Experience level:
1 year
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This question MUST be answered for consideration: This position supports a US Government contract which requires a US Citizenship. Are you a US Citizen?
Work Location: Hybrid remote in Washington, DC 20001",5.0,"Simatree
5.0","Washington, DC",1 to 50 Employees,2018,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
715,Data Engineer,Employer Provided Salary:$77K - $113K,"Eaton’s Electrical division is currently seeking a Business Analyst to work out of our Raleigh, NC location.

Relocation within the United States will be offered.

The expected annual salary range for this role is $77,249 - $113,299 a year.
Please note the salary information shown above is a general guideline only. Salaries are based upon candidate skills, experience, and qualifications, as well as market and business considerations.
What you’ll do:

The Data Engineer will collaborate with cross-functional teams to understand data requirements, design and implement efficient data pipelines, and ensure the availability, reliability, and scalability of our data solutions. This position necessitates proficiency in data manipulation, data analysis, SQL (writing queries), data modeling, and visualization skills.

Job responsibilities
Gather data from various sources. Design and write SQL queries to retrieve and analyze data stored in various databases.
Develop ETL processes to clean, transform, and enrich data in PowerQuery, ensuring data quality, consistency, and accuracy for use in the PowerBI's Data model.
Design and develop data model, transform raw data into meaningful insights, create relationships, hierarchies, and calculated measures in PowerBI’s Data Model
Document data integration and transformation processes for future reference and troubleshooting.
Assists in the implementation of data governance processes and systems.
Qualifications:
Basic Qualifications:
Bachelor's degree from an accredited educational institution
Minimum three (3) years of SQL and Power BI experience
Must be legally authorized to work in the United States without company sponsorship now or in the future

Preferred Qualifications:
Bachelor’s degree in computer science or software engineering
PowerBI (PowerQuery, Modelling, DAX, creating interactive visuals), Relational databases, Data Governance, Visualization
Problem-solving, Communication, Attention to detail
#LI-JM3
We are committed to ensuring equal employment opportunities for all job applicants and employees. Employment decisions are based upon job-related reasons regardless of an applicant's race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, marital status, genetic information, protected veteran status, or any other status protected by law.
Eaton considers qualified applicants regardless of criminal histories, consistent with local laws. To request a disability-related reasonable accommodation to assist you in your job search, application or interview process, please call us at 1-800-836-6345 to discuss your specific need. Only accommodation requests will be accepted by this phone number.
We know that good benefit programs are important to employees and their families. Eaton provides various Health and Welfare benefits as well as Retirement benefits, and several programs that provide for paid and unpaid time away from work. Click here for more detail: Eaton Benefits Overview. Please note that specific programs and options available to an employee may depend on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.",3.9,"Eaton
3.9","Raleigh, NC",10000+ Employees,1911,Company - Public,Electronics Manufacturing,Manufacturing,$10+ billion (USD)
716,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
717,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
718,Sr. Data Engineer (with ETL testing experience)(W2 only),Employer Provided Salary:$50.00 - $65.00 Per Hour,"Good engineers who have knowledge of programming in Python/Pyspark – functional programming experience is a definite plus.
· Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
· Should know how to program distributed systems
· Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
· Knowledge of Test Automation preferred
Responsibilities:
Develop and maintain data pipelines and ETL processes using Python and PySpark.
Design and implement scalable and efficient data processing systems for large datasets.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and ensure data quality.
Work on building data flow pipelines in a cloud environment (AWS, preferred), including data extraction, transformation, and loading.
Implement functional programming concepts to ensure robust and maintainable code.
Program distributed systems and utilize parallel processing to optimize data processing tasks.
Contribute to the design and development of data warehouses, such as Snowflake and Redshift.
Perform quality engineering of data pipelines and ensure data accuracy, integrity, and consistency.
Develop and execute test plans for data pipelines and automation scripts to validate data.
Collaborate with DevOps teams to ensure the smooth deployment and monitoring of data pipelines.
Stay updated with the latest trends and technologies in data engineering and cloud computing.
Experience Required:
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Strong programming skills in Python and experience with PySpark for data processing.
Functional programming experience is a significant advantage.
Proficiency in cloud platforms such as AWS (preferred), Azure, or Google Cloud.
Knowledge of distributed systems and their principles.
Hands-on experience with building and optimizing data pipelines.
Familiarity with data warehousing concepts and tools like Snowflake, Redshift, etc.
Understanding of data quality and data integration best practices.
Knowledge of test automation frameworks and methodologies.
Strong problem-solving skills and ability to work with large datasets.
Excellent communication skills and the ability to collaborate with cross-functional teams.
Strong attention to detail and commitment to delivering high-quality data solutions.
Job Type: Contract
Salary: $50.00 - $65.00 per hour
Benefits:
Paid time off
Compensation package:
Performance bonus
Experience level:
5 years
Schedule:
8 hour shift
Experience:
Informatica: 4 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 6 years (Preferred)
ETL: 4 years (Preferred)
Work Location: Remote",4.0,"Fourth Technologies, Inc.
4.0",Remote,51 to 200 Employees,1987,Company - Private,Business Consulting,Management & Consulting,$5 to $25 million (USD)
719,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
720,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
721,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
722,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
723,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
724,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
725,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
726,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
727,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
728,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
729,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
730,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
731,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
732,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
733,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
734,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
735,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
736,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
737,"Data Engineer, Google Nest",-1,"Minimum qualifications:
Bachelor's degree in Computer Science, a related technical field, or equivalent practical experience.
Experience analyzing data and creating reports with database query (e.g., SQL) and visualization tools (e.g., Tableau, Dashboards).
Experience with one or more general purpose programming languages: Python, C/C++, or Java.

Preferred qualifications:
Knowledge of Machine Learning (ML) and Big Data Infrastructure.
Excellent communication and presentation skills.
About the job
The Business Strategy & Operations organization provides business critical insights using analytics, ensures cross functional alignment of goals and execution, and helps teams drive strategic partnerships and new initiatives forward. We stay focused on aligning the highest-level company priorities with effective day-to-day operations, and help evolve early stage ideas into future-growth initiatives.
The Google Nest team focuses on hardware, software, and services for the home, ranging from Nest thermostats to Nest smart displays. The Google Nest team develops, designs, and develops new technologies and hardware to make users’ homes more helpful. Our mission is the helpful home: to create a home that cares for the people inside it and the world around it.
The US base salary range for this full-time position is $115,000-$169,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.
Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google.

Responsibilities
Collaborate with Google Home App (GHA), Home Graph and other Platform engineering team to ensure infrastructure supports key analyses by identifying logging gaps. Build or Modify new/existing data pipelines for tracking GHA metrics.
Conduct end-to-end analysis that includes data gathering and requirements specification, processing, analysis, ongoing deliverables, and presentations.
Act as a thought partner to produce insights and metrics for various technical and business stakeholders across the Smart Home Platforms teams.
Deliver presentations of findings and recommendations to leadership, creating visual displays of quantitative information.
Make business recommendations with the presentations of findings at multiple levels of stakeholders through visual displays of quantitative information.
Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form.",4.4,"Google
4.4","Mountain View, CA",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
738,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
739,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
740,Data Engineer,Employer Provided Salary:$100K - $160K,"Position Summary
The Effectual Data Engineer builds pipelines that are used to transport date from a data source to a data warehouse. These pipelines are crucial: they are what allow us to access and analyze an organizations data and use the insights to help them make decisions. Data pipelines transport and transform data according to established business rules or a line of exploratory analysis the business wants to undertake. As a Data Engineer, you will prepare and organize the data that organizations have built in their databases and other formats.
A Glimpse into the Daily Routine of a Data Engineer
As a Data Engineer, specializing in Confluent Kafka, you will take on big data challenges in an agile way. You will build data pipelines, utilizing Confluent Kafka, that enables our clients and their vision. You will also build data models to deliver insightful analytics while ensuring the highest standard in data integrity. You are encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.
Essential Duties and Responsibilities
Design, build and launch extremely efficient and reliable data pipelines, utilizing Confluent Kafka, to move data across several platforms including Data Lakes, Data Warehouses, and real-time systems.
Develop, construct, test and maintain data architectures from the data architect.
Analyze organic and raw data.
Build data systems and pipelines.
Build the infrastructure required for extraction, transformation, and loading of data from different data sources using SQL and AWS 'big data' technologies.
Write scripts for data architects, data scientists, and data quality engineers.
Data acquisition.
Identify ways to improve data reliability, efficiency, and quality.
Develop dataset processes.
Prepare data for prescriptive and predictive modeling.
Automate the data collection and analysis processes, data releasing and reporting tools.
Build algorithms and prototypes.
Develop analytical tools and programs.
Qualifications
MUST HAVE hands-on experience with Confluent Kafka including both administration and development.
Either Confluent Certified Administrator for Apache Kafka (CCAAK) or Confluent Certified Developer (CCDAK) for Apache Kafka certificates, however, both are preferred.
Bachelor's or master's degree in computer science, Engineering or a related field.
Experience working as a Data Engineer in a professional services or consulting environment.
Proficiency in programming languages such as Python, Java, or Scala, with expertise in data processing frameworks and libraries (e.g., Spark, Hadoop, SQL).
In-depth knowledge of database systems (relational and NoSQL), data modeling, and data warehousing concepts.
Strong knowledge of data architectures and data modeling and data infrastructure ecosystem.
Experience with cloud-based data platforms and services (e.g., AWS, Azure) including familiarity with relevant tools (e.g., S3, Redshift, BigQuery, etc.).
Proficiency in designing and implementing ETL processes and data integration workflows using tools like Apache Airflow, Informatica, or Talend.
Familiarity with data governance practices, data quality frameworks, and data security principles
Work minimal direction and turn a clients want and need into working stories, epics which can be performed upon during a sprint.
A firm understanding of the SDLC process.
An understanding of object-oriented programming.
The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex.
The ability to thrive in a dynamic environment. That means being flexible and willing to jump in and do whatever it takes to be successful.
Ability to travel, with strong preference to mid-west time zone or east coast.
Nice-to-Have Skills and Experience
Knowledge of batch and streaming data architectures.
Product mindset to understand business needs and come up with scalable engineering solutions.
AWS Certified Cloud Practitioner
AWS Certified Data Analytics Specialty
AWS Certified Machine Learning Specialty
AWS Certified Database Specialty
SnowPro Core Certification
Databricks Certified Data Engineer Associate
Company Offered Benefits
Full-time employees are eligible to participate in our employee benefit programs:
Medical, dental, and vision health insurances,
Short term disability, long term disability and life insurances,
401k with Company match
Paid time off (PTO) (120 hours PTO that accrue over one year)
Paid time off for major holidays (14 days per year)
These and any other employee benefit offerings are subject to management's discretion and may change at any time.
Physical Demands and Work Environment
The work is generally performed in an office environment. Physical demands include sitting, keyboarding, verbal communication, written communication. Employees are occasionally required to stand; walk; reach with hands and arms; climb or balance; and stoop, kneel, crouch, or crawl. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this position. Reasonable accommodation may be made to enable individuals with disabilities to perform the functions.
Salary Range for this position: $100,000-$160,000
""Salary ranges provided are for informational purposes only and may vary depending on factors such as experience, qualifications, and geographic location. The final salary offer will be determined based on the candidate's skills and alignment with the role requirements.""
This job description may not be inclusive of all assigned duties, responsibilities, or aspects of the job described, and may be amended anytime at the sole discretion of the Employer. Duties and responsibilities are subject to possible modification to reasonably accommodate individuals with disabilities. To perform this job successfully, the incumbents will possess the skills, aptitudes, and abilities to perform each duty proficiently. This document does not create an employment contract, implied or otherwise, other than an ""at will"" relationship. Effectual Inc. is an EEO employer and does not discriminate on the basis of any protected classification in its hiring, promoting, or any other job-related opportunity.",4.0,"Effectual
4.0",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
741,"Senior Data Engineer, IT Analytics",-1,"Location: DFW Headquarters Building 8 (DFW-SV08)
Additional Locations: None
Requisition ID: 68799
Intro
Are you ready to explore a world of possibilities, both at work and during your time off? Join our American Airlines family, and you’ll travel the world, grow your expertise and become the best version of you. As you embark on a new journey, you’ll tackle challenges with flexibility and grace, learning new skills and advancing your career while having the time of your life. Feel free to enrich both your personal and work life and hop on board!
Why you'll love this job
You will help enable data engineering solutions at AA
You will be part of a team that innovates.
This role is a part of the Data Engineering and Analytics team within our Technology group. You’ll bring your data engineering, collaboration and analytics skills to help cultivate a data driven culture by designing and delivering analytics solutions and making data analytics easier and more effective for American Airlines.
What you'll do
As noted above, this list is intended to reflect the current job but there may be additional essential functions (and certainly non-essential job functions) that are not referenced. Management will modify the job or require other tasks be performed whenever it is deemed appropriate to do so, observing, of course, any legal obligations including any collective bargaining obligations.
Work closely with source data application teams and product owners to design, implement and support analytics solutions that provide insights to make better decisions
Implement data migration and data engineering solutions using Azure products and services: (Azure Data Lake Storage, Azure Data Factory, Azure Functions, Event Hub, Azure Stream Analytics, Azure Databricks, etc.) and traditional data warehouse tools.
Perform multiple aspects involved in the development lifecycle – design, cloud engineering (Infrastructure, network, security, and administration), ingestion, preparation, data modeling, testing, CICD pipelines, performance tuning, deployments, consumption, BI, alerting, prod support.
Provide technical leadership and collaborate within a team environment as well as work independently.
Be a part of a DevOps team that completely owns and supports their product
Implement batch and streaming data pipelines using cloud technologies
Leads development of coding standards, best practices and privacy and security guidelines.
Mentors others on technical and domain skills to create multi-functional teams
All you'll need for success
Minimum Qualifications- Education & Prior Job Experience
Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training
3 years software solution development using agile, DevOps, operating in a product model that includes designing, developing, and implementing large-scale applications or data engineering solutions
3 years data analytics experience using SQL
2 years of cloud development and data lake experience (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Power BI.
Combination of Development, Administration & Support experience in several of the following tools/platforms required:
Scripting: Python, Spark, Unix, SQL
Data Platforms: Teradata, Cassandra, MongoDB, Oracle, SQL Server, ADLS, Snowflake
Azure Data Explorer. Administration skills a plus
Azure Cloud Technologies: Azure Data Factory, Azure Databricks, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Azure Functions
CI/CD: GitHub, Jenkins, Azure DevOps, Terraform
BI Analytics Tool Stack - Cognos, Tableau, Power BI, Alteryx, Denodo, and Grafana
Data Warehousing: DataStage, Informatica
Data Governance and Privacy: Informatica Axon and EDC, BigID
Preferred Qualifications- Education & Prior Job Experience
5+ years software solution development using agile, dev ops, product model that includes designing, developing, and implementing large-scale applications or data engineering solutions.
5+ years data analytics experience using SQL
3+ years full-stack development experience, preferably in Azure
3+ years of cloud development and data lake experience (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Functions, ADX, ASA, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Power BI.
Airline Industry Experience
Skills, Licenses & Certifications
Expertise with the Azure Technology stack for data management, data ingestion, capture, processing, curation and creating consumption layers.
Expertise in providing practical direction within the Azure Native cloud services.
Azure Development Track Certification (preferred)
Spark Certification (preferred)
What you'll get
Feel free to take advantage of all that American Airlines has to offer:
Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.
Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more.
Wellness Programs: We want you to be the best version of yourself – that’s why our wellness programs provide you with all the right tools, resources and support you need.
401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.
Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more
Feel free to be yourself at American
From the team members we hire to the customers we serve, inclusion and diversity are the foundation of the dynamic workforce at American Airlines. Our 20+ Employee Business Resource Groups are focused on connecting our team members to our customers, suppliers, communities and shareholders, helping team members reach their full potential and creating an inclusive work environment to meet and exceed the needs of our diverse world.

Are you ready to feel a tremendous sense of pride and satisfaction as you do your part to keep the largest airline in the world running smoothly as we care for people on life’s journey? Feel free to be yourself at American.
Additional Locations: None
Requisition ID: 68799",3.8,"American Airlines
3.8","Dallas, TX",10000+ Employees,1926,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
742,Senior Software Engineer - Data Engineering,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: The merchant Reporting and Reconciliation team is looking for a senior Back-end engineer responsible for providing technical leadership to the team and executing the unification projects end to end. Your primary focus will be developing massively scalable, distributed software systems that require high availability to our business
Job Description:
Merchant reporting is crucial for our Merchants so that they can close their account books timely and accurately with complete payment data in core markets. We are looking for people who have a passion for developing massively scalable, distributed software systems that require high availability to our business. As a member on the Merchant Reporting and Reconciliation team, you thrive in a fast-paced environment and enjoy driving innovation through rapid prototyping and iterative development. You will work directly with our Product Owners and Domain Technical Leads to create outstanding solutions and deliver incredible reporting products. You will be involved from ideation to rollout.
Your day-to-day:
Work with Product Managers and other business partners to identify opportunities for improvement
Analyze data based on product requirements
Create reports for internal teams and/or external clients
Use graphs, infographics, and other methods to visualize data
Structure large data sets to find usable information
Work with a team of analysts and other associates to process information
Create presentations and reports based on recommendations and findings
Define validation queries when needed and how to identify discrepancies in the data as they arise
Write queries for runbooks that automate the discrepancy identification process
Implement the reporting data model
Deliver within schedule in an Agile software development using test-driven development methodologies.
Participate in development life cycle activities like design, coding, testing, and production release.
Be proactive with identifying areas for improvement and innovation to improve development productivity
What do you need to bring:
BS in EE/CS or equivalent work experience and successful completion of major projects for which you can show code examples.
5+ years of hands-on data/software engineering experience
Experience working with coding languages—preferably SQL, Java, Spark-SQL, Pyspark, Python
Experience working with SQL and NoSQL DataBase
High proficiency in MS Excel, MS PowerPoint, GIT, Apache Airflow
Have a passion for quality and writing clean and solid code that scales and performs well.
Strong desire to learn, push the envelope and share knowledge with others.
Excellent analytical and time management skills
Teamwork skills with a problem-solving attitude
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
743,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
744,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
745,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
746,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
747,Data Engineer,-1,"SalesIntel is the top revenue intelligence platform on the market. Our unique approach to data collection, enhancement, verification and growth distinguishes us from the market, solidifying our position as the best B2B data partner.

The Data Engineer will be responsible for building and extending our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys working with big data and building systems from the ground up.

You will collaborate with our software engineers our software engineers, database architects, data analysts and data scientists to ensure our data delivery architecture is consistent throughout the platform. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

What You’ll Be Doing:
Design and build parts of our data pipeline architecture for extraction, transformation, and loading of data from a wide variety of data sources using the latest Big Data technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with machine learning, data, and analytics experts to drive innovation, accuracy and greater functionality in our data system.
Qualifications:
Bachelor's degree in Engineering, Computer Science, or relevant field.
5+ years of experience in a Data Engineer role.
3+ years experience with Apache Spark and solid understanding of the fundamentals.
Deep understanding of Big Data concepts and distributed systems.
Strong coding skills with Scala, Python, Java and other languages and the ability to quickly switch between them with ease.
Advanced working SQL knowledge and experience working with a variety of relational databases such as Postgres and MySQL.
Experience working with data stored in many formats including TSV, CSV, JSON, Parquet.
Cloud Experience with DataBricks or similar service: S3, Athena, EC2, EMR, RDS, Redshift.
Comfortable working in a linux shell environment and writing scripts as needed.
Comfortable working in an Agile environment
Machine Learning knowledge is a plus.
Must be capable of working independently and delivering stable, efficient and reliable software.
Excellent written and verbal communication skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

This is a remote position.",-1,"SalesIntel Research, Inc.",Remote,51 to 200 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
748,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
749,Data Engineer,-1,"Duration: 11+ months

Job Description:

Aviation connects the world and Connected Aviation Solutions (CAS) connects Aviation. Sustainably. Seamlessly. Securely. The Data Management & Data Science (DM&DS) team is tasked with the end to end responsibility to ensure that CAS data assets are managed with integrity and quality prior to consumption by our critical customer facing applications - whether via APIs, analytics and/or data visualizations. As a senior data engineer on the DM&DS team, you will be responsible for the design, development and maintenance of data processes and pipelines supporting critical CAS Strategic Business Unit (SBU) Data initiatives in support of the Digital Transformation of CAS as well as for the cross-SBU Connected Ecosystem. In this endeavor, you will be working closely with data architecture, data analytics & visualization teams leaders across CAS, SBU and Digital Technology (DT) teams to ensure the technical solutions are efficient, scalable and meet long term Connected Ecosystem needs.

Primary Responsibilities:
Design, develop and support the processes and pipelines for moving data throughout the CAS and cross SBU environments.
Develop automation and monitoring processes that support the data pipelines
Work closely with the architecture team to implement modern data repositories that support the CAS use cases (Pipelines, APIs, Data Science, Applications and Visualizations)
Work with internal business customers and software development teams to gather and document requirements for data publishing and data consumption via data warehouse, data lake, and analytics solutions
Support the operation of the CAS and Digital Technology owned Data Platforms, Data Warehouse and Data Lakes with a view to leveraging capabilities and resources over-time
Work with the CAS and DT Enterprise Data Architects to automate cloud deployments, as well as build CI/CD pipeline to support Cloud-Based workloads. Develop views, materialized views, and SQL scripts
Work with the CAS and DT Enterprise Data Architects to recommend investments or changes in technology, resources, procedures, equipment, systems, or other assets to improve the quality of the organizations projects.
May travel domestically and internationally up to 15%.

Qualifications / Required Skills:
Bachelors degree and 5 years of prior relevant experience OR Advanced Degree in a related technical field and minimum years 3 experience OR In absence of a degree, 10 years of relevant experience is required
3+ years of demonstrated engineering leadership in a relevant engineering function, such as software/service development and deployment, system design and integration, or data analytics.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.",3.8,"Capgemini
3.8",Remote,10000+ Employees,1967,Company - Public,Enterprise Software & Network Solutions,Information Technology,$10+ billion (USD)
750,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
751,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
752,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
753,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
754,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
755,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
756,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the organization.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department. The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
757,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
758,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
759,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
760,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
761,Data Warehouse & Artificial Intelligence Engineer,$83K - $120K (Glassdoor est.),"Job Description:

Design, Implement and Maintain data warehouse.

Duties and Responsibilities:

Develop, test, and deploy extraction, transformation and loading (ETL) routines to load the data warehouse using SSIS, stored procedures, etc.
Develop analytics including reports, dashboards, and data visualizations
Troubleshoot BI-related problems and tune for performance
Implement and oversee a company-wide data governance program to support master data management and resolve data definition discrepancies among stakeholders
Plan the technical approach for implementing analytics strategy across multiple channels
Plan and implement activities to maintain data confidence
Provide technical direction and technical support for all member of the team
Architect and implement analytics solutions to meet data collection and reporting needs.
Work with team to recommend analytic software
Provide general troubleshooting assistance for analytic reports and break fixes
Wrangle data (a.k.a. data engineering) from a wide variety of complex data sets large volumes of data into normalized and enriched data sets.
Assemble and evaluate data such that new insights, solutions, and visualizations can be derived.
Perform data discoveries to understand data formats, source systems, file sizes, etc. and engage with internal and external business partners in this discovery process.
Deliver data acquisition, transformations, normalization, mapping, and loading of data into a variety of data models.
Collaborate extensively with Technology to design data ingestion, data models, and automated operational metrics for consistently high quality data
Supports the research, analysis and presentation of information by: producing reports, compiling and summarizing information, producing supporting documentation and exhibits, and verifying information received from external sources.
Assists other associates with preparation of reports and use of information systems, software and related sources of information. Trains other users on report preparation and database access.
Perform other duties as assigned.

Required Knowledge, skills and abilities:

Ability to manage change successfully in a fast-paced environment
A passion for learning and the ability to do so on the fly
Must be strategic, dynamic, and detail-oriented. The ideal candidate is an analytical thinker, a self-starter and a quick learner.
Ability to design and architect method of extracting data from its original source to another location (i.e. a data lake) to make that data consumable for a variety of downstream purposes such as applications, visualizations, analytics and other related data science activities.
Basic understanding of data science
Ability to transforming and map one ""raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.
Exceptional ability to clearly and effectively communicate with IT peers and users at all levels of technical and business acumen.
Strong time management skills and multi-tasking capabilities
Ability to understand business rules governing financial and budgeting process
Exceptional oral and written communication skills
Self-motivated, ability to set priorities and meet deadlines

Education and Experience:

Bachelor's degree in Computer Science, Business, Economics, Information Science, or Analytics
Minimum of 3 years of experience with data analytics across multiple channels
Experienced with the technologies required to implement Analytics across multiple channels, (for example, Data Layer, Custom JavaScript, JAVA, AJAX, HTML, Objective-C, Swift)
Knowledge of Tableau is a plus
Familiarity with Microsoft Power BI
Extensive experience with ETL and/or other Big Data processes.
Contract manufacturing experience is a plus.

Foxconn Assembly, LLC is an Equal Opportunity Employer (EOE). All qualified candidates will receive consideration without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or marital status in accordance with applicable federal, state and local laws.
Foxconn Assembly, LLC participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

NbXn0DyWY6",3.4,"Foxconn Corporation
3.4","Houston, TX",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$5 to $25 million (USD)
762,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
763,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
764,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
765,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
766,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
767,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
768,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
769,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
770,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
771,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
772,Data Engineer,$79K - $116K (Glassdoor est.),"Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
Sr. Consultant /Data Engineer – Data Architecture & Engineering
Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
A Senior Consultant in the Data and Analytics Practice is someone that looks forward to being challenged and continuously grow their technical and team collaboration skills. You'll provide technical guidance, as needed, to other team members for quality assurance, mentoring and growth opportunities. If this is a path that excites you, let's chat!
Responsibilities
Technical delivery in both Agile and Waterfall methodologies
Implement solutions using the Microsoft Power BI tool suite and related technologies
Leverage advanced database systems including data warehouses, data marts, and models needed for business, financial and operational analysis and/or reporting
Develop high performing, reliable and scalable solutions
Ability to clearly communicate technical details to business and management personnel
Perform data analysis that will support and enhance Information Management systems
Author, oversee and gain approval of design documents for projects assigned
Develop reports, dashboards & scorecards, data exploration and visual analytics using existing and emerging tools and technologies.
Work independently or as part of a team to design and develop solutions
Assist business development team with pre-sales activities and RFPs
Provide meaningful feedback and coaching of other team members to successfully overcome technological challenges
Leading and demonstrating emerging technologies and concepts to teams
Requirements
Bachelor's Degree desired in Computer Science, Information Technology, or related field
2+ years of experience with data visualization
Expert knowledge of Data Management, Business Intelligence and Analytics concepts including:
Extract, Transform, Load (ETL)
Data Warehousing, Data Lakes, Data Marts, Data Stores, and Cubes
Data Quality and Profiling
Data Governance, Master Data Management, and Metadata Management
Data Visualization, Dashboard, Reporting, Self Service Business Intelligence (BI), Key Performance Indicators (KPI), and Scorecards
Predictive, Prescription, and Descriptive Analytics
Expert knowledge of Microsoft Business Intelligence stack including Power BI, Excel and SQL Server (SSAS, SSRS, SSIS)
Minimum of 3-5 years of experience (of which 2-3 years in consulting)
Experience with Azure Data Estate (Azure SQL DB, Cosmos DB, Azure SQL DW, Azure Data Lake) is a plus
Azure ML Studio/Services experience a plus
Certifications are a plus
Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and tuning is a plus
Knowledge of relational and dimensional database structures, theories, principles, and practice is a plus
Experience with Data Science tools, technologies and techniques (R, Python, algorithms) is a plus
Passionate about learning new technologies
Ability to learn new concepts and software quickly
Analytical approach to problem-solving; ability to use technology to solve business problems
Familiarity with database-centric applications
Ability to communicate effectively in both a technical and non-technical manner
Ability to work in a fast-paced environment
Ability to travel up to 30%
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways.
At this time, we cannot sponsor applicants for work visas. Candidates should be based in the United States.
About 3Cloud
3Cloud is a ""born in the cloud"" Gold-Certified Microsoft Azure technology consulting firm and Azure Expert Managed Services Provider that provides cloud strategy, design, implementation, acquisition and ongoing managed services to its clients across various industries. To meet the needs of its clients, 3Cloud offers comprehensive services across Cloud Infrastructure, DevOps & Automation, Cloud Application Development and Data, Analytics and AI. Founded by former Microsoft technology leaders, 3Cloud uniquely offers: (1) highly experienced and proven cloud architects and technologists, (2) strong business strategy, financial acumen and operational proficiency and (3) deep relationship and network into Microsoft engineering and field leadership.
3Cloud Total Rewards Highlights Include:
Flexible work location with a virtual first approach to work!
401(K) with match up to 50% of your 6% contributions of eligible pay
Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days
Three medical plan options to allow you the choice to elect what works best for you!
Option for vision and dental coverage
100% employer premium coverage for STD and LTD
Paid leave for birth parents and non-birth parents
Option for FSA, HSA, HRA and Dependent Care
$67.00 monthly tech and home office allowance
Utilization and/or discretionary bonus eligibility based on role
Robust Employee Assistance Program to help with everyday challenges
3Cloud offers competitive compensation. In addition to base pay employees are eligible to receive an annual discretionary/utilization bonus. If you are hired at 3Cloud your final base salary is based on factors such as skills, education, experience and/or geographic location. Please keep in mind that the range mentioned above includes the full base salary range for the role. It is not typical for offers to be made at or near the top of the range.
Base Salary Range
$90,000—$183,000 USD
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway.
At this time, we cannot sponsor applicants for work visas.",4.3,"3Cloud
4.3","Downers Grove, IL",501 to 1000 Employees,2016,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
773,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
774,Data Warehouse Engineer,$88K - $119K (Glassdoor est.),"At Apiture, our mission is to empower financial institutions to know and serve their clients with the care of a traditional community institution at the scale, speed, and efficiency required in today's digital world. With more than 300 clients throughout the U.S., we deliver comprehensive online and mobile solutions that support banks and credit unions, ranging from small community financial institutions to new, innovative direct banks.

Summary:
Reporting to the Data Engineering Manager, the Senior Data Warehouse Engineer will work closely with the Data Architect to implement the Data Model. You will be working with a team of highly talented engineers to develop practical, scalable data reporting and analytics solutions.

Location (Wilmington, NC, Austin, TX, Remote):
We have offices in Wilmington, NC and Austin, TX and while some positions are office based, we will also consider remote candidates depending on their time zone.

Responsibilities:
Core tasks will involve writing SQL and some Python to transform raw data from the staging layer into the human readable Data Model.
Write new fact and dimension tables and add to existing ones.
Review data in source systems, in databases, or APIs to understand how the data comes into the warehouse and which transformations are needed.
Work with the Data Architect to define and enforce data warehouse standards that align with the larger data management guidelines in place.
Work with data analysts and data scientists to define and refine data analysis goals and implement needed changes in the data warehouse.
Identify and pursue opportunities to automate processes and execute validation strategies to maintain high standards of efficiency and data quality.
Build and maintain documentation around data sets, data classes, data flows, transformations, etc.
Work with the information security and compliance teams at Apiture to build, monitor, and enforce data cataloging, asset tracking, and privacy rules/metrics.
Provide input and feedback to support continuous improvement in data governance processes.

Requirements:
Bachelor's in computer science or equivalent work experience.
4+ years of hands-on experience with coding data transformations in SQL that involved large data sets.
Well-versed with Advanced SQL scripting.
Experience with programming languages: Python, Java, Scala, etc.
Experience building data pipelines that integrate data from structured and unstructured data sources.
Experience troubleshooting data integrations to visualization platforms like Domo, Tableau or PowerBI.
Hands-on experience with data warehousing platforms like Snowflake, RedShift, or Synapse Analytics.
Excellent understanding of ETL/ELT fundamentals and building efficient data pipelines.
Excellent verbal and written communication skills.

Nice To Have:
Experience with cloud technologies (Strong preference for AWS technologies like Lambda, DMS, etc.)
Experience with Data Management platforms like OneTrust
Experience working with REST APIs, Streaming APIs, or other Data Ingress techniques.
Experience with data engineering and monitoring for ML applications.
Exposure to test-driven development and automated testing frameworks.",4.0,"Apiture
4.0","Atlanta, GA",201 to 500 Employees,2017,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
775,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1,Radiant System,Remote,-1,-1,-1,-1,-1,-1
776,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
777,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
778,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
779,Data Engineer III,-1,"We are seeking a Data Engineer III to join our Data Platform team. This role will report to the director of data platform and be involved in the planning, design, and implementation of our centralized data lake solution supporting analytics, products and applications across the company.
Qualifications:
To be successful in this role the candidate needs to have the following qualifications:
Deep knowledge of Scala and Spark
Experience with Databricks is preferred.
Deep knowledge of modern orchestration frameworks such as Apache airflow
Experience working with SQL and NoSQL database systems.
Experience with cloud environments (Azure Preferred)
Experience with acquiring and preparing data from primary and secondary disparate data sources.
Experience with agile project management methodology
Healthcare industry experience preferred, including exposure to different EMR systems, revenue cycle management.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12 is a plus but is not required.
Responsibilities:
Be part of an engineering team in building data adaptors to expedite the data onboarding process from various health systems.
Work with product management and business analysts on design reusable and configurable data orchestration pipelines.
Work with data specialist in develop and design data transformation to standardize the data model and support data enrichment activities.
As a subject matter expert, hosts information sharing session with teams within data platform or larger R1 organization.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",3.4,"R1 RCM, Inc.
3.4",Remote,10000+ Employees,2003,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
780,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
781,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
782,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the organization.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department. The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
783,"Data Engineer, Launch Program 2024 - United States",-1,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a
culture of inclusion
for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title and Summary
Data Engineer, Launch Program 2024 - United States
Be part of the Data & Services Technology Team at Mastercard, Data and Services

The Data Engineer I is a full time role within Mastercard Launch, a cohort based, graduate development program designed to build the skills you’ll leverage most as an innovator in the payments space. Eligibility requires that you currently be a graduating senior, pursuing a relevant degree.

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Make an Impact as a Data Engineer

Data Engineers are fundamental to the success of our client engagements by acting as the bridge between raw client data and Mastercard's software. Data Engineers work closely with both Consultants while working on consulting engagements and Software Development Engineers while working to develop internal capabilities. Data Engineers are responsible for:

Designing processes to extract, transform, and load (ETL) terabytes of data into Mastercard's analytics platform
Sourcing data from clients, government and other third-party data sources, and Mastercard's transaction data warehouse
Working across multiple teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set
Tackling big data problems across various industries

Here are just a few of the benefits that you will get to experience during your first year as a Data Engineer:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies and to explore a variety of solutions
Staffing on external-facing consulting projects where you will have the opportunity to work directly with clients
Flexibility to work on many new and challenging projects across diverse industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven leadership team
Cross-functional collaboration with others throughout Mastercard

Bring your passion and expertise

About You:

Currently enrolled in your final year of a bachelor’s or accelerated master’s program with an established history of academic success
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Strong troubleshooting and problem solving capabilities
Demonstrated analytical and quantitative skills

The role also involves these skills. We don't require them, but it's helpful if you already have them:

Understanding of relational databases, SQL, and ETL Processes
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",4.3,"Mastercard
4.3","Arlington, VA",10000+ Employees,1966,Company - Public,Financial Transaction Processing,Financial Services,Unknown / Non-Applicable
784,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
785,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
786,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
787,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
788,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
789,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
790,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
791,Data Warehouse & Artificial Intelligence Engineer,$83K - $120K (Glassdoor est.),"Job Description:

Design, Implement and Maintain data warehouse.

Duties and Responsibilities:

Develop, test, and deploy extraction, transformation and loading (ETL) routines to load the data warehouse using SSIS, stored procedures, etc.
Develop analytics including reports, dashboards, and data visualizations
Troubleshoot BI-related problems and tune for performance
Implement and oversee a company-wide data governance program to support master data management and resolve data definition discrepancies among stakeholders
Plan the technical approach for implementing analytics strategy across multiple channels
Plan and implement activities to maintain data confidence
Provide technical direction and technical support for all member of the team
Architect and implement analytics solutions to meet data collection and reporting needs.
Work with team to recommend analytic software
Provide general troubleshooting assistance for analytic reports and break fixes
Wrangle data (a.k.a. data engineering) from a wide variety of complex data sets large volumes of data into normalized and enriched data sets.
Assemble and evaluate data such that new insights, solutions, and visualizations can be derived.
Perform data discoveries to understand data formats, source systems, file sizes, etc. and engage with internal and external business partners in this discovery process.
Deliver data acquisition, transformations, normalization, mapping, and loading of data into a variety of data models.
Collaborate extensively with Technology to design data ingestion, data models, and automated operational metrics for consistently high quality data
Supports the research, analysis and presentation of information by: producing reports, compiling and summarizing information, producing supporting documentation and exhibits, and verifying information received from external sources.
Assists other associates with preparation of reports and use of information systems, software and related sources of information. Trains other users on report preparation and database access.
Perform other duties as assigned.

Required Knowledge, skills and abilities:

Ability to manage change successfully in a fast-paced environment
A passion for learning and the ability to do so on the fly
Must be strategic, dynamic, and detail-oriented. The ideal candidate is an analytical thinker, a self-starter and a quick learner.
Ability to design and architect method of extracting data from its original source to another location (i.e. a data lake) to make that data consumable for a variety of downstream purposes such as applications, visualizations, analytics and other related data science activities.
Basic understanding of data science
Ability to transforming and map one ""raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.
Exceptional ability to clearly and effectively communicate with IT peers and users at all levels of technical and business acumen.
Strong time management skills and multi-tasking capabilities
Ability to understand business rules governing financial and budgeting process
Exceptional oral and written communication skills
Self-motivated, ability to set priorities and meet deadlines

Education and Experience:

Bachelor's degree in Computer Science, Business, Economics, Information Science, or Analytics
Minimum of 3 years of experience with data analytics across multiple channels
Experienced with the technologies required to implement Analytics across multiple channels, (for example, Data Layer, Custom JavaScript, JAVA, AJAX, HTML, Objective-C, Swift)
Knowledge of Tableau is a plus
Familiarity with Microsoft Power BI
Extensive experience with ETL and/or other Big Data processes.
Contract manufacturing experience is a plus.

Foxconn Assembly, LLC is an Equal Opportunity Employer (EOE). All qualified candidates will receive consideration without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or marital status in accordance with applicable federal, state and local laws.
Foxconn Assembly, LLC participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

NbXn0DyWY6",3.4,"Foxconn Corporation
3.4","Houston, TX",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$5 to $25 million (USD)
792,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
793,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
794,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
795,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
796,AWS Data Engineer/ Architect,Employer Provided Salary:$130K - $160K,"Working knowledge and experience with AWS Services including EMR, DynamoDB, Kinesis Data Stream, Lambda, S3, Salesforce
Focus on AWS and Salesforce – Sales & Service cloud.
Mastery and hands-on experience with Data Engineering technologies and scripting languages.
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Intimately involved in numerous end to end cloud migration projects.
Strong knowledge and experience in designing for and implementing solutions in the cloud (AWS). This can range from custom code on IaaS/PaaS to SaaS solution integrations.
Languages: Java, Angular, node.js, Python
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Working knowledge of application architectures, software development tools, and methodologies.
History of successful technical consulting, and architecture engagements with numerous customers simultaneously.
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Washington, DC 20001",-1,Omniware LLC,"Washington, DC",-1,-1,-1,-1,-1,-1
797,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
798,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
799,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
800,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
801,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
802,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
803,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
804,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
805,Data Warehouse Engineer,$88K - $119K (Glassdoor est.),"At Apiture, our mission is to empower financial institutions to know and serve their clients with the care of a traditional community institution at the scale, speed, and efficiency required in today's digital world. With more than 300 clients throughout the U.S., we deliver comprehensive online and mobile solutions that support banks and credit unions, ranging from small community financial institutions to new, innovative direct banks.

Summary:
Reporting to the Data Engineering Manager, the Senior Data Warehouse Engineer will work closely with the Data Architect to implement the Data Model. You will be working with a team of highly talented engineers to develop practical, scalable data reporting and analytics solutions.

Location (Wilmington, NC, Austin, TX, Remote):
We have offices in Wilmington, NC and Austin, TX and while some positions are office based, we will also consider remote candidates depending on their time zone.

Responsibilities:
Core tasks will involve writing SQL and some Python to transform raw data from the staging layer into the human readable Data Model.
Write new fact and dimension tables and add to existing ones.
Review data in source systems, in databases, or APIs to understand how the data comes into the warehouse and which transformations are needed.
Work with the Data Architect to define and enforce data warehouse standards that align with the larger data management guidelines in place.
Work with data analysts and data scientists to define and refine data analysis goals and implement needed changes in the data warehouse.
Identify and pursue opportunities to automate processes and execute validation strategies to maintain high standards of efficiency and data quality.
Build and maintain documentation around data sets, data classes, data flows, transformations, etc.
Work with the information security and compliance teams at Apiture to build, monitor, and enforce data cataloging, asset tracking, and privacy rules/metrics.
Provide input and feedback to support continuous improvement in data governance processes.

Requirements:
Bachelor's in computer science or equivalent work experience.
4+ years of hands-on experience with coding data transformations in SQL that involved large data sets.
Well-versed with Advanced SQL scripting.
Experience with programming languages: Python, Java, Scala, etc.
Experience building data pipelines that integrate data from structured and unstructured data sources.
Experience troubleshooting data integrations to visualization platforms like Domo, Tableau or PowerBI.
Hands-on experience with data warehousing platforms like Snowflake, RedShift, or Synapse Analytics.
Excellent understanding of ETL/ELT fundamentals and building efficient data pipelines.
Excellent verbal and written communication skills.

Nice To Have:
Experience with cloud technologies (Strong preference for AWS technologies like Lambda, DMS, etc.)
Experience with Data Management platforms like OneTrust
Experience working with REST APIs, Streaming APIs, or other Data Ingress techniques.
Experience with data engineering and monitoring for ML applications.
Exposure to test-driven development and automated testing frameworks.",4.0,"Apiture
4.0","Atlanta, GA",201 to 500 Employees,2017,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
806,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
807,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
808,Data Engineer,Employer Provided Salary:$70.00 - $75.00 Per Hour,"Job ID: ROCGJP00022864
Role: Data Engineer/Data Scientist
Location: SSF, CA94080(Hybrid, 3 days a week onsite)
Duration: 12+months
.
NOTE: It's a W2 contract role so NO C2C candidates please
.
Must have skills
Outstanding communication skills
Self-Starter Starter/Self-managed
.
Technical skills:
5 years plus experience as Data Engineer/Data Scientist.
SQL background. Able to run Queries. Extract data, understand table structures – Redshift and Oracle
Understand database performance.
AWS platform – Knowledge of Redshift, S3 pockets
Ability to look at data and connect it with other data.
Understand how API’s work. External and internal vendors
Tableau – we have 10 to 20 reports.
Reports on data source and manage them.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Day shift
Application Question(s):
It is a W2 contract role so NO C2C candidates please.
Experience:
Overall IT: 5 years (Required)
Data Engineer/Data Scientist: 4 years (Required)
SQL: 3 years (Required)
AWS: 2 years (Required)
Tableau: 1 year (Required)
Work Location: In person",2.0,"Norwin
2.0","South San Francisco, CA",1 to 50 Employees,-1,Company - Private,Machinery Manufacturing,Manufacturing,Unknown / Non-Applicable
809,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
810,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
811,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
812,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
813,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
814,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
815,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
816,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
817,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
818,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
819,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
820,Data Warehouse & Artificial Intelligence Engineer,$83K - $120K (Glassdoor est.),"Job Description:

Design, Implement and Maintain data warehouse.

Duties and Responsibilities:

Develop, test, and deploy extraction, transformation and loading (ETL) routines to load the data warehouse using SSIS, stored procedures, etc.
Develop analytics including reports, dashboards, and data visualizations
Troubleshoot BI-related problems and tune for performance
Implement and oversee a company-wide data governance program to support master data management and resolve data definition discrepancies among stakeholders
Plan the technical approach for implementing analytics strategy across multiple channels
Plan and implement activities to maintain data confidence
Provide technical direction and technical support for all member of the team
Architect and implement analytics solutions to meet data collection and reporting needs.
Work with team to recommend analytic software
Provide general troubleshooting assistance for analytic reports and break fixes
Wrangle data (a.k.a. data engineering) from a wide variety of complex data sets large volumes of data into normalized and enriched data sets.
Assemble and evaluate data such that new insights, solutions, and visualizations can be derived.
Perform data discoveries to understand data formats, source systems, file sizes, etc. and engage with internal and external business partners in this discovery process.
Deliver data acquisition, transformations, normalization, mapping, and loading of data into a variety of data models.
Collaborate extensively with Technology to design data ingestion, data models, and automated operational metrics for consistently high quality data
Supports the research, analysis and presentation of information by: producing reports, compiling and summarizing information, producing supporting documentation and exhibits, and verifying information received from external sources.
Assists other associates with preparation of reports and use of information systems, software and related sources of information. Trains other users on report preparation and database access.
Perform other duties as assigned.

Required Knowledge, skills and abilities:

Ability to manage change successfully in a fast-paced environment
A passion for learning and the ability to do so on the fly
Must be strategic, dynamic, and detail-oriented. The ideal candidate is an analytical thinker, a self-starter and a quick learner.
Ability to design and architect method of extracting data from its original source to another location (i.e. a data lake) to make that data consumable for a variety of downstream purposes such as applications, visualizations, analytics and other related data science activities.
Basic understanding of data science
Ability to transforming and map one ""raw” data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.
Exceptional ability to clearly and effectively communicate with IT peers and users at all levels of technical and business acumen.
Strong time management skills and multi-tasking capabilities
Ability to understand business rules governing financial and budgeting process
Exceptional oral and written communication skills
Self-motivated, ability to set priorities and meet deadlines

Education and Experience:

Bachelor's degree in Computer Science, Business, Economics, Information Science, or Analytics
Minimum of 3 years of experience with data analytics across multiple channels
Experienced with the technologies required to implement Analytics across multiple channels, (for example, Data Layer, Custom JavaScript, JAVA, AJAX, HTML, Objective-C, Swift)
Knowledge of Tableau is a plus
Familiarity with Microsoft Power BI
Extensive experience with ETL and/or other Big Data processes.
Contract manufacturing experience is a plus.

Foxconn Assembly, LLC is an Equal Opportunity Employer (EOE). All qualified candidates will receive consideration without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or marital status in accordance with applicable federal, state and local laws.
Foxconn Assembly, LLC participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

NbXn0DyWY6",3.4,"Foxconn Corporation
3.4","Houston, TX",1 to 50 Employees,-1,Company - Private,Computer Hardware Development,Information Technology,$5 to $25 million (USD)
821,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
822,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the organization.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department. The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
823,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
824,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
825,AWS Data Engineer/ Architect,Employer Provided Salary:$130K - $160K,"Working knowledge and experience with AWS Services including EMR, DynamoDB, Kinesis Data Stream, Lambda, S3, Salesforce
Focus on AWS and Salesforce – Sales & Service cloud.
Mastery and hands-on experience with Data Engineering technologies and scripting languages.
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Intimately involved in numerous end to end cloud migration projects.
Strong knowledge and experience in designing for and implementing solutions in the cloud (AWS). This can range from custom code on IaaS/PaaS to SaaS solution integrations.
Languages: Java, Angular, node.js, Python
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Working knowledge of application architectures, software development tools, and methodologies.
History of successful technical consulting, and architecture engagements with numerous customers simultaneously.
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Washington, DC 20001",-1,Omniware LLC,"Washington, DC",-1,-1,-1,-1,-1,-1
826,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
827,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
828,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
829,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
830,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
831,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
832,Data Engineer II,-1,"Career Area:
Information Analytics
Job Description:
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do – but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here – we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Your Work Shapes the World
Whether it be groundbreaking products, best in class solutions or creating a lifelong career, you can do the work that matters at Caterpillar. With a 95-year legacy of quality and innovation and 150 locations in countries around the world, your impact spans the globe.
When you join Caterpillar, you are joining a team of makers, innovators and doers. We are the people who roll up our sleeves and do the work to build a better world. We don’t just talk about progress and innovation. We make it happen. And we are proud of that because it helps our customers build and power the world we live in – the roads, hospitals, homes and infrastructure. Without a dedicated workforce Caterpillar could not effectively meet our customer’s needs. Join us.
Description:
As a data engineer, you will use methods in data engineering, cloud computing, and computer science to support application development, data pipelines, and automation routines for the Global Parts Pricing and Sales Variance team.
Job Duties/Responsibilities:
Data engineer on the parts pricing analytics team
Build and maintain infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Create datasets and infrastructure to process data at scale
Create solutions and methods to monitor systems and solutions
Develop and support parts pricing analytic workflows in a dedicated AWS instance
Implement parts pricing automation routines of medium to high complexity
Required Qualifications:
Bachelor's degree in computer science, information systems, data engineering, analytics or related field
2+ years of experience in data engineering, software engineering, computer science, or analytics positions
2+ years of experience working in databases using SQL
Desired Qualifications:
Master's degree in data engineering, computer science, analytics, or related field
Experience working with AWS cloud tools: Lambda, EMR, Spark, Athena, Glue
Experience developing data pipelines or automation routines in python/pyspark
Experience programming in Python with working knowledge of modern data engineering packages
Experience managing and deploying code through Azure Dev Ops or similar tool
Comfortable managing git repositories
Experience with Caterpillar aftermarket parts data
Additional Details:
Preferred location: Peoria, IL
Domestic relocation is available to those who qualify (Peoria, IL)
Employee benefit details:
Our goal at Caterpillar is for you to have a rewarding career. Our teams are critical to the success of our customers who build a better world.
Here you earn more than just a salary, because we value your performance. We offer a total rewards package that provides day one benefits [medical, dental, vision, RX, and 401(k)] along with the potential of an annual bonus.
Additional benefits include paid vacation days and paid holidays (prorated based upon hire date).
#LI
#BI
Final details:
Please frequently check the email associated with your application, including the junk/spam folder, as this is the primary correspondence method. If you wish to know the status of your application – please use the candidate log-in on our career website as it will reflect any updates to your status.
This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S. which can be found through our employment website at www.Caterpillar.com/Careers
Relocation is available for this position.Visa Sponsorship is not available for this position. This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, H, L, TN, F, J, E, O. As a global company, Caterpillar offers many job opportunities outside of the U.S which can be found through our employment website at www.caterpillar.com/careers.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",4.0,"Caterpillar
4.0",Illinois,10000+ Employees,1925,Company - Public,Machinery Manufacturing,Manufacturing,$10+ billion (USD)
833,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
834,Senior Data Engineer,-1,"We are a digitally native technology services company where innovation, design and engineering meet scale. We use some of the latest technologies in the digital and cognitive field to empower organizations in every aspect.

Right now, we are looking for Data Engineers to join our team at Globant!

You will get the chance to:
Work with professionals who have created some of the most revolutionary solutions in their fields.
Make an impact. Work in large-scale projects globally.
Develop your career in our Studios. Each Studio represents deep pockets of expertise on the latest technologies and trends and delivers tailored solutions focused on specific challenges.
Develop your career within an industry or multiple industries.
Work in the city you want, and be nourished by cultural exchanges.
Design, build, and manage the data platforms and data pipelines necessary for optimal extraction, transformation, and loading of data from various sources.
Work with stakeholders to assist with data-related needs.
Create data products to support multiple consumers' needs.
Conduct complex data analysis and data modeling tasks.
Develop and implement data quality, profiling, and cleansing activities
Be empowered to choose your career path: we have more than 600 simultaneous projects, so you can choose where and how to work.
Be part of an agile pod. Driven by a culture of self-regulated teamwork, each team -or POD- works directly with our customers with a full maturity path that evolves as they increase speed, quality and autonomy.

What would help you succeed:
Believe and love what you do.
Enjoy solving problems.
Have imagination and love innovating.
Being flexible & goal oriented.
Advanced English skills.
High performance analytics.
Batch and Streaming processing
Experience with SQL and NoSQL databases, like MongoDB, CosmosDB
Knowledge at Azure Databricks, Azure Data Factory, Azure SQL and Azure Synapse
Experience with Java or Python, + Scala.
Practical experience in AWS cloud, using Spark, Databricks, Snowflake.
Nice to have: Azure Purview and Data Mesh

At Globant we believe that an inclusive culture and a diverse environment makes us stronger. We encourage people to have an inclusive spirit as our global footprint expands. We seek to generate a place of inspiration and growth for everyone. A safe space, based on equity as a value, where everyone's careers can be promoted and developed in the same way. There is no innovation without diversity and there is no improvement without plurality.

Are you ready?",4.3,"Globant
4.3",Arkansas,10000+ Employees,2003,Company - Public,Software Development,Information Technology,$1 to $5 billion (USD)
835,Data Engineer III,-1,"We are seeking a Data Engineer III to join our Data Platform team. This role will report to the director of data platform and be involved in the planning, design, and implementation of our centralized data lake solution supporting analytics, products and applications across the company.
Qualifications:
To be successful in this role the candidate needs to have the following qualifications:
Deep knowledge of Scala and Spark
Experience with Databricks is preferred.
Deep knowledge of modern orchestration frameworks such as Apache airflow
Experience working with SQL and NoSQL database systems.
Experience with cloud environments (Azure Preferred)
Experience with acquiring and preparing data from primary and secondary disparate data sources.
Experience with agile project management methodology
Healthcare industry experience preferred, including exposure to different EMR systems, revenue cycle management.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12 is a plus but is not required.
Responsibilities:
Be part of an engineering team in building data adaptors to expedite the data onboarding process from various health systems.
Work with product management and business analysts on design reusable and configurable data orchestration pipelines.
Work with data specialist in develop and design data transformation to standardize the data model and support data enrichment activities.
As a subject matter expert, hosts information sharing session with teams within data platform or larger R1 organization.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",3.4,"R1 RCM, Inc.
3.4",Remote,10000+ Employees,2003,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
836,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
837,Senior Data Engineer,-1,"(https://socialdiscoverygroup.com/about-us/)
Social Discovery Group (https://socialdiscoverygroup.com/about-us)is the world's largest group of social discovery companies which unites more than 50 brands. For more than 20 years, we have been creating premium international dating services and social discovery apps with a focus on video streaming, AI technologies, entertainment, and game mechanics. Our product portfolio includes Dating.com, Cupid Media, Dil Mil, and many others. The products are already used by more than 500 million users in 150 countries around the world.
SDG Invests (https://socialdiscoverygroup.com/investments) in social discovery technology startups around the world. Our Investments (https://socialdiscoverygroup.com/investments) include Open AI, Patreon, Flo, Wildly, RAW, EVA AI, Clubhouse, Magnet, Tubit, Woebot, BamBam, Flure, Astry, Coursera, Academia, Harbour, Space, Auto1, DocSend, AppAnnie, Rapyd, Boom Supersonic, Trading, View, K-Health and many others.
We solve the problem of loneliness, isolation, and disconnection with the help of digital reality.
Our digital nomad team of more than 800 professionals works all over the world. Our international team of like-minded people and professionals solves ambitious daily tasks and creates truly global products. We value focusing on results, a proactive approach, and we are always looking for new and unconventional ideas.
Our teams of digital nomads live and work remotely from Cyprus, Malta, the USA, Thailand, Indonesia, Hong Kong, Australia, Poland, Israel, Türkiye, Latvia and many others.
At present, we are in search of a Senior Data Engineer to join our analytics team dedicated to our investment products.
Your main tasks will be:
Development and maintenance of data warehouse and ETL processes;
Integration with internal/external sources (new backends, advertising platforms, and other services);
Establishment of data quality control infrastructure (Data Quality + Alerts + SLA);
Crafting and maintaining documentation: outlining data architecture (core tables in Confluence) + identification of ""sources of truth"" + glossary of metrics (dbt/datahub);
Collaborating with the team to devise a roadmap for enhancing analytical infrastructure — selecting technologies and architecting the framework (considering the need for specific task clusters).
We expect from you:
Proficiency in SQL (window functions, subqueries, various joins) + at least one year of experience with MS SQL;
Proficiency in Python (minimum 1 year of experience);
Prior experience with Airflow;
Query optimization skills;
Experience in data architecture development.
Would be a plus:
Spark (Scala or Java knowledge considered advantageous);
dbt;
Experience with BigQuery, ClickHouse is appreciated;
Familiarity with Docker / Kubernetes.
What do we offer:

REMOTE OPPORTUNITY to work full time;
7 wellness days per year (time off) that can be used to deal with household issues, to lie down and recover without taking sick leave;
Bonuses up to $5000 for recommending successful applicants for positions in the company;
Full payment for professional training, international conferences and meetings;
Corporate discount for English lessons;
Health benefits. If you are not eligible for Corporate Medical Insurance, the company will compensate up to $1000 gross per year per employee according to the paychecks. This can be spent on self-purchase of health insurance, or on doctor's fees for yourself and close relatives (spouse, children);
Workplace organization. The company provides all employees with an equipped workplace and all the necessary equipment (table, armchair, wifi, etc.) in the locations where we have offices or co-working. In the other locations, the company provides reimbursement of workplace costs up to $ 1000 gross once every 3 years according to the paychecks. This money can be spent on the rent of the co-working room, on equipping the working place at home (desk, chair, Internet, etc.) during those 3 years;
Internal gamified gratitude system: receive bonuses from colleagues and exchange them for time off, merch, team building activities, massage certificates, etc.
Sounds good? Join us now!",4.3,"Social Discovery Group
4.3",Remote,501 to 1000 Employees,1998,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
838,Data Engineer,Employer Provided Salary:$77K - $113K,"Eaton’s Electrical division is currently seeking a Business Analyst to work out of our Raleigh, NC location.

Relocation within the United States will be offered.

The expected annual salary range for this role is $77,249 - $113,299 a year.
Please note the salary information shown above is a general guideline only. Salaries are based upon candidate skills, experience, and qualifications, as well as market and business considerations.
What you’ll do:

The Data Engineer will collaborate with cross-functional teams to understand data requirements, design and implement efficient data pipelines, and ensure the availability, reliability, and scalability of our data solutions. This position necessitates proficiency in data manipulation, data analysis, SQL (writing queries), data modeling, and visualization skills.

Job responsibilities
Gather data from various sources. Design and write SQL queries to retrieve and analyze data stored in various databases.
Develop ETL processes to clean, transform, and enrich data in PowerQuery, ensuring data quality, consistency, and accuracy for use in the PowerBI's Data model.
Design and develop data model, transform raw data into meaningful insights, create relationships, hierarchies, and calculated measures in PowerBI’s Data Model
Document data integration and transformation processes for future reference and troubleshooting.
Assists in the implementation of data governance processes and systems.
Qualifications:
Basic Qualifications:
Bachelor's degree from an accredited educational institution
Minimum three (3) years of SQL and Power BI experience
Must be legally authorized to work in the United States without company sponsorship now or in the future

Preferred Qualifications:
Bachelor’s degree in computer science or software engineering
PowerBI (PowerQuery, Modelling, DAX, creating interactive visuals), Relational databases, Data Governance, Visualization
Problem-solving, Communication, Attention to detail
#LI-JM3
We are committed to ensuring equal employment opportunities for all job applicants and employees. Employment decisions are based upon job-related reasons regardless of an applicant's race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, marital status, genetic information, protected veteran status, or any other status protected by law.
Eaton considers qualified applicants regardless of criminal histories, consistent with local laws. To request a disability-related reasonable accommodation to assist you in your job search, application or interview process, please call us at 1-800-836-6345 to discuss your specific need. Only accommodation requests will be accepted by this phone number.
We know that good benefit programs are important to employees and their families. Eaton provides various Health and Welfare benefits as well as Retirement benefits, and several programs that provide for paid and unpaid time away from work. Click here for more detail: Eaton Benefits Overview. Please note that specific programs and options available to an employee may depend on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.",3.9,"Eaton
3.9","Raleigh, NC",10000+ Employees,1911,Company - Public,Electronics Manufacturing,Manufacturing,$10+ billion (USD)
839,"Senior Data Engineer, IT Analytics",-1,"Location: DFW Headquarters Building 8 (DFW-SV08)
Additional Locations: None
Requisition ID: 68799
Intro
Are you ready to explore a world of possibilities, both at work and during your time off? Join our American Airlines family, and you’ll travel the world, grow your expertise and become the best version of you. As you embark on a new journey, you’ll tackle challenges with flexibility and grace, learning new skills and advancing your career while having the time of your life. Feel free to enrich both your personal and work life and hop on board!
Why you'll love this job
You will help enable data engineering solutions at AA
You will be part of a team that innovates.
This role is a part of the Data Engineering and Analytics team within our Technology group. You’ll bring your data engineering, collaboration and analytics skills to help cultivate a data driven culture by designing and delivering analytics solutions and making data analytics easier and more effective for American Airlines.
What you'll do
As noted above, this list is intended to reflect the current job but there may be additional essential functions (and certainly non-essential job functions) that are not referenced. Management will modify the job or require other tasks be performed whenever it is deemed appropriate to do so, observing, of course, any legal obligations including any collective bargaining obligations.
Work closely with source data application teams and product owners to design, implement and support analytics solutions that provide insights to make better decisions
Implement data migration and data engineering solutions using Azure products and services: (Azure Data Lake Storage, Azure Data Factory, Azure Functions, Event Hub, Azure Stream Analytics, Azure Databricks, etc.) and traditional data warehouse tools.
Perform multiple aspects involved in the development lifecycle – design, cloud engineering (Infrastructure, network, security, and administration), ingestion, preparation, data modeling, testing, CICD pipelines, performance tuning, deployments, consumption, BI, alerting, prod support.
Provide technical leadership and collaborate within a team environment as well as work independently.
Be a part of a DevOps team that completely owns and supports their product
Implement batch and streaming data pipelines using cloud technologies
Leads development of coding standards, best practices and privacy and security guidelines.
Mentors others on technical and domain skills to create multi-functional teams
All you'll need for success
Minimum Qualifications- Education & Prior Job Experience
Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training
3 years software solution development using agile, DevOps, operating in a product model that includes designing, developing, and implementing large-scale applications or data engineering solutions
3 years data analytics experience using SQL
2 years of cloud development and data lake experience (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Power BI.
Combination of Development, Administration & Support experience in several of the following tools/platforms required:
Scripting: Python, Spark, Unix, SQL
Data Platforms: Teradata, Cassandra, MongoDB, Oracle, SQL Server, ADLS, Snowflake
Azure Data Explorer. Administration skills a plus
Azure Cloud Technologies: Azure Data Factory, Azure Databricks, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Azure Functions
CI/CD: GitHub, Jenkins, Azure DevOps, Terraform
BI Analytics Tool Stack - Cognos, Tableau, Power BI, Alteryx, Denodo, and Grafana
Data Warehousing: DataStage, Informatica
Data Governance and Privacy: Informatica Axon and EDC, BigID
Preferred Qualifications- Education & Prior Job Experience
5+ years software solution development using agile, dev ops, product model that includes designing, developing, and implementing large-scale applications or data engineering solutions.
5+ years data analytics experience using SQL
3+ years full-stack development experience, preferably in Azure
3+ years of cloud development and data lake experience (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Functions, ADX, ASA, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Power BI.
Airline Industry Experience
Skills, Licenses & Certifications
Expertise with the Azure Technology stack for data management, data ingestion, capture, processing, curation and creating consumption layers.
Expertise in providing practical direction within the Azure Native cloud services.
Azure Development Track Certification (preferred)
Spark Certification (preferred)
What you'll get
Feel free to take advantage of all that American Airlines has to offer:
Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.
Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more.
Wellness Programs: We want you to be the best version of yourself – that’s why our wellness programs provide you with all the right tools, resources and support you need.
401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.
Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more
Feel free to be yourself at American
From the team members we hire to the customers we serve, inclusion and diversity are the foundation of the dynamic workforce at American Airlines. Our 20+ Employee Business Resource Groups are focused on connecting our team members to our customers, suppliers, communities and shareholders, helping team members reach their full potential and creating an inclusive work environment to meet and exceed the needs of our diverse world.

Are you ready to feel a tremendous sense of pride and satisfaction as you do your part to keep the largest airline in the world running smoothly as we care for people on life’s journey? Feel free to be yourself at American.
Additional Locations: None
Requisition ID: 68799",3.8,"American Airlines
3.8","Dallas, TX",10000+ Employees,1926,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
840,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
841,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
842,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
843,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
844,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
845,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
846,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
847,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
848,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
849,Intelligence Data Engineer,$82K - $114K (Glassdoor est.),"Overview:
The Data Engineer is a member of the Intelligence team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of the customer. This work will be done as part of a team creating and supporting ViaPath's enterprise data lake from a large variety of data sources.

This remote/virtual position can be performed from anywhere in the continental US.
Responsibilities:
Develop and build data infrastructure for our customers to support data analytic needs.
Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines
Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data
Leverage experience in computer science to develop and actively implement best practices for data gath-ering, auditing, and analysis
Support efforts to strengthen current tools and identify, pilot, and implement best in class tools where needed
Architect and maintain the Data IQ database storage structure to include the creation of stored procedures to gather and organize data from disparate data sources
Develop, implement and maintain Python algorithm for user creation to combine and leverage 3rd party API’s
Develop and maintain Python Flask user interface to facilitate forensic data scrubbing and uploads
Research, develop and maintain a solution related to artificial intelligence object detection through open-source tools
Document your work, system architecture and processes in JIRA, Confluence, and other documentation tools.
Respond to user requests and document work done on service tickets in ticketing system
Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc.
Provide end user support to include training, password resets, etc. as necessary.
Qualifications:
Bachelor’s degree in a quantitative discipline: computer science, statistics, informatics, engineering or data analytics or comparable experience in lieu of degree
A minimum of 2 years of experience as an Intelligence Analyst, Data Engineer, BI Engineer, or Data Warehouse Engineer
Strong experience with SQL/relational databases
Background in at least one programming language (e.g., Python, Java, Scala, PHP, JavaScript)
Experience with either data workflows/modeling, front-end engineering, or backend engineering
Practical experience for investigations
Excellent presentation and communication skills
Familiarity with Business Intelligence concepts
Solid knowledge of distributed computing
Strong organizational skills with attention to detail

ViaPath, an innovation leader in correctional technology, education solutions that assist in rehabilitating in-mates, and payment services solutions for government. ViaPath leads the fields of correctional technology, ed-ucation, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates.

ViaPath is committed to a policy of Equal Employment Opportunity and will not discriminate against an appli-cant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or preg-nancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sex-ual orientation, marital status, or any other characteristic or category protected by federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for em-ployment, verify identity and maintain employment statistics on applicants.",3.4,"ViaPath Technologies
3.4","Dallas, TX",1001 to 5000 Employees,1989,Company - Private,Telecommunications Services,Telecommunications,$500 million to $1 billion (USD)
850,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
851,Junior Data Engineer,$71K - $101K (Glassdoor est.),"Junior Data Engineer
Position Summary:
The Atlanta Falcons are looking to add a detail-oriented, methodical, and collaborative Junior Data Engineer to our team. The ideal candidate will have knowledge of application databases, data warehouses, APIs, and ETL processes.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the organization.
As a Junior Data Engineer, you will work closely with our data and application teams to design, develop, and optimize data pipelines, ensuring data quality and accessibility for various data-driven functions across the football technology department. The person in this role will also assist in the maintenance of our web applications and help build additional tools to assist our analytics team. We seek an individual who wants to be a part of a truly integrated high-performance team. The position is based in Flowery Branch, Georgia.
Roles and Responsibilities:
Data Pipeline Development
Assist in designing, building, and maintaining scalable data pipelines using ETL tools and practices.
Work closely with the team to optimize and enhance the data ingestion and processing tasks.
Assist in maintaining and updating SSIS packages, python scripts, and various API integrations.
Database Management
Support application databases, ensuring high performance and responsiveness.
Assist with database schema design and optimization for application performance.
Ensure that data is stored in an accessible manner.
Data Warehousing
Contribute to the development and maintenance of the data warehouse architecture.
Ensure that data is structured and stored in an accessible, reliable, and efficient manner.
API Integration o Develop and integrate APIs to fetch and send data to external systems.
Collaborate with application developers to ensure seamless data exchange between systems.
Data Quality and Compliance
Monitor data quality, ensuring consistency, accuracy, and reliability.
Adhere to data governance standards and compliance requirements.
Collaboration and Communication
Work alongside other data professionals and cross-functional teams.
Effectively communicate complex technical concepts to non-technical stakeholders.
Qualifications and Education Requirements:
Bachelor's degree in computer science, Mathematics, or related field preferred, or demonstrated experience
Hands on experience in the following technologies
SQL and Data Warehouses.
Building and maintaining SSIS packages with Visual Studio
Building API integrations with both C# and Python
Version control.
Skills & Knowledge
Database design and management
Data mining and Data QA/QC
Strong analytical and quantitative skills • Strong SQL skills
Good communication skills
Problem-solving and troubleshooting
Data warehousing
Desire to learn and constantly improve
A passion and understanding of sports and entertainment",3.9,"AMB Sports + Entertainment
3.9","Atlanta, GA",51 to 200 Employees,-1,Subsidiary or Business Segment,Sports & Recreation,"Arts, Entertainment & Recreation",Unknown / Non-Applicable
852,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
853,Senior Data Engineer,-1,"About Pinterest:
Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.
Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.
Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.
The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.
What you'll do:
Build Scripts in Python to Implement privacy and security requirements on Snowflake
Build Airflow operators in python to ingest data and trigger transformations on Snowflake
Define and advocate the best practices for storing and analyzing data inside Snowflake
Become a Subject Matter Expert and advocate for Enterprise Data Platform
What we're looking for:
Strong skills in Python and SQL
Experience with Building scripts in python for managing Data Warehouse
Experience with any Cloud Data Warehouse either as an Administrator or a Developer
Experience working on privacy and security requirements of a Cloud Data Warehouse
This position is not eligible for relocation assistance.
#LI-REMOTE
#LI-AJ2
At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.
Information regarding the culture at Pinterest and benefits available for this position can be found here.
US based applicants only
$114,750—$236,000 USD
Our Commitment to Diversity:
Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.",3.8,"Pinterest
3.8",Remote,1001 to 5000 Employees,2010,Company - Public,Internet & Web Services,Information Technology,$100 to $500 million (USD)
854,Data Engineer I,-1,"Overview:
This position sits within our Product Development division, which develops, tests, and improves our software solutions in an innovative and collaborative environment.

ConstructConnect is looking for a full-time Data Engineer based in the United States. We support full and/or partial remote work depending on the area you live in.

ConstructConnect has built up multiple sources of data over the course of many years and acquisitions. Our data engineers work to build bridges and pathways between data sources. Our goal of one source of truth is accomplished with a myriad of tools as we build out the company’s data lake and data warehouse. This role has the unique opportunity to consume and manage the nation’s largest collection of construction data.

The ideal candidate for this role is excited about data in all its forms, desires to build a source of data truth, and is looking forward to leveraging Google Cloud’s vast array of data technologies.

Opportunity

The Data Engineer enhances our data pipelines using a combination of technologies and standard engineering practices. They are responsible for working with the software engineering, marketing, product, data science, and customer service teams to ensure our data is delivered in a timely and scalable fashion.
Responsibilities:
What You’ll Be Doing
Assembling large, complex sets of data that meet business requirements
Modernizing and innovating data delivery pipelines using cloud technologies
Ensure data is properly curated and transformed for analysis and consumption
Collaborate with data scientists and architects
This job description in no way implies that the duties listed here are the only ones that team members can be required to perform
Qualifications:
What You Bring to the Team
Creative problem-solver who is passionate about digging into complex problems and devising new approaches to reach results
Strong communication skills and experience distilling and presenting complex quantitative analysis into action-oriented recommendations
Understanding of programming languages such as Python, JavaScript, C#, or other language
Understanding of SQL Queries, Stored Procedures, Views, and Triggers
Understanding of the concepts of ETL or ELT (Extract, Transform, Load) processes
Familiarity with cloud-based data technologies
1 – 3 years professional experience managing Data Lakes and Data WarehousesBachelor’s Degree or equivalent experience in Computer Sciences, Database Development, or a related discipline
Physical Demands and Work Environment

The physical activities of this position include frequent sitting, telephone communication, working on a computer for extended periods of time. Visual acuity is required to perform activities close to the eyes.
This position is fully remote with only occasional travel to the office for team meetings and events. Team members are expected to have an established workspace.
Ability to work remotely in the United States or Canada.
E-Verify Statement

ConstructConnect utilizes the E-Verify program with every potential new hire. This makes it possible for us to make certain that every employee who works for ConstructConnect is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website. E-Verify® is a registered trademark of the United States Department of Homeland Security.

Privacy Notice",3.9,"ConstructConnect
3.9",Remote,1001 to 5000 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
855,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
856,Data Engineer,Employer Provided Salary:$134K - $194K,"At Meta, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Meta Data Center’s Data Science team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Meta's Data Center organization. You will be responsible for creating the technology and data architecture that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team.


Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data



Minimum Qualifications:
BS/MS in Computer Science or a related technical field
5+ years of Python or other modern programming language development experience
5+ years of SQL and relational databases experience
5+ years experience in custom ETL design, implementation and maintenance
3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years experience with Data Modeling
Experience working with cloud or on-premises Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.



Preferred Qualifications:
Experience with more than one coding language
Experience designing and implementing real-time pipelines
Experience with data quality and validation
Experience with SQL performance tuning and end-to-end process optimization
Experience with anomaly/outlier detection
Experience with notebook-based Data Science workflow
Experience with Airflow
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.





Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3.9,"Meta
3.9","Fremont, CA",10000+ Employees,2004,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
857,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
858,Senior Data Engineer,-1,"At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.
One mission. One team. That's OneStudyTeam.
Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.
We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.
What You'll Be Working On
Build reliable and robust data integrations with external partners
Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures
Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions
Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems
Building practical data onboarding tooling and process automation solutions
Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations
Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people
What You'll Bring to OneStudyTeam
4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)
Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform
Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated
Experience or interest in developing and managing enterprise-scale data, distributed data architectures
Able to independently ship medium-to-large features and start to support or participate in architectural design
Excellent written and verbal communication skills
Strong attention to detail is key, especially when considering correctness, security, and compliance
Solid software testing, documentation, and debugging practices in the context of distributed systems
Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits
We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.
Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.
As a condition of employment, you will abide by all organizational security and privacy policies.
For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).",3.2,"OneStudyTeam
3.2",Remote,201 to 500 Employees,2012,Company - Private,Biotech & Pharmaceuticals,Pharmaceutical & Biotechnology,Unknown / Non-Applicable
859,Senior Data Engineer,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Qualifications/requirements:
Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception)
5 years or more experience as data engineer
5 years or more Experience with programming skills with Python
5 years or more Experience with data systems and complex query writing
Experience with Azure is a plus
Experience with NIFI is a plus
Sense of ownership and pride in your performance and its impact on company’s success
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Experience:
ETL: 5 years (Preferred)
Python: 5 years (Preferred)
MySQL: 5 years (Preferred)
Work Location: Remote",3.8,"Anlage Infotech
3.8",Remote,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
860,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
861,Data Engineer,-1,"Data Engineer
Role Info:
Hands on BI Data Engineer role with strong
Creating KPI Dashboards - Interactive Dashboards using tableau / Quicksight
Top Skills:
BI Tableau / AWS Quicksight exp is a must
AWS (IAM, RDS, S3, Lambda) exp is a must
SQL exp is a must
Python knowledge is required
Nice to have:
AWS Certified Solutions Architect
About the Company:
NucleusTeq is a software services, solutions & products company empowering & transforming customer's business through the use of digital technologies such as Big-Data, Analytics, Cloud, Enterprise Automation, Block-chain, Mobility, etc.
We are enabling several fortune 1000 clients in the USA, Canada, UK & India to navigate their digital transformation.",4.7,"NucleusTeq
4.7",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
862,Data Engineer III,-1,"We are seeking a Data Engineer III to join our Data Platform team. This role will report to the director of data platform and be involved in the planning, design, and implementation of our centralized data lake solution supporting analytics, products and applications across the company.
Qualifications:
To be successful in this role the candidate needs to have the following qualifications:
Deep knowledge of Scala and Spark
Experience with Databricks is preferred.
Deep knowledge of modern orchestration frameworks such as Apache airflow
Experience working with SQL and NoSQL database systems.
Experience with cloud environments (Azure Preferred)
Experience with acquiring and preparing data from primary and secondary disparate data sources.
Experience with agile project management methodology
Healthcare industry experience preferred, including exposure to different EMR systems, revenue cycle management.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12 is a plus but is not required.
Responsibilities:
Be part of an engineering team in building data adaptors to expedite the data onboarding process from various health systems.
Work with product management and business analysts on design reusable and configurable data orchestration pipelines.
Work with data specialist in develop and design data transformation to standardize the data model and support data enrichment activities.
As a subject matter expert, hosts information sharing session with teams within data platform or larger R1 organization.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",3.4,"R1 RCM, Inc.
3.4",Remote,10000+ Employees,2003,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
863,AWS Data Engineer/ Architect,Employer Provided Salary:$130K - $160K,"Working knowledge and experience with AWS Services including EMR, DynamoDB, Kinesis Data Stream, Lambda, S3, Salesforce
Focus on AWS and Salesforce – Sales & Service cloud.
Mastery and hands-on experience with Data Engineering technologies and scripting languages.
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Intimately involved in numerous end to end cloud migration projects.
Strong knowledge and experience in designing for and implementing solutions in the cloud (AWS). This can range from custom code on IaaS/PaaS to SaaS solution integrations.
Languages: Java, Angular, node.js, Python
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Working knowledge of application architectures, software development tools, and methodologies.
History of successful technical consulting, and architecture engagements with numerous customers simultaneously.
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Washington, DC 20001",-1,Omniware LLC,"Washington, DC",-1,-1,-1,-1,-1,-1
864,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
865,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
866,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
867,Snowflake Data Engineer,-1,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",-1,Radiant System,Remote,-1,-1,-1,-1,-1,-1
868,Data Warehouse Engineer,$88K - $119K (Glassdoor est.),"At Apiture, our mission is to empower financial institutions to know and serve their clients with the care of a traditional community institution at the scale, speed, and efficiency required in today's digital world. With more than 300 clients throughout the U.S., we deliver comprehensive online and mobile solutions that support banks and credit unions, ranging from small community financial institutions to new, innovative direct banks.

Summary:
Reporting to the Data Engineering Manager, the Senior Data Warehouse Engineer will work closely with the Data Architect to implement the Data Model. You will be working with a team of highly talented engineers to develop practical, scalable data reporting and analytics solutions.

Location (Wilmington, NC, Austin, TX, Remote):
We have offices in Wilmington, NC and Austin, TX and while some positions are office based, we will also consider remote candidates depending on their time zone.

Responsibilities:
Core tasks will involve writing SQL and some Python to transform raw data from the staging layer into the human readable Data Model.
Write new fact and dimension tables and add to existing ones.
Review data in source systems, in databases, or APIs to understand how the data comes into the warehouse and which transformations are needed.
Work with the Data Architect to define and enforce data warehouse standards that align with the larger data management guidelines in place.
Work with data analysts and data scientists to define and refine data analysis goals and implement needed changes in the data warehouse.
Identify and pursue opportunities to automate processes and execute validation strategies to maintain high standards of efficiency and data quality.
Build and maintain documentation around data sets, data classes, data flows, transformations, etc.
Work with the information security and compliance teams at Apiture to build, monitor, and enforce data cataloging, asset tracking, and privacy rules/metrics.
Provide input and feedback to support continuous improvement in data governance processes.

Requirements:
Bachelor's in computer science or equivalent work experience.
4+ years of hands-on experience with coding data transformations in SQL that involved large data sets.
Well-versed with Advanced SQL scripting.
Experience with programming languages: Python, Java, Scala, etc.
Experience building data pipelines that integrate data from structured and unstructured data sources.
Experience troubleshooting data integrations to visualization platforms like Domo, Tableau or PowerBI.
Hands-on experience with data warehousing platforms like Snowflake, RedShift, or Synapse Analytics.
Excellent understanding of ETL/ELT fundamentals and building efficient data pipelines.
Excellent verbal and written communication skills.

Nice To Have:
Experience with cloud technologies (Strong preference for AWS technologies like Lambda, DMS, etc.)
Experience with Data Management platforms like OneTrust
Experience working with REST APIs, Streaming APIs, or other Data Ingress techniques.
Experience with data engineering and monitoring for ML applications.
Exposure to test-driven development and automated testing frameworks.",4.0,"Apiture
4.0","Atlanta, GA",201 to 500 Employees,2017,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
869,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
870,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
871,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
872,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
873,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
874,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
875,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
876,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
877,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
878,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
879,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
880,AWS Data Engineer/ Architect,Employer Provided Salary:$130K - $160K,"Working knowledge and experience with AWS Services including EMR, DynamoDB, Kinesis Data Stream, Lambda, S3, Salesforce
Focus on AWS and Salesforce – Sales & Service cloud.
Mastery and hands-on experience with Data Engineering technologies and scripting languages.
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Intimately involved in numerous end to end cloud migration projects.
Strong knowledge and experience in designing for and implementing solutions in the cloud (AWS). This can range from custom code on IaaS/PaaS to SaaS solution integrations.
Languages: Java, Angular, node.js, Python
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Working knowledge of application architectures, software development tools, and methodologies.
History of successful technical consulting, and architecture engagements with numerous customers simultaneously.
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Washington, DC 20001",-1,Omniware LLC,"Washington, DC",-1,-1,-1,-1,-1,-1
881,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
882,Data Engineer,$79K - $116K (Glassdoor est.),"Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
Sr. Consultant /Data Engineer – Data Architecture & Engineering
Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
A Senior Consultant in the Data and Analytics Practice is someone that looks forward to being challenged and continuously grow their technical and team collaboration skills. You'll provide technical guidance, as needed, to other team members for quality assurance, mentoring and growth opportunities. If this is a path that excites you, let's chat!
Responsibilities
Technical delivery in both Agile and Waterfall methodologies
Implement solutions using the Microsoft Power BI tool suite and related technologies
Leverage advanced database systems including data warehouses, data marts, and models needed for business, financial and operational analysis and/or reporting
Develop high performing, reliable and scalable solutions
Ability to clearly communicate technical details to business and management personnel
Perform data analysis that will support and enhance Information Management systems
Author, oversee and gain approval of design documents for projects assigned
Develop reports, dashboards & scorecards, data exploration and visual analytics using existing and emerging tools and technologies.
Work independently or as part of a team to design and develop solutions
Assist business development team with pre-sales activities and RFPs
Provide meaningful feedback and coaching of other team members to successfully overcome technological challenges
Leading and demonstrating emerging technologies and concepts to teams
Requirements
Bachelor's Degree desired in Computer Science, Information Technology, or related field
2+ years of experience with data visualization
Expert knowledge of Data Management, Business Intelligence and Analytics concepts including:
Extract, Transform, Load (ETL)
Data Warehousing, Data Lakes, Data Marts, Data Stores, and Cubes
Data Quality and Profiling
Data Governance, Master Data Management, and Metadata Management
Data Visualization, Dashboard, Reporting, Self Service Business Intelligence (BI), Key Performance Indicators (KPI), and Scorecards
Predictive, Prescription, and Descriptive Analytics
Expert knowledge of Microsoft Business Intelligence stack including Power BI, Excel and SQL Server (SSAS, SSRS, SSIS)
Minimum of 3-5 years of experience (of which 2-3 years in consulting)
Experience with Azure Data Estate (Azure SQL DB, Cosmos DB, Azure SQL DW, Azure Data Lake) is a plus
Azure ML Studio/Services experience a plus
Certifications are a plus
Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and tuning is a plus
Knowledge of relational and dimensional database structures, theories, principles, and practice is a plus
Experience with Data Science tools, technologies and techniques (R, Python, algorithms) is a plus
Passionate about learning new technologies
Ability to learn new concepts and software quickly
Analytical approach to problem-solving; ability to use technology to solve business problems
Familiarity with database-centric applications
Ability to communicate effectively in both a technical and non-technical manner
Ability to work in a fast-paced environment
Ability to travel up to 30%
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways.
At this time, we cannot sponsor applicants for work visas. Candidates should be based in the United States.
About 3Cloud
3Cloud is a ""born in the cloud"" Gold-Certified Microsoft Azure technology consulting firm and Azure Expert Managed Services Provider that provides cloud strategy, design, implementation, acquisition and ongoing managed services to its clients across various industries. To meet the needs of its clients, 3Cloud offers comprehensive services across Cloud Infrastructure, DevOps & Automation, Cloud Application Development and Data, Analytics and AI. Founded by former Microsoft technology leaders, 3Cloud uniquely offers: (1) highly experienced and proven cloud architects and technologists, (2) strong business strategy, financial acumen and operational proficiency and (3) deep relationship and network into Microsoft engineering and field leadership.
3Cloud Total Rewards Highlights Include:
Flexible work location with a virtual first approach to work!
401(K) with match up to 50% of your 6% contributions of eligible pay
Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days
Three medical plan options to allow you the choice to elect what works best for you!
Option for vision and dental coverage
100% employer premium coverage for STD and LTD
Paid leave for birth parents and non-birth parents
Option for FSA, HSA, HRA and Dependent Care
$67.00 monthly tech and home office allowance
Utilization and/or discretionary bonus eligibility based on role
Robust Employee Assistance Program to help with everyday challenges
3Cloud offers competitive compensation. In addition to base pay employees are eligible to receive an annual discretionary/utilization bonus. If you are hired at 3Cloud your final base salary is based on factors such as skills, education, experience and/or geographic location. Please keep in mind that the range mentioned above includes the full base salary range for the role. It is not typical for offers to be made at or near the top of the range.
Base Salary Range
$90,000—$183,000 USD
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway.
At this time, we cannot sponsor applicants for work visas.",4.3,"3Cloud
4.3","Downers Grove, IL",501 to 1000 Employees,2016,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
883,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
884,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
885,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
886,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
887,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
888,Data Engineer,Employer Provided Salary:$80K - $85K,"Simatree is searching for a Data Engineer Consultant to support our client’s operational requirements on a mission focused program within a challenging, dynamic setting. Successful candidates in this position will collaborate with a diverse group of individuals across the homeland security community and directly contribute to management and analysis of operational data using data science tools. This role supports a US Government contract. An US Citizenship is required.
The Role
The Data Engineer will work alongside a team to generate management tools that effectively control programs and generate reports while supporting the programmatic development of operational applications of the tools or analysis that are focused on operations research and document findings. They will conduct research into management, organizational, and business practices to implement improvements to existing processes. They will utilize commercial or open-source software to manage tasks and present findings to a broad audience, including agency executives. They will maintain a database to produce monthly financial reports, tracks processes, distribute deliverables, and monitor comment status. The Data Engineer will be required to use Microsoft Office to perform queries, analyses, and produce reports. Other responsibilities include:
Use R and Python to perform analyses and develop new tools using popular data visualization libraries such as ggplot2, plotly, matplotlib, or bokeh.
Use Databricks to Extract, Transform, and Load (ETL) data from different sources and transform it into a usable, reliable resource.
Manage data collection and transformation from federal law enforcement and other databases to be used in a wide spectrum of analyses including statistical modeling, performance metrics analysis, crosschecking data with other federal agencies, and risk analysis.
Use ArcGIS to perform analyses and develop new web map applications that summarize results from deep learning/AI algorithms.
Maintain and enhance business intelligence (BI) dashboards using best practices in UI/UX design
Develop and test new methodologies and algorithms which would increase the efficiency and accuracy of existing models, including pilot deployments of automated scripts
Requirements
This role supports a US Government contract. A US citizenship is required.
Strong Experience with SQL programming
Experience with R statistical programming and Python programming
Experience with BI software such as Qlik, Tableau or PowerBI
Demonstrated experience in performing data analysis and modeling on an independent basis
Demonstrated experience in developing and applying quantitative methods to find patterns and relationships in large data sets using statistical and graphical packages
Ability to translate complex, technical, or analytic findings into an easily understood narrative—tell a story with graphical, verbal or written form
3 years experience in supporting quantitative and analytical efforts
Bachelors degree in relevant fields such as statistics, mathematics, computer science, physical science, economics or engineering
Job Type: Full-time
Pay: $80,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Experience level:
1 year
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This question MUST be answered for consideration: This position supports a US Government contract which requires a US Citizenship. Are you a US Citizen?
Work Location: Hybrid remote in Washington, DC 20001",5.0,"Simatree
5.0","Washington, DC",1 to 50 Employees,2018,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
889,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
890,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
891,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
892,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
893,Data Engineer,Employer Provided Salary:$112K - $154K,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

As a Data Engineer at DocuSign you will work with a group of like peers to help support the existing storage infrastructure, while designing the next version of the infrastructure. We have no down time and demand very high availability from our system (we strive for five 9s). When not supporting the existing systems, you will help collaborate with engineering, architecture and product management to create the next great platform features that will drive impact. You will share in an “On Call” rotation to support the platform 24X7. We are pushing the boundaries of what’s possible with file, NoSQL, Full text search and data warehouse storage. This position will push your skills to the next level.

This position is an individual contributor reporting to the Sr. Director, Database and Storage Solutions.

Responsibility
Protect the data under our control by making sure we meet all data storage best practices and policy requirements, which includes patching and other industry best practices
Ensure all storage systems meet (RPO) policies
Ensure all storage systems meet (RTO) policies
Operate and manage multiple 7x24 enterprise storage platforms
Maintain storage platform performance by identifying and resolving production and application development problems; calculating optimum performance measures; evaluating, integrating, and installing new releases, completing maintenance
Provide support by coding utilities, responding to user questions, and resolving problems
Troubleshoot storage service issues as they occur, including after-hours and weekends as part of an “On Call rotation”
Ensure ISO, Fed RAMP and PCI information security compliance
Plan and execute expansion to support rapid transaction growth
Work with engineering to design and optimize storage solutions
Work with the operations team to manage, install and configure server’s in multiple data centers around the world
Create and analyze operational reporting
Document the company’s storage environment
Update job knowledge by participating in educational opportunities, reading professional publications, maintaining personal networks, participating in professional organizations


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)

Positions at DocuSign are assigned a job designation of either In Office, Hybrid or Remote and are specific to the role/job. Preferred job designations are not guaranteed when changing positions within DocuSign. DocuSign reserves the right to change a position's job designation depending on business needs and as permitted by local law.


What you bring

Basic
5+ years experience as a server administrator and production support administrator
Understanding of server setup and configuration for optimal resource usage
Experience storage server monitoring, performance tuning, troubleshooting, and capacity planning
Experience with storage design, maintenance, security, management and analysis
Preferred
Experience with Cloud based storage tools and platforms
High level knowledge of Windows and Windows security
Excellent analytical and problem-solving skills
Excellent written and verbal communication skills
Self-motivated, able to work independently as well as in a collaborative environment
Experience with PowerShell scripting
Strong Kusto querying skills


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $118,300.00 - $182,775.00 base salary

Illinois and Colorado: $111,600.00 - $153,525.00 base salary

Washington and New York (including NYC metro area): $111,600.00 - $162,625.00 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.",3.7,"DocuSign
3.7","Seattle, WA",5001 to 10000 Employees,2003,Company - Public,Information Technology Support Services,Information Technology,$1 to $5 billion (USD)
894,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
895,Data Engineer,Employer Provided Salary:$77K - $113K,"Eaton’s Electrical division is currently seeking a Business Analyst to work out of our Raleigh, NC location.

Relocation within the United States will be offered.

The expected annual salary range for this role is $77,249 - $113,299 a year.
Please note the salary information shown above is a general guideline only. Salaries are based upon candidate skills, experience, and qualifications, as well as market and business considerations.
What you’ll do:

The Data Engineer will collaborate with cross-functional teams to understand data requirements, design and implement efficient data pipelines, and ensure the availability, reliability, and scalability of our data solutions. This position necessitates proficiency in data manipulation, data analysis, SQL (writing queries), data modeling, and visualization skills.

Job responsibilities
Gather data from various sources. Design and write SQL queries to retrieve and analyze data stored in various databases.
Develop ETL processes to clean, transform, and enrich data in PowerQuery, ensuring data quality, consistency, and accuracy for use in the PowerBI's Data model.
Design and develop data model, transform raw data into meaningful insights, create relationships, hierarchies, and calculated measures in PowerBI’s Data Model
Document data integration and transformation processes for future reference and troubleshooting.
Assists in the implementation of data governance processes and systems.
Qualifications:
Basic Qualifications:
Bachelor's degree from an accredited educational institution
Minimum three (3) years of SQL and Power BI experience
Must be legally authorized to work in the United States without company sponsorship now or in the future

Preferred Qualifications:
Bachelor’s degree in computer science or software engineering
PowerBI (PowerQuery, Modelling, DAX, creating interactive visuals), Relational databases, Data Governance, Visualization
Problem-solving, Communication, Attention to detail
#LI-JM3
We are committed to ensuring equal employment opportunities for all job applicants and employees. Employment decisions are based upon job-related reasons regardless of an applicant's race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, marital status, genetic information, protected veteran status, or any other status protected by law.
Eaton considers qualified applicants regardless of criminal histories, consistent with local laws. To request a disability-related reasonable accommodation to assist you in your job search, application or interview process, please call us at 1-800-836-6345 to discuss your specific need. Only accommodation requests will be accepted by this phone number.
We know that good benefit programs are important to employees and their families. Eaton provides various Health and Welfare benefits as well as Retirement benefits, and several programs that provide for paid and unpaid time away from work. Click here for more detail: Eaton Benefits Overview. Please note that specific programs and options available to an employee may depend on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.",3.9,"Eaton
3.9","Raleigh, NC",10000+ Employees,1911,Company - Public,Electronics Manufacturing,Manufacturing,$10+ billion (USD)
896,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
897,Data Engineer,Employer Provided Salary:$100K - $105K,"Job Description
▪ 4+ years of experience in Python with Data background
▪ BA/BS Computer Science or a technical/quantitative degree
▪ Excellent communication and collaboration skills
▪ Interest in learning and adopting new tools and techniques
▪ Humble enough to learn from others, confident enough to teach others new things
Full time position, Salary (+ Benefits ( Healthcare Insurance + PTO + Bonus + 401k) )
Job Type: Full-time
Salary: $100,000.00 - $105,000.00 per year
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",-1,Konnectingtree.Inc,Remote,-1,-1,-1,-1,-1,-1
898,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
899,Data Engineer,Employer Provided Salary:$80K - $120K,"As the Data Engineer at Digible, you will be responsible for designing, developing, and maintaining our data pipeline, ensuring that data is properly collected, processed, and analyzed to inform business decisions. You will work closely with cross-functional teams in order to understand their data needs and ensure that our data infrastructure supports those needs. You will also be responsible for ensuring the integrity and security of our data.
About Us:
Privately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.
At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.
We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.
What you'll do:
Design and implement scalable data pipelines, including data ingestion, transformation, and integration.
Collaborate with stakeholders to understand data requirements and develop data models aligned with Digible's business goals.
Build and maintain ETL/ELT workflows using dbt (Data Build Tool) and Prefect to transform raw data into structured datasets for analysis, reporting, and sustainability tracking purposes.
Implement data quality controls and validation processes to ensure the accuracy and integrity of Digible's data.
Monitor data pipelines, perform debugging and troubleshooting, and implement proactive measures to ensure data availability, accuracy, and efficient resource utilization.
Optimize and tune data pipelines for performance, scalability, and reliability.
Stay up to date on the latest advancements in data engineering technologies, tools, and best practices, and propose their adoption when they align with Digible's objectives
You should have:
Proven experience as a Data Engineer or similar role, with a strong understanding of data engineering concepts, data modeling, and database systems.
Cloud experience (i.e. AWS, GCP, Azure, etc.)
Proficiency with Git and source control
Proficiency with data transformation and modeling tools (i.e. dbt, dataform, PySpark, etc.)
Strong programming skills in Python and familiarity with related libraries and best practices.
Experience with SQL and working knowledge of relational databases and/or columnar databases.
Familiarity with data orchestration techniques and tools (i.e. Prefect, Dagster, Airflow, AWS Glue, etc.)
Data Warehouse Experience (i.e. BigQuery, Snowflake, Redshift, etc..)
Knowledge of data warehousing concepts and dimensional modeling.
Strong problem-solving and analytical skills, with the ability to work in a fast-paced and collaborative environment.
Familiarity with Docker
Experience with marketing and advertising API's & data (i.e. Google Suite, Facebook, etc.) is a big plus
Application experience of CI/CD implementations is a plus
Passion to make an impact and do some awesome work.
Digible's Core Values
Authenticity: The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.
Curiosity: The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.
Focus: The collective will to remain completely devoted and ultimately accountable to our deliverables.
Humility: The recognition and daily practice that ""we"" is always greater than ""I"".
Happiness: The decision to prioritize passion and love for what we do above everything else.
Pay, perks, and such:
Annual Salary of $80,000 - $120,000 depending on level, breadth and years of experience
4 Day work week (32 hour per week)
WFA (Work From Anywhere)
Profit Sharing Bonus
We offer 3 weeks of PTO as well as Sick leave, and Bereavement.
We offer 11 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Day before Thanksgiving, Thanksgiving, Day after Thanksgiving, Christmas Eve, and Christmas)
401(k) + 5% employer match
50% employer paid health benefits, including Medical, Dental, and Vision.
We provide $75/ month reimbursement for Physical Wellness
We provide $75/ month reimbursement for Mental Wellness
$1000/year travel fund for employees who have been with Digible 3+ years
Monthly subscription for financial wellness
Dog-Friendly Office
Paid Parental Leave
Company Sponsored Social Events
Company Provided weekly lunches and snacks for in office employees
Employee Development Program",4.7,"Digible
4.7",Remote,51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,$5 to $25 million (USD)
900,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
901,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
902,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
903,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
904,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
905,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
906,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
907,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
908,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
909,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
910,AWS Data Engineer/ Architect,Employer Provided Salary:$130K - $160K,"Working knowledge and experience with AWS Services including EMR, DynamoDB, Kinesis Data Stream, Lambda, S3, Salesforce
Focus on AWS and Salesforce – Sales & Service cloud.
Mastery and hands-on experience with Data Engineering technologies and scripting languages.
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Intimately involved in numerous end to end cloud migration projects.
Strong knowledge and experience in designing for and implementing solutions in the cloud (AWS). This can range from custom code on IaaS/PaaS to SaaS solution integrations.
Languages: Java, Angular, node.js, Python
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Working knowledge of application architectures, software development tools, and methodologies.
History of successful technical consulting, and architecture engagements with numerous customers simultaneously.
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Washington, DC 20001",-1,Omniware LLC,"Washington, DC",-1,-1,-1,-1,-1,-1
911,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
912,Data Engineer,$79K - $116K (Glassdoor est.),"Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
Sr. Consultant /Data Engineer – Data Architecture & Engineering
Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
A Senior Consultant in the Data and Analytics Practice is someone that looks forward to being challenged and continuously grow their technical and team collaboration skills. You'll provide technical guidance, as needed, to other team members for quality assurance, mentoring and growth opportunities. If this is a path that excites you, let's chat!
Responsibilities
Technical delivery in both Agile and Waterfall methodologies
Implement solutions using the Microsoft Power BI tool suite and related technologies
Leverage advanced database systems including data warehouses, data marts, and models needed for business, financial and operational analysis and/or reporting
Develop high performing, reliable and scalable solutions
Ability to clearly communicate technical details to business and management personnel
Perform data analysis that will support and enhance Information Management systems
Author, oversee and gain approval of design documents for projects assigned
Develop reports, dashboards & scorecards, data exploration and visual analytics using existing and emerging tools and technologies.
Work independently or as part of a team to design and develop solutions
Assist business development team with pre-sales activities and RFPs
Provide meaningful feedback and coaching of other team members to successfully overcome technological challenges
Leading and demonstrating emerging technologies and concepts to teams
Requirements
Bachelor's Degree desired in Computer Science, Information Technology, or related field
2+ years of experience with data visualization
Expert knowledge of Data Management, Business Intelligence and Analytics concepts including:
Extract, Transform, Load (ETL)
Data Warehousing, Data Lakes, Data Marts, Data Stores, and Cubes
Data Quality and Profiling
Data Governance, Master Data Management, and Metadata Management
Data Visualization, Dashboard, Reporting, Self Service Business Intelligence (BI), Key Performance Indicators (KPI), and Scorecards
Predictive, Prescription, and Descriptive Analytics
Expert knowledge of Microsoft Business Intelligence stack including Power BI, Excel and SQL Server (SSAS, SSRS, SSIS)
Minimum of 3-5 years of experience (of which 2-3 years in consulting)
Experience with Azure Data Estate (Azure SQL DB, Cosmos DB, Azure SQL DW, Azure Data Lake) is a plus
Azure ML Studio/Services experience a plus
Certifications are a plus
Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and tuning is a plus
Knowledge of relational and dimensional database structures, theories, principles, and practice is a plus
Experience with Data Science tools, technologies and techniques (R, Python, algorithms) is a plus
Passionate about learning new technologies
Ability to learn new concepts and software quickly
Analytical approach to problem-solving; ability to use technology to solve business problems
Familiarity with database-centric applications
Ability to communicate effectively in both a technical and non-technical manner
Ability to work in a fast-paced environment
Ability to travel up to 30%
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways.
At this time, we cannot sponsor applicants for work visas. Candidates should be based in the United States.
About 3Cloud
3Cloud is a ""born in the cloud"" Gold-Certified Microsoft Azure technology consulting firm and Azure Expert Managed Services Provider that provides cloud strategy, design, implementation, acquisition and ongoing managed services to its clients across various industries. To meet the needs of its clients, 3Cloud offers comprehensive services across Cloud Infrastructure, DevOps & Automation, Cloud Application Development and Data, Analytics and AI. Founded by former Microsoft technology leaders, 3Cloud uniquely offers: (1) highly experienced and proven cloud architects and technologists, (2) strong business strategy, financial acumen and operational proficiency and (3) deep relationship and network into Microsoft engineering and field leadership.
3Cloud Total Rewards Highlights Include:
Flexible work location with a virtual first approach to work!
401(K) with match up to 50% of your 6% contributions of eligible pay
Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days
Three medical plan options to allow you the choice to elect what works best for you!
Option for vision and dental coverage
100% employer premium coverage for STD and LTD
Paid leave for birth parents and non-birth parents
Option for FSA, HSA, HRA and Dependent Care
$67.00 monthly tech and home office allowance
Utilization and/or discretionary bonus eligibility based on role
Robust Employee Assistance Program to help with everyday challenges
3Cloud offers competitive compensation. In addition to base pay employees are eligible to receive an annual discretionary/utilization bonus. If you are hired at 3Cloud your final base salary is based on factors such as skills, education, experience and/or geographic location. Please keep in mind that the range mentioned above includes the full base salary range for the role. It is not typical for offers to be made at or near the top of the range.
Base Salary Range
$90,000—$183,000 USD
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway.
At this time, we cannot sponsor applicants for work visas.",4.3,"3Cloud
4.3","Downers Grove, IL",501 to 1000 Employees,2016,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
913,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
914,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
915,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
916,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
917,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
918,Data Engineer,Employer Provided Salary:$80K - $85K,"Simatree is searching for a Data Engineer Consultant to support our client’s operational requirements on a mission focused program within a challenging, dynamic setting. Successful candidates in this position will collaborate with a diverse group of individuals across the homeland security community and directly contribute to management and analysis of operational data using data science tools. This role supports a US Government contract. An US Citizenship is required.
The Role
The Data Engineer will work alongside a team to generate management tools that effectively control programs and generate reports while supporting the programmatic development of operational applications of the tools or analysis that are focused on operations research and document findings. They will conduct research into management, organizational, and business practices to implement improvements to existing processes. They will utilize commercial or open-source software to manage tasks and present findings to a broad audience, including agency executives. They will maintain a database to produce monthly financial reports, tracks processes, distribute deliverables, and monitor comment status. The Data Engineer will be required to use Microsoft Office to perform queries, analyses, and produce reports. Other responsibilities include:
Use R and Python to perform analyses and develop new tools using popular data visualization libraries such as ggplot2, plotly, matplotlib, or bokeh.
Use Databricks to Extract, Transform, and Load (ETL) data from different sources and transform it into a usable, reliable resource.
Manage data collection and transformation from federal law enforcement and other databases to be used in a wide spectrum of analyses including statistical modeling, performance metrics analysis, crosschecking data with other federal agencies, and risk analysis.
Use ArcGIS to perform analyses and develop new web map applications that summarize results from deep learning/AI algorithms.
Maintain and enhance business intelligence (BI) dashboards using best practices in UI/UX design
Develop and test new methodologies and algorithms which would increase the efficiency and accuracy of existing models, including pilot deployments of automated scripts
Requirements
This role supports a US Government contract. A US citizenship is required.
Strong Experience with SQL programming
Experience with R statistical programming and Python programming
Experience with BI software such as Qlik, Tableau or PowerBI
Demonstrated experience in performing data analysis and modeling on an independent basis
Demonstrated experience in developing and applying quantitative methods to find patterns and relationships in large data sets using statistical and graphical packages
Ability to translate complex, technical, or analytic findings into an easily understood narrative—tell a story with graphical, verbal or written form
3 years experience in supporting quantitative and analytical efforts
Bachelors degree in relevant fields such as statistics, mathematics, computer science, physical science, economics or engineering
Job Type: Full-time
Pay: $80,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Experience level:
1 year
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This question MUST be answered for consideration: This position supports a US Government contract which requires a US Citizenship. Are you a US Citizen?
Work Location: Hybrid remote in Washington, DC 20001",5.0,"Simatree
5.0","Washington, DC",1 to 50 Employees,2018,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
919,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
920,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
921,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
922,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
923,Data Engineer,Employer Provided Salary:$112K - $154K,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

As a Data Engineer at DocuSign you will work with a group of like peers to help support the existing storage infrastructure, while designing the next version of the infrastructure. We have no down time and demand very high availability from our system (we strive for five 9s). When not supporting the existing systems, you will help collaborate with engineering, architecture and product management to create the next great platform features that will drive impact. You will share in an “On Call” rotation to support the platform 24X7. We are pushing the boundaries of what’s possible with file, NoSQL, Full text search and data warehouse storage. This position will push your skills to the next level.

This position is an individual contributor reporting to the Sr. Director, Database and Storage Solutions.

Responsibility
Protect the data under our control by making sure we meet all data storage best practices and policy requirements, which includes patching and other industry best practices
Ensure all storage systems meet (RPO) policies
Ensure all storage systems meet (RTO) policies
Operate and manage multiple 7x24 enterprise storage platforms
Maintain storage platform performance by identifying and resolving production and application development problems; calculating optimum performance measures; evaluating, integrating, and installing new releases, completing maintenance
Provide support by coding utilities, responding to user questions, and resolving problems
Troubleshoot storage service issues as they occur, including after-hours and weekends as part of an “On Call rotation”
Ensure ISO, Fed RAMP and PCI information security compliance
Plan and execute expansion to support rapid transaction growth
Work with engineering to design and optimize storage solutions
Work with the operations team to manage, install and configure server’s in multiple data centers around the world
Create and analyze operational reporting
Document the company’s storage environment
Update job knowledge by participating in educational opportunities, reading professional publications, maintaining personal networks, participating in professional organizations


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)

Positions at DocuSign are assigned a job designation of either In Office, Hybrid or Remote and are specific to the role/job. Preferred job designations are not guaranteed when changing positions within DocuSign. DocuSign reserves the right to change a position's job designation depending on business needs and as permitted by local law.


What you bring

Basic
5+ years experience as a server administrator and production support administrator
Understanding of server setup and configuration for optimal resource usage
Experience storage server monitoring, performance tuning, troubleshooting, and capacity planning
Experience with storage design, maintenance, security, management and analysis
Preferred
Experience with Cloud based storage tools and platforms
High level knowledge of Windows and Windows security
Excellent analytical and problem-solving skills
Excellent written and verbal communication skills
Self-motivated, able to work independently as well as in a collaborative environment
Experience with PowerShell scripting
Strong Kusto querying skills


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $118,300.00 - $182,775.00 base salary

Illinois and Colorado: $111,600.00 - $153,525.00 base salary

Washington and New York (including NYC metro area): $111,600.00 - $162,625.00 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.",3.7,"DocuSign
3.7","Seattle, WA",5001 to 10000 Employees,2003,Company - Public,Information Technology Support Services,Information Technology,$1 to $5 billion (USD)
924,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
925,Data Engineer,Employer Provided Salary:$77K - $113K,"Eaton’s Electrical division is currently seeking a Business Analyst to work out of our Raleigh, NC location.

Relocation within the United States will be offered.

The expected annual salary range for this role is $77,249 - $113,299 a year.
Please note the salary information shown above is a general guideline only. Salaries are based upon candidate skills, experience, and qualifications, as well as market and business considerations.
What you’ll do:

The Data Engineer will collaborate with cross-functional teams to understand data requirements, design and implement efficient data pipelines, and ensure the availability, reliability, and scalability of our data solutions. This position necessitates proficiency in data manipulation, data analysis, SQL (writing queries), data modeling, and visualization skills.

Job responsibilities
Gather data from various sources. Design and write SQL queries to retrieve and analyze data stored in various databases.
Develop ETL processes to clean, transform, and enrich data in PowerQuery, ensuring data quality, consistency, and accuracy for use in the PowerBI's Data model.
Design and develop data model, transform raw data into meaningful insights, create relationships, hierarchies, and calculated measures in PowerBI’s Data Model
Document data integration and transformation processes for future reference and troubleshooting.
Assists in the implementation of data governance processes and systems.
Qualifications:
Basic Qualifications:
Bachelor's degree from an accredited educational institution
Minimum three (3) years of SQL and Power BI experience
Must be legally authorized to work in the United States without company sponsorship now or in the future

Preferred Qualifications:
Bachelor’s degree in computer science or software engineering
PowerBI (PowerQuery, Modelling, DAX, creating interactive visuals), Relational databases, Data Governance, Visualization
Problem-solving, Communication, Attention to detail
#LI-JM3
We are committed to ensuring equal employment opportunities for all job applicants and employees. Employment decisions are based upon job-related reasons regardless of an applicant's race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, marital status, genetic information, protected veteran status, or any other status protected by law.
Eaton considers qualified applicants regardless of criminal histories, consistent with local laws. To request a disability-related reasonable accommodation to assist you in your job search, application or interview process, please call us at 1-800-836-6345 to discuss your specific need. Only accommodation requests will be accepted by this phone number.
We know that good benefit programs are important to employees and their families. Eaton provides various Health and Welfare benefits as well as Retirement benefits, and several programs that provide for paid and unpaid time away from work. Click here for more detail: Eaton Benefits Overview. Please note that specific programs and options available to an employee may depend on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.",3.9,"Eaton
3.9","Raleigh, NC",10000+ Employees,1911,Company - Public,Electronics Manufacturing,Manufacturing,$10+ billion (USD)
926,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
927,Data Engineer,Employer Provided Salary:$100K - $105K,"Job Description
▪ 4+ years of experience in Python with Data background
▪ BA/BS Computer Science or a technical/quantitative degree
▪ Excellent communication and collaboration skills
▪ Interest in learning and adopting new tools and techniques
▪ Humble enough to learn from others, confident enough to teach others new things
Full time position, Salary (+ Benefits ( Healthcare Insurance + PTO + Bonus + 401k) )
Job Type: Full-time
Salary: $100,000.00 - $105,000.00 per year
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",-1,Konnectingtree.Inc,Remote,-1,-1,-1,-1,-1,-1
928,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
929,Data Engineer,Employer Provided Salary:$80K - $120K,"As the Data Engineer at Digible, you will be responsible for designing, developing, and maintaining our data pipeline, ensuring that data is properly collected, processed, and analyzed to inform business decisions. You will work closely with cross-functional teams in order to understand their data needs and ensure that our data infrastructure supports those needs. You will also be responsible for ensuring the integrity and security of our data.
About Us:
Privately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.
At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.
We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.
What you'll do:
Design and implement scalable data pipelines, including data ingestion, transformation, and integration.
Collaborate with stakeholders to understand data requirements and develop data models aligned with Digible's business goals.
Build and maintain ETL/ELT workflows using dbt (Data Build Tool) and Prefect to transform raw data into structured datasets for analysis, reporting, and sustainability tracking purposes.
Implement data quality controls and validation processes to ensure the accuracy and integrity of Digible's data.
Monitor data pipelines, perform debugging and troubleshooting, and implement proactive measures to ensure data availability, accuracy, and efficient resource utilization.
Optimize and tune data pipelines for performance, scalability, and reliability.
Stay up to date on the latest advancements in data engineering technologies, tools, and best practices, and propose their adoption when they align with Digible's objectives
You should have:
Proven experience as a Data Engineer or similar role, with a strong understanding of data engineering concepts, data modeling, and database systems.
Cloud experience (i.e. AWS, GCP, Azure, etc.)
Proficiency with Git and source control
Proficiency with data transformation and modeling tools (i.e. dbt, dataform, PySpark, etc.)
Strong programming skills in Python and familiarity with related libraries and best practices.
Experience with SQL and working knowledge of relational databases and/or columnar databases.
Familiarity with data orchestration techniques and tools (i.e. Prefect, Dagster, Airflow, AWS Glue, etc.)
Data Warehouse Experience (i.e. BigQuery, Snowflake, Redshift, etc..)
Knowledge of data warehousing concepts and dimensional modeling.
Strong problem-solving and analytical skills, with the ability to work in a fast-paced and collaborative environment.
Familiarity with Docker
Experience with marketing and advertising API's & data (i.e. Google Suite, Facebook, etc.) is a big plus
Application experience of CI/CD implementations is a plus
Passion to make an impact and do some awesome work.
Digible's Core Values
Authenticity: The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.
Curiosity: The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.
Focus: The collective will to remain completely devoted and ultimately accountable to our deliverables.
Humility: The recognition and daily practice that ""we"" is always greater than ""I"".
Happiness: The decision to prioritize passion and love for what we do above everything else.
Pay, perks, and such:
Annual Salary of $80,000 - $120,000 depending on level, breadth and years of experience
4 Day work week (32 hour per week)
WFA (Work From Anywhere)
Profit Sharing Bonus
We offer 3 weeks of PTO as well as Sick leave, and Bereavement.
We offer 11 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Day before Thanksgiving, Thanksgiving, Day after Thanksgiving, Christmas Eve, and Christmas)
401(k) + 5% employer match
50% employer paid health benefits, including Medical, Dental, and Vision.
We provide $75/ month reimbursement for Physical Wellness
We provide $75/ month reimbursement for Mental Wellness
$1000/year travel fund for employees who have been with Digible 3+ years
Monthly subscription for financial wellness
Dog-Friendly Office
Paid Parental Leave
Company Sponsored Social Events
Company Provided weekly lunches and snacks for in office employees
Employee Development Program",4.7,"Digible
4.7",Remote,51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,$5 to $25 million (USD)
930,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
931,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
932,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
933,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
934,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
935,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
936,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
937,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
938,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
939,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
940,AWS Data Engineer/ Architect,Employer Provided Salary:$130K - $160K,"Working knowledge and experience with AWS Services including EMR, DynamoDB, Kinesis Data Stream, Lambda, S3, Salesforce
Focus on AWS and Salesforce – Sales & Service cloud.
Mastery and hands-on experience with Data Engineering technologies and scripting languages.
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Intimately involved in numerous end to end cloud migration projects.
Strong knowledge and experience in designing for and implementing solutions in the cloud (AWS). This can range from custom code on IaaS/PaaS to SaaS solution integrations.
Languages: Java, Angular, node.js, Python
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Working knowledge of application architectures, software development tools, and methodologies.
History of successful technical consulting, and architecture engagements with numerous customers simultaneously.
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Washington, DC 20001",-1,Omniware LLC,"Washington, DC",-1,-1,-1,-1,-1,-1
941,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
942,Data Engineer,$79K - $116K (Glassdoor est.),"Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
Sr. Consultant /Data Engineer – Data Architecture & Engineering
Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
A Senior Consultant in the Data and Analytics Practice is someone that looks forward to being challenged and continuously grow their technical and team collaboration skills. You'll provide technical guidance, as needed, to other team members for quality assurance, mentoring and growth opportunities. If this is a path that excites you, let's chat!
Responsibilities
Technical delivery in both Agile and Waterfall methodologies
Implement solutions using the Microsoft Power BI tool suite and related technologies
Leverage advanced database systems including data warehouses, data marts, and models needed for business, financial and operational analysis and/or reporting
Develop high performing, reliable and scalable solutions
Ability to clearly communicate technical details to business and management personnel
Perform data analysis that will support and enhance Information Management systems
Author, oversee and gain approval of design documents for projects assigned
Develop reports, dashboards & scorecards, data exploration and visual analytics using existing and emerging tools and technologies.
Work independently or as part of a team to design and develop solutions
Assist business development team with pre-sales activities and RFPs
Provide meaningful feedback and coaching of other team members to successfully overcome technological challenges
Leading and demonstrating emerging technologies and concepts to teams
Requirements
Bachelor's Degree desired in Computer Science, Information Technology, or related field
2+ years of experience with data visualization
Expert knowledge of Data Management, Business Intelligence and Analytics concepts including:
Extract, Transform, Load (ETL)
Data Warehousing, Data Lakes, Data Marts, Data Stores, and Cubes
Data Quality and Profiling
Data Governance, Master Data Management, and Metadata Management
Data Visualization, Dashboard, Reporting, Self Service Business Intelligence (BI), Key Performance Indicators (KPI), and Scorecards
Predictive, Prescription, and Descriptive Analytics
Expert knowledge of Microsoft Business Intelligence stack including Power BI, Excel and SQL Server (SSAS, SSRS, SSIS)
Minimum of 3-5 years of experience (of which 2-3 years in consulting)
Experience with Azure Data Estate (Azure SQL DB, Cosmos DB, Azure SQL DW, Azure Data Lake) is a plus
Azure ML Studio/Services experience a plus
Certifications are a plus
Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and tuning is a plus
Knowledge of relational and dimensional database structures, theories, principles, and practice is a plus
Experience with Data Science tools, technologies and techniques (R, Python, algorithms) is a plus
Passionate about learning new technologies
Ability to learn new concepts and software quickly
Analytical approach to problem-solving; ability to use technology to solve business problems
Familiarity with database-centric applications
Ability to communicate effectively in both a technical and non-technical manner
Ability to work in a fast-paced environment
Ability to travel up to 30%
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways.
At this time, we cannot sponsor applicants for work visas. Candidates should be based in the United States.
About 3Cloud
3Cloud is a ""born in the cloud"" Gold-Certified Microsoft Azure technology consulting firm and Azure Expert Managed Services Provider that provides cloud strategy, design, implementation, acquisition and ongoing managed services to its clients across various industries. To meet the needs of its clients, 3Cloud offers comprehensive services across Cloud Infrastructure, DevOps & Automation, Cloud Application Development and Data, Analytics and AI. Founded by former Microsoft technology leaders, 3Cloud uniquely offers: (1) highly experienced and proven cloud architects and technologists, (2) strong business strategy, financial acumen and operational proficiency and (3) deep relationship and network into Microsoft engineering and field leadership.
3Cloud Total Rewards Highlights Include:
Flexible work location with a virtual first approach to work!
401(K) with match up to 50% of your 6% contributions of eligible pay
Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days
Three medical plan options to allow you the choice to elect what works best for you!
Option for vision and dental coverage
100% employer premium coverage for STD and LTD
Paid leave for birth parents and non-birth parents
Option for FSA, HSA, HRA and Dependent Care
$67.00 monthly tech and home office allowance
Utilization and/or discretionary bonus eligibility based on role
Robust Employee Assistance Program to help with everyday challenges
3Cloud offers competitive compensation. In addition to base pay employees are eligible to receive an annual discretionary/utilization bonus. If you are hired at 3Cloud your final base salary is based on factors such as skills, education, experience and/or geographic location. Please keep in mind that the range mentioned above includes the full base salary range for the role. It is not typical for offers to be made at or near the top of the range.
Base Salary Range
$90,000—$183,000 USD
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway.
At this time, we cannot sponsor applicants for work visas.",4.3,"3Cloud
4.3","Downers Grove, IL",501 to 1000 Employees,2016,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
943,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
944,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
945,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
946,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
947,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
948,Data Engineer,Employer Provided Salary:$80K - $85K,"Simatree is searching for a Data Engineer Consultant to support our client’s operational requirements on a mission focused program within a challenging, dynamic setting. Successful candidates in this position will collaborate with a diverse group of individuals across the homeland security community and directly contribute to management and analysis of operational data using data science tools. This role supports a US Government contract. An US Citizenship is required.
The Role
The Data Engineer will work alongside a team to generate management tools that effectively control programs and generate reports while supporting the programmatic development of operational applications of the tools or analysis that are focused on operations research and document findings. They will conduct research into management, organizational, and business practices to implement improvements to existing processes. They will utilize commercial or open-source software to manage tasks and present findings to a broad audience, including agency executives. They will maintain a database to produce monthly financial reports, tracks processes, distribute deliverables, and monitor comment status. The Data Engineer will be required to use Microsoft Office to perform queries, analyses, and produce reports. Other responsibilities include:
Use R and Python to perform analyses and develop new tools using popular data visualization libraries such as ggplot2, plotly, matplotlib, or bokeh.
Use Databricks to Extract, Transform, and Load (ETL) data from different sources and transform it into a usable, reliable resource.
Manage data collection and transformation from federal law enforcement and other databases to be used in a wide spectrum of analyses including statistical modeling, performance metrics analysis, crosschecking data with other federal agencies, and risk analysis.
Use ArcGIS to perform analyses and develop new web map applications that summarize results from deep learning/AI algorithms.
Maintain and enhance business intelligence (BI) dashboards using best practices in UI/UX design
Develop and test new methodologies and algorithms which would increase the efficiency and accuracy of existing models, including pilot deployments of automated scripts
Requirements
This role supports a US Government contract. A US citizenship is required.
Strong Experience with SQL programming
Experience with R statistical programming and Python programming
Experience with BI software such as Qlik, Tableau or PowerBI
Demonstrated experience in performing data analysis and modeling on an independent basis
Demonstrated experience in developing and applying quantitative methods to find patterns and relationships in large data sets using statistical and graphical packages
Ability to translate complex, technical, or analytic findings into an easily understood narrative—tell a story with graphical, verbal or written form
3 years experience in supporting quantitative and analytical efforts
Bachelors degree in relevant fields such as statistics, mathematics, computer science, physical science, economics or engineering
Job Type: Full-time
Pay: $80,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Experience level:
1 year
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This question MUST be answered for consideration: This position supports a US Government contract which requires a US Citizenship. Are you a US Citizen?
Work Location: Hybrid remote in Washington, DC 20001",5.0,"Simatree
5.0","Washington, DC",1 to 50 Employees,2018,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
949,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
950,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
951,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
952,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
953,Data Engineer,Employer Provided Salary:$112K - $154K,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

As a Data Engineer at DocuSign you will work with a group of like peers to help support the existing storage infrastructure, while designing the next version of the infrastructure. We have no down time and demand very high availability from our system (we strive for five 9s). When not supporting the existing systems, you will help collaborate with engineering, architecture and product management to create the next great platform features that will drive impact. You will share in an “On Call” rotation to support the platform 24X7. We are pushing the boundaries of what’s possible with file, NoSQL, Full text search and data warehouse storage. This position will push your skills to the next level.

This position is an individual contributor reporting to the Sr. Director, Database and Storage Solutions.

Responsibility
Protect the data under our control by making sure we meet all data storage best practices and policy requirements, which includes patching and other industry best practices
Ensure all storage systems meet (RPO) policies
Ensure all storage systems meet (RTO) policies
Operate and manage multiple 7x24 enterprise storage platforms
Maintain storage platform performance by identifying and resolving production and application development problems; calculating optimum performance measures; evaluating, integrating, and installing new releases, completing maintenance
Provide support by coding utilities, responding to user questions, and resolving problems
Troubleshoot storage service issues as they occur, including after-hours and weekends as part of an “On Call rotation”
Ensure ISO, Fed RAMP and PCI information security compliance
Plan and execute expansion to support rapid transaction growth
Work with engineering to design and optimize storage solutions
Work with the operations team to manage, install and configure server’s in multiple data centers around the world
Create and analyze operational reporting
Document the company’s storage environment
Update job knowledge by participating in educational opportunities, reading professional publications, maintaining personal networks, participating in professional organizations


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)

Positions at DocuSign are assigned a job designation of either In Office, Hybrid or Remote and are specific to the role/job. Preferred job designations are not guaranteed when changing positions within DocuSign. DocuSign reserves the right to change a position's job designation depending on business needs and as permitted by local law.


What you bring

Basic
5+ years experience as a server administrator and production support administrator
Understanding of server setup and configuration for optimal resource usage
Experience storage server monitoring, performance tuning, troubleshooting, and capacity planning
Experience with storage design, maintenance, security, management and analysis
Preferred
Experience with Cloud based storage tools and platforms
High level knowledge of Windows and Windows security
Excellent analytical and problem-solving skills
Excellent written and verbal communication skills
Self-motivated, able to work independently as well as in a collaborative environment
Experience with PowerShell scripting
Strong Kusto querying skills


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $118,300.00 - $182,775.00 base salary

Illinois and Colorado: $111,600.00 - $153,525.00 base salary

Washington and New York (including NYC metro area): $111,600.00 - $162,625.00 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.",3.7,"DocuSign
3.7","Seattle, WA",5001 to 10000 Employees,2003,Company - Public,Information Technology Support Services,Information Technology,$1 to $5 billion (USD)
954,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
955,Data Engineer,Employer Provided Salary:$77K - $113K,"Eaton’s Electrical division is currently seeking a Business Analyst to work out of our Raleigh, NC location.

Relocation within the United States will be offered.

The expected annual salary range for this role is $77,249 - $113,299 a year.
Please note the salary information shown above is a general guideline only. Salaries are based upon candidate skills, experience, and qualifications, as well as market and business considerations.
What you’ll do:

The Data Engineer will collaborate with cross-functional teams to understand data requirements, design and implement efficient data pipelines, and ensure the availability, reliability, and scalability of our data solutions. This position necessitates proficiency in data manipulation, data analysis, SQL (writing queries), data modeling, and visualization skills.

Job responsibilities
Gather data from various sources. Design and write SQL queries to retrieve and analyze data stored in various databases.
Develop ETL processes to clean, transform, and enrich data in PowerQuery, ensuring data quality, consistency, and accuracy for use in the PowerBI's Data model.
Design and develop data model, transform raw data into meaningful insights, create relationships, hierarchies, and calculated measures in PowerBI’s Data Model
Document data integration and transformation processes for future reference and troubleshooting.
Assists in the implementation of data governance processes and systems.
Qualifications:
Basic Qualifications:
Bachelor's degree from an accredited educational institution
Minimum three (3) years of SQL and Power BI experience
Must be legally authorized to work in the United States without company sponsorship now or in the future

Preferred Qualifications:
Bachelor’s degree in computer science or software engineering
PowerBI (PowerQuery, Modelling, DAX, creating interactive visuals), Relational databases, Data Governance, Visualization
Problem-solving, Communication, Attention to detail
#LI-JM3
We are committed to ensuring equal employment opportunities for all job applicants and employees. Employment decisions are based upon job-related reasons regardless of an applicant's race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, marital status, genetic information, protected veteran status, or any other status protected by law.
Eaton considers qualified applicants regardless of criminal histories, consistent with local laws. To request a disability-related reasonable accommodation to assist you in your job search, application or interview process, please call us at 1-800-836-6345 to discuss your specific need. Only accommodation requests will be accepted by this phone number.
We know that good benefit programs are important to employees and their families. Eaton provides various Health and Welfare benefits as well as Retirement benefits, and several programs that provide for paid and unpaid time away from work. Click here for more detail: Eaton Benefits Overview. Please note that specific programs and options available to an employee may depend on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.",3.9,"Eaton
3.9","Raleigh, NC",10000+ Employees,1911,Company - Public,Electronics Manufacturing,Manufacturing,$10+ billion (USD)
956,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
957,Data Engineer,Employer Provided Salary:$100K - $105K,"Job Description
▪ 4+ years of experience in Python with Data background
▪ BA/BS Computer Science or a technical/quantitative degree
▪ Excellent communication and collaboration skills
▪ Interest in learning and adopting new tools and techniques
▪ Humble enough to learn from others, confident enough to teach others new things
Full time position, Salary (+ Benefits ( Healthcare Insurance + PTO + Bonus + 401k) )
Job Type: Full-time
Salary: $100,000.00 - $105,000.00 per year
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",-1,Konnectingtree.Inc,Remote,-1,-1,-1,-1,-1,-1
958,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
959,Data Engineer,Employer Provided Salary:$80K - $120K,"As the Data Engineer at Digible, you will be responsible for designing, developing, and maintaining our data pipeline, ensuring that data is properly collected, processed, and analyzed to inform business decisions. You will work closely with cross-functional teams in order to understand their data needs and ensure that our data infrastructure supports those needs. You will also be responsible for ensuring the integrity and security of our data.
About Us:
Privately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.
At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.
We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.
What you'll do:
Design and implement scalable data pipelines, including data ingestion, transformation, and integration.
Collaborate with stakeholders to understand data requirements and develop data models aligned with Digible's business goals.
Build and maintain ETL/ELT workflows using dbt (Data Build Tool) and Prefect to transform raw data into structured datasets for analysis, reporting, and sustainability tracking purposes.
Implement data quality controls and validation processes to ensure the accuracy and integrity of Digible's data.
Monitor data pipelines, perform debugging and troubleshooting, and implement proactive measures to ensure data availability, accuracy, and efficient resource utilization.
Optimize and tune data pipelines for performance, scalability, and reliability.
Stay up to date on the latest advancements in data engineering technologies, tools, and best practices, and propose their adoption when they align with Digible's objectives
You should have:
Proven experience as a Data Engineer or similar role, with a strong understanding of data engineering concepts, data modeling, and database systems.
Cloud experience (i.e. AWS, GCP, Azure, etc.)
Proficiency with Git and source control
Proficiency with data transformation and modeling tools (i.e. dbt, dataform, PySpark, etc.)
Strong programming skills in Python and familiarity with related libraries and best practices.
Experience with SQL and working knowledge of relational databases and/or columnar databases.
Familiarity with data orchestration techniques and tools (i.e. Prefect, Dagster, Airflow, AWS Glue, etc.)
Data Warehouse Experience (i.e. BigQuery, Snowflake, Redshift, etc..)
Knowledge of data warehousing concepts and dimensional modeling.
Strong problem-solving and analytical skills, with the ability to work in a fast-paced and collaborative environment.
Familiarity with Docker
Experience with marketing and advertising API's & data (i.e. Google Suite, Facebook, etc.) is a big plus
Application experience of CI/CD implementations is a plus
Passion to make an impact and do some awesome work.
Digible's Core Values
Authenticity: The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.
Curiosity: The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.
Focus: The collective will to remain completely devoted and ultimately accountable to our deliverables.
Humility: The recognition and daily practice that ""we"" is always greater than ""I"".
Happiness: The decision to prioritize passion and love for what we do above everything else.
Pay, perks, and such:
Annual Salary of $80,000 - $120,000 depending on level, breadth and years of experience
4 Day work week (32 hour per week)
WFA (Work From Anywhere)
Profit Sharing Bonus
We offer 3 weeks of PTO as well as Sick leave, and Bereavement.
We offer 11 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Day before Thanksgiving, Thanksgiving, Day after Thanksgiving, Christmas Eve, and Christmas)
401(k) + 5% employer match
50% employer paid health benefits, including Medical, Dental, and Vision.
We provide $75/ month reimbursement for Physical Wellness
We provide $75/ month reimbursement for Mental Wellness
$1000/year travel fund for employees who have been with Digible 3+ years
Monthly subscription for financial wellness
Dog-Friendly Office
Paid Parental Leave
Company Sponsored Social Events
Company Provided weekly lunches and snacks for in office employees
Employee Development Program",4.7,"Digible
4.7",Remote,51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,$5 to $25 million (USD)
960,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
961,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
962,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
963,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
964,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
965,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
966,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
967,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
968,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
969,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
970,AWS Data Engineer/ Architect,Employer Provided Salary:$130K - $160K,"Working knowledge and experience with AWS Services including EMR, DynamoDB, Kinesis Data Stream, Lambda, S3, Salesforce
Focus on AWS and Salesforce – Sales & Service cloud.
Mastery and hands-on experience with Data Engineering technologies and scripting languages.
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Intimately involved in numerous end to end cloud migration projects.
Strong knowledge and experience in designing for and implementing solutions in the cloud (AWS). This can range from custom code on IaaS/PaaS to SaaS solution integrations.
Languages: Java, Angular, node.js, Python
Design and implement Data Migration and Data Integration across cloud and hybrid environments.
Working knowledge of application architectures, software development tools, and methodologies.
History of successful technical consulting, and architecture engagements with numerous customers simultaneously.
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Work Location: Hybrid remote in Washington, DC 20001",-1,Omniware LLC,"Washington, DC",-1,-1,-1,-1,-1,-1
971,Data Analytics Engineer,Employer Provided Salary:$60.00 Per Hour,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",-1,Proits Hub LLC,Remote,1 to 50 Employees,-1,Company - Public,-1,-1,Unknown / Non-Applicable
972,Data Engineer,$79K - $116K (Glassdoor est.),"Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
Sr. Consultant /Data Engineer – Data Architecture & Engineering
Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you!
At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other.
A Senior Consultant in the Data and Analytics Practice is someone that looks forward to being challenged and continuously grow their technical and team collaboration skills. You'll provide technical guidance, as needed, to other team members for quality assurance, mentoring and growth opportunities. If this is a path that excites you, let's chat!
Responsibilities
Technical delivery in both Agile and Waterfall methodologies
Implement solutions using the Microsoft Power BI tool suite and related technologies
Leverage advanced database systems including data warehouses, data marts, and models needed for business, financial and operational analysis and/or reporting
Develop high performing, reliable and scalable solutions
Ability to clearly communicate technical details to business and management personnel
Perform data analysis that will support and enhance Information Management systems
Author, oversee and gain approval of design documents for projects assigned
Develop reports, dashboards & scorecards, data exploration and visual analytics using existing and emerging tools and technologies.
Work independently or as part of a team to design and develop solutions
Assist business development team with pre-sales activities and RFPs
Provide meaningful feedback and coaching of other team members to successfully overcome technological challenges
Leading and demonstrating emerging technologies and concepts to teams
Requirements
Bachelor's Degree desired in Computer Science, Information Technology, or related field
2+ years of experience with data visualization
Expert knowledge of Data Management, Business Intelligence and Analytics concepts including:
Extract, Transform, Load (ETL)
Data Warehousing, Data Lakes, Data Marts, Data Stores, and Cubes
Data Quality and Profiling
Data Governance, Master Data Management, and Metadata Management
Data Visualization, Dashboard, Reporting, Self Service Business Intelligence (BI), Key Performance Indicators (KPI), and Scorecards
Predictive, Prescription, and Descriptive Analytics
Expert knowledge of Microsoft Business Intelligence stack including Power BI, Excel and SQL Server (SSAS, SSRS, SSIS)
Minimum of 3-5 years of experience (of which 2-3 years in consulting)
Experience with Azure Data Estate (Azure SQL DB, Cosmos DB, Azure SQL DW, Azure Data Lake) is a plus
Azure ML Studio/Services experience a plus
Certifications are a plus
Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and tuning is a plus
Knowledge of relational and dimensional database structures, theories, principles, and practice is a plus
Experience with Data Science tools, technologies and techniques (R, Python, algorithms) is a plus
Passionate about learning new technologies
Ability to learn new concepts and software quickly
Analytical approach to problem-solving; ability to use technology to solve business problems
Familiarity with database-centric applications
Ability to communicate effectively in both a technical and non-technical manner
Ability to work in a fast-paced environment
Ability to travel up to 30%
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways.
At this time, we cannot sponsor applicants for work visas. Candidates should be based in the United States.
About 3Cloud
3Cloud is a ""born in the cloud"" Gold-Certified Microsoft Azure technology consulting firm and Azure Expert Managed Services Provider that provides cloud strategy, design, implementation, acquisition and ongoing managed services to its clients across various industries. To meet the needs of its clients, 3Cloud offers comprehensive services across Cloud Infrastructure, DevOps & Automation, Cloud Application Development and Data, Analytics and AI. Founded by former Microsoft technology leaders, 3Cloud uniquely offers: (1) highly experienced and proven cloud architects and technologists, (2) strong business strategy, financial acumen and operational proficiency and (3) deep relationship and network into Microsoft engineering and field leadership.
3Cloud Total Rewards Highlights Include:
Flexible work location with a virtual first approach to work!
401(K) with match up to 50% of your 6% contributions of eligible pay
Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days
Three medical plan options to allow you the choice to elect what works best for you!
Option for vision and dental coverage
100% employer premium coverage for STD and LTD
Paid leave for birth parents and non-birth parents
Option for FSA, HSA, HRA and Dependent Care
$67.00 monthly tech and home office allowance
Utilization and/or discretionary bonus eligibility based on role
Robust Employee Assistance Program to help with everyday challenges
3Cloud offers competitive compensation. In addition to base pay employees are eligible to receive an annual discretionary/utilization bonus. If you are hired at 3Cloud your final base salary is based on factors such as skills, education, experience and/or geographic location. Please keep in mind that the range mentioned above includes the full base salary range for the role. It is not typical for offers to be made at or near the top of the range.
Base Salary Range
$90,000—$183,000 USD
Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway.
At this time, we cannot sponsor applicants for work visas.",4.3,"3Cloud
4.3","Downers Grove, IL",501 to 1000 Employees,2016,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
973,Data Engineer,-1,"Core4ce is searching for a Data Engineer to support the Legacy Data Consolidation Solution (LDCS) NIWC Atlantic Team’s ingestion of legacy data into the MHS Information Platform (MIP) within the Defense Health Agency (DHA) Government Cloud Amazon Web Services (AWS) environment. This position requires a self-starter who welcomes a challenge and is comfortable working both independently and as a member of a team. The candidate will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database environments, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

Responsibilities
Perform Data Discovery
Perform Data Analysis such as:
Identifying Volume and Cardinality
Identifying referential relationships not explicit in metadata
Identifying Table counts and Activity Levels
Write data manipulation queries to prepare data for the data modeling and mapping team
Support FHIR team mapping of data elements into FHIR resources.
Validate permissions and access into source AHLTA CDR system.
Create Redundancy and Backup Plans
Establish MIP storage and server compute requirements.
Create test datasets
Assist in de-identification of datasets
Coordinate and complete engineering working group sessions or assessments.
Transform system data into a consolidated solution supporting critical use cases including clinical, release of information, analytical and decision-making.
Utilize Microsoft SQL Server Migration Assistance (SSMA) to migrate system data from Oracle to Microsoft SQL Server.
Develop data queries and reports as needed.
Maintain access and posture within SQL, Dev, and Arkload Servers.
Perform ‘Smoke’ Tests including ability to login, navigate, sign out.
Confirm application layer connectivity to databases within Servers.
Review and support LDCS patch/version management plan as necessary in support of the database team.

Requirements
5+ years of Data experience
Proficient in SQL programming and database management
Proficient in Data Modeling and normalization techniques
Familiarity with relational data querying and manipulation
Familiarity with Oracle and Microsoft SQL Server databases.
Ability to use Excel for data analysis
Ability to manipulate and analyze data
Strong attention to detail and ability to work with large datasets in RDBMS, Excel, or CSV flat file formats
Effective communication and teamwork abilities
Public Trust or ability to obtain

Preferred
Experience with DoD medical systems and databases
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.",4.4,"Core4ce
4.4",Remote,201 to 500 Employees,2018,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
974,"Data Engineer, Sales (Remote)",Employer Provided Salary:$90K - $150K,"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
The Data Engineer is responsible for building and maintaining the data foundations of the Sales, Specialist Sales, and Sales Engineering business units and automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Data & Analytics team, the Data Engineer collaborates with Analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution.
They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment.

In addition to managing the data foundation, the Data Engineer will work across IT, Sales Operations, Marketing Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets.
What You'll Do:
Design, develop, and maintain a data platform that serves as the foundation of the Sales, Specialist Sales, and Sales Engineering business units, automating and aggregating data from internal systems as well as external vendors
Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity
Build data pipelines and reports that enable analysts and other stakeholders across the business unit
Interface with engineers from other systems and applications to ensure proper data collection
Ensure data quality by implementing data detection mechanisms
Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit
Participate in reviewing the data models created by the team and change the existing data models in production as necessary
Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes
Support existing processes running in production and optimize when possible
What You’ll Need:
3+ years of experience as a Data / Analytics Engineer, ETL Developer, or similar functions at a fast-paced company, including designing, building, and maintaining data processing systems
3+ years of experience with production-level SQL, with the ability to write, analyze, and debug SQL queries
2+ years of experience with Python preferred
Experience with Salesforce and Snowflake preferred
Experience with Tableau preferred
Experience with Sales, Alliances, and/or Marketing a plus
BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field
Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more
Demonstrated understanding of development processes and agile methodologies
Intellectually curious and willing to learn
#LI-Remote
#LI-DV1
#LI-HK1
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $90,000 - $150,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work",4.3,"CrowdStrike
4.3",Remote,1001 to 5000 Employees,2011,Company - Public,Enterprise Software & Network Solutions,Information Technology,$500 million to $1 billion (USD)
975,Informatica Data Engineer,Employer Provided Salary:$50.00 Per Hour,"Description
(Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange)
Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs.
Ensure that supported data pipelines meet any predefined SLAs.
Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted.
Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed.
Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs.
Address defects related to support pipelines as needed and assigned.
Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments.
Track job performance to identify increasing processing times and implement optimizations as needed.
Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences.
Document any changes to supported jobs for incident resolution and/or defect fixes.
Provides resolution to a diverse range of recognizable complex problems.
Analysis is required to identify root cause.
Uses judgment within defined boundaries to develop and iterate solutions, both long and short term.
Top Skills Details
1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses
2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services)
3. Strong experience with Informatica PowerCenter and Informatica PowerExchange
4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory)
5. Strong experience working with Snowflake as the Enterprise Data Warehouse
6. Strong experience working with Oracle/SQL Server databases
7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins
Job Type: Contract
Pay: From $50.00 per hour
Experience level:
1 year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",4.1,"Abotts consulting
4.1",Remote,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
976,Snowflake Data Engineer,Employer Provided Salary:$60.00 - $80.00 Per Hour,"Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?
Imagine working at Intellibus to engineer platforms that impact billions of lives around the world. With your passion and focus we will accomplish great things together!
We are looking forward to you joining our Platform Engineering Team.
Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.
We are looking for Engineers who can
Create Data modeling
Work on Snowflake modeling – roles, databases, schemas, ETL toolswith cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level.
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks).
Setup AWS S3/EC2, Configure External stages and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta lake (data bricks)
We work closely with
Data Wrangling
ETL
Talend
Jasper
Java
Python
Unix
AWS
Data Warehousing
Data Modeling
Database Migration
ECreLT
RBAC model
Data migration
Our Process
Schedule a 15 min Video Call with someone from our Team
4 Proctored GQ Tests (< 2 hours)
30-45 min Final Video Interview
Receive Job Offer
If you are interested in reaching out to us, please apply and our team will contact you within the hour.
Job Type: Full-time
Pay: $60.00 - $80.00 per hour
Schedule:
Monday to Friday
Experience:
Data Wrangling: 7 years (Preferred)
Snowflake: 7 years (Preferred)
ETL: 7 years (Preferred)
Work Location: In person",4.6,"Intellibus
4.6","Silver Spring, MD",Unknown,2015,Unknown,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
977,Senior Data Engineer I,$121K - $155K (Glassdoor est.),"Description
At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.

As part of the Spoonflower Data Engineering team you will partner with cross-functional teams in Product Management, Marketing, and Business Development to identify and answer complex business questions about Spoonflowers business performance. The individual will be responsible for driving the relationship with stakeholders, delivering actionable insights, developing helpful analytics frameworks, and coordinating with data engineers to create data and reporting solutions.

What You'll Do Here:
Highly skilled at writing and optimizing SQL queries to analyze large data sets in a cloud database environment.
Excellent data storyteller adept at summarizing complex data analysis into easy-to-understand visuals and presentations up to an executive level.
A self-sufficient problem solver with the ability to think critically, understand stakeholders needs, and develop analytics frameworks to answer often ambiguous business questions.
Experience building reports and dashboards in Tableau, Looker, PowerBI or similar technologies
Pull data from various data sources and integrate into the ETL process
Optimize workloads to reduce cost and improve performance
Develop monitoring over BI infrastructure
Translate business requirements to technical specifications
Contribute to the design of the Spoonflower Data Warehouse
Work with a variety of databases (Redshift, Snowflake, Databricks, MySQL)
Monitor the integrity and validity of the data
Work closely with Product analysts, Marketing, Data Scientists and Finance to deliver insights
A self-starter who takes ownership of their domain and establishes processes to organize requests into a manageable workflow.
Great communicator who sets clear expectations and anticipates questions and concerns in advance.
The Skills You'll Bring:
7+ years of experience in a BI Development roll
Thorough understanding of eCommerce and Internet marketing data collection and metrics
Experience in DWH Design
Proficient in SQL
Proficient in BI Technologies (Tableau, Power BI, Looker, etc.)
Bachelor's / Master's degree in Computer Science or equivalent
It is helpful, but not required to have:
Experience with Snowflake/Redshift/Aurora
Experience with NoSQL Databases - MongoDB, Dyanamo DB
Experience with 24/7 production systems and real-time analytics
Experience with Python
Experience with AWS Athena, AWS DMS, Spark-ETL, AWS Glue, Pyspark, EMR, Kinesis, Lambda, S3
Experience working in the AWS Services Ecosystem or relevant Cloud Infrastructures such as Google Cloud or Azure
Experience with Databricks as a compute environment is a plus
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.

The compensation package for this role is based on multiple factors, such as job level, responsibilities,
location, and candidate experience. The base pay ranges included below are specific to the locations
listed, and may not be applicable to other locations.

California: [$101,800-144,850]
Connecticut, New York, and Rhode Island: [$101,800-133,450]
Colorado and Washington: [$101,800-122,550]
Nevada: [$96,600-133,450]

This position may be eligible for a bonus incentive, health benefits, a 401K program, and other employee
perks. More details about our company benefits can be found at https://shutterflyinc.com/benefits/

#SFLYTechnology",3.3,"Shutterfly
3.3","Fort Mill, SC",10000+ Employees,1999,Company - Private,Internet & Web Services,Information Technology,$1 to $5 billion (USD)
978,Data Engineer,Employer Provided Salary:$80K - $85K,"Simatree is searching for a Data Engineer Consultant to support our client’s operational requirements on a mission focused program within a challenging, dynamic setting. Successful candidates in this position will collaborate with a diverse group of individuals across the homeland security community and directly contribute to management and analysis of operational data using data science tools. This role supports a US Government contract. An US Citizenship is required.
The Role
The Data Engineer will work alongside a team to generate management tools that effectively control programs and generate reports while supporting the programmatic development of operational applications of the tools or analysis that are focused on operations research and document findings. They will conduct research into management, organizational, and business practices to implement improvements to existing processes. They will utilize commercial or open-source software to manage tasks and present findings to a broad audience, including agency executives. They will maintain a database to produce monthly financial reports, tracks processes, distribute deliverables, and monitor comment status. The Data Engineer will be required to use Microsoft Office to perform queries, analyses, and produce reports. Other responsibilities include:
Use R and Python to perform analyses and develop new tools using popular data visualization libraries such as ggplot2, plotly, matplotlib, or bokeh.
Use Databricks to Extract, Transform, and Load (ETL) data from different sources and transform it into a usable, reliable resource.
Manage data collection and transformation from federal law enforcement and other databases to be used in a wide spectrum of analyses including statistical modeling, performance metrics analysis, crosschecking data with other federal agencies, and risk analysis.
Use ArcGIS to perform analyses and develop new web map applications that summarize results from deep learning/AI algorithms.
Maintain and enhance business intelligence (BI) dashboards using best practices in UI/UX design
Develop and test new methodologies and algorithms which would increase the efficiency and accuracy of existing models, including pilot deployments of automated scripts
Requirements
This role supports a US Government contract. A US citizenship is required.
Strong Experience with SQL programming
Experience with R statistical programming and Python programming
Experience with BI software such as Qlik, Tableau or PowerBI
Demonstrated experience in performing data analysis and modeling on an independent basis
Demonstrated experience in developing and applying quantitative methods to find patterns and relationships in large data sets using statistical and graphical packages
Ability to translate complex, technical, or analytic findings into an easily understood narrative—tell a story with graphical, verbal or written form
3 years experience in supporting quantitative and analytical efforts
Bachelors degree in relevant fields such as statistics, mathematics, computer science, physical science, economics or engineering
Job Type: Full-time
Pay: $80,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Experience level:
1 year
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC 20001: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This question MUST be answered for consideration: This position supports a US Government contract which requires a US Citizenship. Are you a US Citizen?
Work Location: Hybrid remote in Washington, DC 20001",5.0,"Simatree
5.0","Washington, DC",1 to 50 Employees,2018,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
979,DATA ENGINEER,Employer Provided Salary:$48.00 - $58.00 Per Hour,"STAND 8 is a global leader providing end-to-end IT Solutions. We solve business problems through PEOPLE, PROCESS, and TECHNOLOGY and are looking for individuals to help us scale software projects designed to change the world!

We are currently seeking a Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will also possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes and systems.

Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical designs.
Develop ETL/ELT workflows, mappings, and data pipelines to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Other related duties as directed.
Skill Qualifications
Required
Minimum of 5 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 3 years of SQL query development, preferably in multiple database management platforms, and working with normalized relational databases and dimensional data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS and PowerCenter preferred)
Some experience with cloud-based database technologies required
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals.
Preferred:
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required
BA or BS in Computer Science, Information Systems or related field
5+ years’ experience
The US base range for this contract position is $48-$58/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training",4.4,"STAND 8
4.4",Remote,1 to 50 Employees,-1,Company - Private,-1,-1,Less than $1 million (USD)
980,Data Engineer,-1,"Overview
Veracity Solutions is an IT professional services company focused on helping companies execute their digital transformation projects. We use the latest technologies along with the skill and expertise of our people to transform our clients in tech, healthcare, retail, finance, and more, stay relevant and flourish in the digital era. At Veracity Solutions, we offer a respectful, fun, collaborative, and positive work culture that encourages growth, innovation, hard work, and a happy work-life balance with competitive pay.?
This is a remote position - 6-month contract with a potential extension.
Primary responsibility:
The data engineer plays a key role in the journey toward a modern data stack that enables the app data accessibility, the creation of knowledge and the insight from the data. As a data engineer, you will be part of a high-performing data mission team and operate in a dynamic and deliver-focused environment. You will work closely with data architects, data engineers and platform engineers to design and develop data pipelines on-premise and in the cloud.
Working context:
Analyze, define, and document system requirements for data, workflow, logical processes, interfaces with other systems, auditing, reporting requirements, and production configuration.
Design, develop, implement, deliver and manage ETL processes or data pipelines to enable data accessibility to application, reporting and analytics.
Support the creation of the new cloud infrastructure and data ecosystem in the cloud.
Should be comfortable overlapping significantly with standard Eastern Time business Overtime would not be expected or permitted.
Required Qualifications
Bachelor’s or higher Degree in Computer Science, Engineering or equivalent experience.
An AWS Glue expert with 5+ years of working experience and a background in data engineering developing, implementing, delivering and managing end-to-end data solutions.
Expertise in designing, developing, and maintaining database schema objects including tables, indexes, stored procedures, and user-defined functions for database applications.
Proficiency in cloud data technologies, such as AWS S3, EC2.
Advanced knowledge in designing, developing, implementing and manage data pipelines to deliver data or data insights for application, reporting or analytics.
Strong experience creating and maintaining functional and technical specifications documents.
Strong experience creating test plans, test data sets, and automated testing to ensure all components of the system meet specifications.
Strong SQL technical experience including linking IT applications to databases and creating and handling metadata.
Strong programming skill in Python or Scala.
Preferred Qualifications
Strong experience in NoSQL database such as MongoDB.
Strong experience in streaming technology such as Kafka, databricks streaming.
Strong experience in working in the healthcare industry including dealing with PHI, HIPPA regulations, BAA processes.
Knowledge of Snowflake/Redshift tools
Diversity Statement:
At Veracity Solutions, we commit to candor with compassion, a client-first approach, continuous improvement individually, in our teams and as a company, deliver results with a “wow,” and reward employees who are humble,?hungry, and people smart. We believe that diversity fuels innovation. We are committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.",-1,Veracity Solutions Inc,Remote,1 to 50 Employees,1998,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
981,Data Engineer,$84K - $121K (Glassdoor est.),"Clearance: Requires an active DOD Security Clearance
The Prospective Group (TPG) is looking for a Data Engineer who will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The candidate will have extensive experience supporting software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the data needs of multiple teams, systems and products, to optimize or even re-design the company’s data architecture to support the next generation of products and data initiatives.
Position Responsibilities:
Create data tools for analytics and data scientist team members that assist them in building and optimizing the product into an innovative industry leader
Create and maintain optimal data pipeline architecture
Assemble large, complex datasets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure ‘big data’ technologies
Work with data and analytics experts to strive for greater functionality in the data systems
Position Requirements:
Strong work experience with PowerShell
Hands-on experience with a wide variety of data platforms: PowerShell, SQL, and Azure VMS/networking
Working knowledge of programming languages including PySpark, Python, R, etc.
5+ years of experience as a data engineer or a similar role
Capable of supporting and working with cross-functional teams in a dynamic environment
Technical expertise with data models, data mining, and segmentation techniques
Strong understanding of data pipelines; should be able to work with REST, SOAP, FTP, HTTP, and ODBC
Working knowledge of message queuing, stream processing, and highly scalable data stores
Familiar with using ETL Solutions like Azure Data Factory to assist in the extracting, transforming, and loading of data into databases or other storage types
Familiar with Cloud Platforms
Preferred Skills:
Bachelor’s degree in Computer Science, IT or similar fields or relevant work experience
Data engineering certification (e.g., IBM Certified Data Engineer)
Experience with Python
Basic knowledge of Agile Scrum

The Prospective Group (TPG) is an IT management consulting company providing services internationally to government and commercial entities. Being Prospective - leaning forward with action - defines the TPG culture.
Our focus areas include but are not limited to program/project management, cloud computing, software development, cybersecurity, data science/analytics, and diplomatic business support.
TPG is a women-owned small business (WOSB), appraised at CMMI Level 3 with excellent performance credentials.
The Prospective Group is an Equal Opportunity and Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or orientation, national origin, disability, or protected veteran status.",4.2,"The Prospective Group (TPG)
4.2","Springfield, VA",51 to 200 Employees,-1,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
982,Senior Data Engineer - AWS,Employer Provided Salary:$60.00 - $65.00 Per Hour,"Data Engineer: Contractor
Kaizen Analytix LLC, an analytics services company seeking a qualified Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 16 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data
Contribute to estimating input and time required for data engineering development tasks
Contribute to client demonstrations of solution or presentations on architecture
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Work Location: In person",3.7,"Kaizen Analytix
3.7","Dallas, TX",1 to 50 Employees,-1,Company - Private,Business Consulting,Management & Consulting,Unknown / Non-Applicable
983,Data Engineer,Employer Provided Salary:$112K - $154K,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

As a Data Engineer at DocuSign you will work with a group of like peers to help support the existing storage infrastructure, while designing the next version of the infrastructure. We have no down time and demand very high availability from our system (we strive for five 9s). When not supporting the existing systems, you will help collaborate with engineering, architecture and product management to create the next great platform features that will drive impact. You will share in an “On Call” rotation to support the platform 24X7. We are pushing the boundaries of what’s possible with file, NoSQL, Full text search and data warehouse storage. This position will push your skills to the next level.

This position is an individual contributor reporting to the Sr. Director, Database and Storage Solutions.

Responsibility
Protect the data under our control by making sure we meet all data storage best practices and policy requirements, which includes patching and other industry best practices
Ensure all storage systems meet (RPO) policies
Ensure all storage systems meet (RTO) policies
Operate and manage multiple 7x24 enterprise storage platforms
Maintain storage platform performance by identifying and resolving production and application development problems; calculating optimum performance measures; evaluating, integrating, and installing new releases, completing maintenance
Provide support by coding utilities, responding to user questions, and resolving problems
Troubleshoot storage service issues as they occur, including after-hours and weekends as part of an “On Call rotation”
Ensure ISO, Fed RAMP and PCI information security compliance
Plan and execute expansion to support rapid transaction growth
Work with engineering to design and optimize storage solutions
Work with the operations team to manage, install and configure server’s in multiple data centers around the world
Create and analyze operational reporting
Document the company’s storage environment
Update job knowledge by participating in educational opportunities, reading professional publications, maintaining personal networks, participating in professional organizations


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)

Positions at DocuSign are assigned a job designation of either In Office, Hybrid or Remote and are specific to the role/job. Preferred job designations are not guaranteed when changing positions within DocuSign. DocuSign reserves the right to change a position's job designation depending on business needs and as permitted by local law.


What you bring

Basic
5+ years experience as a server administrator and production support administrator
Understanding of server setup and configuration for optimal resource usage
Experience storage server monitoring, performance tuning, troubleshooting, and capacity planning
Experience with storage design, maintenance, security, management and analysis
Preferred
Experience with Cloud based storage tools and platforms
High level knowledge of Windows and Windows security
Excellent analytical and problem-solving skills
Excellent written and verbal communication skills
Self-motivated, able to work independently as well as in a collaborative environment
Experience with PowerShell scripting
Strong Kusto querying skills


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $118,300.00 - $182,775.00 base salary

Illinois and Colorado: $111,600.00 - $153,525.00 base salary

Washington and New York (including NYC metro area): $111,600.00 - $162,625.00 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.",3.7,"DocuSign
3.7","Seattle, WA",5001 to 10000 Employees,2003,Company - Public,Information Technology Support Services,Information Technology,$1 to $5 billion (USD)
984,Business Data Engineer,$76K - $102K (Glassdoor est.),"IAT Insurance Group has an immediate opening for a Business Data Engineer to map P&C data from source systems to target data warehouse and data marts. This role focuses on documenting source-to-target data mapping, data transformation, and derivation for ETL development. Internally we refer to this role as a Data Integration Analyst.
The ideal candidate will have at least two years of insurance or financial data experience
defining business needs to identify source data and mapping that to target data warehouses.
Internally we refer to this role as a Data Integration Analyst.

Possess a working understanding of business intelligence and data warehousing technologies.
Must be authorized to work in the United States and
must not require, now or in the future, VISA sponsorship for employment purposes.

This role works a hybrid schedule out of an IAT office in Naperville, IL, Rolling Meadows, IL, or Raleigh, NC, to support IAT’s ability to make data-driven decisions. This role will be required to work from the office Monday through Wednesday with the option of working Thursday and Friday remotely. Occasional travel is required. This role is called a Data Integration Analyst within IAT.

Responsibilities:
Document source to target data mapping, data transformation and derivation for ETL development. Develop extract SQL and lookups that will be utilized by the ETL process. Document how data should appear in the target for various source data scenarios such as data updates and deletes.
Analyze data-related business and technical needs, elicit and document requirements related to data integration from source systems to target data warehouse, data marts, data analytic tools, and information delivery solutions.
Develop hands-on experience with our systems to understand policy, claims and billing life cycles and information collected to operate our business.
Gather data needs from business consumers: BI/Analytics, Finance, Operations, etc.; and assess the business “reasonableness” of the requests, as well as getting to the best requirement for the business.
Identify and document data source for these data needs by digging into XML manuscripts, JSON data structures, relational table structures, talking with upstream BAs/architects.
Create functional specs documenting the data requirements and data sources for these requirements.
Perform other duties as assigned
Qualifications:
Must Have:
Bachelors’ degree and at least two years of insurance or financial data experience.
Equivalent is defined as six years of relevant experience.
Relevant experience as a business analyst for technical projects
Experience and proficiency in creating SQL queries out of complex data models.
Understanding of business intelligence/data warehousing technologies
Excellent oral and written communication and presentation skills, capable of presenting complex issues in a clear, concise and persuasive manner to the business and technical professionals, including C-level executives
Strategic mindset with an ability to plan and execute strategies
Demonstrated initiative, self-starter, with strong analytical and problem-solving skills
Strong work ethic and commitment to meeting deadlines
To qualify, all applicants must be authorized to work in the United States and must not require, now or in the future, VISA sponsorship for employment purposes.
Preferred to Have:
Advanced knowledge of TSQL, database schema design, reports, SQL performance tuning.
Strong knowledge of related technologies (Data Analytics, Business Intelligence, Warehousing, Information Governance, etc.).
Ability to troubleshoot difficult and undocumented issues with creative and well thought-out solutions, after gaining manager’s approval.
Strong knowledge of insurance industry data models/modeling relevant to the property & casualty industry and ability to apply models to a newly developed data warehouse and related tools and processes
Our Culture
IAT is the largest private, family- owned property and casualty insurer in the U.S. Insurance Answers Together is how we define IAT, in letter and in spirit. We work together to provide solutions for people and businesses. We collaborate internally and with our partners to provide the best possible insurance and surety options for our customers. It’s about far more than being a workplace.

We’re a work family.

At IAT, our fundamental belief is to a create a culture where all employees are heard and have a sense of belonging. Inclusion and Diversity are not just initiatives, it’s how we live. We’re committed to driving and building an open and supportive culture. By embracing the uniqueness of our employees, investing in their development, listening to, and engaging their ideas, we are stronger as an organization.
Our employees propel IAT forward – driving innovation, stable partnerships and growth. That’s why we continue to build an engaging workplace culture to attract and retain the best talent. We offer comprehensive benefits like:
26 PTO Days (Entry Level) + 12 Company Holidays = 38 Paid Days Off
Healthcare and Wellness Programs
Opportunity to earn performance-based bonuses
7% 401(k) Company Match and additional Discretionary Retirement Contribution
College Loan Assistance Support Plan
Educational Assistance Program
Mentorship Program
Dress for Your Day Policy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. We maintain a drug-free workplace and participate in E-Verify.

Come join the IAT family!

IAT Recruiter:
Steve Morley | Senior Recruiter
steve.morley@iatinsurance.com
www.linkedin.com/in/stevemorley916",4.1,"IAT Insurance Group, Inc.
4.1","Naperville, IL",501 to 1000 Employees,1991,Company - Private,Insurance Carriers,Insurance,$1 to $5 billion (USD)
985,Data Engineer,Employer Provided Salary:$77K - $113K,"Eaton’s Electrical division is currently seeking a Business Analyst to work out of our Raleigh, NC location.

Relocation within the United States will be offered.

The expected annual salary range for this role is $77,249 - $113,299 a year.
Please note the salary information shown above is a general guideline only. Salaries are based upon candidate skills, experience, and qualifications, as well as market and business considerations.
What you’ll do:

The Data Engineer will collaborate with cross-functional teams to understand data requirements, design and implement efficient data pipelines, and ensure the availability, reliability, and scalability of our data solutions. This position necessitates proficiency in data manipulation, data analysis, SQL (writing queries), data modeling, and visualization skills.

Job responsibilities
Gather data from various sources. Design and write SQL queries to retrieve and analyze data stored in various databases.
Develop ETL processes to clean, transform, and enrich data in PowerQuery, ensuring data quality, consistency, and accuracy for use in the PowerBI's Data model.
Design and develop data model, transform raw data into meaningful insights, create relationships, hierarchies, and calculated measures in PowerBI’s Data Model
Document data integration and transformation processes for future reference and troubleshooting.
Assists in the implementation of data governance processes and systems.
Qualifications:
Basic Qualifications:
Bachelor's degree from an accredited educational institution
Minimum three (3) years of SQL and Power BI experience
Must be legally authorized to work in the United States without company sponsorship now or in the future

Preferred Qualifications:
Bachelor’s degree in computer science or software engineering
PowerBI (PowerQuery, Modelling, DAX, creating interactive visuals), Relational databases, Data Governance, Visualization
Problem-solving, Communication, Attention to detail
#LI-JM3
We are committed to ensuring equal employment opportunities for all job applicants and employees. Employment decisions are based upon job-related reasons regardless of an applicant's race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, marital status, genetic information, protected veteran status, or any other status protected by law.
Eaton considers qualified applicants regardless of criminal histories, consistent with local laws. To request a disability-related reasonable accommodation to assist you in your job search, application or interview process, please call us at 1-800-836-6345 to discuss your specific need. Only accommodation requests will be accepted by this phone number.
We know that good benefit programs are important to employees and their families. Eaton provides various Health and Welfare benefits as well as Retirement benefits, and several programs that provide for paid and unpaid time away from work. Click here for more detail: Eaton Benefits Overview. Please note that specific programs and options available to an employee may depend on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.",3.9,"Eaton
3.9","Raleigh, NC",10000+ Employees,1911,Company - Public,Electronics Manufacturing,Manufacturing,$10+ billion (USD)
986,Senior Data Engineer,Employer Provided Salary:$130K - $160K,"About GoodLeap:
We provide friendly financing options for those who dream of living a more sustainable lifestyle and want to save money using modern technology. Our collective mission is to make a positive impact on the planet, build lasting relationships with our valued partners and customers, and deliver a tech-enabled financing experience that is simple, fast, and frictionless.

We are creating a financial ecosystem that connects billions of dollars of capital to millions of homeowners that want to convert their outdated houses into modern, smart, energy-efficient homes. By unlocking access to numerous products that help people achieve better sustainability, we are revolutionizing the home improvement industry and protecting our only planet.

As part of our world-positive initiative, we are also the official sponsor of GivePower – a foundation that uses solar-based solutions to power life’s basic needs for people in developing regions of the world.
If you have an unstoppable desire to make a meaningful impact on our planet, and help mission-driven businesses and consumers achieve a more sustainable future, join us.

Learn more about our perks and culture!
Competitive pay
Comprehensive benefits package

Position Summary
The Senior Data Engineer is a hands-on role with a strong background in Data Analysis, Business Intelligence & development. This role will work closely with cross-functional teams to effectively coordinate the complex interdependencies inherent in the applications.

Essential Job Duties & Responsibilities
Implement data integrations across the organization as well as with business applications
Develop and maintain data-oriented web applications with scalable web services
Participate in the design and development of projects, either independently or on a team
Utilize agile software development lifecycle and DevOps principles
Be the data stewards of the organization upholding quality and availability standards for our downstream consumers
Be self-sufficient and fully own the responsibility of executing projects from inception to delivery
Provide mentorship to team members including pair programming and skills development
Participate in data design and architecture discussions, considering solutions in the context of the larger GoodLeap ecosystem
Required Skills, Knowledge & Abilities
4-7 years of full-time Data Analysis and Software Development experience
Experience with an end to end reporting & analytics technology: data warehousing (SQL,
NoSQL) to BI/Visualization (Tableau, PowerBI, Excel)
Degree in Computer Science or related discipline
Experience with DataBricks
Expertise with relational databases (including functional SQL/stored procedures) and non-relational databases (MongoDB, DynamoDB, Elastic Search)
Experience with orchestrating data pipelines with modern tools such as Airflow
Solid understanding of performance implications and scalability of code
Experience with Amazon Web Services (IAM, Cognito, EC2, S3, RDS, Cloud Formation)
Experience with messaging paradigms and serverless technologies (Lambda, SQS, SNS, SES)
Experiences working with server-less applications on public clouds (e.g. AWS)
Experience with large, complex codebases and know how to maintain them
In addition to the above salary, this role may be eligible for a bonus and equity.
Additional Information Regarding Job Duties and Job Descriptions:

Job duties include additional responsibilities as assigned by one's supervisor or other managers related to the position/department. This job description is meant to describe the general nature and level of work being performed; it is not intended to be construed as an exhaustive list of all responsibilities, duties and other skills required for the position. The Company reserves the right at any time with or without notice to alter or change job responsibilities, reassign or transfer job position or assign additional job responsibilities, subject to applicable law. The Company shall provide reasonable accommodations of known disabilities to enable a qualified applicant or employee to apply for employment, perform the essential functions of the job, or enjoy the benefits and privileges of employment as required by the law.

If you are an extraordinary professional who thrives in a collaborative work culture and values a rewarding career, then we want to work with you! Apply today!",3.7,"GoodLeap
3.7",Remote,1001 to 5000 Employees,2003,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
987,Data Engineer,Employer Provided Salary:$100K - $105K,"Job Description
▪ 4+ years of experience in Python with Data background
▪ BA/BS Computer Science or a technical/quantitative degree
▪ Excellent communication and collaboration skills
▪ Interest in learning and adopting new tools and techniques
▪ Humble enough to learn from others, confident enough to teach others new things
Full time position, Salary (+ Benefits ( Healthcare Insurance + PTO + Bonus + 401k) )
Job Type: Full-time
Salary: $100,000.00 - $105,000.00 per year
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",-1,Konnectingtree.Inc,Remote,-1,-1,-1,-1,-1,-1
988,Data Engineer,Employer Provided Salary:$105K - $121K,"It's fun to work in a company where people truly believe in what they are doing. At Dutch Bros Coffee, we are more than just a coffee company. We are a fun-loving, mind-blowing company that makes a difference one cup at a time.
Being part of the Dutch Family
You are adaptable, a servant leader, and community-minded. You view yourself as an unfinished product on the constant pursuit of personal and professional development. We rely on our people to uphold our core values of speed, quality, and service to protect our culture and ensure our growth remains limitless!

Dutch Bros mission statement
We are a fun-loving, mind-blowing company that makes a massive difference one cup at a time.

Who we are
Dutch Bros puts people first in everything we do. Joining our team gives you the opportunity to build a compelling future while making a massive difference in the lives of our customers and communities.
We love people and we love OUR people! Here’s what we offer
Here at Dutch Bros, we want our employees to feel valued, and we recognize there's more to value than a salary. The following benefits and perks were hand-picked to cater to our diverse employee base:
Medical/Dental/Vision/Short Term Disability/Life insurances
Paid Sick Days
401(k) plan with employer match after one year of employment
Education Benefit Program
Vacation/Floating Holidays/Paid Time Off
Paid Parental Leave
Flexible Schedule
Paid Volunteer Days
Various employee discounts
Office perks, such as hi-lo desks, snacks provided daily, casual dress code, and an in-house coffee bar with a dedicated Broista
Position Overview
The Data Engineer is a lifelong learner with deep knowledge of data warehouse and ETL solutions. This role engages in BI activities which include the design, development, and implementation of data assets, data governance policies, and data management processes. This role works with other teams to understand and collect requirements for designing data assets (data warehouses, pipelines,etc.) and deliver reliable and sustainable data products for internal use.
Key Result Areas (KRAs)
Design, develop, and improve ETL and data warehouse tools at Dutch Bros to deliver reliable, high quality and sustainable data solutions:
Design and develop new ETL solutions
Improve the performance and effectiveness of current ETL processes
Design and implement new data warehouses
Monitor and improve the performance of the current data warehouses
Perform ongoing preventive maintenance on data pipelines and related applications
Develop and improve the data asset documentation:
Build data catalog for the legacy and new data assets
Develop data architecture diagram
Develop data dictionaries for the legacy and new data assets
Categorize and tag the data to democratize the data assets to a wider group
Develop ETL and data warehouse description documentation
Develop and support the data governance efforts:
Develop data policies to manage the access and availability of data assets
Develop data policies to support privacy and security compliance efforts
Monitor the permissions, access and availability of data for different internal and external users
Apply the best practices to improve the data security for the data in motion or at rest
Other duties as assigned
Job Qualifications
Required Qualifications:
Minimum of 3 years of experience in a data engineering role, required
2 additional years of experience developing data warehouses on Snowflake platform, required
Bachelor's degree in Computer Science, Software or Computer Engineering, Applied Math, Physics, Statistics, or a related field, preferred
Experience with data warehousing concepts, SQL, and SQL Analytical functions, required
Experience in using the Azure platform to implement data solutions (ADF, SQL DBs, Purview, Storage Units, etc.), required
Data visualization and dashboarding experience (Power BI, Tableau, Looker, etc.)
Experience in data modeling (dimensional, normalized, key-value pair)
DevOps experience (Azure DevOps or Gitlab) delivering continuous improvements
Experience in management and maintenance of data pipelines in an enterprise setting
Problem-solving orientation with the ability to leverage both quantitative and qualitative analyses to drive decision-making
Preferred Qualifications:
Background and experience working in food and beverage industry
Working knowledge of data programming languages/solutions (Python, Java or R)
Working knowledge of big data and real-time pipelines (such as Spark, Kafka, Airflow, Hive, Elastic Search, etc.)
Experience in working with data catalog/quality/governance solutions (including Informatica, Collibra, Alation)
Familiar with real-time pipeline design and management principles and concepts
Experience building RESTful APIs to enable data consumption
Experience with Action Analytics (Microsoft D365 Analytics solution)
Familiarity with Azure Logic Apps
Preferred Certifications:
Azure platform (Developer/Architect/Data Engineer)
Snowflake platform (SnowPro Core/Advanced)
Competencies
Adaptable
Collaborative
Communication
Effective Prioritization
Functional and Tech. Expertise
Initiative
Physical Requirements
Occasional lifting up to ten pounds
Must be able to work in a climate-controlled office environment
Vision must be good, or corrected to normal, to perform normal job duties
Hearing must be good, or corrected to normal, to have the ability to understand information to perform job duties
Ability to read and write in English in order to process paperwork and follow up on any actions necessary
Sitting for extended periods of time
Manual dexterity needed for keyboarding and other repetitive tasks
This position is eligible for remote work within any state Dutch Bros currently resides in (AL, AZ, CA, CO, ID, KS, KY, MO, NM, NV, OK, OR, TN, TX, UT, and WA)
Compensation:
$104,788.59 - $121,478.69
If you like wild growth and working in a unique and fun environment, surrounded by positive community, you'll enjoy your career with us!",4.1,"Dutch Bros
4.1",Oregon,10000+ Employees,1992,Company - Public,Restaurants & Cafes,Restaurants & Food Service,$500 million to $1 billion (USD)
989,Data Engineer,Employer Provided Salary:$80K - $120K,"As the Data Engineer at Digible, you will be responsible for designing, developing, and maintaining our data pipeline, ensuring that data is properly collected, processed, and analyzed to inform business decisions. You will work closely with cross-functional teams in order to understand their data needs and ensure that our data infrastructure supports those needs. You will also be responsible for ensuring the integrity and security of our data.
About Us:
Privately owned and operated, Digible was founded in 2017 with a mission to bring sophisticated digital marketing solutions to the multifamily industry. We offer a comprehensive suite of digital services as well as a predictive analytics platform, Fiona, that is the first of its kind.
At Digible, Inc. we love to celebrate our diverse group of hardworking employees – and it shows. We pride ourselves on our collaborative, transparent, and authentic culture. These values are pervasive throughout every step of a Digible employee's journey. Starting with our interviews and continuing through our weekly All Hands Transparency Round-up, values are at the heart of working at Digible.
We value diversity and believe forming teams in which everyone can be their authentic self is key to our success. We encourage people from underrepresented backgrounds and different industries to apply. Come join us and find out what the best work of your career could look like here at Digible.
What you'll do:
Design and implement scalable data pipelines, including data ingestion, transformation, and integration.
Collaborate with stakeholders to understand data requirements and develop data models aligned with Digible's business goals.
Build and maintain ETL/ELT workflows using dbt (Data Build Tool) and Prefect to transform raw data into structured datasets for analysis, reporting, and sustainability tracking purposes.
Implement data quality controls and validation processes to ensure the accuracy and integrity of Digible's data.
Monitor data pipelines, perform debugging and troubleshooting, and implement proactive measures to ensure data availability, accuracy, and efficient resource utilization.
Optimize and tune data pipelines for performance, scalability, and reliability.
Stay up to date on the latest advancements in data engineering technologies, tools, and best practices, and propose their adoption when they align with Digible's objectives
You should have:
Proven experience as a Data Engineer or similar role, with a strong understanding of data engineering concepts, data modeling, and database systems.
Cloud experience (i.e. AWS, GCP, Azure, etc.)
Proficiency with Git and source control
Proficiency with data transformation and modeling tools (i.e. dbt, dataform, PySpark, etc.)
Strong programming skills in Python and familiarity with related libraries and best practices.
Experience with SQL and working knowledge of relational databases and/or columnar databases.
Familiarity with data orchestration techniques and tools (i.e. Prefect, Dagster, Airflow, AWS Glue, etc.)
Data Warehouse Experience (i.e. BigQuery, Snowflake, Redshift, etc..)
Knowledge of data warehousing concepts and dimensional modeling.
Strong problem-solving and analytical skills, with the ability to work in a fast-paced and collaborative environment.
Familiarity with Docker
Experience with marketing and advertising API's & data (i.e. Google Suite, Facebook, etc.) is a big plus
Application experience of CI/CD implementations is a plus
Passion to make an impact and do some awesome work.
Digible's Core Values
Authenticity: The commitment to be steadfast and genuine with our actions and communication toward everyone we touch.
Curiosity: The belief that a deep and fundamental curiosity (the ""why"") in our work is vital to company innovation and evolution.
Focus: The collective will to remain completely devoted and ultimately accountable to our deliverables.
Humility: The recognition and daily practice that ""we"" is always greater than ""I"".
Happiness: The decision to prioritize passion and love for what we do above everything else.
Pay, perks, and such:
Annual Salary of $80,000 - $120,000 depending on level, breadth and years of experience
4 Day work week (32 hour per week)
WFA (Work From Anywhere)
Profit Sharing Bonus
We offer 3 weeks of PTO as well as Sick leave, and Bereavement.
We offer 11 paid holidays (New Years Eve, New Years Day, MLK day, Memorial Day, Independence Day, Labor Day, Day before Thanksgiving, Thanksgiving, Day after Thanksgiving, Christmas Eve, and Christmas)
401(k) + 5% employer match
50% employer paid health benefits, including Medical, Dental, and Vision.
We provide $75/ month reimbursement for Physical Wellness
We provide $75/ month reimbursement for Mental Wellness
$1000/year travel fund for employees who have been with Digible 3+ years
Monthly subscription for financial wellness
Dog-Friendly Office
Paid Parental Leave
Company Sponsored Social Events
Company Provided weekly lunches and snacks for in office employees
Employee Development Program",4.7,"Digible
4.7",Remote,51 to 200 Employees,2017,Company - Private,Advertising & Public Relations,Media & Communication,$5 to $25 million (USD)
990,Data Engineer (Remote),-1,"At Bluesight, our mission is to create groundbreaking medication intelligence solutions that increase efficiency, safety and visibility for health systems and pharmaceutical manufacturers. We empower our customers to deliver the right medicine to the right patient at the right time, every time. We are a high growth healthcare information technology company with a start-up 'vibe' but over 2,000 customers tracking medications using our proven solutions.

The Data Engineer has experience working with both internal, and external, consumers of data products, with a skill for building data architectures and platforms that are robust, extensible, and scalable. Prior experience designing and leading an enterprise-wide data architecture is desirable. This role should use a collaborative and consensus-building approach to the work. This role will cover the spectrum of our data product offerings, from data governance and security to extract, transformation and testing and will work closely with product owners, engineering teams.
You can expect to:
Define, design, implement and maintain data architectures and infrastructure that unify our core products and 3rd party tools
Develop and champion good data stewardship and governance practices across the company and work closely with Security to ensure customer assurance
Design and support customer facing data products both with API and interactive components.
You will be responsible for extending a set of tools used for data pipeline development and metadata management.
About You:
A minimum of a BS degree in computer science, software engineering, or related scientific discipline
At least 2 years of experience in a data engineering role gained over several companies or coupled with prior experience as a backend engineer
Ability to communicate clearly with all audiences about data architecture and implementations
Strong written and analytical skills
Experience working on modern ETL tools like Stitch and FiveTran, and DBT
Strong bias for action, documentation and education
Ability to work through ambiguity with open communication and autonomy
Expert-level mastery of SQL (MySql, Redshift, Athena, Postgres) and db denormalization concepts
Proficiency in Spark or other distributed processing frameworks.
Exceptional attention to detail
Great analytical, critical thinking, and problem-solving abilities
Proficiency implementing monitoring instrumentation, triaging incidents, resolving customer issues
Experience with a data pipeline orchestration tools like Dagster, Luigi or Airflow
Working experience in AWS cloud computing environment, preferably with Lambda, S3, MWAA, Athena, SQS, DMS, Glue
Programming experience with Python in a production setting
Nice to Have:
Experience working with PHI or other regulated data
Knowledge of hospital data systems (e.g. EHR, ADC, purchasing reports, etc.)
Knowledge of the pharmaceutical supply chain
What you'll get from us:
Opportunity to build a data team culture within the organization, and to shape a data team that has built an industry-leading product
A strong voice in what we work on, how it works, and how we build it
Trust in your sense of ownership, estimations and design decisions
A budget for training and career development
Coworkers you'll learn from and who are looking to learn from you
Opportunities to solve problems of scale, debt and security to redefine what’s possible in medication intelligence
This position is a remote position and open to applicants in the continental United States.

Why Bluesight?
Bluesight’s culture is built on innovation and teamwork. There’s room to grow and opportunities to take initiative. You will partner with sharp, motivated teammates looking to disrupt a massive industry—and have fun doing it. We truly believe that where you work and what you do matters. Join us as we revolutionize the hospital pharmacy landscape!
Competitive salary
Time off when you need it – unlimited vacation days!
Generous insurance coverage
401k program with a company match
Fun, collaborative culture!

EOE AA M/F/VET/Disability

All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, religion, color, national origin, sex, protected veteran status, disability, or any other basis protected by federal, state or local laws.",4.1,"Kitcheck
4.1",Remote,51 to 200 Employees,2013,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
991,"Associate Engineer, Data Science",-1,"Company Description

We're Nagarro.

We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (18,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!

Job Description

You have begun to prove your worth in our industry! How has the experience been so far? We hope it has been rewarding. At Nagarro, we want to take the word “hope” out of that sentence. At Nagarro, if you are ready to take the next step, we are ready to take you to the next level. Want to work with the coolest tech, the brightest minds, and be able to solve the most challenging problems, then look no further.

Additional Information

Click here to access the application privacy notice",4.1,"Nagarro
4.1",Remote,10000+ Employees,1996,Company - Public,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
992,Data Engineer,-1,"At Brooks, new ideas, new technologies and new ways of thinking are driving our future. Our customer focused culture encourages employees to embrace innovation and challenge the status quo with novel thinking and collaborative work relationships.

All we accomplish is grounded in our core values of World Class, Empowered, Accountable, Respectful, Engaged.
Data Engineer
Job Description
What you’ll do:
Proven analytical and problem solving skills.
Proficient with SQL as a tool to solve problems, analyze data, create ad hoc reports and test.
Nice to have knowledge of Oracle ERP Order management
Should have hands on experience on ETL tools like SSIS/Informatica Cloud
Strong oral and written communication skills.
Proficient with Power BI/Tableau BI tools
Ability to work in a team environment.
Proficient in prioritizing, multi-tasking and managing multiple projects.
Proven independent worker with strong organizational and time management skills.
Flexible and can adapt to change.
What you’ll bring:
5 to 10 Years’ experience as Data Engineer
Bachelor of Science in Computer Engineering or Information Systems
#LI-remote
#LI-CF1
What we Offer:
Competitive compensation and annual bonus incentive
Company-subsidized medical, dental and vision
Income protection – fully-paid life and short-term disability insurance
Flexible working options including hybrid work schedules, unlimited PTO, and company-wide shut down between Christmas and New Year’s Day
401(k) matching
Employee Assistance Program (EAP)
Employee Wellbeing Programs (Headspace & Better Up)
Employee Referral Program
Adoption Assistance
Tuition Assistance/Student Loan Program
Group Legal Assistance
Brooks Automation US, LLC is proud to be an Equal Opportunity Employer. We celebrate diversity and are committed to creating a welcoming and inclusive environment for all. If this role sounds energizing for you, we encourage you to apply regardless of your race, color, ethnicity, national origin, ancestry, religion, gender identity, sex, sexual orientation, citizenship, age, marital status, parental or caregiver status, physical or mental disability, medical condition, military status, or any other perceived limiting factor.
Salary Ranges
If any applicant is unable to complete an application or respond to a job opening because of a disability, please email at
HR.Recruiting@brooks.com
for assistance.
Brooks Automation is an Equal Opportunity Employer. This company considers candidates regardless of race, color, age, religion, gender, sexual orientation, gender identity, national origin, disability or veteran status.",3.9,"Brooks Automation
3.9",Remote,1001 to 5000 Employees,1978,Company - Private,Machinery Manufacturing,Manufacturing,$500 million to $1 billion (USD)
993,Junior Data Engineer,$73K - $107K (Glassdoor est.),"Job Title - Junior Data Engineer
Location - Dallas TX Hybrid ( No Relocation )
Tax Term (W2, C2C) - W2 / C2C

Position Type(Contract/permanent) - Contract
Project Duration - Long term 12+ months



Job description -
The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.
Scripting – Python, KSH, Powershell
Knowledge about Linux, and Windows Server Operating System
AWS basic knowledge",4.7,"HYR Global Source Inc
4.7","Dallas, TX",51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
994,Data Engineer,-1,"Position Purpose
Within the Reporting and Analytics team, the Data Engineer (DE) will liaise with the Cloud DBA and the Infrastructure team. The DE shall work with constructs available within database platforms (managed by Infrastructure) to support analytics and reporting-specific needs. The DE shall implement methods to improve data reliability and quality, combining raw information from various sources to create consistent and machine-readable formats. The DE shall also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling.
At MiHIN, we recognize that our diversity is our greatest strength. We draw on the differences in who we are, what we've experienced, and how we think to serve our stakeholders and communities best. Because our family of companies serves everyone, we believe in including everyone. This means we strive to hire the most talented and qualified employees that are diverse in thinking and by race, gender, gender identity, gender expression, age, religion, sexual orientation, physical abilities, and socio-economic upbringing.
Duties and responsibilities
Work as a team member in the Reporting and Analytics department, collaborating with Data and Reporting Analysts to:
Ensure that Information MiHIN produces, maintains, stores, or transmits remains secure through adherence to MiHIN's Security Policies and Procedures.
Evaluate business needs for reporting and objectives.
Integrate large volumes of data from internal and external sources to enhance insights in support and in sync with business goals.
Working with large volumes of data, ensure ETL solutions architecture will support the requirements of the Reporting and Analytics team and the needs of the business in general.
Develop, test, deploy, and establish monitoring for reporting data repositories
Develop, test, deploy, and establish monitoring for data pipeline processing components to move data from origin sources to reporting repositories.
Identify opportunities for external data acquisition, for value add to analysis insights.
Assists in post-implementation support, resolving issues, and raising defects appropriately.
Drive continuous improvement in data quality and access performance/efficiency.
Employ a variety of languages and tools (e.g., scripting languages) for data management and integration across sources.
Recommend methodologies and technologies to improve data reliability, efficiency, and quality.
Liaison with the infrastructure team for systems/platform level planning and administration.
Create and maintain documentation of knowledge assets and processes (mapping, data flows, repositories, etc.).
Act as a backup for Data and Reporting Analysts; Development of reporting and dashboard visualizations, in collaboration with team members and/or individually, as an overflow resource as that workload fluctuates.
Qualifications
Minimum Requirements include:
Bachelor's degree or advanced certification in Statistics, Data Science, Computer Science, or a related field
6-10 years of experience in data engineering and data analytics
Demonstrable experience with scripting languages such as Python, SQL, and Procedural SQL
Advanced data pipeline development skills:
Strong knowledge of relational databases and SQL. data into a relational database(s). Ability to write complex SQL queries.
Advanced data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets, reformat data between wide and long, etc.
Technical expertise in data models, data mining, and segmentation techniques.
Demonstrated ability to learn, apply, and troubleshoot new techniques in solving data management challenges.
Demonstrated ability to work effectively in teams in both lead and support roles.
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision.
Curiosity to dive below the surface and identify critical strategic implications in the data.
Ability to effectively collaborate with all levels of the organization.
Preferred qualifications include:
Master's degree in statistics, data science, computer science, or related field.
Data engineering certificate (e.g., AWS Certified Big Data – Specialty certification) is a plus
Deep understanding of healthcare messaging and data standards
Experience working with Amazon Web Services cloud infrastructure services: Athena, S3, Glue, Quicksight, Aurora – PostgreSQL, etc.
Tableau Prep, Developer, Server experience.
Direct reports
May supervise an intern",2.8,"Michigan Health Information Network
2.8",Remote,201 to 500 Employees,2010,Nonprofit Organization,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
995,AI/ML Data Automation Engineer,-1,"At PayPal (NASDAQ: PYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives.
Job Description Summary: We are looking for a talented Python Engineer to join our team. The ideal candidate will have a strong understanding of Big Data, Python Scripting, Machine Learning and Artificial Intelligence concepts, and experience applying these concepts on a large-scale data set. They will have the ability to solve complex problems with simple, efficient solutions, and the desire to continually learn and evolve their skills. They will collaborate closely with cross-functional teams to develop and deploy big data and AI powered solutions that optimize employee experiences and drive business outcomes. As part of our team, you'll work with cutting-edge AI/ML technologies and be at the forefront of the latest advancements in the field.
Job Description:
Your way to impact:
As a Python engineer, you will be responsible for designing, developing, and deploying big data, machine learning models and AI applications that solve complex business problems.
In your day to day, you will:
Develop, implement, and maintain machine learning models, including data collection, model design, training, optimization, and validation.
Collaborate closely with product teams and other stakeholders to understand business needs and translate them into technical requirements.
Develop robust data pipelines to collect, clean, and process data from various sources.
Keep up to date with the latest technologies and research in the field of AI and ML to bring innovative solutions and improvements to our products.
Monitor the performance of AI/ML systems and make improvements as necessary.
Work closely with other members of the team to design and implement solutions, including collaborating with domain SMEs to understand the data and design models, working with software engineers to integrate models into products, and collaborating with product managers to define requirements and prioritize work.
Responsible for developing and maintaining data pipelines to support machine learning models, which includes data cleaning, feature engineering, and data storage.
Conduct experiments to evaluate model performance, including designing experiments to evaluate the performance of machine learning models, and then analyze and interpret the results to improve model performance.
Communicate technical concepts to non-technical stakeholders, you will need to be able to communicate complex technical concepts to non-technical stakeholders in a way that is easy to understand.
Provide technical leadership and mentorship in the field of Big data and AI/ML
What do you need to bring:
Bachelor’s degree or higher in Computer Science, Statistics, Mathematics, or a related field
5+ years of experience working on Big Data and AI/ML models in a production environment.
Experience with big data processing frameworks such as Hadoop, Spark, PySpark, or Kafka, and hands-on experience with Google Cloud Platform (GCP)
Proficiency in Python and experience with machine learning libraries such as TensorFlow, PyTorch, Scikit-Learn, or similar.
Understanding of various machine learning techniques (classification, regression, clustering, etc.)
Strong knowledge of statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and algorithms
Experience with data preparation and data engineering - Python engineers should have experience with data preparation and data engineering tasks such as data cleaning, feature engineering, and data transformation.
Our Benefits:

At PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset—you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.

We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com

Who We Are:

To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx

PayPal has remained at the forefront of the digital payment revolution for more than 20 years. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, the PayPal platform is empowering more than 400 million consumers and merchants in more than 200 markets to join and thrive in the global economy. For more information, visit paypal.com.
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.
As part of PayPal’s commitment to employees’ health and safety, we have established in-office Covid-19 protocols and requirements, based on expert guidance. Depending on location, this might include a Covid-19 vaccination requirement for any employee whose role requires them to work onsite. Employees may request reasonable accommodation based on a medical condition or religious belief that prevents them from being vaccinated.",3.9,"PayPal
3.9","Scottsdale, AZ",10000+ Employees,1998,Company - Public,Internet & Web Services,Information Technology,$10+ billion (USD)
996,Data Engineer,Employer Provided Salary:$120K - $150K,"At Dynatron Software, we help automotive service departments increase revenue and profitability with our suite of services. We strive to be a people-first company where employees enjoy coming to work, the people they work with, and are given the autonomy to succeed. Our company culture is built on a foundation of teamwork, accountability, integrity, clear communication, and positive attitudes.
We are currently looking to add new talent to our growing team!
About the Role:
We are seeking a dynamic and experienced US, remote-based Data Engineer to join our newly formed Data Platform Team. As a Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing code in Python, SQL, and occasionally PHP. The ideal candidate will have a strong technical background and a passion for data processing, with the ability to think creatively and strategically to solve complex challenges.
Key Responsibilities:
Write, test, and maintain robust, high-quality code in Python and PHP.
Work with data and analytics experts to strive for greater functionality in our data systems.
Collaborate with cross-functional teams to gather requirements and develop software solutions.
Design, construct, install, test and maintain highly scalable data management systems.
Provide debugging and troubleshooting support for existing systems.
Participate in code reviews to ensure software quality and conformance to standards.
Assist in database design and development using SQL.
Leverage AWS and other cloud technologies for efficient software deployment and scalability.
Occasionally work with PHP for server-side programming tasks.
Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications:
Bachelor's degree in Computer Science, Engineering, or related field.
Minimum 3+ years of experience in software development with a focus on Python and PHP.
Strong understanding of Object-Oriented Programming (OOP) concepts and design.
Knowledge of SQL and experience working with relational databases.
Familiarity with AWS and/or other cloud-based technologies.
Experience with Big Data technologies such as Hadoop, Spark, or similar technologies is a plus.
Excellent problem-solving skills with a strong attention to detail.
Ability to work both independently and as part of a team.
Excellent verbal and written communication skills.
Preferred Qualifications:
Experience contributing to an AI platform in any capacity
Experience with hybrid cloud workflows
If you are passionate about data analysis, have a knack for solving complex problems, and are eager to make a significant impact on a growing organization, we would love to hear from you!
In Return for Your Expertise, You Will Receive:
Excellent benefits including health, dental, and vision insurance, stock options, professional development opportunities, 9 paid holidays, and 15 days PTO.
A welcome “swag bag” with branded clothing as an official welcome to the team.
The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!
Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation Range: $120,000-150,000/yr
IMIVirMHCH",4.3,"Dynatron Software
4.3",Remote,51 to 200 Employees,1997,Company - Private,Enterprise Software & Network Solutions,Information Technology,$25 to $100 million (USD)
997,Data Engineer,Employer Provided Salary:$113K,"Department: Technology

Our Company Promise

We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.

Job Description:Job Summary:
The People of Southwest Airlines come together to deliver on our Purpose, which is to connect People to what’s important in their lives through friendly, reliable, and low-cost air travel. This Team looks for creative ways to solve Business problem s using cutting-edge tools and analytics. The Team is making its journey to Cloud and working on creating a cloud data warehouse that provides real-time data to the Data S cientist for predictive analysis. The data this Team provide s helps Businesses visualize and explain problematic patterns with the operation and improve operational performance. The information also enables planners to adjust the coverage of resources. Data Engineer s are analytical and critical thinker s , and they ’ll be part of meaningful and impactful initiatives that help drive the future of Southwest.

Additional Details:
This role is offered as a remote workplace position, which may require travel for training, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is a limited group of states or localities ineligible for Employees to regularly perform their work.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.

Southwest Airlines is an Equal Opportunity Employer. We continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences .

Responsibilities

Assemble large, complex sets of data that meet non-functional and functional business requirements

Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes

Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies

Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition

Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues

Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues

Generate or adapt equipment and technology to serve user needs

May perform other job duties as directed by Employee's Leaders

Knowledge, Skills and Abilities

Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services

Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models

Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems

Ability to understand new information for both current and future problem-solving and decision-making

Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions

Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem

Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)

Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)

Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)

Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material

Education

Required: High School Diploma or GED

Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training

Experience

Required: Intermediate level experience, fully functioning broad knowledge in:

Cloud infrastructure, DataLake

ETL experience ensuring source to target data integrity

Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque).

ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience.

Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies

Agile development experience and Agile ceremonies and practices

Preferred: 5+ Years of Development Experience

Preferred: 3+ Years of experience in Amazon Web Services Cloud technology

Preferred: 3+ years' experience coding and scripting with Python or Spark

Preferred: 2+ years' experience in AWS GLUE, Data brew, experience extracting data from multiple sources

Preferred: 2+ years of experience working with Agile development practices

Preferred: Working knowledge of AWS services such as S3, Lambda, developing and maintaining data pipelines using AWS Cloud formation

Preferred: Working knowledge of Kinesis, Kafka, or other data streaming platform

Preferred: Experience building solutions for migrating on-prem data warehouses and data lakes to AWS Redshift Cloud Data warehouse

Preferred: Experience building data ingestion pipelines on AWS

Preferred: Experience deploying appropriate CI/CD tools using Git

Preferred: Experience in developing re-usable generic applications for high-volume DWH environments

Preferred: DevOps experience setting up CI/CD pipeline

Licensing/Certification

N/A

Physical Abilities

Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time

Ability to communicate and interact with others in the English language to meet the demands of the job

Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job

Other Qualifications

Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines

Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986

Must be at least 18 years of age

Must be able to comply with Company attendance standards as described in established guidelines

Pay & Benefits:
Competitive market salary from $112,950 per year to $125,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.

Benefits you’ll love:
Fly for free, as a privilege, on any open seat on all Southwest flights (your eligible dependents too)
Up to a 9.3% 401(k) Company match, dollar for dollar, of your eligible pay, per
paycheck *
Potential for annual ProfitSharing contribution toward retirement - when
Southwest profits, you profit**
Explore more Benefits you’ll love: https://careers.southwestair.com/benefits
Pay amount does not guarantee employment for any particular period of time.
**401(k) match contributions are subject to the plan’s vesting schedule and applicable IRS limits
***ProfitSharing contributions are subject to plan’s vesting schedule and are made at the discretion of the Company.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.

Southwest Airlines is an Equal Opportunity Employer.
Please print/save this job description because it won't be available after you apply.",4.1,"Southwest Airlines
4.1","Dallas, TX",10000+ Employees,1967,Company - Public,"Airlines, Airports & Air Transportation",Transportation & Logistics,$10+ billion (USD)
998,Sr. Azure Data Engineer,Employer Provided Salary:$60.00 - $70.00 Per Hour,"Role: Sr. Azure Data Engineer
Job Location: Remote
Type: Long Term Contract
Job Summary:
10+ years of experience
Required Skills:
Azure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven
High-level tasks:
Analyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.
Develop the Ingestion pipeline using Azure Data Factory.
Develop data transformation code using pySpark in Databricks notebooks.
Orchestrate the Ingestion and transformation pipeline using Azure data factory.
Check in/Check out the ADF/notebooks/python/any code using Github.
Deploy and test Azure Data Factory and Databricks code.
Prod deployment support.
Document the prod hand oversteps.
Job Types: Full-time, Contract
Salary: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 10 years (Required)
Scala: 2 years (Required)
Azure Databricks: 4 years (Required)
Total IT: 10 years (Required)
Azure Data Factory: 4 years (Required)
Work Location: Remote",3.3,"Iron Service Global Inc
3.3",Remote,201 to 500 Employees,1987,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
999,Data Engineer,Employer Provided Salary:$112K - $127K,"Description

OPEN DATE: 8/11/2023
CLOSING DATE: 9/8/2023
SALARY RANGE: $112,015 - 126,949
POSITION TYPE: Trust Fund
APPOINTMENT TYPE: Temporary
SCHEDULE: Full Time
DUTY LOCATION: Washington DC
Non-Critical Sensitive/Moderate Risk
Open to all qualified applicants

What are Trust Fund Positions?
Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care).
Conditions of Employment
Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk.
Complete a Probationary Period
Maintain a Bank Account for Direct Deposit/Electronic Transfer.
The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.

OVERVIEW

Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.

DUTIES AND RESPONSIBILITIES

The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines.
The Data Engineer:
Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products.
Performs data profiling and analysis, data cleansing and validation in support of data solutions.
Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards.
Tests and promotes work packages between non-production and production environments both independently, and with team members.
Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs.
Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner.
Creates and manages incident reports as they pertain to data management processes.
Provides advanced technical support for production applications and dataflows.

QUALIFICATION REQUIREMENTS

Successful candidates will have:
Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning.
Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes.
Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases.
Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs.
Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI).
Experience with CRM-like and transactional datasets.
Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services.
Proficiency in programming languages for data and analytics (e.g., SQL, Python and R).
Strong analytical and problem-solving skills.
Strong technical writing skills and experience creating procedural and audit documentation.
Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting.
Demonstrated ability to innovate and drive assigned tasks to successful completion.
Curious, creative, and collaborative approach to challenges.
Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package.
Any false statement in your application may result in your application being rejected and may also result in termination after employment begins.
The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""

Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.

What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager.
Relocation expenses are not paid.

The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures.
The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.

About Smithsonian Institution
Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,"Smithsonian Institution
3.9","Washington, DC",5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
